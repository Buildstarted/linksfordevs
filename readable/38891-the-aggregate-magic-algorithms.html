<!DOCTYPE html>
<html lang="en">
<head>
    <title>
The Aggregate Magic Algorithms -
linksfor.dev(s)
    </title>
    <link rel="alternate" type="application/rss+xml" title="Linksfor.dev(s) feed" href="https://linksfor.dev/feed.rss" />
    <meta charset="utf-8">
    <meta name="Description" content="A curated source of links that devs might find interesting. Updated around the clock." />
    <meta name="google" value="notranslate">
    <link rel="stylesheet" type="text/css" href="/style.css" />
    <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon">
    <link rel="icon" href="/favicon.ico" type="image/x-icon">
</head>
<body>
    <div class="grid">
        
<div class="readable">
    <div id="readOverlay" class="style-ebook"><div id="readInner" class="margin-medium size-medium"><h1>The Aggregate Magic Algorithms</h1><div><div><p align="CENTER"><a href="http://www.uky.edu/"><img src="/IMG/ukwidesig.gif" width="263" heght="54" border="0"></a></p><p>

There are lots of people and places that create and collect
algorithms of all types (<a href="#Other People's Magic">here</a> are a few WWW sites).  Unfortunately, in building
systems hardware and software, we in <a href="http://aggregate.org/">The Aggregate</a> often have found
it necessary to do relatively obscure low-level things very
efficiently.  Many of the tricks we've devised or collected
either require assembly language coding or are not entirely
portable when coded in HLLs like C, but these techniques are
still valuable because they can yield significant performance
improvements over the more obvious ways of doing things.

</p><p>

None of the following coding tricks came from proprietary
sources; further, we believe that each of the tricks we did not
invent is essentially "standard engineering practice" in the
specialized niche where it applies.  Thus, although we have not
conducted patent searches, etc., to confirm it, we believe that
these are tricks that freely can be used for any purpose.  Of
course, The Aggregate accepts no responsibility for your use of
these tricks; you must confirm that the trick does what you want
and that you can use it as you intend.  That said, we do intend
to maintain this page by adding new algorithms and/or correcting
existing entries.  If you have any comments, please contact <a href="http://aggregate.org/hankd/">Professor Hank Dietz</a>.

</p><p>

This document should be cited using something like the following bibtex entry,
but with the date fetched from this site inserted:

</p><pre>@techreport{magicalgorithms,
author={Henry Gordon Dietz},
title={{The Aggregate Magic Algorithms}},
institution={University of Kentucky},
howpublished={Aggregate.Org online technical report},
URL={http://aggregate.org/MAGIC/}
}</pre><hr><hr><p>

IEEE floating point uses an explicit sign bit, so the absolute
value can be taken by a bitwise AND with the complement of the
sign bit.  For IA32 32-bit, the sign bit is an <tt>int</tt>
value of 0x80000000, for IA32 64-bit, the sign bit is the
<tt>long long int</tt> value 0x8000000000000000.  Of course, if
you prefer to use just <tt>int</tt> values, the IA32 64-bit sign
bit is 0x80000000 at an <tt>int</tt> address offset of +1.  For
example:

</p><pre>double x;

/* make x = abs(x) */
*(((int *) &amp;x) + 1) &amp;= 0x7fffffff;
</pre><hr><p>

Alignment of pointers is a pretty common problem, and there are
several relevant tricks, so, at the suggestion of <a href="euler@free.fr">Jean-Charles Meyrignac</a> (who also
provided an example of the upward alignment described below),
I've added a little description here.

</p><p>

It is fairly obvious that the downward alignment of an address
<var>a</var> to a multiple-of-<var>b</var> boundary, where
<var>b</var> is a power of 2, is simply <var>(a &amp; ~(b-1))</var>.
Of course, <var>~(b-1)</var> is also <var>-b</var>, so <var>(a &amp;
-b)</var> also works (the difference is usually nothing; if
<var>b</var> is a constant, most compilers will fold the first
into the second form).

</p><p>

For upward alignment, we simply add <var>b-1</var>:
<var>((a + (b-1)) &amp; -b)</var>.

</p><p>

Of course, there are a few complications.  First, languages like
C, which allow pointer arithmetic, generally scale pointer
offsets by the size of the target object -- which would keep our
math from working.  It used to be that casting a pointer as a
<code>(char *)</code> would turn-off this scaling, but with
<code>long char</code> and such out there, a cast as <code>(void
*)</code> is probably a safer bet.  Unfortunately, C doesn't
define bitwise operations on pointers of any flavor, so you'll
need to cast to an appropriately-large integer type before doing
a bitwise AND.

</p><p>

Secondly, aligning an address doesn't help unless you allocated
a large enough chunk of memory so that you can treat your data
structure as starting at the aligned address.  In general, if
you wish to create a <var>b</var>-aligned data structure with
<var>c</var> bytes, you would do something like:
<code>a=((typeof(a))(((int)(((void
*)malloc(c+(b-1)))+(b-1)))&amp;-b))</code>.  Please excuse my use of
the GCC <code>typeof()</code>.  Anyway, this is particularly
useful for cache-line alignment of data structures.  One little
annoyance:  you can't call <var>free(a)</var>; you'll need to
keep a copy of the original block address for that.

</p><hr><p>

This is actually an extension of the "well known" fact that for
binary integer values <tt>x</tt> and <tt>y</tt>, <tt>(x+y)</tt>
equals <tt>((x&amp;y)+(x|y))</tt> equals <tt>((x^y)+2*(x&amp;y))</tt>.

</p><p>

Given two integer values <tt>x</tt> and <tt>y</tt>, the (floor
of the) average normally would be computed by <tt>(x+y)/2</tt>;
unfortunately, this can yield incorrect results due to overflow.
A very sneaky alternative is to use <tt>(x&amp;y)+((x^y)/2)</tt>.
If we are aware of the potential non-portability due to the fact
that C does not specify if shifts are signed, this can be
simplified to <tt>(x&amp;y)+((x^y)&gt;&gt;1)</tt>.  In either case, the
benefit is that this code sequence cannot overflow.

</p><hr><p>

Reversing the bits in an integer <tt>x</tt> is somewhat painful,
but here's a SWAR algorithm for a 32-bit value:

</p><pre>unsigned int
reverse(register unsigned int x)
{
	x = (((x &amp; 0xaaaaaaaa) &gt;&gt; 1) | ((x &amp; 0x55555555) &lt;&lt; 1));
	x = (((x &amp; 0xcccccccc) &gt;&gt; 2) | ((x &amp; 0x33333333) &lt;&lt; 2));
	x = (((x &amp; 0xf0f0f0f0) &gt;&gt; 4) | ((x &amp; 0x0f0f0f0f) &lt;&lt; 4));
	x = (((x &amp; 0xff00ff00) &gt;&gt; 8) | ((x &amp; 0x00ff00ff) &lt;&lt; 8));
	return((x &gt;&gt; 16) | (x &lt;&lt; 16));

}
</pre><p>

It also is possible to re-write this algorithm to use 4 instead
of 8 constants, thus saving some instruction bandwidth.  On my
1.2GHz Athlon (a Thunderbird), the difference is too small to
measure reliably.  Here's the other version:

</p><pre>unsigned int
reverse(register unsigned int x)
{
        register unsigned int y = 0x55555555;
        x = (((x &gt;&gt; 1) &amp; y) | ((x &amp; y) &lt;&lt; 1));
        y = 0x33333333;
        x = (((x &gt;&gt; 2) &amp; y) | ((x &amp; y) &lt;&lt; 2));
        y = 0x0f0f0f0f;
        x = (((x &gt;&gt; 4) &amp; y) | ((x &amp; y) &lt;&lt; 4));
        y = 0x00ff00ff;
        x = (((x &gt;&gt; 8) &amp; y) | ((x &amp; y) &lt;&lt; 8));
        return((x &gt;&gt; 16) | (x &lt;&lt; 16));
}
</pre><hr><p>

IEEE floating point has a number of nice properties, including
the ability to use 2's complement integer comparisons to compare
floating point values, provided the native byte order is
consistent between float and integer values.  The only
complication is the use of sign+magnitude representation in
floats.  The <a href="http://www.amd.com/us-en/assets/content_type/white_papers_and_tech_docs/22007.pdf"><em>AMD Athlon Processor x86 Code Optimization Guide</em></a>
gives a nice summary on Page 43.  Here's a set of C routines
that embody the same logic:

</p><pre>#define FasI(f)  (*((int *) &amp;(f)))
#define FasUI(f) (*((unsigned int *) &amp;(f)))

#define	lt0(f)	(FasUI(f) &gt; 0x80000000U)
#define	le0(f)	(FasI(f) &lt;= 0)
#define	gt0(f)	(FasI(f) &gt; 0)
#define	ge0(f)	(FasUI(f) &lt;= 0x80000000U)
</pre><hr><p>

In many cases, it is useful to convert the result of a
comparison, which is either 0 or some non-zero bit pattern, into
either a "clean" 0 or -1 bit mask.

</p><p>

For many systems, this can be efficienty done by C code that
simply uses the logic operators and negation: <tt>-(x!=0)</tt>
or <tt>-!!x</tt>. This is a very well known and old method,
really a direct consequence of (and partial motivation for) the
C concept of conditional results being integers.  However, for
some compilers and instruction sets (especially SWAR targets),
the code generated for logicals is terrible, sometimes even
including conditional branches!  Where this obvious coding
doesn't work well, here are some alternatives.

</p><p>

If the messy <strong>non-negative</strong> integer value is
<tt>x</tt>, the sanitized version is:

</p><pre>(((int) (-x)) &gt;&gt; (WORDBITS - 1))
</pre><p>

To remove the constraint that the messy value be non-negative,
use:

</p><pre>(((int) (x | -x)) &gt;&gt; (WORDBITS - 1))
</pre><p>

Logically, this works because the shift by <tt>(WORDBITS-1)</tt>
replicates the sign bit to create a mask -- be aware, however,
that the C language does <em>not</em> require that shifts are
signed even if their operands are signed, so there is a
potential portability problem.  Additionally, one might think
that a shift by any number greater than or equal to
<tt>WORDBITS</tt> would have the same effect, but many
instruction sets have shifts that behave strangely when such
shift distances are specified.

</p><p>

Of course, the opposite condition can be tested using:

</p><pre>(((int) ~(x | -x)) &gt;&gt; (WORDBITS - 1))
</pre><p>

If you prefer the C-standard 0 or 1 comparison result, simply
use the <tt>unsigned</tt> shift:

</p><pre>(((unsigned int) (x | -x)) &gt;&gt; (WORDBITS - 1))
</pre><p>

The opposite condition can be obtained using:

</p><pre>(((unsigned int) ~(x | -x)) &gt;&gt; (WORDBITS - 1))
</pre><hr><p>

Normally, a dual-linked circular list would contain both
previous and next pointer fields and the current position in the
list would be identified by a single pointer.  By using two
current pointers, one to the node in question and the other to
the one just before/after it, it becomes possible to store only
a single pointer value in each node.  The value stored in each
node is the XOR of the next and previous pointers that normally
would have been stored in each node.  Decoding is obvious.

</p><p>

Unfortunately, using this trick in C is awkward because the XOR
operation is not defined for pointers.

</p><hr><p>

Joe Ibershoff, one of my students, pointed-out that integer
divide normally yields the floor, but both ceiling and
round-to-nearest are easy and useful.  I thought these were
fairly well-known tricks closely related to the <a href="#Alignment of Pointers">Alignment of Pointers</a> magic,
but perhaps they aren't so obvious...?  He points out that
<tt>Ceiling(a/b)</tt> is <tt>(a+b-1)/b</tt> and
<tt>RoundToNearest(a/b)</tt> is <tt>(a+(b/2))/b</tt>.  Of
course, these tricks also work if divide is implemented in less
obvious ways, such as shifts or shift-and-subtract sequences.

</p><hr><p>

A basic SIMD operation, "any" is a logical-OR reduction that
returns true if any of its inputs are true.  On SIMD hardware,
this is usually very easy...  but not so on GPUs (Graphics
Processing Units).  NVIDIA's CUDA has recently added hardware
support, but there is a more portable way that is just as fast.
The <tt>p_any(flag)</tt> operation within a block can be
accomplished by:

</p><pre>if (flag) sharedtemp = serial; /* maskable store */
__syncthreads(); /* optional with right block size */
p_any = (sharedtemp == (serial++));
</pre><p>

We first publically announced this at SC08, and we're pretty
sure we invented it.  The trick is that NVIDIA's hardware seems
to take constant time to resolve N threads storing into the same
object, i.e., it picks a winner.  This behaviour is not
documented, but has been experimentally observed; this
<tt>p_any(flag)</tt> will run on any of the CUDA hardware, and
takes essentially the same time as the atomic any that was added
to later CUDA hardware.  There are actually quite a few useful
operations that can be built using variations on this trick....

</p><hr><p>

The most fundamental aggregate function (or you might
call it a collective communication) is a barrier synchronization.
On SIMD hardware, this is usually implicit...  but not so on GPUs
(Graphics Processing Units).  Within a Block, NVIDIA's CUDA
provides a barrier called <tt>__syncthreads()</tt>. Across
Blocks -- if you are running a number of Blocks that the GPU
can timeshare rather than batch process -- you can synchronize
using this:

</p><pre>/* First, sync within each Block */
__syncthreads();
/* Pick a representative from each (here, 1D) block */
if (threadIdx.x == 0) {
  /* Get my barrier number */
  int barno = barnos[blockIdx.x] + 1;
  int hisbarno;
  int who = (blockIdx.x + 1) % gridDim.x;
  /* Check in at barrier */
  barnos[blockIdx.x] = barno;
  /* Scan for all here or somebody passed */
  do {
    /* Wait for who */
    do {
      hisbarno = barnos[who];
    } while (hisbarno &lt; barno);
    /* Bump to next who */
    if (++who &gt;= gridDim.x) who = 0;
  } while ((hisbarno == barno) &amp;&amp; (who != blockIdx.x));
  /* Tell others we are all here */
  barnos[blockIdx.x] = barno + 1;
}
/* Rejoin with rest of my Block */
__syncthreads();
</pre><p>

The above code assumes that <tt>barnos[]</tt> is a
<tt>volatile</tt> (forced memory access) array in GPU global
memory that is initialized to 0. The type can be either
<tt>int</tt> or <tt>float</tt>; it is not critical because
either way wrap-around will take longer than GPUs will let one
kernel run by default. Cost is O(number of Blocks) if all arrive
at the same time, but O(1) for the last to arrive if there is
any temporal skew. The O(1) behavior is due to counting by 2 per
barrier; if all Blocks typically arrive roughly simultaneously,
the algorithm can be simplified to count by 1. The OpenCL
version of this algorithm has been tested on both NVIDIA and ATI
GPUs with good performance. This algorithm also is the obvious
basis for efficient within-a-kernel reductions and scans....

</p><p>

We first publically showed various GPU variants of this algorithm
at SC08, and it was published within the MS thesis of two of
Dietz's students in <a href="http://hdl.handle.net/10225/1085">July 2009</a> and <a href="http://archive.uky.edu/handle/10225/1158">September
2009</a>.  Actually, it is a trivial variation on the lockless
shared memory barrier that we developed for SHMAPERS and <a href="http://www.springerlink.com/content/wq4n8g070771554m/">published</a> over a decade ago. (In fact, it took less time to
synchronize four processors in a Sun server than for one of
those processors to execute a single atomic memory instruction!)
I note the dates because late in 2009 somebody else published
and claimed to have invented what is an inferior variant of this
algorithm and did not cite us....

</p><hr><p>

A Gray code is any binary coding sequence in which only a single
bit position changes as we move from one value to the next.
There are many such codes, but the traditional one is computed
such that the <tt>K</tt>th Gray code is <tt>K^(K&gt;&gt;1)</tt>.

</p><p>

The well-known algorithm for conversion from Gray to binary is a
linear sequence of XORs that makes it seem each bit must be
dealt with separately.  Fortunately, that is equivalent to a
parallel prefix XOR that can be computed using SWAR techniques
in log time.  For 32-bit Gray code values produced as described
above, the conversion from Gray code back to unsigned binary is:

</p><pre>unsigned int
g2b(unsigned int gray)
{
        gray ^= (gray &gt;&gt; 16);
        gray ^= (gray &gt;&gt; 8);
        gray ^= (gray &gt;&gt; 4);
        gray ^= (gray &gt;&gt; 2);
        gray ^= (gray &gt;&gt; 1);
        return(gray);
}
</pre><hr><p>

Given an integer value <tt>x</tt> and an integer or floating
point value <tt>y</tt>, the value of <tt>x*y</tt> can be
computed efficiently using a sequence derived from the binary
value of <tt>x</tt>.  For example, if <tt>x</tt> is 5 (4 + 1):

</p><pre>y2 = y + y;
y4 = y2 + y2;
result = y + y4;
</pre><p>

In the special case that <tt>y</tt> is an integer, this can be
done with shifts:

</p><pre>y4 = (y &lt;&lt; 2);
result = y + y4;
</pre><hr><p>

Given 2's complement integer values <tt>x</tt> and <tt>y</tt>,
the minimum can be computed without any branches as
<tt>x+(((y-x)&gt;&gt;(WORDBITS-1))&amp;(y-x))</tt>.  Logically, this works
because the shift by <tt>(WORDBITS-1)</tt> replicates the sign
bit to create a mask -- be aware, however, that the C language
does <em>not</em> require that shifts are signed even if their
operands are signed, so there is a potential portability
problem.  Additionally, one might think that a shift by any
number greater than or equal to <tt>WORDBITS</tt> would have the
same effect, but many instruction sets have shifts that behave
strangely when such shift distances are specified.

</p><p>

Of course, maximum can be computed using the same trick:
<tt>x-(((x-y)&gt;&gt;(WORDBITS-1))&amp;(x-y))</tt>.

</p><p>

Actually, the <a href="#Integer Selection">Integer
Selection</a> coding trick is just as efficient in encoding
minimum and maximum....

</p><hr><p>

Given an integer value <tt>x</tt> and an integer or floating
point value <tt>y</tt>, the value of <tt>y</tt> to the
<tt>x</tt> power can be computed efficiently using a sequence
derived from the binary value of <tt>x</tt>.  For example,
if <tt>x</tt> is 5 (4 + 1):

</p><pre>y2 = y * y;
y4 = y2 * y2;
result = y * y4;
</pre><hr><p>

A branchless, lookup-free, alternative to code like <tt>if (a&amp;LT;b)
x=c; else x=d;</tt> is <tt>((((a-b) &amp;GT;&amp;GT; (WORDBITS-1)) &amp; (c^d)) ^
d)</tt>.  This code assumes that the shift is signed, which, of
course, C does not promise.

</p><hr><p>

A non-negative binary integer value <tt>x</tt> is a power of 2
iff <tt>(x&amp;(x-1))</tt> is 0 using 2's complement arithmetic.

</p><hr><p>

Some machines have had single instructions that count the number
of leading zero bits in an integer; such an operation can be an
artifact of having floating point normalization hardware around.
Clearly, floor of base 2 log of <tt>x</tt> is
<tt>(WORDBITS-lzc(x))</tt>.  In any case, this operation has
found its way into quite a few algorithms, so it is useful to
have an efficient implementation:

</p><pre>unsigned int
lzc(register unsigned int x)
{
        x |= (x &gt;&gt; 1);
        x |= (x &gt;&gt; 2);
        x |= (x &gt;&gt; 4);
        x |= (x &gt;&gt; 8);
        x |= (x &gt;&gt; 16);
        return(WORDBITS - ones(x));
}
</pre><hr><p>

This can be useful for extracting the lowest numbered element of
a bit set.  Given a 2's complement binary integer value
<tt>x</tt>, <tt>(x&amp;-x)</tt> is the least significant 1 bit.
(This was pointed-out by Tom May.)  The reason this works is
that it is equivalent to <tt>(x &amp; ((~x) + 1))</tt>; any trailing
zero bits in <tt>x</tt> become ones in <tt>~x</tt>, adding 1 to
that carries into the following bit, and AND with <tt>x</tt>
yields only the flipped bit...  the original position of the
least significant 1 bit.

</p><p>

Alternatively, since <tt>(x&amp;(x-1))</tt> is actually <tt>x</tt>
stripped of its least significant 1 bit, the least significant 1
bit is also <tt>(x^(x&amp;(x-1)))</tt>.

</p><hr><p>

Given a binary integer value <tt>x</tt>, the floor of the base 2
log of that number efficiently can be computed by the
application of two variable-precision SWAR algorithms.  The
first "folds" the upper bits into the lower bits to construct a
bit vector with the same most significant 1 as <tt>x</tt>, but
all 1's below it.  The second SWAR algorithm is population
count, defined elsewhere in this document.  However, we must
consider the issue of what the log2(0) should be; the log of 0
is undefined, so how that value should be handled is unclear.
The following code for handling a 32-bit value gives two
options:  if <tt>LOG0UNDEFINED</tt>, this code returns -1 for
log2(0); otherwise, it returns 0 for log2(0).  For a 32-bit
value:

</p><pre>unsigned int
floor_log2(register unsigned int x)
{
        x |= (x &gt;&gt; 1);
        x |= (x &gt;&gt; 2);
        x |= (x &gt;&gt; 4);
        x |= (x &gt;&gt; 8);
        x |= (x &gt;&gt; 16);
#ifdef	LOG0UNDEFINED
        return(ones32(x) - 1);
#else
	return(ones32(x &gt;&gt; 1));
#endif
}
</pre><p>

Suppose instead that you want the ceiling of the base 2 log.
The floor and ceiling are identical if <tt>x</tt> is a power of
two; otherwise, the result is 1 too small.  This can be
corrected using the power of 2 test followed with the
comparison-to-mask shift used in integer minimum/maximum.  The
result is:

</p><pre>unsigned int
log2(register unsigned int x)
{
	register int y = (x &amp; (x - 1));

	y |= -y;
	y &gt;&gt;= (WORDBITS - 1);
        x |= (x &gt;&gt; 1);
        x |= (x &gt;&gt; 2);
        x |= (x &gt;&gt; 4);
        x |= (x &gt;&gt; 8);
        x |= (x &gt;&gt; 16);
#ifdef	LOG0UNDEFINED
        return(ones(x) - 1 - y);
#else
	return(ones(x &gt;&gt; 1) - y);
#endif
}
</pre><hr><p>

Given a binary integer value <tt>x</tt>, the next largest power
of 2 can be computed by a SWAR algorithm that recursively
"folds" the upper bits into the lower bits.  This process yields
a bit vector with the same most significant 1 as <tt>x</tt>, but
all 1's below it.  Adding 1 to that value yields the next
largest power of 2.  For a 32-bit value:

</p><pre>unsigned int
nlpo2(register unsigned int x)
{
        x |= (x &gt;&gt; 1);
        x |= (x &gt;&gt; 2);
        x |= (x &gt;&gt; 4);
        x |= (x &gt;&gt; 8);
        x |= (x &gt;&gt; 16);
        return(x+1);
}
</pre><hr><p>

Given a binary integer value <tt>x</tt>, the most significant 1
bit (highest numbered element of a bit set) can be computed
using a SWAR algorithm that recursively "folds" the upper bits
into the lower bits.  This process yields a bit vector with the
same most significant 1 as <tt>x</tt>, but all 1's below it.
Bitwise AND of the original value with the complement of the
"folded" value shifted down by one yields the most significant
bit.  For a 32-bit value:

</p><pre>unsigned int
msb32(register unsigned int x)
{
        x |= (x &gt;&gt; 1);
        x |= (x &gt;&gt; 2);
        x |= (x &gt;&gt; 4);
        x |= (x &gt;&gt; 8);
        x |= (x &gt;&gt; 16);
        return(x &amp; ~(x &gt;&gt; 1));
}
</pre><hr><p>

For integers used to represent natural data types, simply
shifting right works well for conversion to a lower precision,
but shifting left is not very effective for converting to a
higher precision.  The problem is simply that if the "new" bits
are taken to be 0s, the maximum value will never be attained.
Likewise, if taken to be any fixed non-0 value, the value zero
will never be obtained.  A good answer to this problem is to
replicate the existing bit pattern in the "new" bits, truncating
or repeating the pattern if more bits are needed.

</p><p>

For example, a 10-bit raw pixel value (e.g., from my Canon G1)
called <tt>x</tt> can be extended to a 16-bit value by the
expression <tt>((x&lt;&lt;6)|(x&gt;&gt;4))</tt>.  This way, both the maximum
and minimum values are reachable, with good linearity throughout
the entire range.

</p><hr><p>

It is fairly obvious, but <tt>x0+x1*x+x2*x*x+x3*x*x*x+...</tt>
always can be rewritten as the usually faster equivalent
<tt>x0+x*(x1+x*(x2+x*(x3+x*(...))))</tt>.  There are various
accuracy and other issues, but this sort of obvious
transformation should not be overlooked.

</p><hr><p>

The population count of a binary integer value <tt>x</tt> is the
number of one bits in the value.  Although many machines have
single instructions for this, the single instructions are
usually microcoded loops that test a bit per cycle; a log-time
algorithm coded in C is often faster.  The following code uses a
variable-precision SWAR algorithm to perform a tree reduction
adding the bits in a 32-bit value:

</p><pre>unsigned int
ones32(register unsigned int x)
{
        /* 32-bit recursive reduction using SWAR...
	   but first step is mapping 2-bit values
	   into sum of 2 1-bit values in sneaky way
	*/
        x -= ((x &gt;&gt; 1) &amp; 0x55555555);
        x = (((x &gt;&gt; 2) &amp; 0x33333333) + (x &amp; 0x33333333));
        x = (((x &gt;&gt; 4) + x) &amp; 0x0f0f0f0f);
        x += (x &gt;&gt; 8);
        x += (x &gt;&gt; 16);
        return(x &amp; 0x0000003f);
}
</pre><p>

It is worthwhile noting that the SWAR population count algorithm
given above can be improved upon for the case of counting the
population of multi-word bit sets.  How?  The last few steps in
the reduction are using only a portion of the SWAR width to
produce their results; thus, it would be possible to combine
these steps across multiple words being reduced.

</p><p>

One additional note:  the AMD Athlon optimization guidelines
suggest a very similar algorithm that replaces the last three
lines with <tt>return((x * 0x01010101) &gt;&gt; 24);</tt>.  For the
Athlon (which has a very fast integer multiply), I would have
expected AMD's code to be faster...  but it is actually 6%
slower according to my benchmarks using a 1.2GHz Athlon (a
Thunderbird).  Why?  Well, it so happens that GCC doesn't use a
multiply instruction - it writes out the equivalent shift and
add sequence!

</p><hr><p>

Sometimes it is necessary to test if two integers, <tt>a</tt>
and <tt>b</tt>, have the same value within a given tolerance,
<tt>c</tt>.  The obvious test would be something like
<tt>((a&gt;b)?(a-b):(b-a))&lt;c</tt>, which isn't horrifically
inefficient, but does involve a conditional branch.
Alternatively, <tt>abs(a-b)&lt;c</tt> would do... but again, it
takes some cleverness to implement <tt>abs()</tt> without a
conditional jump. Here's a branchless alternative.

</p><p>

If <tt>(a-b)&gt;0</tt>, then <tt>(b-a)&lt;0</tt>; similarly, if
<tt>(a-b)&lt;0</tt>, then <tt>(b-a)&gt;0</tt>.  Both can't be
greater than 0 simultaneously.  Suppose that
<tt>(a-b)&gt;0</tt>. Subtracting <tt>((a-b)-c)</tt> will produce
a negative result <em>iff</em><tt>a</tt> and <tt>b</tt> are
within <tt>c</tt> of each other. Of course, our assumption
requires <tt>(b-a)&lt;0</tt>, so <tt>((b-a)-c)</tt> simply
becomes more negative (assuming the value doesn't wrap around).
Generalizing, if either <tt>((a-b)-c)&gt;0</tt> or
<tt>((b-a)-c)&gt;0</tt> then the values of <tt>a</tt> and
<tt>b</tt> are not the same within tolerance <tt>c</tt>. In
other words, they are within tolerance if:

</p><pre>(((a-b-c)&amp;(b-a-c))&lt;0)
</pre><p>

This test can be rewritten a variety of ways. The <tt>&lt;0</tt>
part is really just examining the sign bit, so a mask or shift
could be used to extract the bit value instead. For example,
using 32-bit words, <tt>(((a-b-c)&amp;(b-a-c))&gt;&gt;31)</tt>
using unsigned <tt>&gt;&gt;</tt> will produce the value 1 for
true or 0 for false. It is also possible to factor-out
<tt>t=a-b</tt>, giving:

</p><pre>(((t-c)&amp;(-t-c))&lt;0)
</pre><p>

Which is really equivalent to <tt>abs(t)&lt;c</tt>.

</p><p>

Once again, an excellent alternative computation has been
provided by <a href="euler@free.fr">Jean-Charles Meyrignac</a>,
who observed that checking if <tt>v</tt> is in the interval
[<tt>i</tt> ..  <tt>j</tt>), which one might normally compute by
<tt>(v&gt;=i &amp;&amp; v&lt;j)</tt>, can instead be done as
<tt>((unsigned int)(v-i) &lt; (j-i))</tt>.

</p><p>

Checking if <tt>a</tt> and <tt>b</tt> are the same within
<tt>c</tt> is thus basically checking <tt>((unsigned int)(a-b+c)
&lt; (c+c))</tt>.  Well, almost. It actually considers one more
value to be in the range than it should... and it does odd
things when values wrap around. Still, often close enough....

</p><hr><p>

Rather obviously, if an integer multiply can be implemented by a
shift-and-add sequence, then a shift-and-add sequence can be
implemented by multiplying by the appropriate constant...  with
some speedup on processors like the AMD Athlon.  Unfortunately,
GCC seems to believe constant multiplies should <em>always</em>
be converted into shift-and-add sequences, so there is a problem
in using this optimization in C source code.

</p><hr><p>

Although many instruction sets provide single machine
instructions that implement sign extension of 2's-complement
integers, I've been sent a number of tricks for sign extension.
I've included them here because sign extension instructions
generally work only on the data sizes directly understood by the
processor, whereas these methods work on any bit precisions.

</p><p>

The most obvious method assumes that you have a signed right
shift:  to extend an <var>a</var>-bit number <var>x</var> to
<var>b</var> bits, shift left by <var>b-a</var>, then signed
shift that value right by <var>b-a</var> bits.  I believe
this has been widely known and used for many years -- I know
I didn't invent it, but used it decades ago.

</p><p><a href="mailto:euler@free.fr">Jean-Charles Meyrignac</a>
suggested a shiftless variation that basically does a 1-bit add
to flip high bits:  if <var>n</var> is 2 to the <var>a</var>,
simply compute <var>(((x | -n) + (n/2)) ^ (n/2))</var>. This
version has been posted here for some time....

</p><p>

However, in August 2010, <a href="http://spatula-city.org/~im14u2c/">Joe Zbiciak</a> sent
me a little email with a <em>much</em> cleaner shiftless sign
extension: <var>((x ^ n) - n)</var> where <var>n</var> is the
value of the top bit position in the number to be extended.
Thus, to sign-extend an 8-bit value <var>x</var>, compute
<var>((x ^ 128) - 128)</var>.  It really couldn't be much
simpler or more obvious... at least once you've been told how to
do it.  ;-)

</p><hr><p>

Given two binary integer values, <tt>x</tt> and <tt>y</tt>,
the values can be exchanged without use of a temporary by:

</p><pre>x ^= y; /* x' = (x^y) */
y ^= x;	/* y' = (y^(x^y)) = x */
x ^= y; /* x' = (x^y)^x = y */
</pre><p>

It should be obvious that this can be done with various
operators and their inverses, but xor has the unusual property
that it is it's own inverse. For example, here it is with
modular add and subtract:

</p><pre>x += y; 	/* x' = (x+y) */
y = x - y;	/* y' = (x+y)-y = x */
x -= y;		/* x' = (x+y)-x = y */
</pre><p>

This works on machines that don't have xor instructions. It even
would also appear to work for floating point values, but where
there is a significant difference in magnitude between
<tt>x</tt> and <tt>y</tt>, there can be a serious loss of
accuracy in the value with the smaller magnitude. For example,
if <tt>x</tt> has a much greater magnitude than <tt>y</tt>, then
<tt>(x+y)==x</tt> and we end with <tt>y=0</tt>. The interesting
thing is that you can losslessly swap floating-point values by
treating them as integers and using xor.

</p><hr><p>

Before we coined the name SWAR in Fall 1996, we already had
defined a complete set of basic operations and described how
they could be implemented with good efficiency. On February 4,
1997, we posted <a href="/SWAR/over.html">this</a> fairly complete overview document and there also are
<a href="/SWAR/970213/slide1.html">slides</a> from a seminar presentation I gave at Purdue. These
methods were used in our SWARC compiler and were detailed in a
number of our publications throughout the 1990s. We hadn't
posted them on this page because they were so prominently
published elsewhere.

</p><p>

However, much to our surprize, <a href="http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO1&amp;Sect2=HITOFF&amp;d=PALL&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsrchnum.htm&amp;r=1&amp;f=G&amp;l=50&amp;s1=7039906.PN.&amp;OS=PN/7039906&amp;RS=PN/7039906">United States Patent 7039906, "Compiler for enabling multiple
signed independent data elements per register,"</a> was filed
October 20, 2000 and was issued to IBM on May 2, 2006! By our
reading, this patent appears to seriously overlap my and Randy
Fisher's earlier published work -- much of which was cited by
the patent examiner.  I am posting this note here so that people
who happen to hear about the IBM patent will not be discouraged
from using the prior art technologies developed by us, which, by
being cited in the patent, are explicitly not covered by the
patent.

</p><hr><p>

Given the <a href="#Least Significant 1 Bit">Least Significant
1 Bit</a> and <a href="#Population Count (Ones Count)">Population Count (Ones Count)</a> algorithms, it is trivial to
combine them to construct a trailing zero count (as pointed-out
by Joe Bowbeer):

</p><pre>unsigned int
tzc(register int x)
{
        return(ones((x &amp; -x) - 1));
}
</pre><hr><hr><p>

The following are generally good sources of magic algorithms:

</p><hr><p><a href="http://aggregate.org/"><img src="/IMG/talogos.jpg" width="160" height="32" alt="The Aggregate."></a> The <em>only</em> thing set in stone is our name.
</p></div></div></div></div>
</div>
    </div>
    <footer>
        <div>created by buildstarted &copy; 2020 <a href="/about">about</a></div>
        <div>Share this page on social media: copy and paste this url https://linksfor.dev/</div>
        <div>If you prefer RSS: <a href="https://linksfor.dev/feed.xml">https://linksfor.dev/feed.xml</a></div>
    </footer>
    
    <script>
        (function () {
            var COLLECT_URL = "https://dna.buildstarted.com/t";
            var SITE_ID = "linksfor.devs";
            var GLOBAL_VAR_NAME = "__DNA__";

            window[GLOBAL_VAR_NAME] = {};

            window[GLOBAL_VAR_NAME].sendPageView = function () {
                var path = location.pathname;
                var referrer = document.referrer;

                var url = COLLECT_URL + "?siteid=" + SITE_ID + "&p=" + encodeURIComponent(path) + "&r=" + encodeURIComponent(referrer);

                try { fetch(url, { method: "GET" }); } catch (e) { }
            };

            window[GLOBAL_VAR_NAME].sendPageView();
        })();
    </script>
</body>
</html>