<!DOCTYPE html>
<html lang="en">
<head>
    <title>
Fawkes - linksfor.dev(s)    </title>
    <meta charset="utf-8">
    <link rel="alternate" type="application/rss+xml" title="Linksfor.dev(s) feed" href="https://linksfor.dev/feed.rss" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta name="google" value="notranslate">
    <link rel="stylesheet" type="text/css" href="/style.css" />
    <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon">
    <link rel="icon" href="/favicon.ico" type="image/x-icon">
    <meta property="og:title" content="Fawkes - linksfor.dev(s)"/>
    <meta property="og:description" content="2020 is a watershed year for machine&#xA;              learning. It has seen the true arrival of commodized machine&#xA;              learning, where deep learning models and algorithms are readily&#xA;              available to Internet users. GPUs are cheaper and more readily&#xA;              available than ever, and new training methods like transfer&#xA;              learning have made it possible to train powerful deep learning models using&#xA;              smaller sets of data."/>
    <meta property="og:type" content="website"/>
    <meta property="og:url" content="http://sandlab.cs.uchicago.edu/fawkes/"/>

<meta property="og:site_name" content="linksfor.dev(s)" />
</head>
<body>
    <div class="devring" style="background: #222">
        <div style="text-align:center">Explore other dev related sites in this ring. If you would like to join this ring <a href="https://devring.club">click here</a>.</div>
        <div class="grid">
            <div style="display: grid; grid-template-columns: .5fr 1fr 1fr 1fr; text-align: center;">
                <span class="devring-title"><a href="https://devring.club/">devring.club</a></span>
                <a href="https://devring.club/sites/1/prev" class="devring-previous">Previous</a>
                <a href="https://devring.club/random" class="devring-random">Random</a>
                <a href="https://devring.club/sites/1/next" class="devring-next">Next</a>
            </div>
        </div>
    </div>
    <div class="grid">
        <h1>
<a href="/" style="color:inherit">linksfor.dev(s)</a>
        </h1>
        <title>linksfor.dev(s) - Fawkes</title>
<div class="readable">
        <h1>Fawkes</h1>
            <div>Reading time: 11-14 minutes</div>
        <div>Posted here: 22 Jul 2020</div>
        <p><a href="http://sandlab.cs.uchicago.edu/fawkes/">http://sandlab.cs.uchicago.edu/fawkes/</a></p>
        <hr/>
<div id="readability-page-1" class="page"><div>
          <hr>
<center>
<table>
<tbody><tr><td><a href="#intro">Introduction</a></td>
<td></td>
<td><a href="#paper">Technical Publication</a></td>
<td></td>
<td><a href="#code">Downloads</a></td>
<td></td>
<td><a href="#faq">FAQ</a></td>
</tr></tbody></table>
</center>
          <hr>
        <section>
          <div><p>2020 is a watershed year for machine
              learning. It has seen the true arrival of commodized machine
              learning, where deep learning models and algorithms are readily
              available to Internet users. GPUs are cheaper and more readily
              available than ever, and new training methods like transfer
              learning have made it possible to train powerful deep learning models using
              smaller sets of data. </p>
            <p>But accessible machine learning also has its
              downsides as well. A recent New York Times <a href="https://www.nytimes.com/2020/01/18/technology/clearview-privacy-facial-recognition.html">article</a>
              by Kashmir Hill profiled <em><a href="https://clearview.ai/">clearview.ai</a></em>,
              an unregulated facial recognition service that has now downloaded
              over 3 billion photos of people from the Internet and social
              media, using them to build facial recognition models for millions
              of citizens without their knowledge or permission. Clearview.ai
              demonstrates just how easy it is to build invasive tools for
              monitoring and tracking using deep learning. </p>
            <p>So how do we protect ourselves against
              unauthorized third parties building facial recognition models to
              recognize us wherever we may go? Regulations can and will help
              restrict usage of machine learning by public companies, but will
              have negligible impact on private organizations, individuals, or
              even other nation states with similar goals.</p>
            <p>The <a href="http://sandlab.cs.uchicago.edu/">SAND
                Lab</a> at University of Chicago has developed <em><strong>Fawkes<sup>1</sup></strong></em>,
              an algorithm and software tool (running locally on your computer)
              that gives individuals the ability to limit how their own images
              can be used to track them. At a high level, Fawkes takes your
              personal images, and makes tiny, pixel-level changes to them that
              are invisible to the human eye, in a process we call <em>image
                cloaking</em>. You can then use these "cloaked" photos as you
              normally would, sharing them on social media, sending them to
              friends, printing them or displaying them on digital devices, the
              same way you would any other photo. The difference, however, is
              that if and when someone tries to use these photos to build a
              facial recognition model, "cloaked" images will teach the model an
              highly distorted version of what makes you look like you. The
              cloak effect is not easily detectable, and will not cause errors
              in model training. However, when someone tries to identify you
              using an unaltered image of you (e.g. a photo taken in public),
              and tries to identify you, they will fail.</p>
            <p>Fawkes has been tested extensively and
              proven effective in a variety of environments, and shows 100%
              effectiveness against state of the art facial recognition models 
              (Microsoft Azure Face API, Amazon Rekognition, and Face++). We are
              in the process of adding more material here to explain how and why
              Fawkes works. For now, please see the link below to our technical
              paper, which will be presented at the upcoming <a href="https://www.usenix.org/conference/usenixsecurity20">USENIX
                Security Symposium</a>, to be held on August 12 to 14.</p>
            <p>The Fawkes project is led by two PhD
              students at SAND Lab, <u><a href="https://people.cs.uchicago.edu/~ewillson/">Emily Wenger</a></u> and 
	    <u><a href="https://www.shawnshan.com/">Shawn Shan</a></u>, with important
              contributions from <u><a href="https://jiayunz.github.io/">Jiayun Zhang</a></u> 
	      (SAND Lab visitor and current PhD student at UC San Diego) and 
 	      <u><a href="https://people.cs.uchicago.edu/~huiyingli/">Huiying Li</a></u>, also a SAND Lab PhD
              student. The faculty advisors are SAND Lab co-directors and Neubauer
              Professors <u><a href="https://people.cs.uchicago.edu/~ravenben/">Ben Zhao</a></u> and 
	      <u><a href="https://people.cs.uchicago.edu/~htzheng/">Heather Zheng</a></u>. </p>
            <p><sup>1</sup>The Guy Fawkes mask, a la <a href="https://en.wikipedia.org/wiki/V_for_Vendetta_%28film%29"><strong><em>V
                    for Vendetta</em></strong></a>.</p>
          </div>
          <!-- 
          <div class="image"> <img src="imgs/teaser.png" alt="teaser">            <p>(a) We engineered a wearable ultrasound jammer that can prevent              surrounding microphones from eavesdropping on a conversation. (b)              This is the actual speech that the conversation partner hears,              since our jammer does not disrupt human hearing. However, (c) is              the transcript of what a state-of-the-art speech recognizer makes              out of the jammed conversation.</p>          </div>          <div class="text">            <p> Despite the initial excitement around voice-based smart devices,              consumers are becoming increasingly nervous with the fact that              these interactive devices are, by default, always listening,              recording, and possibly saving sensitive personal information.              Therefore, it is critical to build tools that protect users              against the potential compromise or misuse of microphones in the              age of voice-based smart devices. </p>            <p> Recently, researchers have shown that ultrasonic transducers can              prevent commodity microphones from recording human speech. While              these ultrasonic signals are imperceptible to human ears, they              leak into the audible spectrum after being captured by the              microphones, producing a jamming signal inside the microphone              circuit that jams (disrupts) voice recordings. The leakage is              caused by an inherent, nonlinear property of microphone’s              hardware. </p>            <p> However, all these devices exhibit two key limitations: (1) They              are heavily directional, thus requiring users to point the jammer              precisely at the location where the microphones are. This is not              only impractical, as it interferes with the users’ primary task,              but is also often impossible when microphones are hidden. (2) They              rely on multiple transducers that enlarge their jamming coverage              but introduce blind spots locations were the signals from two or              more transducers cancel each other out. If a microphone is placed              in any of these locations it will not be jammed, rendering the              whole jammer obsolete. </p>          </div>          <div class="image"> <img src="imgs/simulations.png" alt="simulations">            <p>Our simulations depict how different transducer layouts radiate              around the simulated device. We found that, when moving in space,              a wearable jammer outperforms stationary jammers.</p>          </div>          <div class="text">            <p> To tackle these shortcomings, we engineered a wearable jammer              that is worn as a bracelet, which is depicted in Figure 1. By              turning an ultrasonic jammer into a bracelet, our device leverages              natural hand gestures that occur while speaking, gesturing or              moving around to blur out the aforementioned blind spots.              Furthermore, by arranging the transducers in a ring layout, our              wearable jams in multiple directions and protects the privacy of              its user’s voice, anywhere and anytime, without requiring its user              to manually point the jammer to the eavesdropping microphones. </p>          </div>          <div class="image"> <img src="imgs/implementation.png" alt="implementation">            <p>Our prototype is a self-contained wearable comprised of              ultrasonic transducers, a signal generator, a microcontroller, a              battery, a voltage regulator and a 3W amplifier.</p>          </div>          <div class="text">            <p> We confirmed that an ultrasonic microphone jammer is superior to              state-of-the-art and commercial stationary jammers by conducting a              series of technical evaluations and a user study. These              demonstrated that: (1) our wearable jammer outperformed static              jammers in jamming coverage; (2) its jamming is effective even if              the microphones are hidden and covered by various materials, such              as cloths or paper sheets; and, (3) in a life-like situation our              study participants felt that our wearable protected the privacy of              their voice. </p>          </div>          <div class="image"> <img src="imgs/wer.png" alt="wer">            <p>Word error rate (WER) of speech recognition for jamming with our              wearable, planar jammer and i4. We found that the WER for planar              and i4 dropped drastically after 90 degree, while our wearable              maintained a constant jamming effect &gt;87%.</p>          </div>          <div class="image"> <img src="imgs/jamming_hidden.png" alt="wer">            <p>Speech recognition results when the microphone is covered up with              various objects.</p>          </div>   -->
        </section>
        <section>
          <hr>
          <a name="paper"></a><h3>Publication &amp; Presentation</h3>
          <p><strong>Fawkes: Protecting Personal Privacy against Unauthorized
              Deep Learning Models. </strong><br>
            Shawn Shan, Emily Wenger, Jiayun Zhang, Huiying Li, Haitao Zheng,
            and Ben Y. Zhao. <br>
            In Proceedings of <em>USENIX Security Symposium 2020</em>. ( <a href="http://people.cs.uchicago.edu/~ravenben/publications/abstracts/fawkes-usenix20.html">Download
              PDF here</a> ) </p><p>
	    Technical presentation video forthcoming</p>
	    
        </section>
        <section>
          <hr>
          <a name="code"></a><h3>Downloads and Source Code</h3>
          
          <ul>
            <li>Download the Fawkes Image Cloaking Tool
		<br><a href="http://sandlab.cs.uchicago.edu/fawkes/files/fawkes_binary.zip">Download Mac Binary</a>
		<br><a href="http://sandlab.cs.uchicago.edu/fawkes/files/fawkes_binary_windows.zip">Download Windows Binary</a>
		<br><a href="http://sandlab.cs.uchicago.edu/fawkes/files/fawkes_binary_linux.zip">Download Linux Binary</a>
		<br><a href="https://github.com/Shawn-Shan/fawkes/blob/master/fawkes/README.md">Setup Instructions</a>
		<br>If we have issues setting it up, please feel free to ask us by email or raising an issue in our <a href="https://github.com/Shawn-Shan/fawkes">Github repo</a>.
            </li>
            
            <li>Fawkes <a href="https://github.com/Shawn-Shan/fawkes">Source Code</a> on Github, for development and evaluation <br>
              
            </li>
          </ul>
          
        </section>
        <section>
          <hr>
          <a name="faq"></a><h3>Frequently Asked Questions</h3>
          
          <ul>
            <li><b><i>How effective is Fawkes against 3rd party facial recognition models like ClearView.ai?</i></b><br>

		We have extensive experiments and results in the technical paper (linked above). The short 
version is that we provide strong protection against unauthorized models. Our tests against state of the art 
facial recognition models from Microsoft Azure, Amazon Rekognition, and Face++ are at or near 100%. Protection 
level will vary depending on your willingness to tolerate small tweaks to your photos. Please do remember that 
this is a research effort first and foremost, and while we are trying hard to produce something useful for 
privacy-aware Internet users at large, there are likely issues in configuration, usability in the tool itself, 
and it may not work against all models for all images. 

            
	    </li><li><b><i>How is Fawkes different from things like the invisibility cloak projects at <a href="https://www.cs.umd.edu/~tomg/projects/invisible/">UMaryland</a>, led by Tom Goldstein, and other similar efforts?</i></b><br>

		Fawkes works quite differently from these prior efforts, and we believe it is the first 
practical tool that the average Internet user can make use of. Prior projects like the invisibility cloak 
project involve users wearing a specially <a href="https://www.cs.umd.edu/~tomg/projects/invisible/">printed 
patterned sweater</a>, which then prevents the wearer from being recognized by person-detection models. In 
other cases, the user is asked to wear a <a href="https://arxiv.org/abs/1904.08653">printed placard</a>, or a 
special patterned <a href="https://arxiv.org/abs/1908.08705">hat</a>. One fundamental difference is that these 
approaches can only protect a user when the user is wearing the sweater/hat/placard. Even if users were 
comfortable wearing these unusual objects in their daily lives, these mechanisms are model-specific, that is, 
they are specially encoded to prevent detection against a single specific model (in most cases, it is the YOLO 
model). Someone trying to track you can either use a different model (there are many), or just target users in 
settings where they can't wear these conspicuous accessories. In contrast, Fawkes is different because it 
protects users by targeting the model itself. Once you disrupt the model that's trying to track you, the 
protection is always on no matter where you go or what you wear, and even extends to attempts to identify you from 
static photos of you taken, shared or sent digitally.


<!--            <p></p>
	    <LI><B><I>How can I test if my cloaked images are working correctly?</B></I><BR>
		The short answer is that 
-->
            
	    </li><li><b><i>How can Fawkes be useful when there are so many uncloaked, original images of me on social media that I can't take down?</i></b><br>

		Fawkes works by training the unauthorized model to learn about a cluster of your cloaked images 
in its "feature space." If you, like many of us, already have a significant set of public images online, then a 
model like Clearview.AI has likely already downloaded those images, and used them to learn "what you look like" 
as a cluster in its feature space. However, these models are always adding more training data in order to 
improve their accuracy and keep up with changes in your looks over time. The more cloaked images you "release," 
the larger the cluster of "cloaked features" will be learned by the model. At some point, when your cloaked 
cluster of images grows bigger than the cluster of uncloaked images, the tracker's model will switch its 
definition of you to the new cloaked cluster and abandon the original images as outliers.

            
	    </li><li><b><i>Is Fawkes specifically designed as a response to Clearview.ai?</i></b><br>

		It might surprise some to learn that we started the Fawkes project a while before the New York 
Times article that profiled Clearview.ai in February 2020. Our original goal was to serve as a preventative 
measure for Internet users to inoculate themselves against the possibility of some third-party, unauthorized 
model. Imagine our surprise when we learned 3 months into our project that such companies already existed, and 
had already built up a powerful model trained from massive troves of online photos. It is our belief that 
Clearview.ai is likely only the (rather large) tip of the iceberg. Fawkes is designed to significantly raise 
the costs of building and maintaining accurate models for large-scale facial recognition. If we can reduce the 
accuracy of these models to make them untrustable, or force the model's owners to pay significant per-person 
costs to maintain accuracy, then we would have largely succeeded. For example, someone carefully examining a 
large set of photos of a single user might be able to detect that some of them are cloaked. However, that same 
person is quite possibly capable of identifying the target person in equal or less time using traditional means 
(without the facial recognition model). 

            
	    </li><li><b><i>Can Fawkes be used to impersonate someone else?</i></b><br>
		The goal of Fawkes is to avoid identification by someone with access to an unauthorized facial 
recognition model. While it is possible for Fawkes to make you "look" like someone else (e.g. "person X") in 
the eyes of a recognition model, we would not consider it an impersonation attack, since "person X" is highly 
likely to want to avoid identification by the model themselves. If you cloaked an image of yourself before 
giving it as training data to a legitimate model, the model trainer can simply detect the cloak by asking you 
for a real-time image, and testing it against your cloaked images in the feature space. The key to detecting cloaking is the "ground truth" image of you that a legitmate model can obtain, and unauthorized models cannot.

            
<!--	    <LI><B><I>Can Fawkes protect photos with multiple faces in them?</B></I><BR>
            <p></p>
-->
            </li><li><b><i>How can I distinguish photos that have been cloaked from those that have not?</i></b><br>
		A big part of the goal of Fawkes is to make cloaking as subtle and undetectable as possible and minimize impact on your photos. Thus it is intentionally difficult to tell cloaked images from the originals. We are looking into adding small markers into the cloak as a way to help users identify cloaked photos. More information to come.
              

	    </li><li><b><i>How do I get Fawkes and use it to protect my photos?</i></b><br>
		We are working hard to produce user-friendly versions of Fawkes for use on Mac and Windows platforms. For now, Fawkes is available as source code, and you will need to compile it on your own computer. More information will be made available as we get closer to software releases for MacOS and Windows platforms.
              
	    </li><li>We are adding more Q&amp;A soon. If you don't see your question here, please email us and we will add it to the page soon.

            </li>
          </ul>
          
        </section>
        <!-- <section class="award"> -->
        <!--   <h3>Award</h3> -->
        <!--   <div> -->
        <!--     <li>CHI'2020 - Best Paper Honorable Mention Award</li> -->
        <!--   </div> -->
        <!-- </section> -->
        <!-- <section class="team"> -->
        <!--   <h3>Team</h3> -->
        <!--   <div class="image"> -->
        <!--     <img src="imgs/NYT.jpg" alt="NYT-photo"> -->
        <!--     <p>From left to right: Pedro Lopes, Huiying Li, Yuxin Chen and Shan-Yuan Teng. -->
        <!--       <br> -->
        <!--       Not shown: Steven Nagels, Zhijing Li, Ben Zhao and Heather Zheng. -->
        <!--       <br> -->
        <!--       Photo credits to Petra Ford (New York Times)</p> -->
        <!--   </div> -->
        <!-- </section> -->
        <!-- <section class="press-coverage"> -->
        <!--   <h3>Selected press articles</h3> -->
        <!--   <div> -->
        <!--     <li>New York Times: <a href="https://www.nytimes.com/2020/02/14/technology/alexa-jamming-bracelet-privacy-armor.html">Activate This 'Bracelet of Silence,' and Alexa Can't Eavesdrop</a></li> -->
        <!--   </div> -->
        <!-- </section> --> </div></div></div>
    </div>
    <footer>
        <div>created by <a href="https://buildstarted.com">buildstarted</a> &copy; 2020 <a href="/about">about</a></div>
        <div>Share this page on social media: copy and paste this url https://linksfor.dev/</div>
        <div>If you prefer RSS: <a href="https://linksfor.dev/feed.xml">https://linksfor.dev/feed.xml</a></div>
        <div>Customer satisfaction guaranteed to be optional.</div>
    </footer>
    
    <script async defer>
        _dna = window._dna || {};
        _dna.siteId = "linksfor.devs";
        _dna.outlink = true;

        (function() {
            let dna = document.createElement('script');
            dna.type = 'text/javascript';
            dna.async = true;
            dna.src = '//dna.buildstarted.com/t.js';
            let s = document.getElementsByTagName('script')[0];
            s.parentNode.insertBefore(dna, s);
        })();
    </script>
    <noscript><img src="//dna.buildstarted.com/g?siteId=linksfor.devs"/></noscript>
</body>
</html>