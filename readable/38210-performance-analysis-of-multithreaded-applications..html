<!DOCTYPE html>
<html lang="en">
<head>
    <title>
Performance analysis of multithreaded applications. -
linksfor.dev(s)
    </title>
	<link rel="alternate" type="application/rss+xml" title="Linksfor.dev(s) feed" href="https://linksfor.dev/feed.rss" />
    <meta charset="utf-8">
    <meta name="Description" content="A curated source of links that devs might find interesting. Updated around the clock." />
    <meta name="google" value="notranslate">
    <link rel="stylesheet" type="text/css" href="/style.css" />
</head>
<body>
    <div class="grid">
        
<div class="readable">
    <h1>Performance analysis of multithreaded applications.</h1>
    <div><div class="notebody"> <p><strong>Contents:</strong></p> <p><strong>Subscribe to my <a href="/blog/2019/10/05/Performance-Analysis-Of-MT-apps#mc_embed_signup">mailing list</a> and support me on <a href="https://www.patreon.com/dendibakh">Patreon</a>.</strong></p> <p>Modern CPUs are getting more and more cores each year. As of 2019 you can buy fresh x86 server processor which will have more than 50 cores! And even mid-range desktop with 8 execution threads will not be surprising. Usually the question is how to find the workload to feed those hungry guys.</p> <p>Most of the articles in my blog so far were focused on the performance of a single core, completely ignoring the entire spectrum of multithreaded applications (MT app). I decided to fill this gap and write a <strong>beginner-friendly article</strong> showing how one can quickly jump into analyzing performance of the MT app. Sure, there is a lot of details which is impossible to cover in one article. Here I just want to touch ground on performance analysis of MT apps, give you the checklist and the set of tools which you can use. But be sure, there will be more of it on my blog, just stay tuned.</p> <p>As usual, I provide my articles with examples, so let me start with the benchmark which we will be working on.</p> <h3 id="benchmark-under-the-test">Benchmark under the test</h3> <p>For better illustration I had few constraints in mind when choosing the benchmark: <sup id="fnref:1"><a href="#fn:1" class="footnote">1</a></sup></p>
<ol> <li>It should have explicit parallelism using <code class="highlighter-rouge">pthread/std::thread</code> (no OpenMP).</li> <li>It does not scale with the amount of threads added. Work is not split, equally between threads.</li> <li>Of course, open source and free.</li>
</ol> <p>I took <code class="highlighter-rouge">h264dec</code> benchmark from the <a href="https://www.aes.tu-berlin.de/menue/research/projects/completed_projects/starbench_parallel_benchmark_suite/">Starbench parallel benchmark suite</a>. This benchmark decodes H.264 raw videos and uses pthread library for managing threads.</p> <p>One can run this benchmark like this:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>./h264dec <span class="nt">-i</span> park_joy_2160p.h264 <span class="nt">-t</span> &lt;number of threads&gt; <span class="nt">-o</span> output.file
</code></pre></div></div> <p>In this benchmark there is one main thread (mostly idle), one thread that reads the input, configurable number of worker threads (that do decoding) and one thread that writes the output.</p> <h3 id="performance-scaling">Performance scaling</h3> <p>First thing in MT app that needs to be estimated is how it scales as we throw more cores/threads to it. In fact, this is the most important metric of how successful the future of the app will be. The chart below shows the scaling of <code class="highlighter-rouge">h264dec</code> benchmark. My CPU (Intel Core i5-8259U) has 4 cores/8 threads, so I took upper bound as 8 threads. Notice, that after using 4 threads, performance doesn&#x2019;t scale much.</p> <p><img src="/img/posts/MT_apps/scaling.png" alt class="center-image-width-50"></p> <p>This is very useful information when you try to model performance of the workload. For example, when you estimate what HW you need for the task you are trying to solve. Looking at this picture, I will better choose less cores but higher frequency of a single core. <sup id="fnref:2"><a href="#fn:2" class="footnote">2</a></sup></p> <p>Sometimes we could be asked what is the minimal system configuration that can handle a certain workload. When the application scales linearly (i.e. when threads work on independent piece of data and do not require synchronization) you can provide rough estimation pretty easily. Just run the workload in a single thread and measure the number of cycles executed. Then you can select CPU with specific number of cores and frequency to meet your latency/throughput goals. Situation gets tricky however, when performance of the app doesn&#x2019;t scale well. Here you can&#x2019;t rely on IPC anymore (see later), since one thread can be busy and show high utilization, but in fact all it was doing is just spinning on a lock. There is more information on capacity planning in the book <a href="https://amzn.to/2Vd6GwS">Systems Performance</a> by Brendan Gregg.</p> <h3 id="cpu-utilization">CPU utilization</h3> <p>Next important metric is CPU utilization <sup id="fnref:3"><a href="#fn:3" class="footnote">3</a></sup>. It can tell you how much threads on average were busy doing something. Note, that it is not necessarily useful work. It can be just spinning. Below you can see the chart for <code class="highlighter-rouge">h264dec</code> benchmark.</p> <p><img src="/img/posts/MT_apps/utilization.png" alt class="center-image-width-50"></p> <p>For example, when having 5 worker threads for the workload, on average only 4 were busy. I.e. typically, one thread was always sleeping, which limits scaling. This tells that there is some synchronization going between the threads.</p> <p>To estimate how much overhead is spent on communication between threads we can collect the total number of cycles and instructions:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 1 worker thread</span>
<span class="nv">$ </span>perf stat ./h264dec <span class="nt">-i</span> park_joy_2160p.h264 <span class="nt">-t</span> 1 <span class="nt">-o</span> output.file <span class="nt">-v</span> 261,963,433,544 cycles <span class="c"># 3.766 GHz</span> 485,932,999,948 instructions <span class="c"># 1.85 insn per cycle</span> 66.236645032 seconds <span class="nb">time </span>elapsed 66.290561000 seconds user 3.324930000 seconds sys <span class="c"># 4 worker thread</span>
<span class="nv">$ </span>perf stat ./h264dec <span class="nt">-i</span> park_joy_2160p.h264 <span class="nt">-t</span> 4 <span class="nt">-o</span> output.file <span class="nt">-v</span> 272,518,956,365 cycles <span class="c"># 3.479 GHz</span> 523,079,251,733 instructions <span class="c"># 1.92 insn per cycle</span> 23.643595782 seconds <span class="nb">time </span>elapsed 73.979057000 seconds user 4.402318000 seconds sys <span class="c"># 8 worker thread</span>
<span class="nv">$ </span>perf stat ./h264dec <span class="nt">-i</span> park_joy_2160p.h264 <span class="nt">-t</span> 8 <span class="nt">-o</span> output.file <span class="nt">-v</span> 453,581,394,912 cycles <span class="c"># 3.410 GHz</span> 661,715,307,682 instructions <span class="c"># 1.46 insn per cycle</span> 22.700304401 seconds <span class="nb">time </span>elapsed
     128.122821000 seconds user
       4.883838000 seconds sys
</code></pre></div></div> <p>There is a couple of interesting things here. First, don&#x2019;t be confused by the timings for the case with 8 threads. CPU time is more than 5 times bigger than the wall time. It perfectly correlates with CPU utilization.</p> <p>Second, look at the number of instructions retired for 8 threads case. For 8 worker threads we have 36% more executed instructions than in single-thread which is all can be considered as overhead. All this is likely caused by thread active synchronization (spinning).</p> <h3 id="profiling-multithreaded-apps">Profiling multithreaded apps</h3> <p>To do analysis on a source code level let&#x2019;s run the profiler on it: <sup id="fnref:4"><a href="#fn:4" class="footnote">4</a></sup></p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 1 worker thread</span>
<span class="nv">$ </span>perf record <span class="nt">-o</span> perf1.data <span class="nt">--</span> ./h264dec <span class="nt">-i</span> park_joy_2160p.h264 <span class="nt">-t</span> 1 <span class="nt">-o</span> output.file <span class="nt">-v</span>
<span class="o">[</span> perf record: Captured and wrote 10.790 MB perf1.data <span class="o">(</span>282808 samples<span class="o">)</span> <span class="o">]</span>
<span class="c"># 4 worker thread</span>
<span class="nv">$ </span>perf record <span class="nt">-o</span> perf4.data <span class="nt">--</span> ./h264dec <span class="nt">-i</span> park_joy_2160p.h264 <span class="nt">-t</span> 4 <span class="nt">-o</span> output.file <span class="nt">-v</span>
<span class="o">[</span> perf record: Captured and wrote 12.592 MB perf4.data <span class="o">(</span>330029 samples<span class="o">)</span> <span class="o">]</span>
<span class="c"># 8 worker thread</span>
<span class="nv">$ </span>perf record <span class="nt">-o</span> perf8.data <span class="nt">--</span> ./h264dec <span class="nt">-i</span> park_joy_2160p.h264 <span class="nt">-t</span> 8 <span class="nt">-o</span> output.file <span class="nt">-v</span>
<span class="o">[</span> perf record: Captured and wrote 21.239 MB perf8.data <span class="o">(</span>556685 samples<span class="o">)</span> <span class="o">]</span>
</code></pre></div></div> <p>The number of samples correlates with the user time we saw earlier. I.e. the more workers we have the more <a href="https://easyperf.net/blog/2018/06/01/PMU-counters-and-profiling-basics">interrupts</a> profiler needs to do. If we look into the profiles, they will mostly look similar except one function <code class="highlighter-rouge">ed_rec_thread</code>:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 1 worker thread</span>
<span class="nv">$ </span>perf report <span class="nt">-n</span> <span class="nt">-i</span> perf1.data <span class="nt">--stdio</span>
<span class="c"># Overhead Samples Command Shared Object Symbol </span>
<span class="c"># ........ ............ ....... ................ .....................................</span> 0.18% 524 h264dec h264dec <span class="o">[</span>.] ed_rec_thread <span class="c"># 4 worker thread</span>
<span class="nv">$ </span>perf report <span class="nt">-n</span> <span class="nt">-i</span> perf4.data <span class="nt">--stdio</span>
<span class="c"># Overhead Samples Command Shared Object Symbol </span>
<span class="c"># ........ ............ ....... ................ .....................................</span> 3.62% 11417 h264dec h264dec <span class="o">[</span>.] ed_rec_thread <span class="c"># 8 worker thread</span>
<span class="nv">$ </span>perf report <span class="nt">-n</span> <span class="nt">-i</span> perf8.data <span class="nt">--stdio</span>
<span class="c"># Overhead Samples Command Shared Object Symbol </span>
<span class="c"># ........ ............ ....... ................ .....................................</span> 17.53% 95619 h264dec h264dec <span class="o">[</span>.] ed_rec_thread
</code></pre></div></div> <p>Notice how <code class="highlighter-rouge">ed_rec_thread</code> function takes much more samples with increasing the number of threads. This is likely the bottleneck that prevents us from scaling further with adding more threads.</p> <p>Also, this is one of the methods how we can find synchronization bottlenecks in the MT app. By simply comparing profiles for the different number of workers.</p> <p>And in fact, there is this code in <code class="highlighter-rouge">decode_slice_mb</code> function that is inlined into <code class="highlighter-rouge">ed_rec_thread</code>:</p>
<div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code> <span class="k">while</span> <span class="p">(</span><span class="n">rle</span><span class="o">-&gt;</span><span class="n">mb_cnt</span> <span class="o">&gt;=</span> <span class="n">rle</span><span class="o">-&gt;</span><span class="n">prev_line</span><span class="o">-&gt;</span><span class="n">mb_cnt</span> <span class="o">-</span><span class="mi">1</span><span class="p">);</span>
</code></pre></div></div> <p>where <code class="highlighter-rouge">mb_cnt</code> is defined as volatile. So, it is in fact active spinning happens here.</p> <h3 id="per-thread-view">Per-thread view</h3> <p>Linux perf is powerful enough to collect profiles for every thread that is spawn from the main thread of the process. If you want to look inside the execution of a single thread perf let you do this with <code class="highlighter-rouge">-s</code> option:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>perf record <span class="nt">-s</span> ./h264dec <span class="nt">-i</span> park_joy_2160p.h264 <span class="nt">-t</span> 8 <span class="nt">-o</span> output.file <span class="nt">-v</span>
</code></pre></div></div> <p>Then you can list all the thread IDs along with the number of samples collected for each of them:</p> <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ perf report -n -T
...
#  PID   TID   cycles:ppp
  6602  6607  52758679502
  6602  6603    487183790
  6602  6613  49670283608
  6602  6608  51173619921
  6602  6604    165362635
  6602  6609  38662454026
  6602  6610  31375722931
  6602  6606  48270267494
  6602  6611  53793234480
  6602  6612  25640899076
  6602  6605  14481176486
</code></pre></div></div> <p>Now, if you want to only analyze the samples that were collected for particular software thread, you can do it with <code class="highlighter-rouge">--tid</code> option:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>perf report <span class="nt">-T</span> <span class="nt">--tid</span> 6607 <span class="nt">-n</span> 8.28% 25657 h264dec h264dec <span class="o">[</span>.] decode_cabac_residual_nondc 7.12% 35880 h264dec h264dec <span class="o">[</span>.] put_h264_qpel8_hv_lowpass 6.19% 31430 h264dec h264dec <span class="o">[</span>.] put_h264_qpel8_v_lowpass 5.87% 28874 h264dec h264dec <span class="o">[</span>.] h264_v_loop_filter_luma_c 2.82% 10105 h264dec h264dec <span class="o">[</span>.] ff_h264_decode_mb_cabac 1.19% 4525 h264dec h264dec <span class="o">[</span>.] get_cabac_noinline
</code></pre></div></div> <p>If you are a lucky owner of <a href="https://software.intel.com/en-us/vtune">Intel Vtune Amplifier</a>, you can use it&#x2019;s nice GUI to do the filtering and zooming into particular thread and point in time.</p> <p>When you nailed down the thread that is causing you troubles and you don&#x2019;t care much about the other threads, you can profile only one thread by attaching to it with perf:</p> <h3 id="using-strace-tool">Using strace tool</h3> <p>Another thing which I use regularly is <code class="highlighter-rouge">strace</code> tool:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>strace <span class="nt">-tt</span> <span class="nt">-ff</span> <span class="nt">-T</span> <span class="nt">-o</span> strace-dump <span class="nt">--</span> ./h264dec <span class="nt">-i</span> park_joy_2160p.h264 <span class="nt">-t</span> 8 <span class="nt">-o</span> output.file
</code></pre></div></div> <p>This will generate syscall dumps per running thread with precise time stamps and duration for each syscall. We can further process individual dump file to extract lots of interesting information from it. For example:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Total time spent by parser thread waiting for mutex to unlock</span>
<span class="nv">$ </span><span class="nb">grep </span>futex strace-dump.3740 <span class="o">&gt;</span> futex.dump
<span class="nv">$ </span>sed <span class="nt">-i</span> <span class="s1">&apos;s/.*&lt;//g&apos;</span> futex.dump
<span class="nv">$ </span>sed <span class="nt">-i</span> <span class="s1">&apos;s/&gt;//g&apos;</span> futex.dump
<span class="nv">$ </span>paste <span class="nt">-s</span> <span class="nt">-d</span>+ futex.dump | bc
16.407458
</code></pre></div></div> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Total time spent by parser thread on reading input from the file</span>
<span class="nv">$ </span><span class="nb">grep read </span>strace-dump.3740 <span class="o">&gt;</span> read.dump
<span class="nv">$ </span>sed <span class="nt">-i</span> <span class="s1">&apos;s/.*&lt;//g&apos;</span> read.dump <span class="nv">$ </span>sed <span class="nt">-i</span> <span class="s1">&apos;s/&gt;//g&apos;</span> read.dump <span class="nv">$ </span>paste <span class="nt">-s</span> <span class="nt">-d</span>+ read.dump | bc
2.179850
</code></pre></div></div> <p>Considering total running time of the thread is 21 seconds, almost 80% of the time this thread was blocked. Since this thread provides input for the worker threads, that&#x2019;s likely another thing that limits performance scaling of the benchmark.</p> <h3 id="optimizing-multithreaded-apps">Optimizing multithreaded apps</h3> <p>When dealing with single-threaded application, optimizing one portion of the program usually yields positive results on performance<sup id="fnref:5"><a href="#fn:5" class="footnote">5</a></sup>. However, it&#x2019;s not necessary the case for multithreaded applications. And in fact, it can be very hard to predict. That&#x2019;s where <a href="https://plasma-umass.org/coz/">coz</a> profiler might help. I wasn&#x2019;t able to extract any useful information from using it, but it was the first time I tried it, so maybe I was doing something wrong.</p> <p>If your application scales well or threads work on independent piece of input, then you may see a proportional increase in performance as a result of your optimizations. In this case it&#x2019;s easier to do performance analysis of the application running on a single thread. There is a good chance that optimizations that are valid for single thread will scale well. <sup id="fnref:6"><a href="#fn:6" class="footnote">6</a></sup></p> <p><a href="https://easyperf.net/blog/2019/02/09/Top-Down-performance-analysis-methodology">Top-Down Methodology</a>, our best friend in finding michroarchitectural issues, will become even better in new Intel CPU generations. See the <a href="https://dyninst.github.io/scalable_tools_workshop/petascale2018/assets/slides/TMA%20addressing%20challenges%20in%20Icelake%20-%20Ahmad%20Yasin.pdf">presentation</a> by Ahmad Yasin for more details.</p> <p>However, there are performance problems that are specific to multithreading. In future articles I will touch the topics of lock contention, false sharing and more. Stay tuned!</p> </div></div>
</div>
    </div>
    <footer>
        <div>created by buildstarted &copy; 2019 <a href="/about">about</a></div>
        <div>Share this page on social media: copy and paste this url https://linksfor.dev/</div>
    </footer>
    
    <script>
        (function() {
            var COLLECT_URL = "https://dna.buildstarted.com/t";
            var SITE_ID = "linksfor.devs";
            var GLOBAL_VAR_NAME = "__DNA__";

            window[GLOBAL_VAR_NAME] = {};

            window[GLOBAL_VAR_NAME].sendPageView = function() {
                var path = location.pathname;
                var referrer = document.referrer;

                var url = COLLECT_URL + "?siteid=" + SITE_ID + "&p=" + encodeURIComponent(path) + "&r=" + encodeURIComponent(referrer);

                try { fetch(url, { method: "GET" }); } catch (e) {}
            };

            window[GLOBAL_VAR_NAME].sendPageView();
        })();
    </script>
</body>
</html>