<!DOCTYPE html>
<html lang="en">
<head>
    <title>
Memory Bandwidth Napkin Math - linksfor.dev(s)    </title>
    <meta charset="utf-8">
    <link rel="alternate" type="application/rss+xml" title="Linksfor.dev(s) feed" href="https://linksfor.dev/feed.rss" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta name="google" value="notranslate">
    <link rel="stylesheet" type="text/css" href="/style.css" />
    <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon">
    <link rel="icon" href="/favicon.ico" type="image/x-icon">
    <meta property="og:title" content="Memory Bandwidth Napkin Math - linksfor.dev(s)"/>
    <meta property="og:description" content="An exploration into C&#x2B;&#x2B; memory throughput performance."/>
    <meta property="og:type" content="website"/>
    <meta property="og:url" content="https://www.forrestthewoods.com/blog/memory-bandwidth-napkin-math/"/>

<meta property="og:site_name" content="linksfor.dev(s)" />
</head>
<body>
	<div class="devring" style="background: #222">
		<div class="grid">
			<div style="display: grid; grid-template-columns: .5fr 1fr 1fr 1fr; text-align: center;">
				<span class="devring-title">devring.club</span>
				<a href="https://devring.club/site/1/previous" class="devring-previous">Previous</a>
				<a href="https://devring.club/random" class="devring-random">Random</a>
				<a href="https://devring.club/sites/1/next" class="devring-next">Next</a>
			</div>
		</div>
	</div>
    <div class="grid">
        <h1>
<a href="/" style="color:inherit">linksfor.dev(s)</a>
        </h1>
        <title>linksfor.dev(s) - Memory Bandwidth Napkin Math</title>
<div class="readable">
        <h1>Memory Bandwidth Napkin Math</h1>
            <div>Reading time: 16-20 minutes</div>
        <div>Posted here: 12 Feb 2020</div>
        <p><a href="https://www.forrestthewoods.com/blog/memory-bandwidth-napkin-math/">https://www.forrestthewoods.com/blog/memory-bandwidth-napkin-math/</a></p>
        <hr/>
<div id="readability-page-1" class="page"><div>

        <!-- NON RESPONSIVE, EASY OPTION -->
        <img src="https://forrestthewoods.b-cdn.net/assets/images/header-memory-bandwidth-napkin-math.jpg" alt=" " width="100%">

        <!-- Headline and date -->
        

        <!-- written content -->
        <div>
            <p>A few weeks ago I had lunch with a co-worker. He was complaining about some process being slow. He estimated how many bytes of data were generated, how many processing passes were performed, and ultimately how many bytes of RAM needed to be accessed. He suggested that modern GPUs with upwards of 500 gigabytes per second of memory bandwidth could eat his problem for breakfast.</p>

            <p>I thought his perspective was interesting. That's not how I think about problems.</p>
            
            <p>I know about the processor-memory performance gap. I know how to write cache friendly code. I know approximate latency numbers. But I don't know enough to ballpark memory throughput on a napkin.</p>

            <p><img src="https://www.forrestthewoods.com/blog/memory-bandwidth-napkin-math/assets/img/01.png" alt="CPU vs Memory Bandwidth"></p><p>Here's my thought experiment. Imagine you have a contiguous array of one billion 32-bit integers in memory. That's 4 gigabytes. How long will it take to iterate that array and sum the values? How many bytes of contiguous data can a CPU read from RAM per second? How many bytes of random access? How well can it be parallelized?</p>

            <p>Now you may think these aren't useful questions. Real programs are too complicated for such a naive benchmark to be meaningful. You're not wrong! The real answer is "it depends". </p>

            <p>That said, I think the question is worth a blog post's worth of exploration. I'm not trying to find <i>the answer</i>. But I do think we can identify some upper and lower bounds, some interesting points in the middle, and hopefully learn a few things along the way.</p>



            
            

            <p>If you read programming blogs at some point you've probably come across "numbers every programmer should know". It looks something like this.</p>
<pre>L1 cache reference                           0.5 ns
Branch mispredict                            5   ns
L2 cache reference                           7   ns                      14x L1 cache
Mutex lock/unlock                           25   ns
Main memory reference                      100   ns                      20x L2 cache, 200x L1 cache
Compress 1K bytes with Zippy             3,000   ns        3 us
Send 1K bytes over 1 Gbps network       10,000   ns       10 us
Read 4K randomly from SSD*             150,000   ns      150 us          ~1GB/sec SSD
Read 1 MB sequentially from memory     250,000   ns      250 us
Round trip within same datacenter      500,000   ns      500 us
Read 1 MB sequentially from SSD*     1,000,000   ns    1,000 us    1 ms  ~1GB/sec SSD, 4X memory
Disk seek                           10,000,000   ns   10,000 us   10 ms  20x datacenter roundtrip
Read 1 MB sequentially from disk    20,000,000   ns   20,000 us   20 ms  80x memory, 20X SSD
Send packet CA-&gt;Netherlands-&gt;CA    150,000,000   ns  150,000 us  150 ms
</pre>
            <p>Source: <a href="https://gist.github.com/jboner/2841832">Jonas BonÃ©r</a></p>

            <p>This is a great list. I see it on HackerNews at least once a year. Every programmer should know these numbers.</p>

            <p>But these numbers are focused on a different question. Latency and throughput are not the same.</p>



            
            

            <p>That list is from 2012. This post comes from 2020, and times have changed. Here are figures for an Intel i7 via <a href="https://stackoverflow.com/questions/4087280/approximate-cost-to-access-various-caches-and-main-memory">StackOverflow.</a></p>

<pre>L1 CACHE hit, ~4 cycles(2.1 - 1.2 ns)
L2 CACHE hit, ~10 cycles(5.3 - 3.0 ns)
L3 CACHE hit, line unshared               ~40 cycles(21.4 - 12.0 ns)
L3 CACHE hit, shared line in another core ~65 cycles(34.8 - 19.5 ns)
L3 CACHE hit, modified in another core    ~75 cycles(40.2 - 22.5 ns)
local  RAM                                                   ~60 ns</pre>

            <p>Interesting! What's changed?</p>

            <ul>
                <li>L1 is slower; <code>0.5 ns -&gt; 1.5 ns</code></li>
                <li>L2 is faster; <code>7 ns -&gt; 4.2 ns</code></li>
                <li>L1 vs L2 relative speed is way closer; <code>2.5x vs 14x</code> ðŸ¤¯</li>
                <li>L3 cache is now standard; <code>12 ns to 40 ns</code></li>
                <li>Ram is faster; <code> 100ns -&gt; 60ns</code></li>
            </ul>

            <p>I don't want to draw too many conclusions from this. It's not clear how the original figures were calculated. We're not comparing apples to apples.</p>
            
            <p>Here's some bandwidth and cache sizes for my CPU courtesy of <a href="https://en.wikichip.org/wiki/intel/core_i7/i7-8700k">wikichip</a>.</p>

<pre>Memory Bandwidth: 39.74 gigabytes per second
L1 cache: 192 kilobytes (32 KB per core)
L2 cache: 1.5 megabytes (256 KB per core)
L3 cache: 12 megabytes  (shared; 2 MB per core)
</pre>
            <p>Here's what I'd like to know:</p>

            <ul>
                <li>What is the upper limit of RAM performance?</li>
                <li>What is the lower limit?</li>
                <li>What are the limits for L1/L2/L3 cache?</li>
            </ul>

            
            

            <p>Let's run some tests. I put together a naive C++ benchmark. It looks <i>very roughly</i> like this.</p>

<pre><span>// Generate random elements</span>
std<span>::</span>vector<span>&lt;</span><span>int</span><span>&gt;</span> nums;
<span>for</span> (<span>size_t</span> i <span>=</span> <span>0</span>; i <span>&lt;</span> <span>1024</span><span>*</span><span>1024</span><span>*</span><span>1024</span>; <span>++</span>i) <span>// one billion ints</span>
    nums.push_back(rng() <span>%</span> <span>1024</span>); <span>// small nums to prevent overflow</span>

<span>// Run test with 1 to 12 threads</span>
<span>for</span> (<span>int</span> thread_count <span>=</span> <span>1</span>; thread_count <span>&lt;=</span> MAX_THREADS; <span>++</span>thread_count) {
    <span>auto</span> slice_len <span>=</span> nums.size() <span>/</span> thread_count;
    
    <span>// for-each thread</span>
    <span>for</span> (<span>size_t</span> <span>thread</span> <span>=</span> <span>0</span>; <span>thread</span> <span>&lt;</span> thread_count; <span>++</span><span>thread</span>) {
        
        <span>// partition data</span>
        <span>auto</span> begin <span>=</span> nums.begin() <span>+</span> <span>thread</span> <span>*</span> slice_len;
        <span>auto</span> end <span>=</span> (<span>thread</span> <span>==</span> thread_count <span>-</span> <span>1</span>)
            <span>?</span> nums.end() <span>:</span> begin <span>+</span> slice_len;

        <span>// spawn threads</span>
        futures.push_back(std<span>::</span>async([begin, end] { 
            
            <span>// sum ints sequentially</span>
            <span>int64_t</span> sum <span>=</span> <span>0</span>;
            <span>for</span> (<span>auto</span> ptr <span>=</span> begin; ptr <span>&lt;</span> end; <span>++</span>ptr)
                sum <span>+=</span> <span>*</span>ptr;
            <span>return</span> sum;
        }));
    }

    <span>// combine results</span>
    <span>int64_t</span> sum <span>=</span> <span>0</span>;
    <span>for</span> (<span>auto</span><span>&amp;</span> future <span>:</span> futures)
        sum <span>+=</span> future.get();
}
</pre>

            <p>I'm leaving out a few details. But you get the idea. Create a large, contiguous array of elements. Divide the array into non-overlapping chunks. Process each chunk on a different thread. Accumulate the results.</p>

            <p>I also want to measure random access. This is tricky. I tried a few methods. Ultimately I chose to pre-compute and shuffle indices. Each index exists exactly one. The inner-loop then iterates indices and computes <code>sum += nums[index]</code>.</p>
            
<pre>std<span>::</span>vector<span>&lt;</span><span>int</span><span>&gt;</span> nums <span>=</span> <span>/* ... */</span>;
std<span>::</span>vector<span>&lt;</span><span>uint32_t</span><span>&gt;</span> indices <span>=</span> <span>/* shuffled */</span>;

<span>// random access</span>
<span>int64_t</span> sum <span>=</span> <span>0</span>;
<span>for</span> (<span>auto</span> ptr <span>=</span> indices.begin(); ptr <span>&lt;</span> indices.end(); <span>++</span>ptr) {
    <span>auto</span> idx <span>=</span> <span>*</span>ptr;
    sum <span>+=</span> nums[idx];
}
<span>return</span> sum;
</pre>
            <p>I do <b>not</b> consider the memory of the indices array for my bandwidth calculations. I only count bytes that contribute to <code>sum</code>. I'm not benchmarking my hardware. I'm estimating my ability to work with data sets of different sizes and different access patterns.</p>

            <p>I performed tests with three data types:</p>

            <ul>
                <li><code>int</code> - basic 32-bit integer</li>
                <li><code>matri4x4</code> - contains <code>int[16]</code>; fits on 64-byte cache line</li>
                <li><code>matrix4x4_simd</code> - uses <code>__m256i</code> intrinsics</li>
            </ul>


            
            

            <p>My first test operates on a large block of memory. A <code>1 GB</code> block of <code>T</code> elements is allocated and filled with small, random values. A simple loop iterates the array N times so it accesses <code>N GB</code> worth of memory to produce an <code>int64_t</code> sum. Multi-threading partitions the array and access the exact same number of elements.</p>
        </div>

        <p><img src="https://www.forrestthewoods.com/blog/memory-bandwidth-napkin-math/assets/img/02.png" alt="Large Block - Sequential and Random">
        </p>

        <div>
            <p>Tada! This plot takes the average time to sum elements and converts it from <code>runtime_in_nanoseconds</code> to <code>gigabytes_per_second</code>.</p>

            <p>This is pretty neat. <code>int32</code> can sequentially read <code>11 GB/s</code> on a single thread. It scales linearly until flattening out at <code>38 GB/s</code>. <code>matrix4x4</code> and <code>matrix4x4_simd</code> are faster, but hit a similar ceiling.</p>

            <p>There is a clear and obvious upper limit on how much data we can read from RAM per second. On my system it's roughly <code>40 GB/s</code>. This lines up with modern specifications listed above.</p>

            <p>Looking at the bottom 3 lines, random access is slow. Very very slow. Single-threaded <code>int32</code> performance is a paltry <code>0.46 GB/s</code>. This is <code>24x</code> slower than its <code>11.03 GB/s</code> sequential sum! <code>matrix4x4</code> is better since it operates on full cache-lines. But it's still four to seven times slower than sequential and peaks at just <code>8 GB/s</code>.</p>



            
            

            <p>My system has per-core L1/L2/L3 cache sizes of <code>32 KB</code>, <code>256 KB</code>, and <code>2 MB</code>. What happens if we take a <code>32 KB</code> block of elements and iterate across it 125,000 times? That's <code>4 GB</code> worth of memory access, but we'll ~always hit the cache.</p>
        </div>

        <p><img src="https://www.forrestthewoods.com/blog/memory-bandwidth-napkin-math/assets/img/03.png" alt="int32 - small block - sequential">
        </p>

        <div>
            <p>Awesome! Single-threaded performance is similar to the large block read, <code>~12 GB/s</code>. Except this time multi-threading smashes through the <code>40 GB/s</code> ceiling. This makes perfect sense. The data stays in cache so the RAM bottleneck doesn't come into play. Data sizes that don't fit into L3 cache hit the same <code>~38 GB/s</code> ceiling.</p>

            <p><code>matrix4x4</code> follows a similar pattern, but is even faster; <code>31 GB/s</code> single-thread, <code>171 GB/s</code> multi-threaded.</p>
        </div>

        <p><img src="https://www.forrestthewoods.com/blog/memory-bandwidth-napkin-math/assets/img/04.png" alt="matrix4x4 - small block - sequential">
        </p>

        <p>Now let's look at <code>matrix4x4_simd</code>. Pay close attention to the y-axis.</p>

        <p><img src="https://www.forrestthewoods.com/blog/memory-bandwidth-napkin-math/assets/img/05.png" alt="matrix4x4_simd - small block - sequential">
        </p>

        <div>
            <p><code>matrix4x4_simd</code> is omgwtfbbq fast. It's upwards of 10x faster than <code>int32</code>. When operating on a <code>16 KB</code> block it actually breaks <code>1000 GB/s</code>. ðŸ¤¯</p>

            <p>Obviously this is a superficial synthetic test. Most applications don't perform the same operation on the same data a million times in a row. This is not indicative of real world performance.</p>

            <p>But the lesson is real. Operating on data inside the cache is <i>fast</i>. With a very high ceiling when using simd; <code>&gt;100 GB/s</code> single-thread, <code>&gt;1000 GB/s</code> multi-threaded. Getting data into the cache is slow and has a hard cap, <code>~40 GB/s</code> total.</p>



            
            

            <p>Let's do the same thing, but with random access. This is my favorite part of this post.</p>
        </div>
    
        <p><img src="https://www.forrestthewoods.com/blog/memory-bandwidth-napkin-math/assets/img/06.png" alt="int32 - small block - random">
        </p>
    
        <p>Reading random values RAM is slow, <code>0.46 GB/s</code>. Reading random values from L1 cache is very very fast, <code>13 GB/s</code>. This is <i>faster</i> than the <code>11 GB/s</code> performance we got sequentially reading <code>int32</code> from RAM.</p>
        
        <p><img src="https://www.forrestthewoods.com/blog/memory-bandwidth-napkin-math/assets/img/07.png" alt="matrix4x - small block - random">
        </p>

        <p><code>matrix4x4</code> follows the same pattern, but is ~2x faster than <code>int</code>.</p>

        <p><img src="https://www.forrestthewoods.com/blog/memory-bandwidth-napkin-math/assets/img/08.png" alt="matrix4x_simd - small block - random">
        </p>

        <div>
            <p><code>matrix4x4_simd</code> random access is crazy fast.</p>


        
        

            <p>Random access from RAM is slow. Catastrophically slow. Less than <code>1 GB/s</code> slow for both <code>int32</code>. Random access from the cache is remarkably quick. It's comparable to <b>sequential</b> RAM performance.</p>
        </div>

        <p><img src="https://www.forrestthewoods.com/blog/memory-bandwidth-napkin-math/assets/img/09.png" alt="Single Threaded Comparison">
        </p>

        <div>
            <p>Let this sink in. Random access into the cache has comparable performance to sequential access from RAM. The drop off from sub-L1 <code>16 KB</code> to L2-sized <code>256 KB</code> is 2x or less.</p> 
            
            <p>I think this has profound implications.</p>


            
            

            <p>Pointer chasing is bad. Really, really bad. Just how bad is it? I made an extra test that wraps <code>matrix4x4</code> in <code>std::unique_ptr</code>. Each access has to go through a pointer. Here's the terrible, horrible, no good, very bad result.</p>

<pre>     1 Thread       |   matrix4x4   | unique_ptr |  diff  |
--------------------|---------------|------------|--------|
Large Block - Seq   |   14.8 GB/s   |  0.8 GB/s  |  19x   |
16 KB - Seq         |   31.6 GB/s   |  2.2 GB/s  |  14x   |
256 KB - Seq        |   22.2 GB/s   |  1.9 GB/s  |  12x   |
Large Block - Rand  |    2.2 GB/s   |  0.1 GB/s  |  22x   |
16 KB - Rand        |   23.2 GB/s   |  1.7 GB/s  |  14x   |
256 KB - Rand       |   15.2 GB/s   |  0.8 GB/s  |  19x   |


     6 Threads      |   matrix4x4   | unique_ptr |  diff  |
--------------------|---------------|------------|--------|
Large Block - Seq   |   34.4 GB/s   |  2.5 GB/s  |  14x   |
16 KB - Seq         |  154.8 GB/s   |  8.0 GB/s  |  19x   |
256 KB - Seq        |  111.6 GB/s   |  5.7 GB/s  |  20x   |
Large Block - Rand  |    7.1 GB/s   |  0.4 GB/s  |  18x   |
16 KB - Rand        |   95.0 GB/s   |  7.8 GB/s  |  12x   |
256 KB - Rand       |   58.3 GB/s   |  1.6 GB/s  |  36x   |
</pre>

            <p>Sequentially summing values behind a pointer runs at less than <code>1 GB/s</code>. Random access, which misses the cache twice, runs at just <code>0.1 GB/s</code>.</p>

            <p>Pointer chasing is 10 to 20 times slower. Friends don't let friends used linked lists. Please, think of the <strike>children</strike> cache.</p>

        
            
            
            <p>It's standard for game developers to set CPU and memory allocation budgets for subsystems. But I'm not sure I've ever seen a memory access budget.</p>

            <p>Modern games run at increasingly high frame rates. <code>60 fps</code> is table stakes. VR runs at <code>90 Hz</code>. I have a <code>144 Hz</code> gaming monitor. It's awesome and 60fps feels like crap. I'm never going back. E-Sports/Twitch players use <code>240 Hz</code> monitors. Asus unveiled a <code>360 Hz</code> monster at CES this year.</p>

            <p>My CPU has an upper limit of <code>~40 GB/s</code>. That sounds like a lot! At <code>240 Hz</code> that's just <code>167 MB</code> per frame. A more realistic application might consume <code>5 GB/s</code> at 144 Hz which is just <code>69 MB</code> per frame.</p>

            <p>Here's a table with a few figures.</p>
        </div>

        <div>
<pre>        |   1   |   10   |   30   |   60   |   90   |  144   |  240   |  360   |
--------|-------|--------|--------|--------|--------|--------|--------|--------|
40 GB/s | 40 GB |  4 GB  | 1.3 GB | 667 MB | 444 MB | 278 MB | 167 MB | 111 MB |
10 GB/s | 10 GB |  1 GB  | 333 MB | 166 MB | 111 MB |  69 MB |  42 MB |  28 MB |
 1 GB/s |  1 GB | 100 MB |  33 MB |  17 MB |  11 MB |   7 MB |   4 MB |   3 MB |
</pre>
        </div>

        <div>

            <p>I believe thinking about problems this way is useful. It can help you realize that some ideas aren't feasible. Hitting 240 Hz isn't easy. It won't happen by accident.</p>


            
            
            <p>The "Numbers Every Programmer Should Know" list is out of date. It needs to be refreshed for 2020.</p>
            
            <p>Here are a few numbers for my home PC. They're a blend of AIDA64, Sandra, and my benchmarks. These figures don't tell the whole story and are just a starting point.</p>

<pre>L1 Latency:    1 ns
L2 Latency:    2.5 ns
L3 Latency:    10 ns
RAM Latency:   50 ns

<b>(per core)</b>
L1 Bandwidth:  210 GB/s 
L2 Bandwidth:  80 GB/s
L3 Bandwidth:  60 GB/s

<b>(whole system)</b>
RAM Bandwidth: 45 GB/s 
</pre>

            <p>I think it would be cool if there was a small, simple, open source benchmark tool for this. A single C file that could run on desktop, server, mobile, consoles, etc. I'm not the right person to write that tool.</p>


            
            
            
            <p>Measuring this stuff is hard. Really hard. My code probably has bugs. There are many compounding factors not discussed. If you want to critize my benchmark methodology you probably aren't wrong.</p>

            <p>Ultimately I think that's ok. This post isn't about the precise performance of my home desktop. It's about framing problems from a certain perspective. And learning how to perfrom some basic napkin math.</p>


            
            
            <p>A co-worker shared a way of thinking about programming problems I hadn't considered. That sent me on a journey to explore modern memory performance.</p>
            
            <p>For the purpose of napkin math here's some ballpark figures for a modern desktop PC.</p>
            
<ul>
    <li>RAM Performance
        <ul>
            <li>Upper Limit: <code>45 GB/s</code></li>
            <li>Napkin Estimate: <code>5 GB/s</code></li>
            <li>Lower Limit: <code>1 GB/s</code></li>
        </ul>
    </li>
    <li>Cache Performance â€” L1/L2/L3 (per core)
        <ul>
            <li>Upper Limit (w/ simd): <code>210 GB/s</code> / <code>80 GB/s</code> / <code>60 GB/s</code></li>
            <li>Napkin Estimate: <code>25 GB/s</code> / <code>15 GB/s</code> / <code>9 GB/s</code></li>
            <li>Lower Limit: <code>13 GB/s</code> / <code>8 GB/s</code> / <code>3.5 GB/s</code></li>
        </ul>
    </li>
</ul>   
            <p>Napkin estimates come from observed performance with <code>matrix4x4</code>. RealCodeâ„¢ is never that simple. But for napkin math estimates I think this is a reasonable starting point. You will need to adjust based on the access patterns and characteristics of your hardware and your code.</p>

            <p>That said, the most important thing I learned is a new way to think about problems. Thinking about bytes-per-second or bytes-per-frame is another lens to look through. It's a useful tool to keep in my back pocket.</p>
            
            <p>Thanks for reading.</p>


            
            

            <p><a href="https://gist.github.com/forrestthewoods/4ee84b62c581956bebbe124cd1c26212">C++ Benchmark</a>
            <br><a href="https://gist.github.com/forrestthewoods/6bbf49cd778adb2ff5ef8fd85d2b3add">Python Plot</a>
            <br><a href="https://gist.github.com/forrestthewoods/85572ba1325f19b79faf3bc23c946f85">data.json</a>
            </p>


            
            
            <p>This post just scratches the surface. I probably won't write another post on this subject. But if I did it might cover some of the following:</p>

            <ul>
                <li>Write Performance</li>
                <li>False Sharing</li>
                <li><code>std::atomic</code> performance (or lack thereof)</li>
                <li>Performance counters</li>
                <li><a href="https://stackoverflow.com/questions/10446301/tlb-misses-vs-cache-misses">TLB Performance</a></li>
                <li><a href="https://en.wikipedia.org/wiki/MESIF_protocol">Cache Protocols</a></li>
            </ul>

            
            
            <p>All tests were performed on my home computer. All stock settings, no overclocking.</p>

            <ul>
                <li>OS: Windows 10 v1903 build 18362.</li>
                <li>CPU: Intel i7-8700k @ 3.70 Ghz</li>
                <li>RAM: 2x16 GSkill Ripjaw DDR4-3200 (16-18-18-38 @ 1600 MHz)</li>
                <li>Motherboard: Asus TUF Z370-Plus Gaming</li>
            </ul>

        </div>
    </div></div></div>
    </div>
    <footer>
        <div>created by buildstarted &copy; 2020 <a href="/about">about</a></div>
        <div>Share this page on social media: copy and paste this url https://linksfor.dev/</div>
        <div>If you prefer RSS: <a href="https://linksfor.dev/feed.xml">https://linksfor.dev/feed.xml</a></div>
		<div>Customer satisfaction guaranteed to be optional.</div>
    </footer>
    
    <script async defer>
        _dna = window._dna || {};
        _dna.siteId = "linksfor.devs";
        _dna.outlink = true;

        (function () {
            let dna = document.createElement('script');
            dna.type = 'text/javascript';
            dna.async = true;
            dna.src = '//dna.buildstarted.com/t.js';
            let s = document.getElementsByTagName('script')[0];
            s.parentNode.insertBefore(dna, s);
        })();
    </script>
    <noscript><img src="//dna.buildstarted.com/g?siteId=linksfor.devs"/></noscript>
</body>
</html>