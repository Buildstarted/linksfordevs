<!DOCTYPE html>
<html lang="en">
<head>
    <title>
Packaging CLI programs into Docker images to avoid dependency hell -
linksfor.dev(s)
    </title>
	<link rel="alternate" type="application/rss+xml" title="Linksfor.dev(s) feed" href="https://linksfor.dev/feed.rss" />
    <meta charset="utf-8">
    <meta name="Description" content="A curated source of links that devs might find interesting. Updated around the clock." />
    <meta name="google" value="notranslate">
    <link rel="stylesheet" type="text/css" href="/style.css" />
</head>
<body>
    <div class="grid">
        
<div class="readable">
    <h1>Packaging CLI programs into Docker images to avoid dependency hell</h1>
    <div class="post-content">
<p>In this post, I&apos;m not going to talk about ASP.NET Core for a change. Instead, I&apos;m going to show one way to package CLI tools and their dependencies as Docker images. With a simple helper script, this allows you to run a CLI tool without having to install the dependencies on your host machine. I&apos;ll show how to create a Docker image containing your favourite CLI tool, and a helper script for invoking it.</p>
<p>All the commands in this post describe using Linux containers. The same principal can be applied to Windows containers if you update the commands. However the benefits of isolating your environment come with the downside of large Docker image sizes.</p>
<blockquote>
<p>If you&apos;re looking for a Dockerised version of the AWS CLI specifically, I have an <a href="https://hub.docker.com/r/andrewlock/aws-cli">image on Docker hub</a> which is generated from <a href="https://github.com/andrewlock/aws-cli">this GitHub</a> repository.</p>
</blockquote> <p>For example, take the <a href="https://aws.amazon.com/cli/">AWS CLI</a>. The suggested way to install the CLI on Linux is to use Python and <a href="https://pip.pypa.io/en/stable/">pip</a> (Pip is the package installer for Python; the equivalent of NuGet for .NET). The recommended version to use is Python 3, but you may have other apps that require Python 2, at which point you&apos;re in a world of dependency hell. </p>
<p>Docker containers can completely remove this problem. By packaging all the dependencies of an application into a container (even the operating system) you isolate the apps from both your host machine, and other apps. Each container runs in its own little world, and can have completely different dependencies to every other and the host system.</p>
<p><img src="/content/images/2019/containers.png" alt="Diagram of two containers, isolated from he host OS"></p>
<p>This is obviously one of the big selling points of containers, and is part of the reason they&apos;re seeing such high adoption for production loads. But they can also help with our AWS CLI problem. Instead of installing the CLI on our host machine, we can install it in a Docker container instead, and execute our CLI commands there. </p> <p>So what does it actually take to package up a tool in a Docker container? That depends on the tool in question. Hopefully, the installation instructions include a set of commands for you to run. In most cases, if you&apos;re at all familiar with Docker you can take these commands and convert them into a Dockerfile.</p>
<p>For example, let&apos;s take the AWS CLI instructions. According to <a href="https://docs.aws.amazon.com/cli/latest/userguide/install-linux.html#install-linux-awscli">the installation instructions</a>, you need to have Python and <code>pip</code> installed, after which you can run </p>
<pre class="language-bash"><code class="language-bash">pip3 <span class="token function">install</span> awscli --upgrade --user
</code></pre>
<p>to install the CLI. </p>
<p>One of the main difficulties of packaging your app into a Docker container, is establishing all of the dependencies. Python and <code>pip</code> are clearly required, but depending on which operating system you use for your base image, you may find you need to install additional dependencies. </p>
<p><a href="https://alpinelinux.org/">Alpine Linux</a> is a common candidate for a base OS as it&apos;s tiny, which keeps your final Docker images as small as possible. However Alpine is kept small by not including much in the box. You may well find you need to add some extra dependencies for your target tool to work correctly.</p>
<p>The example Dockerfile below shows how to install the AWS CLI in an Alpine base image. It&apos;s taken from <a href="https://github.com/andrewlock/aws-cli/blob/master/Dockerfile">the aws-cli image</a> which is <a href="https://hub.docker.com/r/andrewlock/aws-cli">available on Docker Hub</a>:</p>
<pre class="language-docker"><code class="language-docker"><span class="token keyword">FROM</span> alpine<span class="token punctuation">:</span>3.6
<span class="token keyword">RUN</span> apk <span class="token punctuation">-</span>v <span class="token punctuation">-</span><span class="token punctuation">-</span>no<span class="token punctuation">-</span>cache add \ python \ py<span class="token punctuation">-</span>pip \ groff \ less \ mailcap \ &amp;&amp; \ pip install <span class="token punctuation">-</span><span class="token punctuation">-</span>upgrade awscli==1.16.206 s3cmd==2.0.2 python<span class="token punctuation">-</span>magic &amp;&amp; \ apk <span class="token punctuation">-</span>v <span class="token punctuation">-</span><span class="token punctuation">-</span>purge del py<span class="token punctuation">-</span>pip
<span class="token keyword">VOLUME</span> /root/.aws
<span class="token keyword">VOLUME</span> /project
<span class="token keyword">WORKDIR</span> /project
<span class="token keyword">ENTRYPOINT</span> <span class="token punctuation">[</span><span class="token string">&quot;aws&quot;</span><span class="token punctuation">]</span>
</code></pre>
<p>This base image uses Alpine 3.6, and starts by installing a bunch of prerequisites:</p>
<ul>
<li><code>python</code>: the Python (3) environment</li>
<li><code>py-pip</code>: the <code>pip</code> package installer we need to install the AWS CLI</li>
<li><code>groff</code>: used for formatting text</li>
<li><code>less</code>: used for controlling the amount of text displayed on a terminal</li>
<li><code>mailcap</code>: used for controlling how to display non-text</li>
</ul>
<p>Next, as part of the same <code>RUN</code> command (to keep the final Docker image as small as possible) we install the AWS CLI using <code>pip</code>. We also install <a href="https://s3tools.org/s3cmd">the tool <code>s3cmd</code></a> (which makes it easier to work with S3 data), and <code>python-magic</code> (which helps with mime-type detection).</p>
<p>As the last step of the <code>RUN</code> command, we uninstall the <code>py-pip</code> package. We only needed it to install the AWS CLI and other tools, and now it&apos;s just taking up space. Deleting (and purging) it helps keep the size of the final Docker image down.</p>
<p>The next two <code>VOLUME</code> commands define locations known by the Docker container when it runs on your machine. The <code>/root/.aws</code> path is where the AWS CLI will look for credential files. The <code>/project</code> path is where we set the working directory (using <code>WORKDIR</code>), so it&apos;s where the AWS CLI commands will be run. We&apos;ll bind that at runtime to wherever we want to run the AWS CLI, as you&apos;ll see shortly. </p>
<p>Finally we set the <code>ENTRYPOINT</code> for the container. This sets the command that will run when the container is executed. So running the Docker container will execute <code>aws</code>, the AWS CLI.</p>
<p>To build the image, run <code>docker build .</code> in the same directory as Dockerfile, and give it a tag:</p>
<pre class="language-bash"><code class="language-bash">docker build -t example/aws-cli <span class="token keyword">.</span>
</code></pre>
<p>You will now have a Docker image containing the AWS CLI. The next step is to use it!</p> <p>You can create a container from your tool image and run it in the most basic form using:</p>
<pre class="language-bash"><code class="language-bash">docker run --rm example/aws-cli
</code></pre>
<p>If you run this, Docker creates a container from your image, executes the <code>aws</code> command, and then exists. The <code>--rm</code> option means that the old container is removed afterwards, so it doesn&apos;t clutter up your drive. In this example, we didn&apos;t provide any command line arguments, so the AWS CLI shows the standard help text:</p>
<pre class="language-bash"><code class="language-bash"><span class="token operator">&gt;</span> docker run --rm example/aws-cli
usage: aws <span class="token punctuation">[</span>options<span class="token punctuation">]</span> <span class="token operator">&lt;</span>command<span class="token operator">&gt;</span> <span class="token operator">&lt;</span>subcommand<span class="token operator">&gt;</span> <span class="token punctuation">[</span><span class="token operator">&lt;</span>subcommand<span class="token operator">&gt;</span> <span class="token punctuation">..</span>.<span class="token punctuation">]</span> <span class="token punctuation">[</span>parameters<span class="token punctuation">]</span>
To see <span class="token function">help</span> text, you can run: aws <span class="token function">help</span> aws <span class="token operator">&lt;</span>command<span class="token operator">&gt;</span> <span class="token function">help</span> aws <span class="token operator">&lt;</span>command<span class="token operator">&gt;</span> <span class="token operator">&lt;</span>subcommand<span class="token operator">&gt;</span> <span class="token function">help</span>
aws: error: too few arguments
</code></pre>
<p>If you want to do something useful, you&apos;ll need to provide some arguments to the CLI. For example, lets try listing the available S3 buckets, by passing the arguments <code>s3 ls</code>:</p>
<pre class="language-bash"><code class="language-bash"><span class="token operator">&gt;</span> docker run --rm example/aws-cli s3 <span class="token function">ls</span>
Unable to <span class="token function">locate</span> credentials. You can configure credentials by running <span class="token string">&quot;aws configure&quot;</span><span class="token keyword">.</span>
</code></pre>
<p>This is where things start to get a bit more tricky. To call AWS, you need to provide credentials. There are a <a href="https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-files.html">variety of ways of doing this</a>, including using credentials files in your profile, or by setting environment variables. The easiest approach is to use environment variables, by exporting them in your host environment: </p>
<pre class="language-bash"><code class="language-bash"><span class="token function">export</span> AWS_ACCESS_KEY_ID<span class="token operator">=</span><span class="token string">&quot;&lt;id&gt;&quot;</span>
<span class="token function">export</span> AWS_SECRET_ACCESS_KEY<span class="token operator">=</span><span class="token string">&quot;&lt;key&gt;&quot;</span>
<span class="token function">export</span> AWS_SESSION_TOKEN<span class="token operator">=</span><span class="token string">&quot;&lt;token&gt;&quot;</span> <span class="token function">export</span> AWS_DEFAULT_REGION<span class="token operator">=</span><span class="token string">&quot;&lt;region&gt;&quot;</span>
</code></pre>
<p>And passing these to the <code>docker run</code> command:</p>
<pre class="language-bash"><code class="language-bash">docker run --rm \ -e AWS_ACCESS_KEY_ID \ -e AWS_SECRET_ACCESS_KEY \ -e AWS_DEFAULT_REGION \ -e AWS_SESSION_TOKEN \ example/aws-cli \ s3 <span class="token function">ls</span>
</code></pre>
<p>I split the command over multiple lines as it&apos;s starting to get a bit unwieldy. If you have your AWS credentials <a href="https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-files.html#cli-configure-files-where">stored in credentials files instead</a> in <code>$HOME/.aws</code> instead of environment variables, you can pass those to the container using:</p>
<pre class="language-bash"><code class="language-bash">docker run --rm \ -v <span class="token string">&quot;<span class="token variable">$HOME</span>/.aws:/root/.aws&quot;</span> \ example/aws-cli \ s3 <span class="token function">ls</span>
</code></pre>
<p>In these examples, we&apos;re just listing out our S3 buckets, so we&apos;re not interacting with the file system directly. But what if you want to copy a file from a bucket to your local file system? To achieve this, you need to bind your working directory to the <code>/project</code> volume inside the container. For example: </p>
<pre class="language-bash"><code class="language-bash">docker run --rm \ -v <span class="token string">&quot;<span class="token variable">$HOME</span>/.aws:/root/.aws&quot;</span> \ -v <span class="token variable">$PWD</span>:/project \ example/aws-cli \ s3 <span class="token function">cp</span> s3://mybucket/test.txt test2.txt
</code></pre>
<p>In this snippet we bind the current directory (<code>$PWD</code>) to the working directory in the container <code>/project</code>. When we use <code>s3 cp</code> to download the <em>test.txt</em> file, it&apos;s written to <code>/project/test2.txt</code> in the container, which in turn writes it to your current directory on the host.</p>
<p>By now you might be getting a bit fatigued - having to run such a long command every time you want to use the AWS CLI sucks. Luckily there&apos;s easy fixes by using a small script</p> <p>Having to pass all those environment variables and volume mounts is a pain. The simplest solution, is to create a basic script that includes all those defaults for you:</p>
<pre class="language-bash"><code class="language-bash"><span class="token shebang important">#!/bin/bash</span> docker run --rm \ -v <span class="token string">&quot;<span class="token variable">$HOME</span>/.aws:/root/.aws&quot;</span> \ -v <span class="token variable">$PWD</span>:/project \ example/aws-cli \ <span class="token string">&quot;<span class="token variable"><a href="/cdn-cgi/l/email-protection" class="__cf_email__">[email&#xA0;protected]</a></span>&quot;</span>
</code></pre>
<p>Note that this script is pretty much the same as the final example from the previous section. The difference is that we&apos;re using the arguments catch-all <code>&quot;<a href="/cdn-cgi/l/email-protection" class="__cf_email__">[email&#xA0;protected]</a>&quot;</code> at the end of the script, which means &quot;paste all of the arguments here as quoted string&quot;. </p>
<p>If you save this script as <em>aws.sh</em> in your home directory (and give it execute permissions by running <code>chmod +x ~/aws.sh</code>), then copying a file becomes almost identical to using the AWS CLI directly:</p>
<pre class="language-bash"><code class="language-bash">
aws.sh s3 <span class="token function">cp</span> s3://mybucket/test.txt test2.txt ~/aws.sh s3 <span class="token function">cp</span> s3://mybucket/test.txt test2.txt
</code></pre>
<p>Much nicer!</p>
<p>You could even go one step further and create an alias for <code>aws</code> to be the contents of the script:</p>
<pre class="language-bash"><code class="language-bash"><span class="token function">alias</span> aws<span class="token operator">=</span><span class="token string">&apos;docker run --rm -v &quot;<span class="token variable">$HOME</span>/.aws:/root/.aws&quot; -v <span class="token variable">$PWD</span>:/project example/aws-cli&apos;</span>
</code></pre>
<p>or alternatively, copy the file into your path:</p>
<pre class="language-bash"><code class="language-bash"><span class="token function">sudo</span> <span class="token function">cp</span> ~/aws.sh /usr/local/bin/aws
</code></pre>
<p>As ever with Linux, there&apos;s a whole host of extra things you could do. You could create different versions of the <em>aws.sh</em> script which is configured to use alternative credentials or regions. But using a Dockerised tool rather than installing the CLI directly on your host means you can also have scripts that use different <em>versions</em> of the CLI. All the while, you&apos;ve avoided polluting your host environment with dependencies!</p> <p>In this post, I showed how you can Dockerise your CLI tools to avoid having to install dependencies in your host environment. I showed how to pass environment variables and arguments to the Dockerised tool, and how to bind to your host&apos;s file system. Finally, I showed how you can use scripts to simplify executing your Docker images.</p>
<p>If you&apos;re looking for a Dockerised version of the AWS CLI specifically, I have an <a href="https://hub.docker.com/r/andrewlock/aws-cli">image on Docker hub</a> which is generated from <a href="https://github.com/andrewlock/aws-cli">this GitHub</a> repository (which is a <a href="https://github.com/mesosphere/aws-cli">fork of an original</a> which fell out of maintenance).</p>
</div>
</div>
    </div>
    <footer>
        <div>created by buildstarted &copy; 2019 <a href="/about">about</a></div>
        <div>Share this page on social media: copy and paste this url https://linksfor.dev/</div>
    </footer>
    
    <script>
        (function() {
            var COLLECT_URL = "https://dna.buildstarted.com/t";
            var SITE_ID = "linksfor.devs";
            var GLOBAL_VAR_NAME = "__DNA__";

            window[GLOBAL_VAR_NAME] = {};

            window[GLOBAL_VAR_NAME].sendPageView = function() {
                var path = location.pathname;
                var referrer = document.referrer;

                var url = COLLECT_URL + "?siteid=" + SITE_ID + "&p=" + encodeURIComponent(path) + "&r=" + encodeURIComponent(referrer);

                try { fetch(url, { method: "GET" }); } catch (e) {}
            };

            window[GLOBAL_VAR_NAME].sendPageView();
        })();
    </script>
</body>
</html>