<!DOCTYPE html>
<html lang="en">
<head>
    <title>
Announcing CUDA on Windows Subsystem for Linux 2 | NVIDIA Developer Blog - linksfor.dev(s)    </title>
    <meta charset="utf-8">
    <link rel="alternate" type="application/rss+xml" title="Linksfor.dev(s) feed" href="https://linksfor.dev/feed.rss" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta name="google" value="notranslate">
    <link rel="stylesheet" type="text/css" href="/style.min.css" />
    <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon">
    <link rel="icon" href="/favicon.ico" type="image/x-icon">
    <meta property="og:title" content="Announcing CUDA on Windows Subsystem for Linux 2 | NVIDIA Developer Blog - linksfor.dev(s)"/>
    <meta property="article:author" content="View all posts by Raphael Boissel"/>
    <meta property="og:description" content="Figure 1. Stack image showing layers involved while running Linux AI frameworks in WSL 2 containers. In response to popular demand, Microsoft announced a new feature of the Windows Subsystem for Linux&#x2026;"/>
    <meta property="og:type" content="website"/>
    <meta property="og:url" content="https://devblogs.nvidia.com/announcing-cuda-on-windows-subsystem-for-linux-2/"/>

<meta property="og:site_name" content="linksfor.dev(s)" />
</head>
<body>
    <div class="devring" style="background: #222">
        <div style="text-align:center">Explore other dev related sites in this ring. If you would like to join this ring <a href="https://devring.club">click here</a>.</div>
        <div class="grid">
            <div style="display: grid; grid-template-columns: .5fr 1fr 1fr 1fr; text-align: center;">
                <span class="devring-title"><a href="https://devring.club/">devring.club</a></span>
                <a href="https://devring.club/sites/1/prev" class="devring-previous">Previous</a>
                <a href="https://devring.club/random" class="devring-random">Random</a>
                <a href="https://devring.club/sites/1/next" class="devring-next">Next</a>
            </div>
        </div>
    </div>
    <div class="grid">
        <h1 style="margin: unset">
<a href="/" style="color:inherit">linksfor.dev(s)</a>
        </h1>
        <title>linksfor.dev(s) - Announcing CUDA on Windows Subsystem for Linux 2 | NVIDIA Developer Blog</title>
<div class="readable">
        <h1>Announcing CUDA on Windows Subsystem for Linux 2 | NVIDIA Developer Blog</h1>
            <div>by View all posts by Raphael Boissel</div>
            <div>Reading time: 19-24 minutes</div>
        <div>Posted here: 19 Jun 2020</div>
        <p><a href="https://devblogs.nvidia.com/announcing-cuda-on-windows-subsystem-for-linux-2/">https://devblogs.nvidia.com/announcing-cuda-on-windows-subsystem-for-linux-2/</a></p>
        <hr/>
<div id="readability-page-1" class="page"><div id="pf-post-view">
    <div>

                  <article id="post-18337">
                

                <div>
                  
<div><figure><img src="https://devblogs.nvidia.com/wp-content/uploads/2020/06/stack-layers-for-linux-ai-in-wsl-2-containers-1-625x829.png" alt="The diagram shows Microsoft Windows GPU machines running on the NVIDIA hardware. For the software layers, it shows the Windows kernel, NVIDIA Windows driver, GPU virtualization, WSL2 environment (Linux kernel), NVIDIA CUDA, and other Linux AI frameworks and apps." width="508" height="673" srcset="https://devblogs.nvidia.com/wp-content/uploads/2020/06/stack-layers-for-linux-ai-in-wsl-2-containers-1-625x829.png 625w, https://devblogs.nvidia.com/wp-content/uploads/2020/06/stack-layers-for-linux-ai-in-wsl-2-containers-1-226x300.png 226w, https://devblogs.nvidia.com/wp-content/uploads/2020/06/stack-layers-for-linux-ai-in-wsl-2-containers-1-87x115.png 87w, https://devblogs.nvidia.com/wp-content/uploads/2020/06/stack-layers-for-linux-ai-in-wsl-2-containers-1-768x1019.png 768w, https://devblogs.nvidia.com/wp-content/uploads/2020/06/stack-layers-for-linux-ai-in-wsl-2-containers-1-1158x1536.png 1158w, https://devblogs.nvidia.com/wp-content/uploads/2020/06/stack-layers-for-linux-ai-in-wsl-2-containers-1-1543x2048.png 1543w, https://devblogs.nvidia.com/wp-content/uploads/2020/06/stack-layers-for-linux-ai-in-wsl-2-containers-1-68x90.png 68w, https://devblogs.nvidia.com/wp-content/uploads/2020/06/stack-layers-for-linux-ai-in-wsl-2-containers-1-362x480.png 362w, https://devblogs.nvidia.com/wp-content/uploads/2020/06/stack-layers-for-linux-ai-in-wsl-2-containers-1-83x110.png 83w, https://devblogs.nvidia.com/wp-content/uploads/2020/06/stack-layers-for-linux-ai-in-wsl-2-containers-1-1024x1359.png 1024w, https://devblogs.nvidia.com/wp-content/uploads/2020/06/stack-layers-for-linux-ai-in-wsl-2-containers-1.png 1884w" sizes="(max-width: 508px) 100vw, 508px"><figcaption><em>Figure 1. Stack image showing layers involved while running Linux AI frameworks in WSL 2 containers.</em></figcaption></figure></div>



<p>In response to popular demand, Microsoft <a href="https://devblogs.microsoft.com/commandline/the-windows-subsystem-for-linux-build-2020-summary" data-wpel-link="external" target="_blank" rel="nofollow external noopener noreferrer">announced</a> a new feature of the <a href="https://docs.microsoft.com/en-us/windows/wsl" target="_blank" rel="noreferrer noopener nofollow external" data-wpel-link="external">Windows Subsystem for Linux 2</a> (WSL 2)—GPU acceleration—at the <strong>Build </strong>conference in May 2020. This feature opens the gate for many compute applications, professional tools, and workloads currently available only on Linux, but which can now run on Windows as-is and benefit from GPU acceleration.</p>



<p>Most importantly, <a href="https://developer.nvidia.com/cuda-zone" data-wpel-link="external" target="_blank" rel="nofollow external noopener noreferrer">NVIDIA CUDA</a> acceleration is now coming to WSL. In this post, we discuss what you can expect from CUDA in the <a href="https://developer.nvidia.com/cuda/wsl" target="_blank" rel="noreferrer noopener nofollow external" data-wpel-link="external">Public Preview for WSL 2</a>.</p>



<h2><strong>What is WSL?</strong></h2>



<p>WSL is a Windows 10 feature that enables you to run native Linux command-line tools directly on Windows, without requiring the complexity of a dual-boot environment. Internally, WSL is a containerized environment that is tightly integrated with the Microsoft Windows operating system. This allows it to run Linux applications alongside traditional Windows desktop and modern store apps.</p>



<p>WSL is primarily a tool for developers. If you are working on a compute workload inside Linux containers, you can develop and test the workload locally on your Windows PC using the same native Linux tools with which you are familiar. Typically, those applications required a lot of hacking, third-party frameworks, and libraries to get them to work on Windows systems. This all changes with WSL 2, which brings full Linux kernel support to the Windows world.</p>



<p>With the WSL 2 and GPU Paravirtualization (GPU-PV) technology, Microsoft is adding another spin to the Linux support on Windows by allowing you to run compute workloads targeting GPU hardware. Later in this post, we cover WSL 2 and how GPU is added there in more detail.</p>



<p>For more information, see <a rel="noreferrer noopener nofollow external" href="https://devblogs.microsoft.com/directx/directx-heart-linux/" target="_blank" data-wpel-link="external">DirectX is coming to the Windows Subsystem for Linux</a> and the <a rel="noreferrer noopener nofollow external" href="https://github.com/microsoft/WSL2-Linux-Kernel/tree/linux-msft-wsl-4.19.y/drivers/gpu" target="_blank" data-wpel-link="external">WSL2-Linux-Kernel/driver/gpu</a> directory in GitHub.</p>



<h2><strong>CUDA in WSL</strong></h2>



<p>To take advantage of the GPU in WSL 2, the target system must have a GPU driver installed that supports the Microsoft <a href="https://docs.microsoft.com/en-us/windows-hardware/drivers/display/windows-vista-display-driver-model-design-guide" data-wpel-link="external" target="_blank" rel="nofollow external noopener noreferrer">WDDM model</a>. These drivers are provided by GPU hardware vendors such as NVIDIA.</p>



<p>CUDA enables you to program NVIDIA GPUs. It has been supported in the WDDM model in Windows graphics for decades. The new Microsoft WSL 2 container delivers GPU acceleration, which CUDA can leverage to enable you to run CUDA workloads inside of WSL. For more information, see the <a rel="noreferrer noopener nofollow external" href="https://docs.nvidia.com/cuda/wsl-user-guide/index.html" target="_blank" data-wpel-link="external">CUDA on WSL User Guide</a>.</p>



<p>Support for CUDA in WSL is included with the NVIDIA display driver targeting the WDDM 2.9 model. You only have to install the drivers on the Windows host. The CUDA user mode driver in WSL (libcuda.so) is automatically mapped inside the container and added to the loader search path there.</p>



<p>The NVIDIA driver development team added support for the WDDM model and GPU-PV to the CUDA driver, to be able to run it on Linux on Windows. It is still a preview driver and will not be released until the official GPU support in WSL in Windows 10 is released. For more information about the release, see <a rel="noreferrer noopener nofollow external" href="https://developer.nvidia.com/cuda/wsl/download" target="_blank" data-wpel-link="external">CUDA WSL 2 Download</a>.</p>



<p>Figure 2 shows a simple diagram of how the CUDA driver is plugged into the new WDDM model within the Linux guest.</p>



<div><figure><img src="https://devblogs.nvidia.com/wp-content/uploads/2020/06/wddm-model-supporting-cuda-user-mode-linux-guest-625x352.png" alt="In the Linux guest, the CUDA user mode library talks to dxgkrnl driver's /dev/dxg device using IoCtls wrapped with libdxcore API. The dxg requests then get forwarded to the Windows host system using VMBus where for those the host dxgkrnl driver makes calls to the KMD (Kernel Mode Driver) DDI handlers." srcset="https://devblogs.nvidia.com/wp-content/uploads/2020/06/wddm-model-supporting-cuda-user-mode-linux-guest-625x352.png 625w, https://devblogs.nvidia.com/wp-content/uploads/2020/06/wddm-model-supporting-cuda-user-mode-linux-guest-300x169.png 300w, https://devblogs.nvidia.com/wp-content/uploads/2020/06/wddm-model-supporting-cuda-user-mode-linux-guest-179x101.png 179w, https://devblogs.nvidia.com/wp-content/uploads/2020/06/wddm-model-supporting-cuda-user-mode-linux-guest-768x432.png 768w, https://devblogs.nvidia.com/wp-content/uploads/2020/06/wddm-model-supporting-cuda-user-mode-linux-guest-500x281.png 500w, https://devblogs.nvidia.com/wp-content/uploads/2020/06/wddm-model-supporting-cuda-user-mode-linux-guest-160x90.png 160w, https://devblogs.nvidia.com/wp-content/uploads/2020/06/wddm-model-supporting-cuda-user-mode-linux-guest-362x204.png 362w, https://devblogs.nvidia.com/wp-content/uploads/2020/06/wddm-model-supporting-cuda-user-mode-linux-guest-196x110.png 196w, https://devblogs.nvidia.com/wp-content/uploads/2020/06/wddm-model-supporting-cuda-user-mode-linux-guest.png 800w" sizes="(max-width: 625px) 100vw, 625px"><figcaption><em>Figure 2. A diagram of the WDDM model supporting CUDA user mode driver running inside Linux guest.</em></figcaption></figure></div>



<p>If you are a developer who installed the WSL distro on the latest Windows build from the Microsoft <a href="https://insider.windows.com/en-us/getting-started/#register" data-wpel-link="external" target="_blank" rel="nofollow external noopener noreferrer">Windows Insider Program</a>’s <a href="https://insider.windows.com/en-us/how-to-pc/#about-rings" data-wpel-link="external" target="_blank" rel="nofollow external noopener noreferrer">Fast Ring</a> (Build 20149 or higher) and set the container to run in WSL 2 mode, and if you are an inspired owner of an NVIDIA GPU in your PC, you can try the driver and run your workloads in WSL 2. All you have to do is to install the driver on your Windows host OS and then open the WSL container. CUDA will be there and working with CUDA apps without any extra effort. Figure 3 shows the screenshot of running the CUDA TensorFlow workload in the WSL 2 container.</p>



<div><figure><img src="https://devblogs.nvidia.com/wp-content/uploads/2020/06/tensorflow-container-on-wsl-2-1.png" alt="The screenshot from the host system shows GPU node activities on the Performance tab of the Windows Task Manager tool while running a TensorFlow workload in the GPU-accelerated WSL 2 container. The picture includes the Task Manager window, WSL 2 container log, and Edge browser running the Jupyter notebook tutorial." width="795" height="463" srcset="https://devblogs.nvidia.com/wp-content/uploads/2020/06/tensorflow-container-on-wsl-2-1.png 1589w, https://devblogs.nvidia.com/wp-content/uploads/2020/06/tensorflow-container-on-wsl-2-1-300x175.png 300w, https://devblogs.nvidia.com/wp-content/uploads/2020/06/tensorflow-container-on-wsl-2-1-625x364.png 625w, https://devblogs.nvidia.com/wp-content/uploads/2020/06/tensorflow-container-on-wsl-2-1-179x104.png 179w, https://devblogs.nvidia.com/wp-content/uploads/2020/06/tensorflow-container-on-wsl-2-1-768x447.png 768w, https://devblogs.nvidia.com/wp-content/uploads/2020/06/tensorflow-container-on-wsl-2-1-1536x894.png 1536w, https://devblogs.nvidia.com/wp-content/uploads/2020/06/tensorflow-container-on-wsl-2-1-500x291.png 500w, https://devblogs.nvidia.com/wp-content/uploads/2020/06/tensorflow-container-on-wsl-2-1-155x90.png 155w, https://devblogs.nvidia.com/wp-content/uploads/2020/06/tensorflow-container-on-wsl-2-1-362x211.png 362w, https://devblogs.nvidia.com/wp-content/uploads/2020/06/tensorflow-container-on-wsl-2-1-189x110.png 189w, https://devblogs.nvidia.com/wp-content/uploads/2020/06/tensorflow-container-on-wsl-2-1-1024x596.png 1024w" sizes="(max-width: 795px) 100vw, 795px"><figcaption><em>Figure 3. TensorFlow container running inside the WSL 2.</em></figcaption></figure></div>



<p>GPU in WSL opens a gate for a variety of CUDA compute applications that currently only run in native Linux environments.</p>



<p>NVIDIA is still actively working on this project and making adjustments. Among other things, we are working on bringing APIs that used to be specific to Linux to the WDDM layer so that more and more applications can work on WSL out of the box.</p>



<p>Another point of focus is performance. As mentioned before, WSL 2 GPU support heavily leverages GPU-PV, which can affect small GPU workloads without any pipelining. Right now, we are reducing as much of this overhead as possible.</p>



<h3>NVML</h3>



<p>NVML is not included in the initial driver package and there are some concerns about this. To address it, we are planning to bring NVML to WSL, along with other libraries.</p>



<p>We started by bringing the core CUDA driver up to let you try most of your existing workloads in this early preview. We realize that some containers and applications leverage NVML to query GPU information even before loading CUDA. This is why we have put NVML on WSL among our top priorities. Stay tuned for more updates on this topic.</p>



<h3><strong>GPU containers in WSL</strong></h3>



<p>In addition to DirectX and CUDA support, NVIDIA is also adding support for the NVIDIA Container Toolkit (previously nvidia-docker2)&nbsp; within WSL 2. Containerized GPU workloads that data scientists prepared to run under Linux on on-premises hardware, or execute in the cloud, can now run as-is inside WSL 2 on Windows PCs.</p>



<p>There is no need for a specific WSL package. The NVIDIA runtime library (libnvidia-container) can dynamically detect libdxcore and use it when it is run in a WSL 2 environment with GPU acceleration. This happens automatically after Docker and NVIDIA Container Toolkit packages are installed, just like on Linux, allowing GPU-accelerated containers to run out of the box.</p>



<p>We do recommend getting the latest version of the Docker tools (19.03 or later) to take advantage of the added support for the <code>--gpus</code> option. To enable WSL 2 support, follow the <a href="https://github.com/NVIDIA/nvidia-docker" data-wpel-link="external" target="_blank" rel="nofollow external noopener noreferrer">README</a> steps on the GitHub repo for your Linux distribution and install the latest version available.</p>



<p>So how does it work? All the WSL 2 specific work is handled by the <a href="https://github.com/NVIDIA/libnvidia-container" data-wpel-link="external" target="_blank" rel="nofollow external noopener noreferrer">libnvidia-container</a> library. This library is now able to detect, at runtime, the presence of libdxcore.so and uses it to detect all the GPUs exposed to this interface.</p>



<p>If these GPUs need to be used in the container, the location of the driver store, a folder that contains all driver libraries for both the Windows host and WSL 2, is queried using libdxcore.so. It is up to libnvidia-container.so to set the container up so that the driver store is mapped correctly and does the setup for the core libraries supported by WSL 2 GPU, as shown in Figure 4.</p>



<div><figure><img src="https://devblogs.nvidia.com/wp-content/uploads/2020/06/discovery-mapping-scheme-used-by-libnvidiacontainer.so_-1-625x253.png" alt="The diagram shows that the DriverStore gets automatically mapped from the host system into the WSL 2 container. The libnvidia-container.so loads the CUDA library (libcuda.so.1.1) from that mapped DriverStore location within the container." srcset="https://devblogs.nvidia.com/wp-content/uploads/2020/06/discovery-mapping-scheme-used-by-libnvidiacontainer.so_-1-625x253.png 625w, https://devblogs.nvidia.com/wp-content/uploads/2020/06/discovery-mapping-scheme-used-by-libnvidiacontainer.so_-1-300x121.png 300w, https://devblogs.nvidia.com/wp-content/uploads/2020/06/discovery-mapping-scheme-used-by-libnvidiacontainer.so_-1-179x72.png 179w, https://devblogs.nvidia.com/wp-content/uploads/2020/06/discovery-mapping-scheme-used-by-libnvidiacontainer.so_-1-500x202.png 500w, https://devblogs.nvidia.com/wp-content/uploads/2020/06/discovery-mapping-scheme-used-by-libnvidiacontainer.so_-1-160x65.png 160w, https://devblogs.nvidia.com/wp-content/uploads/2020/06/discovery-mapping-scheme-used-by-libnvidiacontainer.so_-1-362x147.png 362w, https://devblogs.nvidia.com/wp-content/uploads/2020/06/discovery-mapping-scheme-used-by-libnvidiacontainer.so_-1-272x110.png 272w, https://devblogs.nvidia.com/wp-content/uploads/2020/06/discovery-mapping-scheme-used-by-libnvidiacontainer.so_-1.png 709w" sizes="(max-width: 625px) 100vw, 625px"><figcaption><em>Figure 4. Discovery and mapping scheme used by libnvidia-container.so on WSL 2.</em></figcaption></figure></div>



<p>Also, this deviates from the logic used outside of WSL. This is completely abstracted by libnvidia-container.so and should be as transparent as possible for the end user. One of the limitations of this early version is the lack of GPU selection in a multi-GPU environment: all GPUs are always visible in the container.</p>



<p>Here’s what you can run within the WSL container: any NVIDIA Linux container with which you are currently familiar. NVIDIA supports most existing Linux tools and workflows used by professionals. Download a favorite container workload from <a href="https://ngc.nvidia.com/" data-wpel-link="external" target="_blank" rel="nofollow external noopener noreferrer">NVIDIA NGC</a> and give it a try.</p>



<p>In the next section, we describe how to run TensorFlow and <em>n</em>-body containers within WSL 2 with the workloads accelerated by NVIDIA GPUs.</p>



<h4><strong>Running the N-body container</strong></h4>



<p>Install Docker using the Docker installation script:</p>



<pre><code>user@PCName:/mnt/c$ curl https://get.docker.com | sh</code></pre>



<p>Install the NVIDIA Container Toolkit. WSL 2 support is available starting with nvidia-docker2 v2.3 and the underlying runtime library libnvidia-container 1.2.0-rc.1.</p>



<p>Set up the <code>stable</code> and <code>experimental</code> repositories and the GPG key. The changes to the runtime to support WSL 2 are available in the experimental repository.</p>



<pre><code>user@PCName:/mnt/c$ distribution=$(. /etc/os-release;echo $ID$VERSION_ID)

user@PCName:/mnt/c$ curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add -

user@PCName:/mnt/c$ curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | sudo tee /etc/apt/sources.list.d/nvidia-docker.list

user@PCName:/mnt/c$ curl -s -L https://nvidia.github.io/libnvidia-container/experimental/$distribution/libnvidia-container-experimental.list | sudo tee /etc/apt/sources.list.d/libnvidia-container-experimental.list</code></pre>



<p>Install the NVIDIA runtime packages and their dependencies:</p>



<pre><code>user@PCName:/mnt/c$ sudo apt-get update
user@PCName:/mnt/c$ sudo apt-get install -y nvidia-docker2</code></pre>



<p>Open the WSL container and start the Docker daemon there. You should see the dockerd service output.</p>



<pre><code>user@PCName:/mnt/c$ sudo dockerd</code></pre>



<div><figure><img src="https://devblogs.nvidia.com/wp-content/uploads/2020/06/sudo-dockerd-625x297.png" alt="Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6750 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1070, pci bus id: 0000:01:00.0, compute capability: 6.1)" srcset="https://devblogs.nvidia.com/wp-content/uploads/2020/06/sudo-dockerd-625x297.png 625w, https://devblogs.nvidia.com/wp-content/uploads/2020/06/sudo-dockerd-300x143.png 300w, https://devblogs.nvidia.com/wp-content/uploads/2020/06/sudo-dockerd-179x85.png 179w, https://devblogs.nvidia.com/wp-content/uploads/2020/06/sudo-dockerd-500x238.png 500w, https://devblogs.nvidia.com/wp-content/uploads/2020/06/sudo-dockerd-160x76.png 160w, https://devblogs.nvidia.com/wp-content/uploads/2020/06/sudo-dockerd-362x172.png 362w, https://devblogs.nvidia.com/wp-content/uploads/2020/06/sudo-dockerd-231x110.png 231w, https://devblogs.nvidia.com/wp-content/uploads/2020/06/sudo-dockerd.png 715w" sizes="(max-width: 625px) 100vw, 625px"><figcaption><em>Figure 5. Starting the Docker daemon.</em></figcaption></figure></div>



<p>In another WSL container window, download and start the <em>N</em>-body simulation container. Make sure that the user has sufficient permissions to download the container. You may need to run the following command in sudo. GPU is highlighted in the output.</p>



<pre><code>user@PCName:/mnt/c$ docker run --gpus all nvcr.io/nvidia/k8s/cuda-sample:nbody nbody -gpu -benchmark</code></pre>



<div><figure><img src="https://devblogs.nvidia.com/wp-content/uploads/2020/06/docker-run-625x578.png" alt="Results of docker run command" srcset="https://devblogs.nvidia.com/wp-content/uploads/2020/06/docker-run-625x578.png 625w, https://devblogs.nvidia.com/wp-content/uploads/2020/06/docker-run-300x277.png 300w, https://devblogs.nvidia.com/wp-content/uploads/2020/06/docker-run-124x115.png 124w, https://devblogs.nvidia.com/wp-content/uploads/2020/06/docker-run-325x300.png 325w, https://devblogs.nvidia.com/wp-content/uploads/2020/06/docker-run-97x90.png 97w, https://devblogs.nvidia.com/wp-content/uploads/2020/06/docker-run-362x335.png 362w, https://devblogs.nvidia.com/wp-content/uploads/2020/06/docker-run-119x110.png 119w, https://devblogs.nvidia.com/wp-content/uploads/2020/06/docker-run.png 712w" sizes="(max-width: 625px) 100vw, 625px"><figcaption><em>Figure 6. Starting the <em>N</em>-body simulation container.</em></figcaption></figure></div>



<h4><strong>Running the TensorFlow container</strong></h4>



<p>Try another popular container: TensorFlow in Docker in WSL 2.</p>



<p>Download a TensorFlow Docker image. To avoid Docker connection issues, the command is run in sudo.</p>



<pre><code>user@PCName:/mnt/c$ docker pull tensorflow/tensorflow:latest-gpu-py3</code></pre>



<p>Save a slightly modified version of <a href="https://learningtensorflow.com/using-gpu/" data-wpel-link="external" target="_blank" rel="nofollow external noopener noreferrer">Lesson 15 – Using GPU</a> from TensorFlow Tutorials on your host’s drive C, which is mapped in the WSL 2 container as /mnt/c by default.</p>



<pre><code>user@PCName:/mnt/c$ vi ./matmul.py
import sys
import numpy as np
import tensorflow as tf
from datetime import datetime

device_name = sys.argv[1]  # Choose device from cmd line. Options: gpu or cpu
shape = (int(sys.argv[2]), int(sys.argv[2]))
if device_name == "gpu":
    device_name = "/gpu:0"
else:
    device_name = "/cpu:0"

tf.compat.v1.disable_eager_execution()
with tf.device(device_name):
    random_matrix = tf.random.uniform(shape=shape, minval=0, maxval=1)
    dot_operation = tf.matmul(random_matrix, tf.transpose(random_matrix))
    sum_operation = tf.reduce_sum(dot_operation)


startTime = datetime.now()
with tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(log_device_placement=True)) as session:
        result = session.run(sum_operation)
        print(result)

# Print the results
print("Shape:", shape, "Device:", device_name)
print("Time taken:", datetime.now() - startTime)</code></pre>



<p>The results of running this script, launched from the mounted drive C, on a GPU and a CPU are shown below. For simplicity, the output is reduced.</p>



<pre><code>user@PCName:/mnt/c$ docker run --runtime=nvidia --rm -ti -v "${PWD}:/mnt/c" tensorflow/tensorflow:latest-gpu-jupyter python /mnt/c/matmul.py gpu 20000</code></pre>



<div><figure><img src="https://devblogs.nvidia.com/wp-content/uploads/2020/06/docker-run-matmul-py-625x648.png" alt="Results of running docker run --runtime=nvidia --rm -ti -v &quot;${PWD}:/mnt/c&quot; tensorflow/tensorflow:latest-gpu-jupyter python /mnt/c/matmul.py gpu 20000" srcset="https://devblogs.nvidia.com/wp-content/uploads/2020/06/docker-run-matmul-py-625x648.png 625w, https://devblogs.nvidia.com/wp-content/uploads/2020/06/docker-run-matmul-py-289x300.png 289w, https://devblogs.nvidia.com/wp-content/uploads/2020/06/docker-run-matmul-py-111x115.png 111w, https://devblogs.nvidia.com/wp-content/uploads/2020/06/docker-run-matmul-py-87x90.png 87w, https://devblogs.nvidia.com/wp-content/uploads/2020/06/docker-run-matmul-py-32x32.png 32w, https://devblogs.nvidia.com/wp-content/uploads/2020/06/docker-run-matmul-py-362x375.png 362w, https://devblogs.nvidia.com/wp-content/uploads/2020/06/docker-run-matmul-py-106x110.png 106w, https://devblogs.nvidia.com/wp-content/uploads/2020/06/docker-run-matmul-py.png 716w" sizes="(max-width: 625px) 100vw, 625px"><figcaption><em>Figure 7. Running the matmul.py script.</em></figcaption></figure></div>



<p>There is a significant speedup when GPU is used within the WSL 2 container for the earlier compute scenario.</p>



<p>Here’s another demo to see the work accelerated by the GPU: the Jupyter notebook tutorial. When the container is launched, you should see the link to the notebook server printed.</p>



<pre><code>user@PCName:/mnt/c$ docker run -it --gpus all -p 8888:8888 tensorflow/tensorflow:latest-gpu-py3-jupyter                                                      </code></pre>



<div><figure><img src="https://devblogs.nvidia.com/wp-content/uploads/2020/06/docker-run-jupyter-625x658.png" alt="Results of running docker run -it --gpus all -p 8888:8888 tensorflow/tensorflow:latest-gpu-py3-jupyter" srcset="https://devblogs.nvidia.com/wp-content/uploads/2020/06/docker-run-jupyter-625x658.png 625w, https://devblogs.nvidia.com/wp-content/uploads/2020/06/docker-run-jupyter-285x300.png 285w, https://devblogs.nvidia.com/wp-content/uploads/2020/06/docker-run-jupyter-109x115.png 109w, https://devblogs.nvidia.com/wp-content/uploads/2020/06/docker-run-jupyter-86x90.png 86w, https://devblogs.nvidia.com/wp-content/uploads/2020/06/docker-run-jupyter-362x381.png 362w, https://devblogs.nvidia.com/wp-content/uploads/2020/06/docker-run-jupyter-105x110.png 105w, https://devblogs.nvidia.com/wp-content/uploads/2020/06/docker-run-jupyter.png 727w" sizes="(max-width: 625px) 100vw, 625px"><figcaption><em>Figure 8. Launching the Jupyter notebook.</em></figcaption></figure></div>



<p>Now you should be able to run the demo samples on the Jupyter notebook. Be careful to use localhost from the Microsoft Edge browser instead of 127.0.0.1 when connecting to the notebook.</p>



<p>Navigate to tensorflow-tutorials and run the classification.ipynb notebook.</p>



<p>To see the work accelerated by the GPU of your Windows PC, navigate to the <strong>Cell</strong><em> </em>menu, choose <strong>Run All</strong>, and check the log in the Jupyter notebook’s WSL 2 container.</p>



<div><figure><img src="https://devblogs.nvidia.com/wp-content/uploads/2020/06/jupyter-log-results-625x569.png" alt="Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6750 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1070, pci bus id: 0000:01:00.0, compute capability: 6.1)" srcset="https://devblogs.nvidia.com/wp-content/uploads/2020/06/jupyter-log-results-625x569.png 625w, https://devblogs.nvidia.com/wp-content/uploads/2020/06/jupyter-log-results-300x273.png 300w, https://devblogs.nvidia.com/wp-content/uploads/2020/06/jupyter-log-results-126x115.png 126w, https://devblogs.nvidia.com/wp-content/uploads/2020/06/jupyter-log-results-330x300.png 330w, https://devblogs.nvidia.com/wp-content/uploads/2020/06/jupyter-log-results-99x90.png 99w, https://devblogs.nvidia.com/wp-content/uploads/2020/06/jupyter-log-results-362x329.png 362w, https://devblogs.nvidia.com/wp-content/uploads/2020/06/jupyter-log-results-121x110.png 121w, https://devblogs.nvidia.com/wp-content/uploads/2020/06/jupyter-log-results.png 722w" sizes="(max-width: 625px) 100vw, 625px"><figcaption><em>Figure 9. The Jupyter notebook log.</em></figcaption></figure></div>



<p>This demo and some other demos in this container highlight the current overhead issues with the virtualization layer on small submissions, also mentioned earlier. The submissions associated with these toy models results in a GPU runtime shorter than the synchronization overhead itself. In these extreme small model cases on WSL 2, the CPU time might be better than the GPU time. This is currently being optimized and should be limited to a small, non-pipelined workload.</p>



<h2>WSL overview</h2>



<p>To understand how GPU is added to WSL 2, we now discuss what Linux on Windows is and how the hardware is exposed to the container.</p>



<p>Microsoft introduced WSL at the <strong>Build</strong> conference in 2016. It quickly gained momentum and became a popular tool among Linux developers who wanted to run Windows applications like Office alongside Linux development tools and target workloads.</p>



<p>WSL 1 allowed the running of unmodified Linux binaries. However, it still used a Linux kernel emulation layer, which was implemented as a subsystem within the NT kernel. This subsystem handled the calls from Linux applications by forwarding them to corresponding Windows 10 functionality.</p>



<p>WSL 1 was a useful tool but it was not compatible with all Linux applications as it required the emulation of potentially every Linux syscall. In general, the filesystem accesses were also slow, which resulted in unacceptable performance for some real-world applications.</p>



<p>With that in mind, Microsoft decided to go another route and introduced WSL 2, a new version of the WSL. The WSL 2 container runs full Linux distribution in a virtualized environment while still leveraging the full benefits of the Windows 10 new container system. </p>



<p>While it uses the Hyper-V services of Windows 10, WSL 2 is still not a traditional VM but rather a lightweight utility VM. That utility manages virtual address–backed memory, allowing the WSL 2 container to dynamically allocate memory from the host Windows system.</p>



<p>Some of the main goals for WSL 2 were to increase file system performance and to support full system call compatibility. It also has a better overall Windows host system integration. It allows shortcuts from the Windows shell into the Linux system running inside the container as well as the access to the host file system automatically mounted to selected directories of the container file system. </p>



<p>WSL 2 was enabled as a preview feature of the Windows Insider Program and has been released as the most recent Windows 10 update, version 2004.</p>



<p>There are more improvements made to WSL 2 containers in the latest Windows version, from network stacks to underlying storage VHD. Describing all the details would go beyond the scope of this post. For more information about some of those new interesting and exciting features of WSL 2 containers, see <a href="https://docs.microsoft.com/en-us/windows/wsl/compare-versions" data-wpel-link="external" target="_blank" rel="nofollow external noopener noreferrer">Comparing WSL 2 and WSL 1</a>.</p>



<h3><strong>WSL 2 Linux kernel</strong></h3>



<p>The Linux kernel in WSL 2 is built by Microsoft from the latest stable branch, based on the source available at kernel.org. This kernel has been specially tuned for WSL 2, optimized for size and performance to provide a Linux experience on Windows. The kernel is serviced by Windows Update, which means you get the latest security fixes and kernel improvements without needing to manage it yourselves.</p>



<p>Microsoft supports a few distros of Linux in WSL. Following the rules of the open source community, the WSL 2 kernel source code with required modifications to allow system integration with the Windows 10 host is public and available on the <a href="https://github.com/microsoft/WSL2-Linux-Kernel" data-wpel-link="external" target="_blank" rel="nofollow external noopener noreferrer">WSL2-Linux-Kernel</a> GitHub repo.</p>



<h3><strong>GPU in WSL</strong></h3>



<p>Microsoft developers are bringing real GPU hardware support into the WSL 2 containers through the GPU-PV technology, where the OS graphics kernel (dxgkrnl) marshals calls from user-mode components running inside the guest VM to the kernel mode driver that lives on the host.</p>



<p>Microsoft developed this technology as a feature of their WDDM graphics driver model several Windows releases ago with the help of independent hardware vendors (IHVs) . The NVIDIA graphics driver has supported GPU-PV since the early days of the feature preview in Windows Insider Program for the Windows OS. All currently supported NVIDIA GPUs can be exposed to the Windows OS running inside a Hyper-V VM guest.</p>



<p>For WSL 2 to be able to leverage the power of GPU-PV, Microsoft had to implement the foundation of their graphics framework within the Linux guest: the WDDM model with the GPU-PV protocol. The new Microsoft driver sits behind support for the WDDM model on Linux, called dxgkrnl. It is also available as a source code project in the <a href="https://github.com/microsoft/WSL2-Linux-Kernel/tree/linux-msft-wsl-4.19.y/drivers/gpu" data-wpel-link="external" target="_blank" rel="nofollow external noopener noreferrer">WSL2-Linux-Kernel</a> GitHub repo.</p>



<p>The dxgkrnl driver is expected to bring support for GPU acceleration to WSL 2 containers with the WDDM 2.9 version. Microsoft explains that dxgkrnl is a Linux GPU driver based on the GPU-PV protocol and doesn’t share anything else in common with its similarly named Windows counterpart.</p>



<p>For the time being, you can download the <a rel="noreferrer noopener nofollow external" href="https://developer.nvidia.com/cuda/wsl/download" target="_blank" data-wpel-link="external">preview version of NVIDIA WDDM 2.9 driver</a>. Over the next few months, the NVIDIA WDDM 2.9 driver will be distributed from the WIP version of Windows Update, which makes manually downloading and installing the driver unnecessary.</p>



<h3><strong>GPU-PV in a nutshell</strong></h3>



<p>The dxgkrnl driver exposes the new <strong>/dev/dxg</strong> device to user mode in Linux guest. The D3DKMT kernel service layer, which has been available on Windows, is also being ported to Linux as a part of the dxcore library. It communicates with the dxgkrnl using a set of private IOCTL calls.</p>



<p>The guest Linux version of dxgkrnl connects to the dxg kernel on the Windows host using multiple VM bus channels. The dxg kernel on the host treats Linux process submissions the same way as the process submissions from native Windows apps running in the WDDM model. It sends them to KMD (an IHV-specific kernel mode driver), which prepares them and submits them to the hardware GPU. Figure 10 shows a simplified diagram of such a communication channel.</p>



<div><figure><img src="https://devblogs.nvidia.com/wp-content/uploads/2020/06/windows-host-components-backing-dxg-device-1.png" alt="In the Linux guest, the dxgkrnl driver creates the /dev/dxg device for user mode components to access. The requests that come from GPU applications get forwarded to the Windows host system via VMBus where for those the host dxgkrnl driver makes calls to the KMD (Kernel Mode Driver) DDI handlers." width="624" height="351" srcset="https://devblogs.nvidia.com/wp-content/uploads/2020/06/windows-host-components-backing-dxg-device-1.png 624w, https://devblogs.nvidia.com/wp-content/uploads/2020/06/windows-host-components-backing-dxg-device-1-300x169.png 300w, https://devblogs.nvidia.com/wp-content/uploads/2020/06/windows-host-components-backing-dxg-device-1-179x101.png 179w, https://devblogs.nvidia.com/wp-content/uploads/2020/06/windows-host-components-backing-dxg-device-1-500x281.png 500w, https://devblogs.nvidia.com/wp-content/uploads/2020/06/windows-host-components-backing-dxg-device-1-160x90.png 160w, https://devblogs.nvidia.com/wp-content/uploads/2020/06/windows-host-components-backing-dxg-device-1-362x204.png 362w, https://devblogs.nvidia.com/wp-content/uploads/2020/06/windows-host-components-backing-dxg-device-1-196x110.png 196w" sizes="(max-width: 624px) 100vw, 624px"><figcaption><em>Figure 10.</em> <em>A simplified diagram showing Windows host components backing the new graphics dxg device in Linux guest.</em></figcaption></figure></div>



<p>NVIDIA drivers have supported Windows 10 GPU-PV with Windows guests for many releases. NVIDIA GPUs can be used to accelerate compute and graphics within all end-user Windows 10 applications that use the Microsoft virtualization layer and add vGPU using the GPU-PV feature:</p>



<ul><li><a href="https://docs.microsoft.com/en-us/windows/security/threat-protection/windows-sandbox/windows-sandbox-overview" data-wpel-link="external" target="_blank" rel="nofollow external noopener noreferrer">Windows Sandbox</a></li><li><a href="https://docs.microsoft.com/en-us/deployedge/microsoft-edge-security-windows-defender-application-guard" data-wpel-link="external" target="_blank" rel="nofollow external noopener noreferrer">Microsoft Defender Application Guard</a></li><li><a href="https://docs.microsoft.com/en-us/windows/mixed-reality/install-the-tools" data-wpel-link="external" target="_blank" rel="nofollow external noopener noreferrer">Microsoft HoloLens 2 emulator</a></li></ul>



<p>Figure 11 shows an example of running a sample DirectX app within the Windows Sandbox container on a NVIDIA GeForce GTX 1070 GPU.</p>



<div><figure><img src="https://devblogs.nvidia.com/wp-content/uploads/2020/06/windows-sandbox-container-gpu-acceleration-1.png" alt="The picture shows two instances of the Edge browser running inside a GPU-accelerated Hyper-V VM of the Windows Sandbox app as well as an instance of the sample DirectX app (ClassicD3D). Real GPU used for the acceleration is highlighted in the running virtualized app outputs. " srcset="https://devblogs.nvidia.com/wp-content/uploads/2020/06/windows-sandbox-container-gpu-acceleration-1.png 624w, https://devblogs.nvidia.com/wp-content/uploads/2020/06/windows-sandbox-container-gpu-acceleration-1-300x229.png 300w, https://devblogs.nvidia.com/wp-content/uploads/2020/06/windows-sandbox-container-gpu-acceleration-1-150x115.png 150w, https://devblogs.nvidia.com/wp-content/uploads/2020/06/windows-sandbox-container-gpu-acceleration-1-392x300.png 392w, https://devblogs.nvidia.com/wp-content/uploads/2020/06/windows-sandbox-container-gpu-acceleration-1-118x90.png 118w, https://devblogs.nvidia.com/wp-content/uploads/2020/06/windows-sandbox-container-gpu-acceleration-1-362x277.png 362w, https://devblogs.nvidia.com/wp-content/uploads/2020/06/windows-sandbox-container-gpu-acceleration-1-144x110.png 144w" sizes="(max-width: 624px) 100vw, 624px"><figcaption><em>Figure 11. Windows Sandbox container gets GPU acceleration on an NVIDIA GeForce GTX 1070 GPU.</em></figcaption></figure></div>



<h3><strong>User mode support</strong></h3>



<p>To enable graphics in WSL, the Windows graphics team also ported a user mode component to Linux: dxcore.</p>



<p>The dxcore library provides an API function to enumerate the graphics adapters in the system that are WDDM-compliant. It is intended to be the cross-platform, low-level replacement for the DXGI adapter enumeration both in Windows and Linux. It also abstracts access to the dxgkrnl services (IOCTLs on Linux and GDI calls on Windows) with the D3DKMT layer API, which is used by CUDA and other user mode components that rely on WDDM model support in WSL.</p>



<p>According to Microsoft, the dxcore (libdxcore.so) library will be available on both Windows and Linux. NVIDIA plans to add support for DirectX 12 and CUDA APIs to the driver, targeting the new WSL feature of the WDDM 2.9 model. Both API libraries will link to the dxcore so that they can instruct the dxg kernel to marshal their requests to the KMD on the host.</p>



<h2><strong>Try it today</strong></h2>



<p>If you want to use your Windows PC to do real ML and AI development from the comfort of the Linux environment, support for CUDA in WSL brings you an exciting opportunity. WSL is where Docker CUDA containers have proved to be one of the most popular compute environments among data scientists.</p>



<ul><li>Join the Microsoft <a href="https://insider.windows.com/en-us/" data-wpel-link="external" target="_blank" rel="nofollow external noopener noreferrer">Windows Insider program</a> to get access to the WSL 2 preview with GPU acceleration enabled. </li><li>Download the <a rel="noreferrer noopener nofollow external" href="https://developer.nvidia.com/cuda/wsl/download" target="_blank" data-wpel-link="external">latest NVIDIA driver</a>, install it, and try running CUDA-containerized workloads within WSL 2.</li></ul>



<p>Learn more about <a rel="noreferrer noopener nofollow external" href="https://developer.nvidia.com/cuda/wsl" target="_blank" data-wpel-link="external">CUDA on WSL</a>, and share your comments, feedback, and ideas on our community <a rel="noreferrer noopener nofollow external" href="https://forums.developer.nvidia.com/c/accelerated-computing/cuda/cuda-on-windows-subsystem-for-linux/303" target="_blank" data-wpel-link="external">forum</a>.</p>

                    

                    
                </div>
            </article>
        
    </div>
</div></div></div>
    </div>
    <footer>
        <div>created by <a href="https://buildstarted.com">buildstarted</a> &copy; 2020 <a href="/about">about</a></div>
        <div>Share this page on social media: copy and paste this url https://linksfor.dev/</div>
        <div>If you prefer RSS: <a href="https://linksfor.dev/feed.xml">https://linksfor.dev/feed.xml</a></div>
        <div>Customer satisfaction guaranteed to be optional.</div>
    </footer>
    
    <script async defer>
        _dna = window._dna || {};
        _dna.siteId = "linksfor.devs";
        _dna.outlink = true;

        (function() {
            let dna = document.createElement('script');
            dna.type = 'text/javascript';
            dna.async = true;
            dna.src = '//dna.buildstarted.com/t.js';
            let s = document.getElementsByTagName('script')[0];
            s.parentNode.insertBefore(dna, s);
        })();
    </script>
    <noscript><img src="//dna.buildstarted.com/g?siteId=linksfor.devs"/></noscript>
</body>
</html>