<!DOCTYPE html>
<html lang="en">
<head>
    <title>
Materials for Masses: SVBRDF Acquisition with a Single Mobile Phone Image - linksfor.dev(s)    </title>
    <meta charset="utf-8">
    <link rel="alternate" type="application/rss+xml" title="Linksfor.dev(s) feed" href="https://linksfor.dev/feed.rss" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta name="google" value="notranslate">
    <link rel="stylesheet" type="text/css" href="/style.css" />
    <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon">
    <link rel="icon" href="/favicon.ico" type="image/x-icon">
    <meta property="og:title" content="Materials for Masses: SVBRDF Acquisition with a Single Mobile Phone Image - linksfor.dev(s)"/>
    <meta property="article:author" content="NeuroHive"/>
    <meta property="og:description" content="Wide variety of images around us are the outcome of interactions between lighting, shapes and materials. In recent years, the advent of&#x2026;"/>
    <meta property="og:type" content="website"/>
    <meta property="og:url" content="https://medium.com/neurohive-computer-vision/materials-for-masses-svbrdf-acquisition-with-a-single-mobile-phone-image-a7ad8bbadb32"/>

<meta property="og:site_name" content="linksfor.dev(s)" />
</head>
<body>
    <div class="grid">
        <h1>
                <span style="cursor: default" title="linksfor.dev(s) has been running for 1 year! :partypopper:">ðŸŽ‰</span>
<a href="/" style="color:inherit">linksfor.dev(s)</a>
        </h1>
        <title>linksfor.dev(s) - Materials for Masses: SVBRDF Acquisition with a Single Mobile Phone Image</title>
<div class="readable">
        <h1>Materials for Masses: SVBRDF Acquisition with a Single Mobile Phone Image</h1>
            <div>by NeuroHive</div>
            <div>Watching time: 8-10 minutes</div>
        <div>Posted here: 27 Feb 2019</div>
        <p><a href="https://medium.com/neurohive-computer-vision/materials-for-masses-svbrdf-acquisition-with-a-single-mobile-phone-image-a7ad8bbadb32">https://medium.com/neurohive-computer-vision/materials-for-masses-svbrdf-acquisition-with-a-single-mobile-phone-image-a7ad8bbadb32</a></p>
        <hr/>
<div id="readability-page-1" class="page"><div><section><div><div><div><div><div><div><p><a rel="noopener" href="https://medium.com/@neurohive?source=post_page-----a7ad8bbadb32----------------------"><img alt="NeuroHive" src="https://miro.medium.com/fit/c/96/96/1*QkwHhMJFcpYGM17yLpDIaw.png" width="48" height="48"></a></p></div></div></div></div></div></div><div><div><div><figure><div><div><div><div><p><img src="https://miro.medium.com/max/60/1*xe9xHXTVxGA8BAY9RUegzA.png?q=20" width="2050" height="1192" role="presentation"></p><p><img width="2050" height="1192" role="presentation" src="https://miro.medium.com/max/2050/1*xe9xHXTVxGA8BAY9RUegzA.png"></p></div></div></div></div><figcaption data-selectable-paragraph=""><em>Figure 1: Example of Material Types</em></figcaption></figure></div></div></div><div><div><p id="80df" data-selectable-paragraph="">Wide variety of images around us are the outcome of interactions between lighting, shapes and materials. In recent years, the advent of convolutional neural networks (CNNs) has led to significant advances in recovering shape using just a single image. One of the problems which didnâ€™t get much attention is <strong>material estimation</strong> which has not seen as much progress, which might be attributed to multiple causes. First, material properties can be more complex. Even discounting more complex global illumination effects, materials are represented by a <strong>spatially-varying bidirectional reflectance distribution function (SVBRDF)</strong>, which is an unknown high-dimensional function that depends on incident lighting directions. Secondly, pixel observations in a single image contain entangled information from factors such as shape and lighting, besides material, which makes estimation ill-posed.</p><p id="5c7f" data-selectable-paragraph="">The researchers from <a href="https://research.adobe.com/" target="_blank" rel="noopener nofollow">Adobe</a> developed a state-of-the-art technique to recover SVBRDF from a single image of a near-planer surface, acquired using the camera of the mobile phone. This is contrast to conventional BRDF captures setups that usually require significant equipment and expenses. Convolutional Neural Networks is specifically designed to account for the physical form of NDRFs and the interaction of light with materials.</p><p id="9d74" data-selectable-paragraph="">A state of the art novel architecture that encodes the input image into a latent representation, which is decoded into components corresponding to surface normal, diffuse texture and specular roughness. The experiments demonstrate advantages over several baselines and prior works in quantitative comparisons, while also achieving superior qualitative results. The generalization ability of this network trained on the synthetic BRDF dataset is demonstrated by strong performance on real images, acquired in the wild, in both indoor and outdoor environments, using multiple different phone cameras. Given the estimated BRDF parameters, authors also demonstrate applications such as material editing and relighting of novel shapes. To summarise, authors propose the following contributions:</p><ul><li id="8db0" data-selectable-paragraph="">A novel lightweight SVBRDF acquisition method that produces state-of-the-art reconstruction quality.</li><li id="dc7d" data-selectable-paragraph="">A CNN architecture that exploits domain knowledge for joint SVBRDF reconstruction and material classification.</li><li id="f55e" data-selectable-paragraph="">Novel DCRF-based post-processing that accounts for the microfacet BRDF model to refine network outputs.</li><li id="b615" data-selectable-paragraph="">An SVBRDF dataset that is large-scale and specifically attuned to estimation of spatially-varying materials.</li></ul><figure><div><div><div><div><p><img src="https://miro.medium.com/max/60/1*0nwoEyB46Du5xVqpw1LTaA.png?q=20" width="1430" height="482" role="presentation"></p><p><img width="1430" height="482" role="presentation"></p></div></div></div></div><figcaption data-selectable-paragraph=""><em>Figure 2: Distribution of materials in our Training and test sets</em></figcaption></figure><p id="ba4b" data-selectable-paragraph=""><strong><em>Setup</em></strong><em>: </em>Our goal is to reconstruct the spatially-varying BRDF of a near planar surface from a single image captured by a mobile phone with the flash turned on for illumination. Authors assume that the z-axis of the camera is approximately perpendicular to the planar surface (they explicitly evaluate against this assumption in our experiments). For most mobile devices, the position of the flash light is usually very close to the position of the camera, which provides us a univariate sampling of a isotropic BRDF. Our surface appearance is represented by a microfacet parametric BRDF model. Let <em>di, ni, ri</em> be the diffuse color, normal and roughness, respectively, at pixel <em>i</em>. The BRDF model is defined as:</p><figure><div><div><div><div><p><img src="https://miro.medium.com/max/60/1*xLaKob1Q9-6QdANC6SHBIQ.png?q=20" width="1002" height="156" role="presentation"></p><p><img width="1002" height="156" role="presentation"></p></div></div></div></div></figure><p id="424b" data-selectable-paragraph="">Where <em>vi </em>and <em>li </em>are the view and light directions and <em>hi</em> is the half angle vector. Given an observed image <em>I (di, ni, ri, </em><strong><em>L</em></strong><em>)</em>, captured under unknown illumination L, scientists wish to recover the parameters <em>di, ni</em> and <em>ri</em> for each pixel <em>i </em>in the image.</p><p id="4df8" data-selectable-paragraph=""><strong><em>Dataset: </em></strong>The dataset has been used is Adobe Stock 3D Material <a href="https://stock.adobe.com/3d-assets" target="_blank" rel="noopener nofollow">dataset</a> which contain 688 materials with high resolution (4096 x 4096) spatially-varying BRDFs. Scientists use 588 materials for training and 100 materials for testing. For data augmentation, authors randomly crop 12, 8, 4, 2, 1 image patches of size 512, 1024, 2048, 3072, 4096. The distribution is shown in figure 2.</p></div></div><div><div><div><figure><div><div><div><div><p><img src="https://miro.medium.com/max/60/0*EZDtqAQqVIlLIb-E.?q=20" width="1453" height="655" role="presentation"></p><p><img width="1453" height="655" role="presentation"></p></div></div></div></div></figure></div></div></div><div><div><p id="0b9a" data-selectable-paragraph="">The basic network architecture consists of a single encoder and three decoders which reconstruct the three spatially-varying BRDF parameters: diffuse color <em>di</em>, normal <em>ni</em> and roughness <em>ri</em>. The intuition behind using a single encoder is that different BRDF parameters are correlated, thus, representations learned for one should be useful to infer the others, which allows significant reduction in the size of the network. The input to the network is an RGB image, augmented with the pixel coordinates as a fourth channel. Authors add the pixel coordinates since the distribution of light intensities is closely related to the location of pixels, for instance, the center of the image will usually be much brighter. Since CNNs are spatially invariant, they need the extra signal to let the network learn to behave differently for pixels at different locations. Skip links are added to connect the encoder and decoders to preserve details of BRDF parameters. To this end, our encoder network has seven convolutional layers of stride 2, so that the receptive field of every output pixel covers the entire image.</p><p id="dba1" data-selectable-paragraph="">For each BRDF parameter, authors have an L2 loss for direct supervision. For each batch, researchers create novel lights by randomly sampling the point light source on the upper hemisphere. This ensures that the network does not overfit to collocated illumination and is able to reproduce appearance under other light conditions. The final loss function for the encoder-decoder part of our network is:</p><figure><div><div><div><div><p><img src="https://miro.medium.com/max/60/1*ZEBsoUVvpjQmD3fNiDwaIw.png?q=20" width="730" height="88" role="presentation"></p><p><img width="730" height="88" role="presentation"></p></div></div></div></div></figure><p id="e9ae" data-selectable-paragraph="">Where</p><figure><div><div><div><div><p><img src="https://miro.medium.com/max/60/1*RWmr4LGeObWUHLvOYpixpQ.png?q=20" width="402" height="60" role="presentation"></p><p><img width="402" height="60" role="presentation"></p></div></div></div></div></figure><p id="0fab" data-selectable-paragraph="">are the L2 losses for diffuse, normal, roughness and rendered image predictions, respectively. Given the highest level of features extracted by the encoder, the features are send to a classifier to predict its material type. Then to evaluate the BRDF parameters for each material type and use the classification results as weights (the output of SoftMax layer). This averages the prediction from different material types to obtain the final BRDF reconstruction results. The classifier is trained together with the encoder and decoder from scratch, with the weights of each label set to be inversely proportional to the number of examples in figure 2 to balance different material types in the loss function. The overall loss function of our network with the classifier is</p><figure><div><div><div><div><p><img src="https://miro.medium.com/max/60/1*ylFYQqCDclqG-b7rteidvg.png?q=20" width="916" height="84" role="presentation"></p><p><img width="916" height="84" role="presentation"></p></div></div></div></div></figure><p id="f2b3" data-selectable-paragraph=""><strong><em>Acquisition setup: </em></strong>To verify the generalizabity of our method to real data, we show results on real images captured with different mobile devices in both indoor and outdoor environments. Authors capture linear RAW images (with potentially clipped highlights) with the flash enabled, using the Adobe Lightroom Mobile app. The mobile phones were hand-held, and the optical axis of the camera was only approximately perpendicular to the surfaces (See fig 4).</p><figure><div><div><div><div><p><img src="https://miro.medium.com/max/60/1*AJIkTBP1rnN66OgNPR6EsQ.png?q=20" width="1474" height="1136" role="presentation"></p><p><img width="1474" height="1136" role="presentation"></p></div></div></div></div><figcaption data-selectable-paragraph="">Figure 4</figcaption></figure><p id="8ce3" data-selectable-paragraph=""><strong><em>Qualitative results with different mobile phones:</em></strong><em> </em>Figure 5 presents SVBRDF and normal estimation results for real images captured with three different mobile devices: Huawei P9, Google Tango and iPhone 6s. Scientists observe that even with a single image, our network successfully predicts the SVBRDF and normals, with images rendered using the predicted parameters appear very similar to the input. Also, the exact same network generalizes well to different mobile devices, which shows that our data augmentation successfully helps the network factor out variations across devices. For some materials with specular highlights, the network can hallucinate information lost due to saturation. The network can also reconstruct reasonable normals even for complex instances.</p><figure><div><div><div><div><p><img src="https://miro.medium.com/max/60/1*ye6lY4yCgzanhpSKvArZKw.png?q=20" width="986" height="280" role="presentation"></p><p><img width="986" height="280" role="presentation"></p></div></div></div></div><figcaption data-selectable-paragraph=""><em>Figure 5: A failure case, due to incorrect material classification into metal, which causes the specularity to be over-smoothed</em></figcaption></figure><figure><div><div><div><div><p><img src="https://miro.medium.com/max/60/1*BjYnxr0rjDor8lwnHzp-QA.png?q=20" width="2330" height="1190" role="presentation"></p><p><img width="2330" height="1190" role="presentation"></p></div></div></div></div><figcaption data-selectable-paragraph=""><em>Figure 6: BRDF reconstruction results on real data. Authors tried different mobile devices to capture raw images using Adobe Lightroom. The input images in were captured using Huawei P9 (first three rows), Google Tango (fourth row) and iPhone 6s (fifth row), all with handheld mobile phone where the z-axis of camera was only approximately perpendicular to the sample surface</em>.</figcaption></figure><p id="9b8a" data-selectable-paragraph=""><a target="_blank" rel="noopener" href="https://medium.com/@muneeb2405"><em>Muneeb ul Hassan</em></a></p></div></div></section></div></div></div>
    </div>
    <footer>
        <div>created by buildstarted &copy; 2020 <a href="/about">about</a></div>
        <div>Share this page on social media: copy and paste this url https://linksfor.dev/</div>
        <div>If you prefer RSS: <a href="https://linksfor.dev/feed.xml">https://linksfor.dev/feed.xml</a></div>
    </footer>
    
    <script async defer>
        _dna = window._dna || {};
        _dna.siteId = "linksfor.devs";
        _dna.outlink = true;

        (function () {
            let dna = document.createElement('script');
            dna.type = 'text/javascript';
            dna.async = true;
            dna.src = '//dna.buildstarted.com/t.js';
            let s = document.getElementsByTagName('script')[0];
            s.parentNode.insertBefore(dna, s);
        })();
    </script>
    <noscript><img src="//dna.buildstarted.com/g?siteId=linksfor.devs"/></noscript>
</body>
</html>