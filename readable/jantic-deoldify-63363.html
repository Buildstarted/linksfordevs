<!DOCTYPE html>
<html lang="en">
<head>
    <title>
jantic/DeOldify - linksfor.dev(s)    </title>
    <meta charset="utf-8">
    <link rel="alternate" type="application/rss+xml" title="Linksfor.dev(s) feed" href="https://linksfor.dev/feed.rss" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta name="google" value="notranslate">
    <link rel="stylesheet" type="text/css" href="/style.min.css" />
    <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon">
    <link rel="icon" href="/favicon.ico" type="image/x-icon">
    <meta property="og:title" content="jantic/DeOldify - linksfor.dev(s)"/>
    <meta property="og:description" content="A Deep Learning based project for colorizing and restoring old images (and video!) - jantic/DeOldify"/>
    <meta property="og:type" content="website"/>
    <meta property="og:url" content="https://github.com/jantic/DeOldify"/>

<meta property="og:site_name" content="linksfor.dev(s)" />
</head>
<body>
    <div class="devring" style="background: #222">
        <div style="text-align:center">Explore other dev related sites in this ring. If you would like to join this ring <a href="https://devring.club">click here</a>.</div>
        <div class="grid">
            <div style="display: grid; grid-template-columns: .5fr 1fr 1fr 1fr; text-align: center;">
                <span class="devring-title"><a href="https://devring.club/">devring.club</a></span>
                <a href="https://devring.club/sites/1/prev" class="devring-previous">Previous</a>
                <a href="https://devring.club/random" class="devring-random">Random</a>
                <a href="https://devring.club/sites/1/next" class="devring-next">Next</a>
            </div>
        </div>
    </div>
    <div class="grid">
        <h1 style="margin: unset">
<a href="/" style="color:inherit">linksfor.dev(s)</a>
        </h1>
        <title>linksfor.dev(s) - jantic/DeOldify</title>
<div class="readable">
        <h1>jantic/DeOldify</h1>
            <div>Reading time: 20-25 minutes</div>
        <div>Posted here: 24 Jun 2020</div>
        <p><a href="https://github.com/jantic/DeOldify">https://github.com/jantic/DeOldify</a></p>
        <hr/>
<div id="readability-page-1" class="page"><div id="readme">
    
        

      <div>
        <article itemprop="text">
<p><strong>Quick Start</strong>: The easiest way to colorize images using DeOldify (for free!) is here: <a href="https://deepai.org/machine-learning-model/colorizer" rel="nofollow">DeOldify Image Colorization on DeepAI</a></p>
<p>The <strong>most advanced</strong> version of DeOldify image colorization is available here, exclusively.  Try a few images for free! <a href="https://www.myheritage.com/incolor" rel="nofollow">MyHeritiage In Color</a></p>
<hr>
<p>Image (artistic) <a href="https://colab.research.google.com/github/jantic/DeOldify/blob/master/ImageColorizerColab.ipynb" rel="nofollow"><img src="https://camo.githubusercontent.com/52feade06f2fecbf006889a904d221e6a730c194/68747470733a2f2f636f6c61622e72657365617263682e676f6f676c652e636f6d2f6173736574732f636f6c61622d62616467652e737667" data-canonical-src="https://colab.research.google.com/assets/colab-badge.svg"></a> |
Video <a href="https://colab.research.google.com/github/jantic/DeOldify/blob/master/VideoColorizerColab.ipynb" rel="nofollow"><img src="https://camo.githubusercontent.com/52feade06f2fecbf006889a904d221e6a730c194/68747470733a2f2f636f6c61622e72657365617263682e676f6f676c652e636f6d2f6173736574732f636f6c61622d62616467652e737667" data-canonical-src="https://colab.research.google.com/assets/colab-badge.svg"></a></p>
<p><strong>NEW</strong> Having trouble with the default image colorizer, aka "artistic"?  Try the "stable" one below.  It generally won't produce colors that are as interesting as "artistic", but the glitches are noticeably reduced.</p>
<p>Image (stable) <a href="https://colab.research.google.com/github/jantic/DeOldify/blob/master/ImageColorizerColabStable.ipynb" rel="nofollow"><img src="https://camo.githubusercontent.com/52feade06f2fecbf006889a904d221e6a730c194/68747470733a2f2f636f6c61622e72657365617263682e676f6f676c652e636f6d2f6173736574732f636f6c61622d62616467652e737667" data-canonical-src="https://colab.research.google.com/assets/colab-badge.svg"></a></p>
<p>Instructions on how to use the Colabs above have been kindly provided in video tutorial form by Old Ireland in Colour's John Breslin.  It's great! Click video image below to watch.</p>
<p><a href="http://www.youtube.com/watch?v=VaEl0faDw38" rel="nofollow"><img src="https://camo.githubusercontent.com/9d812131195cc524d5fe03696fdc284208bedbde/687474703a2f2f696d672e796f75747562652e636f6d2f76692f5661456c306661447733382f302e6a7067" alt="" data-canonical-src="http://img.youtube.com/vi/VaEl0faDw38/0.jpg"></a></p>
<p>Get more updates on <a href="https://twitter.com/citnaj" rel="nofollow">Twitter <img src="https://github.com/jantic/DeOldify/raw/master/resource_images/Twitter_Social_Icon_Rounded_Square_Color.svg?sanitize=true" width="16"></a>.</p>
<h2>Table of Contents</h2>
<ul>
<li><a href="#about-deoldify">About DeOldify</a></li>
<li><a href="#example-videos">Example Videos</a></li>
<li><a href="#example-images">Example Images</a></li>
<li><a href="#stuff-that-should-probably-be-in-a-paper">Stuff That Should Probably Be In A Paper</a>
<ul>
<li><a href="#how-to-achieve-stable-video">How to Achieve Stable Video</a></li>
<li><a href="#what-is-nogan">What is NoGAN?</a></li>
</ul>
</li>
<li><a href="#why-three-models">Why Three Models?</a></li>
<li><a href="#the-technical-details">Technical Details</a></li>
<li><a href="#this-project-going-forward">Going Forward</a></li>
<li><a href="#getting-started-yourself">Getting Started Yourself</a>
<ul>
<li><a href="#easiest-approach">Easiest Approach</a></li>
<li><a href="#your-own-machine-not-as-easy">Your Own Machine</a></li>
</ul>
</li>
<li><a href="#docker">Docker</a></li>
<li><a href="#pretrained-weights">Pretrained Weights</a></li>
</ul>
<h2>About DeOldify</h2>
<p>Simply put, the mission of this project is to colorize and restore old images and film footage.
We'll get into the details in a bit, but first let's see some pretty pictures and videos!</p>
<h3>New and Exciting Stuff in DeOldify</h3>
<ul>
<li>Glitches and artifacts are almost entirely eliminated</li>
<li>Better skin (less zombies)</li>
<li>More highly detailed and photorealistic renders</li>
<li>Much less "blue bias"</li>
<li><strong>Video</strong> - it actually looks good!</li>
<li><strong>NoGAN</strong> - a new and weird but highly effective way to do GAN training for image to image.</li>
</ul>
<h2>Example Videos</h2>
<p><strong>Note:</strong>  Click images to watch</p>
<h4>Facebook F8 Demo</h4>
<p><a href="http://www.youtube.com/watch?v=l3UXXid04Ys" rel="nofollow"><img src="https://camo.githubusercontent.com/95e149f839667ddcd87e0a1970e3870f6a61c24a/687474703a2f2f696d672e796f75747562652e636f6d2f76692f6c335558586964303459732f302e6a7067" alt="" data-canonical-src="http://img.youtube.com/vi/l3UXXid04Ys/0.jpg"></a></p>
<h4>Silent Movie Examples</h4>
<p><a href="http://www.youtube.com/watch?v=EXn-n2iqEjI" rel="nofollow"><img src="https://camo.githubusercontent.com/24d210457f7e8b57ef701788f013f2f72d2eda1c/687474703a2f2f696d672e796f75747562652e636f6d2f76692f45586e2d6e326971456a492f302e6a7067" alt="" data-canonical-src="http://img.youtube.com/vi/EXn-n2iqEjI/0.jpg"></a></p>
<h2>Example Images</h2>
<p>"Migrant Mother" by Dorothea Lange (1936)</p>
<p><a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/cf0b5cd16cd934cba884172370a78b40b28db00a/68747470733a2f2f692e696d6775722e636f6d2f427430766e6b652e6a7067"><img src="https://camo.githubusercontent.com/cf0b5cd16cd934cba884172370a78b40b28db00a/68747470733a2f2f692e696d6775722e636f6d2f427430766e6b652e6a7067" alt="Migrant Mother" data-canonical-src="https://i.imgur.com/Bt0vnke.jpg"></a></p>
<p>Woman relaxing in her livingroom in Sweden (1920)</p>
<p><a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/8ae04c8fc773e163705fd8ec24d3a9271806980c/68747470733a2f2f692e696d6775722e636f6d2f31353864306f552e6a7067"><img src="https://camo.githubusercontent.com/8ae04c8fc773e163705fd8ec24d3a9271806980c/68747470733a2f2f692e696d6775722e636f6d2f31353864306f552e6a7067" alt="Sweden Living Room" data-canonical-src="https://i.imgur.com/158d0oU.jpg"></a></p>
<p>"Toffs and Toughs" by Jimmy Sime (1937)</p>
<p><a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/0e3d002bbc787b75359789f8ade0c43b637cded3/68747470733a2f2f692e696d6775722e636f6d2f565975617634492e6a7067"><img src="https://camo.githubusercontent.com/0e3d002bbc787b75359789f8ade0c43b637cded3/68747470733a2f2f692e696d6775722e636f6d2f565975617634492e6a7067" alt="Class Divide" data-canonical-src="https://i.imgur.com/VYuav4I.jpg"></a></p>
<p>Thanksgiving Maskers (1911)</p>
<p><a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/ba7b6ae2cc2e908346ba56f06ea54061b9b1ee6e/68747470733a2f2f692e696d6775722e636f6d2f6e3871564a35632e6a7067"><img src="https://camo.githubusercontent.com/ba7b6ae2cc2e908346ba56f06ea54061b9b1ee6e/68747470733a2f2f692e696d6775722e636f6d2f6e3871564a35632e6a7067" alt="Thanksgiving Maskers" data-canonical-src="https://i.imgur.com/n8qVJ5c.jpg"></a></p>
<p>Glen Echo Madame Careta Gypsy Camp in Maryland (1925)</p>
<p><a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/83d69aafb3b306643f99566d08d805099c741e98/68747470733a2f2f692e696d6775722e636f6d2f316f59724a52492e6a7067"><img src="https://camo.githubusercontent.com/83d69aafb3b306643f99566d08d805099c741e98/68747470733a2f2f692e696d6775722e636f6d2f316f59724a52492e6a7067" alt="Gypsy Camp" data-canonical-src="https://i.imgur.com/1oYrJRI.jpg"></a></p>
<p>"Mr. and Mrs. Lemuel Smith and their younger children in their farm house, Carroll County, Georgia." (1941)</p>
<p><a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/f016893e9d37cab0175d218547699364d9c30f76/68747470733a2f2f692e696d6775722e636f6d2f49326a38796e6d2e6a7067"><img src="https://camo.githubusercontent.com/f016893e9d37cab0175d218547699364d9c30f76/68747470733a2f2f692e696d6775722e636f6d2f49326a38796e6d2e6a7067" alt="Georgia Farmhouse" data-canonical-src="https://i.imgur.com/I2j8ynm.jpg"></a></p>
<p>"Building the Golden Gate Bridge" (est 1937)</p>
<p><a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/3b1aca12e6009a5b8a47bcfbbc84cd533b22a1de/68747470733a2f2f692e696d6775722e636f6d2f365362466a66712e6a7067"><img src="https://camo.githubusercontent.com/3b1aca12e6009a5b8a47bcfbbc84cd533b22a1de/68747470733a2f2f692e696d6775722e636f6d2f365362466a66712e6a7067" alt="Golden Gate Bridge" data-canonical-src="https://i.imgur.com/6SbFjfq.jpg"></a></p>
<blockquote>
<p><strong>Note:</strong>  What you might be wondering is while this render looks cool, are the colors accurate? The original photo certainly makes it look like the towers of the bridge could be white. We looked into this and it turns out the answer is no - the towers were already covered in red primer by this time. So that's something to keep in mind- historical accuracy remains a huge challenge!</p>
</blockquote>
<p>"Terrasse de café, Paris" (1925)</p>
<p><a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/ae76951da1b7106193d81c44d7da2a0b74d60077/68747470733a2f2f692e696d6775722e636f6d2f577072517750352e6a7067"><img src="https://camo.githubusercontent.com/ae76951da1b7106193d81c44d7da2a0b74d60077/68747470733a2f2f692e696d6775722e636f6d2f577072517750352e6a7067" alt="Cafe Paris" data-canonical-src="https://i.imgur.com/WprQwP5.jpg"></a></p>
<p>Norwegian Bride (est late 1890s)</p>
<p><a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/03ab876e5b758529725e98bceea87f0e610106df/68747470733a2f2f692e696d6775722e636f6d2f4d6d7476725a6d2e6a7067"><img src="https://camo.githubusercontent.com/03ab876e5b758529725e98bceea87f0e610106df/68747470733a2f2f692e696d6775722e636f6d2f4d6d7476725a6d2e6a7067" alt="Norwegian Bride" data-canonical-src="https://i.imgur.com/MmtvrZm.jpg"></a></p>
<p>Zitkála-Šá (Lakota: Red Bird), also known as Gertrude Simmons Bonnin (1898)</p>
<p><a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/60080246c37e01c042194b2d87f4360a25637a7b/68747470733a2f2f692e696d6775722e636f6d2f7a49474d3034332e6a7067"><img src="https://camo.githubusercontent.com/60080246c37e01c042194b2d87f4360a25637a7b/68747470733a2f2f692e696d6775722e636f6d2f7a49474d3034332e6a7067" alt="Native Woman" data-canonical-src="https://i.imgur.com/zIGM043.jpg"></a></p>
<p>Chinese Opium Smokers (1880)</p>
<p><a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/5a05086ca8215de683081c6fb29998045fee0ddf/68747470733a2f2f692e696d6775722e636f6d2f6c5647713856712e6a7067"><img src="https://camo.githubusercontent.com/5a05086ca8215de683081c6fb29998045fee0ddf/68747470733a2f2f692e696d6775722e636f6d2f6c5647713856712e6a7067" alt="Opium Real" data-canonical-src="https://i.imgur.com/lVGq8Vq.jpg"></a></p>
<h2>Stuff That Should Probably Be In A Paper</h2>
<h3>How to Achieve Stable Video</h3>
<p>NoGAN training is crucial to getting the kind of stable and colorful images seen in this iteration of DeOldify. NoGAN training combines the benefits of GAN training (wonderful colorization) while eliminating the nasty side effects (like flickering objects in video). Believe it or not, video is rendered using isolated image generation without any sort of temporal modeling tacked on. The process performs 30-60 minutes of the GAN portion of "NoGAN" training, using 1% to 3% of imagenet data once.  Then, as with still image colorization, we "DeOldify" individual frames before rebuilding the video.</p>
<p>In addition to improved video stability, there is an interesting thing going on here worth mentioning. It turns out the models I run, even different ones and with different training structures, keep arriving at more or less the same solution.  That's even the case for the colorization of things you may think would be arbitrary and unknowable, like the color of clothing, cars, and even special effects (as seen in "Metropolis").</p>
<p><a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/ea1738479cfd9811faa49b7dc78bb59606e74cfb/68747470733a2f2f7468756d62732e6766796361742e636f6d2f48656176794c6f6e65426c6f77666973682d73697a655f726573747269637465642e676966"><img src="https://camo.githubusercontent.com/ea1738479cfd9811faa49b7dc78bb59606e74cfb/68747470733a2f2f7468756d62732e6766796361742e636f6d2f48656176794c6f6e65426c6f77666973682d73697a655f726573747269637465642e676966" alt="Metropolis Special FX" data-canonical-src="https://thumbs.gfycat.com/HeavyLoneBlowfish-size_restricted.gif"></a></p>
<p>My best guess is that the models are learning some interesting rules about how to colorize based on subtle cues present in the black and white images that I certainly wouldn't expect to exist.  This result leads to nicely deterministic and consistent results, and that means you don't have track model colorization decisions because they're not arbitrary.  Additionally, they seem remarkably robust so that even in moving scenes the renders are very consistent.</p>
<p><a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/007128e9e871429b96bca83aae7f2dfa9f3d9ecc/68747470733a2f2f7468756d62732e6766796361742e636f6d2f46616d696c6961724a7562696c616e744173702d73697a655f726573747269637465642e676966"><img src="https://camo.githubusercontent.com/007128e9e871429b96bca83aae7f2dfa9f3d9ecc/68747470733a2f2f7468756d62732e6766796361742e636f6d2f46616d696c6961724a7562696c616e744173702d73697a655f726573747269637465642e676966" alt="Moving Scene Example" data-canonical-src="https://thumbs.gfycat.com/FamiliarJubilantAsp-size_restricted.gif"></a></p>
<p>Other ways to stabilize video add up as well. First, generally speaking rendering at a higher resolution (higher render_factor) will increase stability of colorization decisions.  This stands to reason because the model has higher fidelity image information to work with and will have a greater chance of making the "right" decision consistently.  Closely related to this is the use of resnet101 instead of resnet34 as the backbone of the generator- objects are detected more consistently and correctly with this. This is especially important for getting good, consistent skin rendering.  It can be particularly visually jarring if you wind up with "zombie hands", for example.</p>
<p><a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/2b18ba56365c70078a0672e7aaa2b402e2a25eea/68747470733a2f2f7468756d62732e6766796361742e636f6d2f54687269667479496e666572696f7249736162656c6c696e6577686561746561722d73697a655f726573747269637465642e676966"><img src="https://camo.githubusercontent.com/2b18ba56365c70078a0672e7aaa2b402e2a25eea/68747470733a2f2f7468756d62732e6766796361742e636f6d2f54687269667479496e666572696f7249736162656c6c696e6577686561746561722d73697a655f726573747269637465642e676966" alt="Zombie Hand Example" data-canonical-src="https://thumbs.gfycat.com/ThriftyInferiorIsabellinewheatear-size_restricted.gif"></a></p>
<p>Additionally, gaussian noise augmentation during training appears to help but at this point the conclusions as to just how much are bit more tenuous (I just haven't formally measured this yet).  This is loosely based on work done in style transfer video, described here:  <a href="https://medium.com/element-ai-research-lab/stabilizing-neural-style-transfer-for-video-62675e203e42" rel="nofollow">https://medium.com/element-ai-research-lab/stabilizing-neural-style-transfer-for-video-62675e203e42</a>.</p>
<p>Special thanks go to Rani Horev for his contributions in implementing this noise augmentation.</p>
<h3>What is NoGAN?</h3>
<p>This is a new type of GAN training that I've developed to solve some key problems in the previous DeOldify model. It provides the benefits of GAN training while spending minimal time doing direct GAN training.  Instead, most of the training time is spent pretraining the generator and critic separately with more straight-forward, fast and reliable conventional methods.  A key insight here is that those more "conventional" methods generally get you most of the results you need, and that GANs can be used to close the gap on realism. During the very short amount of actual GAN training the generator not only gets the full realistic colorization capabilities that used to take days of progressively resized GAN training, but it also doesn't accrue nearly as much of the artifacts and other ugly baggage of GANs. In fact, you can pretty much eliminate glitches and artifacts almost entirely depending on your approach. As far as I know this is a new technique. And it's incredibly effective.</p>
<p><strong>Original DeOldify Model</strong></p>
<p><a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/5f92319233179b2f204b8739173abf98a69ef39a/68747470733a2f2f7468756d62732e6766796361742e636f6d2f436f6f7264696e6174656456656e657261746564486f676765742d73697a655f726573747269637465642e676966"><img src="https://camo.githubusercontent.com/5f92319233179b2f204b8739173abf98a69ef39a/68747470733a2f2f7468756d62732e6766796361742e636f6d2f436f6f7264696e6174656456656e657261746564486f676765742d73697a655f726573747269637465642e676966" alt="Before Flicker" data-canonical-src="https://thumbs.gfycat.com/CoordinatedVeneratedHogget-size_restricted.gif"></a></p>
<p><strong>NoGAN-Based DeOldify Model</strong></p>
<p><a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/410aabdcd548bde894635617caf09eaa678a7e80/68747470733a2f2f7468756d62732e6766796361742e636f6d2f4f696c79426c61636b417263746963686172652d73697a655f726573747269637465642e676966"><img src="https://camo.githubusercontent.com/410aabdcd548bde894635617caf09eaa678a7e80/68747470733a2f2f7468756d62732e6766796361742e636f6d2f4f696c79426c61636b417263746963686172652d73697a655f726573747269637465642e676966" alt="After Flicker" data-canonical-src="https://thumbs.gfycat.com/OilyBlackArctichare-size_restricted.gif"></a></p>
<p>The steps are as follows: First train the generator in a conventional way by itself with just the feature loss. Next, generate images from that, and train the critic on distinguishing between those outputs and real images as a basic binary classifier. Finally, train the generator and critic together in a GAN setting (starting right at the target size of 192px in this case).  Now for the weird part:  All the useful GAN training here only takes place within a very small window of time.  There's an inflection point where it appears the critic has transferred everything it can that is useful to the generator. Past this point, image quality oscillates between the best that you can get at the inflection point, or bad in a predictable way (orangish skin, overly red lips, etc).  There appears to be no productive training after the inflection point.  And this point lies within training on just 1% to 3% of the Imagenet Data!  That amounts to about 30-60 minutes of training at 192px.</p>
<p>The hard part is finding this inflection point.  So far, I've accomplished this by making a whole bunch of model save checkpoints (every 0.1% of data iterated on) and then just looking for the point where images look great before they go totally bonkers with orange skin (always the first thing to go). Additionally, generator rendering starts immediately getting glitchy and inconsistent at this point, which is no good particularly for video. What I'd really like to figure out is what the tell-tale sign of the inflection point is that can be easily automated as an early stopping point.  Unfortunately, nothing definitive is jumping out at me yet.  For one, it's happening in the middle of training loss decreasing- not when it flattens out, which would seem more reasonable on the surface.</p>
<p>Another key thing about NoGAN training is you can repeat pretraining the critic on generated images after the initial GAN training, then repeat the GAN training itself in the same fashion.  This is how I was able to get extra colorful results with the "artistic" model.  But this does come at a cost currently- the output of the generator becomes increasingly inconsistent and you have to experiment with render resolution (render_factor) to get the best result.  But the renders are still glitch free and way more consistent than I was ever able to achieve with the original DeOldify model. You can do about five of these repeat cycles, give or take, before you get diminishing returns, as far as I can tell.</p>
<p>Keep in mind- I haven't been entirely rigorous in figuring out what all is going on in NoGAN- I'll save that for a paper. That means there's a good chance I'm wrong about something.  But I think it's definitely worth putting out there now because I'm finding it very useful- it's solving basically much of my remaining problems I had in DeOldify.</p>
<p>This builds upon a technique developed in collaboration with Jeremy Howard and Sylvain Gugger for Fast.AI's Lesson 7 in version 3 of Practical Deep Learning for Coders Part I. The particular lesson notebook can be found here: <a href="https://github.com/fastai/course-v3/blob/master/nbs/dl1/lesson7-superres-gan.ipynb">https://github.com/fastai/course-v3/blob/master/nbs/dl1/lesson7-superres-gan.ipynb</a></p>
<h2>Why Three Models?</h2>
<p>There are now three models to choose from in DeOldify. Each of these has key strengths and weaknesses, and so have different use cases.  Video is for video of course.  But stable and artistic are both for images, and sometimes one will do images better than the other.</p>
<p>More details:</p>
<ul>
<li><strong>Artistic</strong> - This model achieves the highest quality results in image coloration, in terms of interesting details and vibrance. The most notable drawback however is that it's a bit of a pain to fiddle around with to get the best results (you have to adjust the rendering resolution or render_factor to achieve this).  Additionally, the model does not do as well as stable in a few key common scenarios- nature scenes and portraits.  The model uses a resnet34 backbone on a UNet with an emphasis on depth of layers on the decoder side.  This model was trained with 5 critic pretrain/GAN cycle repeats via NoGAN, in addition to the initial generator/critic pretrain/GAN NoGAN training, at 192px.  This adds up to a total of 32% of Imagenet data trained once (12.5 hours of direct GAN training).</li>
<li><strong>Stable</strong> - This model achieves the best results with landscapes and portraits. Notably, it produces less "zombies"- where faces or limbs stay gray rather than being colored in properly.  It generally has less weird miscolorations than artistic, but it's also less colorful in general.  This model uses a resnet101 backbone on a UNet with an emphasis on width of layers on the decoder side.  This model was trained with 3 critic pretrain/GAN cycle repeats via NoGAN, in addition to the initial generator/critic pretrain/GAN NoGAN training, at 192px.  This adds up to a total of 7% of Imagenet data trained once (3 hours of direct GAN training).</li>
<li><strong>Video</strong> - This model is optimized for smooth, consistent and flicker-free video.  This would definitely be the least colorful of the three models, but it's honestly not too far off from "stable". The model is the same as "stable" in terms of architecture, but differs in training.  It's trained for a mere 2.2% of Imagenet data once at 192px, using only the initial generator/critic pretrain/GAN NoGAN training (1 hour of direct GAN training).</li>
</ul>
<p>Because the training of the artistic and stable models was done before the "inflection point" of NoGAN training described in "What is NoGAN???" was discovered,  I believe this amount of training on them can be knocked down considerably. As far as I can tell, the models were stopped at "good points" that were well beyond where productive training was taking place.  I'll be looking into this in the future.</p>
<p>Ideally, eventually these three models will be consolidated into one that has all these good desirable unified.  I think there's a path there, but it's going to require more work!  So for now, the most practical solution appears to be to maintain multiple models.</p>
<h2>The Technical Details</h2>
<p>This is a deep learning based model.  More specifically, what I've done is combined the following approaches:</p>
<h3><a href="https://arxiv.org/abs/1805.08318" rel="nofollow">Self-Attention Generative Adversarial Network</a></h3>
<p>Except the generator is a <strong>pretrained U-Net</strong>, and I've just modified it to have the spectral normalization and self-attention.  It's a pretty straightforward translation.</p>
<h3><a href="https://arxiv.org/abs/1706.08500" rel="nofollow">Two Time-Scale Update Rule</a></h3>
<p>This is also very straightforward – it's just one to one generator/critic iterations and higher critic learning rate.
This is modified to incorporate a "threshold" critic loss that makes sure that the critic is "caught up" before moving on to generator training.
This is particularly useful for the "NoGAN" method described below.</p>
<h3>NoGAN</h3>
<p>There's no paper here! This is a new type of GAN training that I've developed to solve some key problems in the previous DeOldify model.
The gist is that you get the benefits of GAN training while spending minimal time doing direct GAN training.
More details are in the <a href="#what-is-nogan">What is NoGAN?</a> section (it's a doozy).</p>
<h3>Generator Loss</h3>
<p>Loss during NoGAN learning is two parts:  One is a basic Perceptual Loss (or Feature Loss) based on VGG16 – this just biases the generator model to replicate the input image.
The second is the loss score from the critic.  For the curious – Perceptual Loss isn't sufficient by itself to produce good results.
It tends to just encourage a bunch of brown/green/blue – you know, cheating to the test, basically, which neural networks are really good at doing!
Key thing to realize here is that GANs essentially are learning the loss function for you – which is really one big step closer to toward the ideal that we're shooting for in machine learning.
And of course you generally get much better results when you get the machine to learn something you were previously hand coding.
That's certainly the case here.</p>
<p><strong>Of note:</strong>  There's no longer any "Progressive Growing of GANs" type training going on here.  It's just not needed in lieu of the superior results obtained by the "NoGAN" technique described above.</p>
<p>The beauty of this model is that it should be generally useful for all sorts of image modification, and it should do it quite well.
What you're seeing above are the results of the colorization model, but that's just one component in a pipeline that I'm developing with the exact same approach.</p>
<h2>This Project, Going Forward</h2>
<p>So that's the gist of this project – I'm looking to make old photos and film look reeeeaaally good with GANs, and more importantly, make the project <em>useful</em>.
In the meantime though this is going to be my baby and I'll be actively updating and improving the code over the foreseeable future.
I'll try to make this as user-friendly as possible, but I'm sure there's going to be hiccups along the way.</p>
<p>Oh and I swear I'll document the code properly...eventually.  Admittedly I'm <em>one of those</em> people who believes in "self documenting code" (LOL).</p>
<h2>Getting Started Yourself</h2>
<h3>Easiest Approach</h3>
<p>The easiest way to get started is to go straight to the Colab notebooks:</p>
<p>Image <a href="https://colab.research.google.com/github/jantic/DeOldify/blob/master/ImageColorizerColab.ipynb" rel="nofollow"><img src="https://camo.githubusercontent.com/52feade06f2fecbf006889a904d221e6a730c194/68747470733a2f2f636f6c61622e72657365617263682e676f6f676c652e636f6d2f6173736574732f636f6c61622d62616467652e737667" data-canonical-src="https://colab.research.google.com/assets/colab-badge.svg"></a>
| Video <a href="https://colab.research.google.com/github/jantic/DeOldify/blob/master/VideoColorizerColab.ipynb" rel="nofollow"><img src="https://camo.githubusercontent.com/52feade06f2fecbf006889a904d221e6a730c194/68747470733a2f2f636f6c61622e72657365617263682e676f6f676c652e636f6d2f6173736574732f636f6c61622d62616467652e737667" data-canonical-src="https://colab.research.google.com/assets/colab-badge.svg"></a></p>
<p>Special thanks to Matt Robinson and María Benavente for their image Colab notebook contributions, and Robert Bell for the video Colab notebook work!</p>
<h3>Your Own Machine (not as easy)</h3>
<h4>Hardware and Operating System Requirements</h4>
<ul>
<li><strong>(Training Only) BEEFY Graphics card</strong>.  I'd really like to have more memory than the 11 GB in my GeForce 1080TI (11GB).  You'll have a tough time with less.  The Generators and Critic are ridiculously large.</li>
<li><strong>(Colorization Alone) A decent graphics card</strong>. Approximately 4GB+ memory video cards should be sufficient.</li>
<li><strong>Linux (or maybe Windows 10)</strong>  I'm using Ubuntu 16.04, but nothing about this precludes Windows 10 support as far as I know.  I just haven't tested it and am not going to make it a priority for now.</li>
</ul>
<h4>Easy Install</h4>
<p>You should now be able to do a simple install with Anaconda. Here are the steps:</p>
<p>Open the command line and navigate to the root folder you wish to install.  Then type the following commands</p>
<div><pre><span>git clone https://github.com/jantic/DeOldify.git DeOldify</span>
<span>cd DeOldify</span>
<span>conda env create -f environment.yml</span></pre></div>
<p>Then start running with these commands:</p>
<div><pre><span>source activate deoldify</span>
<span>jupyter lab</span></pre></div>
<p>From there you can start running the notebooks in Jupyter Lab, via the url they provide you in the console.</p>
<blockquote>
<p><strong>Note:</strong> You can also now do "conda activate deoldify" if you have the latest version of conda and in fact that's now recommended. But a lot of people don't have that yet so I'm not going to make it the default instruction here yet.</p>
</blockquote>
<h4>Note on test_images Folder</h4>
<p>The images in the <code>test_images</code> folder have been removed because they were using Git LFS and that costs a lot of money when GitHub actually charges for bandwidth on a popular open source project (they had a billing bug for while that was recently fixed).  The notebooks that use them (the image test ones) still point to images in that directory that I (Jason) have personally and I'd like to keep it that way because, after all, I'm by far the primary and most active developer.  But they won't work for you.  Still, those notebooks are a convenient template for making your own tests if you're so inclined.</p>
<h4>Typical training</h4>
<p>The notebook <code>ColorizeTrainingWandb</code> has been created to log and monitor results through <a href="https://www.wandb.com/" rel="nofollow">Weights &amp; Biases</a>. You can find a description of typical training by consulting <a href="https://app.wandb.ai/borisd13/DeOldify/reports?view=borisd13%2FDeOldify" rel="nofollow">W&amp;B Report</a>.</p>
<h2>Docker</h2>
<h3>Docker for Jupyter</h3>
<p>You can build and run the docker using the following process:</p>
<p>Cloning</p>
<div><pre><span>git clone https://github.com/jantic/DeOldify.git DeOldify</span></pre></div>
<p>Building Docker</p>
<div><pre><span>cd DeOldify &amp;&amp; docker build -t deoldify_jupyter -f Dockerfile .</span></pre></div>
<p>Running Docker</p>
<div><pre><span>echo "http://$(curl ifconfig.io):8888" &amp;&amp; nvidia-docker run --ipc=host --env NOTEBOOK_PASSWORD="pass123" -p 8888:8888 -it deoldify_jupyter</span></pre></div>
<h3>Docker for API</h3>
<p>You can build and run the docker using the following process:</p>
<p>Cloning</p>
<div><pre><span>git clone https://github.com/jantic/DeOldify.git DeOldify</span></pre></div>
<p>Building Docker</p>
<div><pre><span>cd DeOldify &amp;&amp; docker build -t deoldify_api -f Dockerfile-api .</span></pre></div>
<p>Running Docker</p>
<div><pre><span>echo "http://$(curl ifconfig.io):5000" &amp;&amp; nvidia-docker run --ipc=host -p 5000:5000 -d deoldify_api</span></pre></div>
<p>Calling the API for image processing</p>
<div><pre><span>curl -X POST "http://MY_SUPER_API_IP:5000/process" -H "accept: image/png" -H "Content-Type: application/json" -d "{"source_url":"http://www.afrikanheritage.com/wp-content/uploads/2015/08/slave-family-P.jpeg", "render_factor":35}" --output colorized_image.png</span></pre></div>
<p>Calling the API for video processing</p>
<div><pre><span>curl -X POST "http://MY_SUPER_API_IP:5000/process" -H "accept: application/octet-stream" -H "Content-Type: application/json" -d "{"source_url":"https://v.redd.it/d1ku57kvuf421/HLSPlaylist.m3u8", "render_factor":35}" --output colorized_video.mp4</span></pre></div>
<blockquote>
<p><strong>Note:</strong> If you don't have Nvidia Docker, <a href="https://github.com/nvidia/nvidia-docker/wiki/Installation-(version-2.0)#installing-version-20">here</a> is the installation guide.</p>
</blockquote>
<h3>Installation Details</h3>
<p>This project is built around the wonderful Fast.AI library.  Prereqs, in summary:</p>
<ul>
<li><strong>Fast.AI 1.0.51</strong> (and its dependencies).  If you use any higher version you'll see grid artifacts in rendering and tensorboard will malfunction. So yeah...don't do that.</li>
<li><strong>PyTorch 1.0.1</strong> Not the latest version of PyTorch- that will not play nicely with the version of FastAI above.  Note however that the conda install of FastAI 1.0.51 grabs the latest PyTorch, which doesn't work.  This is patched over by our own conda install but fyi.</li>
<li><strong>Jupyter Lab</strong> <code>conda install -c conda-forge jupyterlab</code></li>
<li><strong>Tensorboard</strong> (i.e. install Tensorflow) and <strong>TensorboardX</strong> (<a href="https://github.com/lanpa/tensorboardX">https://github.com/lanpa/tensorboardX</a>).  I guess you don't <em>have</em> to but man, life is so much better with it.  FastAI now comes with built in support for this- you just  need to install the prereqs: <code>conda install -c anaconda tensorflow-gpu</code> and <code>pip install tensorboardX</code></li>
<li><strong>ImageNet</strong> – Only if you're training, of course. It has proven to be a great dataset for my purposes.  <a href="http://www.image-net.org/download-images" rel="nofollow">http://www.image-net.org/download-images</a></li>
</ul>
<h2>Pretrained Weights</h2>
<p>To start right away on your own machine with your own images or videos without training the models yourself, you'll need to download the "Completed Generator Weights" listed below and drop them in the /models/ folder.</p>
<p>The colorization inference notebooks should be able to guide you from here. The notebooks to use are named ImageColorizerArtistic.ipynb, ImageColorizerStable.ipynb, and VideoColorizer.ipynb.</p>
<h3>Completed Generator Weights</h3>
<ul>
<li><a href="https://www.dropbox.com/s/zkehq1uwahhbc2o/ColorizeArtistic_gen.pth?dl=0" rel="nofollow">Artistic</a></li>
<li><a href="https://www.dropbox.com/s/mwjep3vyqk5mkjc/ColorizeStable_gen.pth?dl=0" rel="nofollow">Stable</a></li>
<li><a href="https://www.dropbox.com/s/336vn9y4qwyg9yz/ColorizeVideo_gen.pth?dl=0" rel="nofollow">Video</a></li>
</ul>
<h3>Completed Critic Weights</h3>
<ul>
<li><a href="https://www.dropbox.com/s/8g5txfzt2fw8mf5/ColorizeArtistic_crit.pth?dl=0" rel="nofollow">Artistic</a></li>
<li><a href="https://www.dropbox.com/s/7a8u20e7xdu1dtd/ColorizeStable_crit.pth?dl=0" rel="nofollow">Stable</a></li>
<li><a href="https://www.dropbox.com/s/0401djgo1dfxdzt/ColorizeVideo_crit.pth?dl=0" rel="nofollow">Video</a></li>
</ul>
<h3>Pretrain Only Generator Weights</h3>
<ul>
<li><a href="https://www.dropbox.com/s/9zexurvrve141n9/ColorizeArtistic_PretrainOnly_gen.pth?dl=0" rel="nofollow">Artistic</a></li>
<li><a href="https://www.dropbox.com/s/mdnuo1563bb8nh4/ColorizeStable_PretrainOnly_gen.pth?dl=0" rel="nofollow">Stable</a></li>
<li><a href="https://www.dropbox.com/s/avzixh1ujf86e8x/ColorizeVideo_PretrainOnly_gen.pth?dl=0" rel="nofollow">Video</a></li>
</ul>
<h3>Pretrain Only Critic Weights</h3>
<ul>
<li><a href="https://www.dropbox.com/s/lakxe8akzjgjnmh/ColorizeArtistic_PretrainOnly_crit.pth?dl=0" rel="nofollow">Artistic</a></li>
<li><a href="https://www.dropbox.com/s/b3wka56iyv1fvdc/ColorizeStable_PretrainOnly_crit.pth?dl=0" rel="nofollow">Stable</a></li>
<li><a href="https://www.dropbox.com/s/j7og84cbhpa94gs/ColorizeVideo_PretrainOnly_crit.pth?dl=0" rel="nofollow">Video</a></li>
</ul>
<h2>Want the Old DeOldify?</h2>
<p>We suspect some of you are going to want access to the original DeOldify model for various reasons.  We have that archived here:  <a href="https://github.com/dana-kelley/DeOldify">https://github.com/dana-kelley/DeOldify</a></p>
<h2>Want More?</h2>
<p>Follow <a href="https://twitter.com/search?q=%23Deoldify" rel="nofollow">#DeOldify</a> or <a href="https://twitter.com/citnaj" rel="nofollow">Jason Antic</a> on Twitter.</p>
<h2>License</h2>
<p>All code in this repository is under the MIT license as specified by the LICENSE file.</p>
<p>The model weights listed in this readme under the "Pretrained Weights" section are trained by ourselves and are released under the MIT license.</p>
</article>
      </div>
  </div></div></div>
    </div>
    <footer>
        <div>created by <a href="https://buildstarted.com">buildstarted</a> &copy; 2020 <a href="/about">about</a></div>
        <div>Share this page on social media: copy and paste this url https://linksfor.dev/</div>
        <div>If you prefer RSS: <a href="https://linksfor.dev/feed.xml">https://linksfor.dev/feed.xml</a></div>
        <div>Customer satisfaction guaranteed to be optional.</div>
    </footer>
    
    <script async defer>
        _dna = window._dna || {};
        _dna.siteId = "linksfor.devs";
        _dna.outlink = true;

        (function() {
            let dna = document.createElement('script');
            dna.type = 'text/javascript';
            dna.async = true;
            dna.src = '//dna.buildstarted.com/t.js';
            let s = document.getElementsByTagName('script')[0];
            s.parentNode.insertBefore(dna, s);
        })();
    </script>
    <noscript><img src="//dna.buildstarted.com/g?siteId=linksfor.devs"/></noscript>
</body>
</html>