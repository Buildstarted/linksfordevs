<!DOCTYPE html>
<html lang="en">
<head>
    <title>
LSTM for Programmers - linksfor.dev(s)    </title>
    <meta charset="utf-8">
    <link rel="alternate" type="application/rss+xml" title="Linksfor.dev(s) feed" href="https://linksfor.dev/feed.rss" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta name="google" value="notranslate">
    <link rel="stylesheet" type="text/css" href="/style.css" />
    <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon">
    <link rel="icon" href="/favicon.ico" type="image/x-icon">
    <meta property="og:title" content="LSTM for Programmers - linksfor.dev(s)"/>
    <meta property="og:description" content="import torchimport tensorflow as tf"/>
    <meta property="og:type" content="website"/>
    <meta property="og:url" content="https://blog.zhen-zhang.com/2020/06/14/lstm-note.html"/>

<meta property="og:site_name" content="linksfor.dev(s)" />
</head>
<body>
	<div class="devring" style="background: #222">
		<div class="grid">
			<div style="display: grid; grid-template-columns: .5fr 1fr 1fr 1fr; text-align: center;">
				<span class="devring-title">devring.club</span>
				<a href="https://devring.club/site/1/previous" class="devring-previous">Previous</a>
				<a href="https://devring.club/random" class="devring-random">Random</a>
				<a href="https://devring.club/sites/1/next" class="devring-next">Next</a>
			</div>
		</div>
	</div>
    <div class="grid">
        <h1>
<a href="/" style="color:inherit">linksfor.dev(s)</a>
        </h1>
        <title>linksfor.dev(s) - LSTM for Programmers</title>
<div class="readable">
        <h1>LSTM for Programmers</h1>
            <div>Reading time: 6-8 minutes</div>
        <div>Posted here: 14 Jun 2020</div>
        <p><a href="https://blog.zhen-zhang.com/2020/06/14/lstm-note.html">https://blog.zhen-zhang.com/2020/06/14/lstm-note.html</a></p>
        <hr/>
<div id="readability-page-1" class="page"><div>
      <div>
        <div>

  

  <article>
    <div><div><pre><code><span>import</span> <span>torch</span>
<span>import</span> <span>tensorflow</span> <span>as</span> <span>tf</span>
</code></pre></div></div>

<p>First we define some parameters:</p>

<ul>
  <li><code>seq_len</code> (Sequence length): The length of sequence. For RNN, it can vary
<em>from batch to batch</em>. As a first step, we will define it as a constant and
show how LSTM works in a single batch. We also call this timestep length, or
timesteps.</li>
  <li><code>input_size</code> (Input feature size): For each input, e.g. a word in a sentence,
we use a vector of <code>input_size</code>-long to represent it. We normally call this
the <em>embedding</em> of the actual individual data (word).</li>
  <li><code>batch_size</code> (Batch size): how many copies of data in a single batch</li>
  <li><code>hidden_size</code> (Hidden units size): the number of features in hidden state.
Note that hidden state is the <em>output</em> features.</li>
</ul>

<div><div><pre><code><span>seq_len</span><span>,</span> <span>batch_size</span><span>,</span> <span>input_size</span><span>,</span> <span>hidden_size</span> <span>=</span> <span>17</span><span>,</span> <span>2</span><span>,</span> <span>10</span><span>,</span> <span>7</span>

<span>I_tc</span> <span>=</span> <span>torch</span><span>.</span><span>randn</span><span>(</span><span>seq_len</span><span>,</span> <span>batch_size</span><span>,</span> <span>input_size</span><span>)</span> <span># Input for PyTorch
</span><span>I_tf</span> <span>=</span> <span>tf</span><span>.</span><span>random</span><span>.</span><span>normal</span><span>([</span><span>batch_size</span><span>,</span> <span>seq_len</span><span>,</span> <span>input_size</span><span>])</span> <span># Input for TensorFlow
</span></code></pre></div></div>

<h2 id="simple-lstms">Simple LSTMs</h2>

<p>Now, let’s show a simple <a href="https://pytorch.org/docs/stable/nn.html#lstm">LSTM layer in PyTorch</a> first:</p>

<div><div><pre><code><span>lstm_tc_simple</span> <span>=</span> <span>torch</span><span>.</span><span>nn</span><span>.</span><span>LSTM</span><span>(</span><span>input_size</span><span>,</span> <span>hidden_size</span><span>)</span>
<span>O_tc_simple</span><span>,</span> <span>(</span><span>H_tc_simple</span><span>,</span> <span>C_tc_simple</span><span>)</span> <span>=</span> <span>lstm_tc_simple</span><span>(</span><span>I_tc</span><span>)</span>
</code></pre></div></div>

<div><div><pre><code><span>assert</span> <span>O_tc_simple</span><span>.</span><span>shape</span> <span>==</span> <span>torch</span><span>.</span><span>Size</span><span>((</span><span>seq_len</span><span>,</span> <span>batch_size</span><span>,</span> <span>hidden_size</span><span>))</span>
</code></pre></div></div>

<div><div><pre><code><span>assert</span> <span>H_tc_simple</span><span>.</span><span>shape</span> <span>==</span> <span>torch</span><span>.</span><span>Size</span><span>((</span><span>1</span><span>,</span> <span>batch_size</span><span>,</span> <span>hidden_size</span><span>))</span>
</code></pre></div></div>

<div><div><pre><code><span>assert</span> <span>C_tc_simple</span><span>.</span><span>shape</span> <span>==</span> <span>torch</span><span>.</span><span>Size</span><span>((</span><span>1</span><span>,</span> <span>batch_size</span><span>,</span> <span>hidden_size</span><span>))</span>
</code></pre></div></div>

<p>Next, another example in <a href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM">TensorFlow</a>:</p>

<div><div><pre><code><span>lstm_tf_simple</span> <span>=</span> <span>tf</span><span>.</span><span>keras</span><span>.</span><span>layers</span><span>.</span><span>LSTM</span><span>(</span><span>hidden_size</span><span>)</span>
<span>O_tf_simple</span> <span>=</span> <span>lstm_tf_simple</span><span>(</span><span>I_tf</span><span>)</span>
</code></pre></div></div>

<div><div><pre><code><span>assert</span> <span>O_tf_simple</span><span>.</span><span>shape</span> <span>==</span> <span>tf</span><span>.</span><span>TensorShape</span><span>((</span><span>batch_size</span><span>,</span> <span>hidden_size</span><span>))</span>
</code></pre></div></div>

<p>As you can see, the input of two <code>lstm</code>s are identical (modulo the order).
But, there are several questions we can ask about the <em>difference</em> between the
outputs:</p>

<ul>
  <li>What does <code>O_tc_simple</code>, <code>H_tc_simple</code> and <code>C_tc_simple</code> mean?</li>
  <li>What does <code>O_tf_simple</code> mean? How does it correspond to the PyTorch gang?</li>
</ul>

<p>Let’s look at some diagrams from <a href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/">Colah’s post on LSTM</a>:</p>

<p><img src="https://izgzhen.github.io/assets/images/RNN-unrolled.png" alt=""></p>

<p>The above is a general structure of RNN:</p>

<blockquote>
  <p>In the above diagram, a chunk of neural network, <span id="MathJax-Element-1-Frame" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>A</mi></math>" role="presentation"><span role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>A</mi></math></span></span>, looks at some input <span id="MathJax-Element-2-Frame" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>x</mi><mi>t</mi></msub></math>" role="presentation"><span role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>x</mi><mi>t</mi></msub></math></span></span> and outputs a value <span id="MathJax-Element-3-Frame" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>h</mi><mi>t</mi></msub></math>" role="presentation"><span role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>h</mi><mi>t</mi></msub></math></span></span>. A loop allows information to be passed from one step of the network to the next.</p>
</blockquote>

<p>So, LSTM (or any general RNN)’s basic functionality is converting
a <em>sequence</em> of <span id="MathJax-Element-4-Frame" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>x</mi></math>" role="presentation"><span role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>x</mi></math></span></span> to a <em>sequence</em> of <span id="MathJax-Element-5-Frame" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>h</mi></math>" role="presentation"><span role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>h</mi></math></span></span>.</p>

<p><code>O_tc_simple</code> contains a <em>sequence</em> of hidden state (or output features) <span id="MathJax-Element-6-Frame" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>h</mi></math>" role="presentation"><span role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>h</mi></math></span></span>
multiplied with batch size.</p>

<p><code>H_tc_simple</code> contains the hidden state for most recent timestep,
in shape of <code>(num_layers * num_directions, batch_size, hidden_size)</code>.
Since by default we constructed LSTM of only one layer and one direction,
the first dimension is just 1.</p>

<p>Similar to <code>H_tc_simple</code>’s shape,
<code>C_tc_simple</code> contains the <em>cell</em> state. You can think of
each hidden state is associated with a cell state <span id="MathJax-Element-7-Frame" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>c</mi></math>" role="presentation"><span role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>c</mi></math></span></span>:</p>

<p><img src="https://izgzhen.github.io/assets/images/LSTM3-C-line.png" alt=""></p>

<p>In fact, we can verify that when there is only one layer, <code>O_tc_simple</code> contains <code>H_tc_simple</code>:</p>

<div><div><pre><code><span>torch</span><span>.</span><span>eq</span><span>(</span><span>O_tc_simple</span><span>[</span><span>-</span><span>1</span><span>,:,:],</span> <span>H_tc_simple</span><span>)</span>
</code></pre></div></div>

<div><div><pre><code>tensor([[[True, True, True, True, True, True, True],
         [True, True, True, True, True, True, True]]])
</code></pre></div></div>

<p>Tensorflow’s <code>O_tf_simple</code> is actuall just the flatten version of
<code>H_tc_simple</code>. There is another
configuration of TF’s LSTM to make it output differently:</p>

<div><div><pre><code><span>lstm_tf_simple2</span> <span>=</span> <span>tf</span><span>.</span><span>keras</span><span>.</span><span>layers</span><span>.</span><span>LSTM</span><span>(</span><span>hidden_size</span><span>,</span> <span>return_sequences</span><span>=</span><span>True</span><span>,</span> <span>return_state</span><span>=</span><span>True</span><span>)</span>
<span>O_tf_simple2</span><span>,</span> <span>H_tf_simple2</span><span>,</span> <span>C_tf_simple2</span> <span>=</span> <span>lstm_tf_simple2</span><span>(</span><span>I_tf</span><span>)</span>
</code></pre></div></div>

<div><div><pre><code><span>assert</span> <span>O_tf_simple2</span><span>.</span><span>shape</span> <span>==</span> <span>tf</span><span>.</span><span>TensorShape</span><span>((</span><span>batch_size</span><span>,</span> <span>seq_len</span><span>,</span> <span>hidden_size</span><span>))</span>
</code></pre></div></div>

<div><div><pre><code><span>assert</span> <span>H_tf_simple2</span><span>.</span><span>shape</span> <span>==</span> <span>tf</span><span>.</span><span>TensorShape</span><span>((</span><span>batch_size</span><span>,</span> <span>hidden_size</span><span>))</span>
</code></pre></div></div>

<div><div><pre><code><span>assert</span> <span>C_tf_simple2</span><span>.</span><span>shape</span> <span>==</span> <span>tf</span><span>.</span><span>TensorShape</span><span>((</span><span>batch_size</span><span>,</span> <span>hidden_size</span><span>))</span>
</code></pre></div></div>

<p>Now, correspondence between PyTorch’s and TF’s outputs is much clearer.</p>

<h2 id="bidirectional-and-stacked-lstms">Bidirectional and Stacked LSTMs</h2>

<p>However, it looks like PyTorch’s <code>LSTM</code> is much more powerful. You can specify
two directions:</p>

<div><div><pre><code><span>num_directions</span> <span>=</span> <span>2</span>
<span>bilstm_tc</span> <span>=</span> <span>torch</span><span>.</span><span>nn</span><span>.</span><span>LSTM</span><span>(</span><span>input_size</span><span>,</span> <span>hidden_size</span><span>,</span> <span>bidirectional</span><span>=</span><span>True</span><span>)</span>
<span>O_tc_bidir</span><span>,</span> <span>(</span><span>H_tc_bidir</span><span>,</span> <span>C_tc_bidir</span><span>)</span> <span>=</span> <span>bilstm_tc</span><span>(</span><span>I_tc</span><span>)</span>
</code></pre></div></div>

<div><div><pre><code><span>assert</span> <span>O_tc_bidir</span><span>.</span><span>shape</span> <span>==</span> <span>torch</span><span>.</span><span>Size</span><span>((</span><span>seq_len</span><span>,</span> <span>batch_size</span><span>,</span> <span>num_directions</span> <span>*</span> <span>hidden_size</span><span>))</span>
</code></pre></div></div>

<div><div><pre><code><span>assert</span> <span>H_tc_bidir</span><span>.</span><span>shape</span> <span>==</span> <span>torch</span><span>.</span><span>Size</span><span>((</span><span>num_directions</span><span>,</span> <span>batch_size</span><span>,</span> <span>hidden_size</span><span>))</span>
</code></pre></div></div>

<div><div><pre><code><span>assert</span> <span>C_tc_bidir</span><span>.</span><span>shape</span> <span>==</span> <span>torch</span><span>.</span><span>Size</span><span>((</span><span>num_directions</span><span>,</span> <span>batch_size</span><span>,</span> <span>hidden_size</span><span>))</span>
</code></pre></div></div>

<p>Or stack multiple layers together:</p>

<div><div><pre><code><span>num_layers</span> <span>=</span> <span>3</span>
<span>bi_lstm_tc_stacked</span> <span>=</span> <span>torch</span><span>.</span><span>nn</span><span>.</span><span>LSTM</span><span>(</span><span>input_size</span><span>,</span> <span>hidden_size</span><span>,</span> <span>num_layers</span><span>=</span><span>num_layers</span><span>,</span> <span>bidirectional</span><span>=</span><span>True</span><span>)</span>
<span>O_tc_bidir_stacked</span><span>,</span> <span>(</span><span>H_tc_bidir_stacked</span><span>,</span> <span>C_tc_bidir_stacked</span><span>)</span> <span>=</span> <span>bi_lstm_tc_stacked</span><span>(</span><span>I_tc</span><span>)</span>
</code></pre></div></div>

<div><div><pre><code><span>assert</span> <span>O_tc_bidir_stacked</span><span>.</span><span>shape</span> <span>==</span> <span>torch</span><span>.</span><span>Size</span><span>((</span><span>seq_len</span><span>,</span> <span>batch_size</span><span>,</span> <span>num_directions</span> <span>*</span> <span>hidden_size</span><span>)),</span> <span>O1</span><span>.</span><span>shape</span>
</code></pre></div></div>

<div><div><pre><code><span>assert</span> <span>H_tc_bidir_stacked</span><span>.</span><span>shape</span> <span>==</span> <span>torch</span><span>.</span><span>Size</span><span>((</span><span>num_directions</span> <span>*</span> <span>num_layers</span><span>,</span> <span>batch_size</span><span>,</span> <span>hidden_size</span><span>))</span>
</code></pre></div></div>

<div><div><pre><code><span>assert</span> <span>C_tc_bidir_stacked</span><span>.</span><span>shape</span> <span>==</span> <span>torch</span><span>.</span><span>Size</span><span>((</span><span>num_directions</span> <span>*</span> <span>num_layers</span><span>,</span> <span>batch_size</span><span>,</span> <span>hidden_size</span><span>))</span>
</code></pre></div></div>

<p>How to do the same extension in TF?</p>

<p><a href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/Bidirectional">Bidirectional</a> is a wrapper over an RNN layer.</p>

<div><div><pre><code><span>bilstm_tf</span> <span>=</span> <span>tf</span><span>.</span><span>keras</span><span>.</span><span>layers</span><span>.</span><span>Bidirectional</span><span>(</span><span>tf</span><span>.</span><span>keras</span><span>.</span><span>layers</span><span>.</span><span>LSTM</span><span>(</span><span>hidden_size</span><span>),</span> <span>input_shape</span><span>=</span><span>I_tf</span><span>.</span><span>shape</span><span>)</span>
<span>O_tf_bidir</span> <span>=</span> <span>bilstm_tf</span><span>(</span><span>I_tf</span><span>)</span>
</code></pre></div></div>

<div><div><pre><code><span>assert</span> <span>O_tf_bidir</span><span>.</span><span>shape</span> <span>==</span> <span>tf</span><span>.</span><span>TensorShape</span><span>((</span><span>batch_size</span><span>,</span> <span>2</span> <span>*</span> <span>hidden_size</span><span>))</span>
</code></pre></div></div>

<p>Stacking LSTM layers in TF requires composing them in a sequential model.</p>

<div><div><pre><code><span># Stack 3 layers
</span>
<span>model_tf_stacked</span> <span>=</span> <span>tf</span><span>.</span><span>keras</span><span>.</span><span>Sequential</span><span>()</span>
<span>model_tf_stacked</span><span>.</span><span>add</span><span>(</span><span>tf</span><span>.</span><span>keras</span><span>.</span><span>layers</span><span>.</span><span>LSTM</span><span>(</span><span>hidden_size</span><span>,</span> <span>return_sequences</span><span>=</span><span>True</span><span>,</span> <span>input_shape</span><span>=</span><span>(</span><span>seq_len</span><span>,</span> <span>input_size</span><span>)))</span>
<span>model_tf_stacked</span><span>.</span><span>add</span><span>(</span><span>tf</span><span>.</span><span>keras</span><span>.</span><span>layers</span><span>.</span><span>LSTM</span><span>(</span><span>hidden_size</span><span>,</span> <span>return_sequences</span><span>=</span><span>True</span><span>))</span>
<span>model_tf_stacked</span><span>.</span><span>add</span><span>(</span><span>tf</span><span>.</span><span>keras</span><span>.</span><span>layers</span><span>.</span><span>LSTM</span><span>(</span><span>hidden_size</span><span>))</span>
<span>model_tf_stacked</span><span>.</span><span>compile</span><span>(</span><span>optimizer</span><span>=</span><span>'adam'</span><span>,</span> <span>loss</span><span>=</span><span>'mse'</span><span>)</span>
<span>model_tf_stacked</span><span>.</span><span>summary</span><span>()</span>
</code></pre></div></div>

<div><div><pre><code>Model: "sequential"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
lstm_5 (LSTM)                (None, 17, 7)             504       
_________________________________________________________________
lstm_6 (LSTM)                (None, 17, 7)             420       
_________________________________________________________________
lstm_7 (LSTM)                (None, 7)                 420       
=================================================================
Total params: 1,344
Trainable params: 1,344
Non-trainable params: 0
_________________________________________________________________
</code></pre></div></div>

<div><div><pre><code><span>O_tf_stacked</span> <span>=</span> <span>model_tf_stacked</span><span>.</span><span>predict</span><span>(</span><span>I_tf</span><span>)</span>
</code></pre></div></div>

<div><div><pre><code><span>assert</span> <span>O_tf_stacked</span><span>.</span><span>shape</span> <span>==</span> <span>tf</span><span>.</span><span>TensorShape</span><span>((</span><span>batch_size</span><span>,</span> <span>hidden_size</span><span>))</span>
</code></pre></div></div>

<blockquote>
  <p><a href="https://github.com/izgzhen/ml-notebooks/blob/master/lstm.ipynb">Source code</a>. Please
star it if you like it! You can open an issue if you want to see more articles like this.</p>
</blockquote>

  </article>
    
  <!-- Disqus -->
  
  
  <!-- Disqus -->
</div>

      </div>
    </div></div></div>
    </div>
    <footer>
        <div>created by buildstarted &copy; 2020 <a href="/about">about</a></div>
        <div>Share this page on social media: copy and paste this url https://linksfor.dev/</div>
        <div>If you prefer RSS: <a href="https://linksfor.dev/feed.xml">https://linksfor.dev/feed.xml</a></div>
		<div>Customer satisfaction guaranteed to be optional.</div>
    </footer>
    
    <script async defer>
        _dna = window._dna || {};
        _dna.siteId = "linksfor.devs";
        _dna.outlink = true;

        (function () {
            let dna = document.createElement('script');
            dna.type = 'text/javascript';
            dna.async = true;
            dna.src = '//dna.buildstarted.com/t.js';
            let s = document.getElementsByTagName('script')[0];
            s.parentNode.insertBefore(dna, s);
        })();
    </script>
    <noscript><img src="//dna.buildstarted.com/g?siteId=linksfor.devs"/></noscript>
</body>
</html>