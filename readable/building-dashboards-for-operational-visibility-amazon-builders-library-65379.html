<!DOCTYPE html>
<html lang="en">
<head>
    <title>
Building dashboards for operational visibility | Amazon Builders&#x27; Library - linksfor.dev(s)    </title>
    <meta charset="utf-8">
    <link rel="alternate" type="application/rss+xml" title="Linksfor.dev(s) feed" href="https://linksfor.dev/feed.rss" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta name="google" value="notranslate">
    <link rel="stylesheet" type="text/css" href="/style.min.css" />
    <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon">
    <link rel="icon" href="/favicon.ico" type="image/x-icon">
    <meta property="og:title" content="Building dashboards for operational visibility | Amazon Builders&#x27; Library - linksfor.dev(s)"/>
    <meta property="article:author" content="About the author"/>
    <meta property="og:description" content="Building dashboards to monitor, dive deep, audit, and review distributed services and automated systems."/>
    <meta property="og:type" content="website"/>
    <meta property="og:url" content="https://aws.amazon.com/builders-library/building-dashboards-for-operational-visibility/"/>

<meta property="og:site_name" content="linksfor.dev(s)" />
</head>
<body>
    <div class="devring" style="background: #222">
        <div style="text-align:center">Explore other dev related sites in this ring. If you would like to join this ring <a href="https://devring.club">click here</a>.</div>
        <div class="grid">
            <div style="display: grid; grid-template-columns: .5fr 1fr 1fr 1fr; text-align: center;">
                <span class="devring-title"><a href="https://devring.club/">devring.club</a></span>
                <a href="https://devring.club/sites/1/prev" class="devring-previous">Previous</a>
                <a href="https://devring.club/random" class="devring-random">Random</a>
                <a href="https://devring.club/sites/1/next" class="devring-next">Next</a>
            </div>
        </div>
    </div>
    <div class="grid">
        <h1 style="margin: unset">
<a href="/" style="color:inherit">linksfor.dev(s)</a>
        </h1>
        <title>linksfor.dev(s) - Building dashboards for operational visibility | Amazon Builders&#x27; Library</title>
<div class="readable">
        <h1>Building dashboards for operational visibility | Amazon Builders&#x27; Library</h1>
            <div>by About the author</div>
            <div>Reading time: 15-19 minutes</div>
        <div>Posted here: 08 Aug 2020</div>
        <p><a href="https://aws.amazon.com/builders-library/building-dashboards-for-operational-visibility/">https://aws.amazon.com/builders-library/building-dashboards-for-operational-visibility/</a></p>
        <hr/>
<div id="readability-page-1" class="page"><div> 
     <div> 
      <div> 
       <h2 id="High-level_dashboards"> High-level dashboards</h2> 
       <h3 id="Customer_experience_dashboards"> Customer experience dashboards</h3> 
       <p>At Amazon, our most important and widely used dashboards are customer experience dashboards. These dashboards are designed for use by a broad group of users that includes service operators and many other stakeholders. They efficiently present metrics on overall service health and adherence to goals. They display monitoring data that is sourced from the service itself and also from client instrumentation (such as AWS SDK metrics), continuous testers (such as Amazon CloudWatch Synthetics canaries), and automated remediation systems. These dashboards also contain data that help users answer questions about the depth and breadth of impact. Some of these questions are likely to be “How many customers are impacted?” and “Which customers are most impacted?”</p> 
       <h3 id="System-level_dashboards"> System-level dashboards</h3> 
       <div> 
        <p>The entry points to our web-based services are typically UI and API endpoints, so dedicated system-level dashboards must contain enough data for operators to see how the system and its customer-facing endpoints are behaving. These dashboards primarily display interface-level monitoring data. These dashboards display three categories of monitoring data for each API:</p> 
        <ul> 
         <li>Input-related monitoring data. This can include counts of requests received or work polled from queues/streams, request byte size percentiles, and authentication/authorization failure counts.</li> 
         <li>Processing-related monitoring data. This can include multi-modal business logic path/branch execution counts, backend microservice request counts/failure/latency percentiles, fault and error log output, and request trace data.</li> 
         <li>Output-related monitoring data. This can include response type counts (with breakdowns for error/fault responses by customer), size of responses, and percentiles for time-to-write first response byte and time-to-write complete response.</li> 
        </ul> 
        <p>In general, we aim to keep these customer experience and system-level dashboards as high level as possible. We deliberately avoid the temptation to add too many metrics to these dashboards because information overload can distract from the core message that these dashboards need to convey.</p> 
       </div> 
       <h3 id="Service_instance_dashboards"> Service instance dashboards</h3> 
       <p>We build some dashboards to facilitate fast and comprehensive evaluation of the customer experience within a single service instance (partition or cell). This narrow view ensures that operators who work on a single service instance are not overloaded with irrelevant data from other service instances.</p> 
       <h3 id="Service_audit_dashboards"> Service audit dashboards</h3> 
       <p>We also build customer experience dashboards that intentionally display data for all instances of a service, across all Availability Zones and Regions. These service audit dashboards are used by operators to audit automated alarming across all service instances. These alarms can also be reviewed during the weekly operations meetings mentioned earlier.</p> 
       <h3 id="Capacity_planning_and_forecasting_dashboards"> Capacity planning and forecasting dashboards</h3> 
       <p>For longer-term use cases, we also build dashboards for capacity planning and forecasting to help us visualize the growth of our services.</p> 
      </div> 
     </div> 
    </div><div> 
     <div> 
      <div> 
       <h2 id="Low-level_dashboards"> Low-level dashboards</h2> 
       <p>Amazon APIs are typically implemented by orchestrating requests across backend microservices. These microservices can be owned by different teams, each of which is responsible for some specific aspect of processing the request. For example, some microservices are dedicated to request authentication and authorization, throttling/limit enforcement, usage metering, creating/updating/deleting resources, retrieving resources from datastores, and starting asynchronous workflows. Teams typically build at least one dedicated microservice-specific dashboard that shows metrics for each API, or unit of work if the service is asynchronously processing data. <br> </p> 
       <h3 id="Microservice-specific_dashboards"> Microservice-specific dashboards</h3> 
       <p>Dashboards for microservices typically show implementation-specific monitoring data that requires deep knowledge of the service. These dashboards are primarily used by the teams that own the services. However, because our services are heavily instrumented, we need to present data from this instrumentation in a way that won’t overwhelm the operators. So, these dashboards typically display some data in aggregated form. When operators identify anomalies in the aggregated data, they typically use a variety of other tools to dive deeper, executing ad-hoc queries on the underlying monitoring data that de-aggregate the data, trace requests, and reveal related or correlated data.<br> </p> 
       <h3 id="Infrastructure_dashboards"> Infrastructure dashboards</h3> 
       <p>Our services run on AWS infrastructure that typically emits metrics, so we also have dedicated infrastructure dashboards. These dashboards focus primarily on the metrics emitted by the compute resources our systems run on, such as Amazon Elastic Compute Cloud (EC2) instances, Amazon Elastic Container Service (ECS)/Amazon Elastic Kubernetes Service (EKS) containers, and AWS Lambda functions. Metrics such as CPU utilization, network traffic, disk IO, and space utilization are commonly used in these dashboards, along with any related cluster, Auto Scaling, and quota metrics that are relevant to these compute resources. <br> </p> 
       <h3 id="Dependency_dashboards"> Dependency dashboards</h3> 
       <p>In addition to compute resources, in many cases microservices depend on other microservices. Even if the teams that own those dependencies already have their own dashboards, each microservice owner typically creates dedicated dependency dashboards to provide a view of how both upstream dependencies (for example, proxies and load balancers) and downstream dependencies (for example, data stores, queues, and streams) are behaving, as measured by their service. These dashboards can also be used to track other critical metrics, such as security certificate expiry dates and other dependency quota usage.<br> </p> 
      </div> 
     </div> 
    </div><div> 
     <div> 
      <div> 
       <h2 id="Dashboard_design"> Dashboard design</h2> 
       <div> 
        <p>At Amazon, we consider consistency in the presentation of data to be critical to the successful creation of a dashboard. To be effective, consistency must be achieved in each dashboard and also across all dashboards. Over the years, we have identified, adapted, and refined a common set of design idioms and conventions that we believe make dashboards accessible to the broadest audience, ultimately increasing their value to our organization. We’ve even found subtle ways to measure and improve these design conventions over time. For example, if a new operator can quickly understand and use the data presented in dashboards to learn how a service works, that is an indication that those dashboards are presenting the right information in the right way.</p> 
        <p>A very common tendency when designing dashboards is to overestimate or underestimate the domain knowledge of the target user. It’s easy to build a dashboard that makes total sense to its creator. However, this dashboard might not provide value to users. We use the technique of working backwards from the customer (in this case, the dashboard users) to eliminate this risk.</p> 
        <p>We have adopted a design convention that standardizes the layout of data in a dashboard. Dashboards render from top to bottom, and users tend to interpret the initially rendered graphs (visible when the dashboard loads) as the most important. So, our design convention advises placing the most important data at the top of the dashboard. We have found that aggregated/summary availability graphs and end-to-end latency percentile graphs are typically the most important dashboards for web services.</p> 
        <p>Here’s a screenshot of the top of a dashboard for a hypothetical Foo Service:<br> </p> 
       </div> 
       <figure> 
        <p><img alt="Dashboard design" title="Dashboard design" src="https://d1.awsstatic.com/builderslibrary/architecture-images/building-dashboards-for-operational-visibility-2.e6fa8f4ee8d52e540e166e67d4b82c5b4567557a.png"> 
        </p> 
       </figure> 
       <h3 id="We_use_larger_graphs_for_the_most_important_metrics"> We use larger graphs for the most important metrics</h3> 
       <p>If we have many metrics in a graph, we ensure that the graph legends don’t vertically or horizontally squeeze the visible graph data. If we are using search queries in graphs, we make sure to allow for a larger than normal set of metrics results.<br> </p> 
       <h3 id="We_lay_out_graphs_for_the_expected_minimum_display_resolution"> We lay out graphs for the expected minimum display resolution</h3> 
       <p>This avoids forcing users to scroll horizontally. An oncall operator on a laptop at 3 AM might not notice the horizontal scroll bar without an obvious visual clue that there are more graphs to the right.<br> </p> 
       <h3 id="We_display_the_time_zone"> We display the time zone</h3> 
       <p>For dashboards that display date and time data, we make sure that the related time zone is visible in the dashboard. For dashboards that are used concurrently by operators in different time zones, we default to one time zone (UTC) that all users can relate to. This way users can communicate with each other using a single time zone, saving them the time and effort of doing excessive mental time zone translations. <br> </p> 
       <h3 id="We_use_the_shortest_time_interval_and_datapoint_period"> We use the shortest time interval and datapoint period</h3> 
       <div> 
        <p>We default to the time interval and datapoint period that is relevant to the most common use cases. We ensure that all graphs on a dashboard initially display data for the same time range and resolution. We find that it’s beneficial if all graphs within a section of the dashboard have the same horizontal size. This allows easy time correlation between graphs.</p> 
        <p>We also avoid plotting too many datapoints in graphs because this slows down the dashboard load time. In addition, we’ve observed that displaying excessive datapoints to the user can actually reduce visibility into anomalies. For example, a graph of a three-hour interval of one-minute resolution datapoints with just 180 values per metric will render clearly in even small dashboard widgets. This number of datapoints also provides enough context to operators who are triaging ongoing operational events. <br> </p> 
       </div> 
       <h3 id="We_enable_the_ability_to_adjust_time_interval_and_metric_period"> We enable the ability to adjust time interval and metric period</h3> 
       <div> 
        <p>Our dashboards provide controls for quickly adjusting both the time interval and metric period for all graphs. Other common interval x resolution ratios that we use in our dashboards are:</p> 
        <ul> 
         <li>1-hour x 1 minute (60 datapoints) – useful for zooming in to observe ongoing events</li> 
         <li>12-hours x 1 minute (720 datapoints)</li> 
         <li>1-day x 5 minutes (288 datapoints) – useful for viewing daily trends</li> 
         <li>3-days x 5 minutes (864 datapoints)</li> 
         <li>1-week x 1 hour (168 datapoints) – useful for viewing weekly trends</li> 
         <li>1-month x 1 hour (744 datapoints)</li> 
         <li>3-months x 1 day (90 datapoints) – useful for viewing quarterly trends</li> 
         <li>9-months x 1 day (270 datapoints)</li> 
         <li>15-months x 1 day (450 datapoints) – useful for long-term capacity reviews<br> </li> 
        </ul> 
       </div> 
       <figure> 
        <p><img alt="Dashboards with time intervals" title="Dashboards with time intervals" src="https://d1.awsstatic.com/builderslibrary/architecture-images/building-dashboards-for-operational-visibility-3.d1665be25cced4054128e70b7aa049683e006710.png"> 
        </p> 
       </figure> 
       <h3 id="We_annotate_graphs_with_alarm_thresholds"> We annotate graphs with alarm thresholds</h3> 
       <p>When we graph metrics that have related automated alarms, if the alarm thresholds are static, we annotate graphs with horizontal lines. If the alarm thresholds are dynamic, that is, based on forecasts or predictions generated using artificial intelligence (AI) or machine learning (ML), we display both the actual and threshold metrics in the same graph. If a graph shows a metric that measures an aspect of the service that has known limits (such as a “maximum tested” limit or a hard resource limit), we annotate the graph with a horizontal line indicating where the known or tested limits are. For metrics that have goals, we add horizontal lines to make those goals immediately visible to the user.<br> </p> 
       <h3 id="We_avoid_adding_horizontal_lines_to_graphs_that_already_use_both_a_left_and_a_right_y-axis"> We avoid adding horizontal lines to graphs that already use both a left and a right y-axis</h3> 
       <p>If you add horizontal lines to these graphs, users might find it difficult to know which y-axis the horizontal line relates to. To avoid this ambiguity, we split graphs like this into two graphs that use a single horizontal axis only and add the horizontal lines only to the appropriate graph. <br> </p> 
       <figure> 
        <p><img alt="Dashboards with horizontal lines" title="Dashboards with horizontal lines" src="https://d1.awsstatic.com/builderslibrary/architecture-images/building-dashboards-for-operational-visibility-4.10edcfae0421d011dc88e3d776850b934a7a3043.png"> 
        </p> 
       </figure> 
       <h3 id="We_avoid_overloading_a_y-axis_with_multiple_metrics_that_have_very_disparate_value_ranges"> We avoid overloading a y-axis with multiple metrics that have very disparate value ranges</h3> 
       <p>We avoid this situation because it can result in reduced visibility into the variance of one or more metrics. An example of this is when we plot p0 (minimum) and p100 (maximum) latencies on the same graph where the values of the p100 datapoints may be orders of magnitude larger than p0 datapoints. <br> </p> 
       <figure> 
        <p><img alt="Dashboards with a y-axis with multiple metrics" title="Dashboards with a y-axis with multiple metrics" src="https://d1.awsstatic.com/builderslibrary/architecture-images/building-dashboards-for-operational-visibility-5.963571313ed7ace436f59467c9b0a0efcb08f3e8.png"> 
        </p> 
       </figure> 
       <h3 id="We_are_wary_of_shrinking_the_y-axis_bounds_to_the_current_datapoint_value_range_only"> We are wary of shrinking the y-axis bounds to the current datapoint value range only</h3> 
       <p>A casual glance at a graph with a y-axis range limited to datapoint values can make a metric look far more variable than it actually is.<br> </p> 
       <h3 id="We_avoid_overloading_single_graphs"> We avoid overloading single graphs</h3> 
       <div> 
        <p>We want to ensure that we don’t have too many statistics or unrelated metrics in a single graph. For example, when adding graphs for request processing, we typically create separate adjacent graphs in the dashboard for the following:</p> 
        <ul> 
         <li>Availability % (Faults/Requests * 100)</li> 
         <li>p10, Average, p90 latencies</li> 
         <li>p99.9 and Maximum (p100) latencies<br> </li> 
        </ul> 
       </div> 
       <h3 id="We_don.27t_assume_the_user_knows_exactly_what_every_metric_or_widget_means"> We don't assume the user knows exactly what every metric or widget means</h3> 
       <div> 
        <p>This applies in particular to implementation-specific metrics. We want to provide sufficient context in dashboard text, for example with description text beside or below each graph. The operator can read this text to understand what the metric means. Then the operator can interpret what "normal" looks like and what it could mean if the graph isn't "normal." In this text, we provide links to related resources that an operator can use to determine the root cause. Here are a few examples of the types of links we provide:</p> 
        <ul> 
         <li>To runbooks. For subject-matter experts, the dashboard can be the runbook.</li> 
         <li>To related “dive deep” dashboards.</li> 
         <li>To equivalent dashboards for other clusters or partitions.</li> 
         <li>To deployment pipelines.</li> 
         <li>To contact information for dependencies.<br> </li> 
        </ul> 
       </div> 
       <figure> 
        <p><img alt="Don't assume users know every metric in a dashboard" title="Don't assume users know every metric in a dashboard" src="https://d1.awsstatic.com/builderslibrary/architecture-images/building-dashboards-for-operational-visibility-6.0c036582ffe6990d2f461638eb6e44eba3755c07.png"> 
        </p> 
       </figure> 
       <h3 id="We_use_alarm_status.2C_simple_numbers.2C_and.2For_time_series_graph_widgets_where_appropriate"> We use alarm status, simple numbers, and/or time series graph widgets where appropriate</h3> 
       <p>Depending on the use cases for the dashboard, we find that displaying a widget that contains a single number (for example, the latest value of a metric) or alarm status is sometimes more appropriate than displaying a complex time series graph of all recent datapoints.<br> </p> 
       <figure> 
        <p><img alt="Use simple numbers where appropriate" title="Use simple numbers where appropriate" src="https://d1.awsstatic.com/builderslibrary/architecture-images/fooservice-at-a-glance.5cbaead79dda7ca1da16c969859f1bb41f89bdd9.png"> 
        </p> 
       </figure> 
       <h3 id="We_avoid_relying_on_graphs_that_display_sparse_metrics"> We avoid relying on graphs that display sparse metrics</h3> 
       <p>Sparse metrics are metrics that are only emitted when certain error conditions exist. Although it can be efficient to instrument services to emit these metrics only when necessary, dashboard users can be confused by empty or almost empty graphs. When we encounter such metrics while designing dashboards, we typically modify the service to continually emit safe (that is, zero) values for these metrics in the absence of the error condition. Operators can then easily understand that the absence of data implies that the service is not emitting telemetry correctly.<br> </p> 
       <h3 id="We_add_additional_graphs_that_display_per-mode_metrics"> We add additional graphs that display per-mode metrics</h3> 
       <div> 
        <p>We do this when we’re displaying graphs for metrics that aggregate multi-model behavior in our systems. Some circumstances when we might do this include:</p> 
        <ul> 
         <li>If a service supports variable-sized requests, we might create a graph for the overall latencies of requests. Plus, we might also create graphs that display metrics for small, medium, and large requests.</li> 
         <li>If a service executes requests in variable ways depending on the values (or combinations) of input parameters, then we might add graphs for metrics that capture each mode of execution.<br> </li> 
        </ul> 
       </div> 
      </div> 
     </div> 
    </div></div></div>
    </div>
    <footer>
        <div>created by <a href="https://buildstarted.com">buildstarted</a> &copy; 2020 <a href="/about">about</a></div>
        <div>Share this page on social media: copy and paste this url https://linksfor.dev/</div>
        <div>If you prefer RSS: <a href="https://linksfor.dev/feed.xml">https://linksfor.dev/feed.xml</a></div>
        <div>Customer satisfaction guaranteed to be optional.</div>
    </footer>
    
    <script async defer>
        _dna = window._dna || {};
        _dna.siteId = "linksfor.devs";
        _dna.outlink = true;

        (function() {
            let dna = document.createElement('script');
            dna.type = 'text/javascript';
            dna.async = true;
            dna.src = '//dna.buildstarted.com/t.js';
            let s = document.getElementsByTagName('script')[0];
            s.parentNode.insertBefore(dna, s);
        })();
    </script>
    <noscript><img src="//dna.buildstarted.com/g?siteId=linksfor.devs"/></noscript>
</body>
</html>