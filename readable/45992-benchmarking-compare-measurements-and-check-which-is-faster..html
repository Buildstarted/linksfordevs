<!DOCTYPE html>
<html lang="en">
<head>
    <title>
Benchmarking: compare measurements and check which is faster. -
linksfor.dev(s)
    </title>
	<link rel="alternate" type="application/rss+xml" title="Linksfor.dev(s) feed" href="https://linksfor.dev/feed.rss" />
    <meta charset="utf-8">
    <meta name="Description" content="A curated source of links that devs might find interesting. Updated around the clock." />
    <meta name="google" value="notranslate">
    <link rel="stylesheet" type="text/css" href="/style.css" />
</head>
<body>
    <div class="grid">
        
<div class="readable">
    <h1>Benchmarking: compare measurements and check which is faster.</h1>
    <div><div class="notebody"> <p><strong>Contents:</strong></p> <p><strong>Subscribe to my <a href="/blog/2019/12/30/Comparing-performance-measurements#mc_embed_signup">mailing list</a> and support me on <a href="https://www.patreon.com/dendibakh">Patreon</a>.</strong></p> <p>When doing performance improvements in our code, we need a way to prove that we actually made it better. Or when we commit a regular code change, we want to make sure performance did not regress. Typically, we do this by 1) measure the baseline performance, 2) measure performance of the modified program and 3) compare them with each other.</p> <p>Here is the exact scenario for this article: compare performance of two different versions of the same <strong>functional</strong> program. For example, we have a program that recursively calculates Fibonacci numbers and we decided to rewrite it in an iterative fashion. Both are functionally correct and yield the same numbers. Now we need to compare two programs for performance.</p> <p>It is very much recommended to get not just a single measurement, but to run the benchmark multiple times. So, we have N measurements for the baseline and N measurements for the modified version of the program. Now we need a way to compare those 2 sets of measurements to decide which one is faster.</p> <p>Don&#x2019;t want to scare you upfront, but it is a harder problem than you might think. Regardless, we know how to compare two scalar numbers, right? So, we can simplify this problem if we aggregate each set of measurements with a single value. Then it will boil down to just comparing 2 numbers. Easy-peasy!</p> <p>If you ask any data scientist, (s)he will tell you that you should <strong>not</strong> rely on a single metric (min/mean/median, etc.). However, the classic statistical methods don&#x2019;t always work well in performance world which makes the problem of automation harder. Likely manual intervention is needed in this case which makes it very time consuming. If you have nightly performance testing in place and multiple benchmarks in a suite, you can&#x2019;t compare them manually every day, so automation is needed. I think that sometimes you can (and should) allow yourself to drive less accurate conclusions from the measurements in order to save your precious time.</p> <p>The easiest way to automate such comparisons is by aggregating each set of measurements and pick one representative value from each. And more often than not we take&#x2026; &#x201C;Average, you say?&#x201D;</p> <p>Let&#x2019;s take a look at example. Suppose we have a set of 3 measurements:</p> <p>Here is what we get if we were to visualize the measurements:</p> <p><img src="/img/posts/ProcessMeas/measurements.jpg" alt class="center-image-width-60"></p> <p>Third measurement is way off but imagine that the network got disconnected during the 3rd iteration and it took a long time to recover.</p> <p>Taking average(mean) would be a bad choice in this situation. Taking median<sup id="fnref:1"><a href="#fn:1" class="footnote">1</a></sup> would be much better. Also taking minimum would be acceptable:</p> <p><img src="/img/posts/ProcessMeas/average-min.jpg" alt class="center-image-width-60"></p> <p>From a statistical point of view we should be collecting more samples, because the <a href="https://en.wikipedia.org/wiki/Standard_deviation">standard deviation</a> is very high (<code class="language-plaintext highlighter-rouge">stddev = 420s</code>). I.e. we should be running our benchmark more times until we get more accurate and repeatable results. However, sometimes you cannot afford it. For example, some of the <a href="http://spec.org/cpu2017/Docs/overview.html#benchmarks">SPEC benchmarks</a> run for more than 10 minutes on a modern machines. That means it takes 1 hour to produce just 3 samples: 30 minutes for each version of the program. Imagine that you have not just a single benchmark in your suite, which may make it very expensive to collect statistically sufficient data.</p> <p>Let me defer explanation on why I think taking average is a bad choice and talk about minimums first. Usually they may give the BEST representative value and it is for a reason. By &#x201C;representative value&#x201D; I mean some value within the measurement range which will yield the most accurate comparison with the other version of a program. Mathematically speaking, we want to pick 2 values from 2 distributions such that the ratio between them would be close to the ratio between <a href="https://www.quora.com/What-is-the-difference-between-mean-and-true-mean-in-statistics">true means</a><sup id="fnref:2"><a href="#fn:2" class="footnote">2</a></sup> from those 2 distributions.</p> <h3 id="closer-look-at-minimum">Closer look at Minimum</h3> <p>Vast majority of compute-bound<sup id="fnref:3"><a href="#fn:3" class="footnote">3</a></sup> applications tend to have right-skewed distribution:</p> <p><img src="/img/posts/ProcessMeas/right-skewed.jpg" alt class="center-image-width-60"></p> <p>And it makes perfect sense. Imagine application runs for some time <code class="language-plaintext highlighter-rouge">t = t1 + t2 + t3 ...</code>, where <code class="language-plaintext highlighter-rouge">t1</code> is the actual time the program needs, <code class="language-plaintext highlighter-rouge">t2</code> is the time the program waited due to a missing entry in the filesystem cache, <code class="language-plaintext highlighter-rouge">t3</code> is the time the program waited due to context switch by the OS, and so on. <code class="language-plaintext highlighter-rouge">t1</code> is always constant in deterministic environment and is the minimum time the program needs when executed on a certain platform. But because there is a lot of non-determinism in modern systems, we have a lot of variables in the equation. Consider the situation when the application we measure, downloads some file from the internet<sup id="fnref:4"><a href="#fn:4" class="footnote">4</a></sup>. It may be very quickly if we were lucky or very slow otherwise.</p> <p>If you look at the chart, you may drive certain guesses from it. Sometimes we were lucky enough to run very fast (minimum). Those were few iterations where no cache misses happened, the process was never context-switched out, etc. But more often, we were not so lucky, and we had some performance &#x201C;road blocks&#x201D; on our way (spike around <code class="language-plaintext highlighter-rouge">102 sec</code>). And also, there were iterations when we had some major performance problems (measurements greater than <code class="language-plaintext highlighter-rouge">110 sec</code>), like, for example, we ran out of memory and OS performed memory swapping to the disk.</p> <p>When we compare performance of two versions of the program, we really want to compare their <code class="language-plaintext highlighter-rouge">t1</code> components and throw away all the other variables. Minimum value from 2 sets of measurements could yield the most accurate comparison in this situation.</p> <p>Now, going back to our first example&#x2026; Remember I wrote that taking average is a bad choice? But why? Mathematically, it is the best choice the one could make if not considering that there could be&#x2026; Outliers. We know the underlying nature of the environment we are working in and it suggests us that the rightmost sample (<code class="language-plaintext highlighter-rouge">1000s</code>) is likely to be an outlier. The issue with the average is that it accounts for outliers which we should be removing first. Minimum is free from taking into account right hand side outliers, which allows to have smaller number of iterations. Median is resistant to both left- and right-hand side outliers, but sometimes does not give the most accurate comparisons.</p> <p>That said, on practice minimum works well for right-skewed distributions, e.g. compute-bound workloads. However, there are cases when minimum can also be a bad choice. For example:<sup id="fnref:5"><a href="#fn:5" class="footnote">5</a></sup></p> <p><img src="/img/posts/ProcessMeas/normal-dist.jpg" alt class="center-image-width-60"></p> <p>In this scenario Minimum is an outlier itself. Dah! I know, I might confused you at this point and you want to ask&#x2026;</p> <h3 id="so-which-one-to-pick">So, which one to pick?</h3> <p>It turns out that there is no right or wrong answer. There is no easy way to compare distributions. And it is not always easy to do. We can only hypothesize that some version of a program is faster than the other, because the measurement set is endless. We can always produce additional sample which could be way faster than all the previous. We can only say that <strong>we think</strong> the version A is faster than version B with some probability P. You see, it gets complicated really quickly&#x2026;</p> <p>So, what do you do? You look at the distribution.</p> <p><img src="/img/posts/ProcessMeas/comp-dist.jpg" alt class="center-image-width-45-no-block"> <img src="/img/posts/ProcessMeas/box-plot.jpg" alt class="center-image-width-35-no-block"></p> <p>By looking at those diagrams we can make conclusions with somewhat high level of confidence. On the left chart we can say that version of the program that corresponds to orange line is faster than the blue one.</p> <p>Box plots are widely used for comparing distributions of many benchmarks on a single chart. We can say that version A is faster than version C and C is faster than B. Usually scientific papers tend to present their results with box plots and let readers drive their own conclusion out of them.</p> <p>That&#x2019;s why it is essential to know the distribution of the benchmark you are working with. If you know characteristics (distribution) for your benchmark, you can do the visualization once, decide on what you will choose as a &#x201C;representative value&#x201D; and then automate it. For example, I work exclusively with compute bound workloads, so I know their distribution is right-skewed, in which case it&#x2019;s okay to take the minimum or median.</p> <h3 id="interestingness-threshold">Interestingness threshold</h3> <p>When comparing two versions of the same program, some fluctuation in performance is inevitable. Imagine, we have a nightly performance testing in place. And quite often you will see performance variations as developers commit their changes. Sometimes small performance degradations can arise even without code changes due to HW instability, I/O, environment, etc. Sometimes even a harmless code change can trigger performance variation of your benchmark. Example of such HW effect can be found in my earlier article <a href="https://easyperf.net/blog/2018/01/18/Code_alignment_issues">Code placement issues</a>.</p> <p>Since usually we cannot do much about instability described above, it would be good to filter somehow the noise from the real regressions resulted from a bad code change. The last thing we want is to be bothered with everyday performance changes of 0.5% up or down. Understanding whether regression comes from a bad code change or from other sources can be quite time consuming, especially if it a small regression (&lt;2%).</p> <p>To deal with such kind of issues we need to define some threshold which we will use to filter small performance variation. For example, if we set the interestingness threshold to 2%, it means that every performance change below it will be considered as noise and can be ignored. Keep in mind, that by using such a criterion, sometimes you can ignore the real performance regressions. This is a business decision to make and is a trade-off. On the one hand you want to save your time and avoid analyzing small regressions that often are just noise. On the other hand, you might miss the bad code change.</p> <p>The actual value of this threshold usually depends on 1) how much you care about performance regressions/improvements and 2) how flaky your benchmark is. For compute bound workloads I see people set their threshold in the range from 2% to 5%.</p> <p>Be sure to track absolute numbers as well, not just ratios. E.g. if you have 4 consecutive 1.5% regressions, they will all be filtered by the interestingness threshold, but they will sum up to 6% over 4 days. You don&#x2019;t want to skip such long-term degradation, so make sure you track the absolute numbers from time to time to see the trend in which performance of your benchmark is going.</p> <h3 id="conclusion-and-practical-advices">Conclusion and practical advices</h3> <p>Let me repeat the caveat in case you jumped directly to conclusions and skipped the whole article. :)</p> <p>If you ask any data scientist, (s)he will tell you that you should <strong>not</strong> rely on a single metric (min/mean/median, etc.). However, the classic statistical methods don&#x2019;t always work well in performance world which makes the problem of automation harder. Likely manual intervention is needed in this case which makes it very time consuming. If you have nightly performance testing in place and multiple benchmarks in a suite, you can&#x2019;t compare them manually every day, so automation is needed. I think sometimes you can (and should) allow yourself to drive less accurate conclusions from the measurements in order to save your precious time. If you decide to go without automation, likely box plot is your best friend here. Alternatively you can compare distributions by representing them with at least 3 numbers: { <a href="https://en.wikipedia.org/wiki/Quartile">Q1</a>; median ; <a href="https://en.wikipedia.org/wiki/Quartile">Q3</a> }.</p> <p>Below are the practical advices. They may not work for every possible scenario, but I hope will work well in practice.</p> <ol> <li>Do the visualization once for your benchmark and see which measurement is the most representative.</li> <li>For compute-bound applications take the minimum if you cannot afford collecting many samples. Take median if you want to be on a safe side (when you think distribution is not 100% right-skewed).</li> <li>If you can afford rich collection of samples (at least 30) it&#x2019;s okay to compare means (averages) but only if the <a href="https://en.wikipedia.org/wiki/Standard_error">standard error</a> is small. You can also adjust the number of iterations dynamically by checking standard error after each collected sample. I.e. you collect samples until you get standard error lie in a certain range.</li> <li>Alternatively, you can fix the number of iterations but go through the process of removing outliers, which I will not discuss in this article. After which you can again consider comparing means. However, you cannot always remove outliers. For example, if you are benchmarking latency of a web service request, you might care a lot about outliers if you have time limits for processing each request.</li> <li>Define the interestingness threshold for your measurements.</li>
</ol> <p>What if you visualized distribution of performance measurements for your benchmark and see something like this?</p> <p><img src="/img/posts/ProcessMeas/bimodal-dist.jpg" alt class="center-image-width-60"></p> <p>If you encountered bimodal distribution, probably there is something wrong with the benchmark. It actually means that you have 2 different types of behavior that occur with equal probability. Consider this situation when it might happen: 1) you want to make 100 runs for the benchmark, 2) first 50 iterations ran fast, 3) starting from iteration #51 someone logged in and started his own experiments which sucked computational resources from your benchmark process.</p> <p>What we want to do in this situation is to isolate those 2 behaviors and test them separately.</p> <h3 id="references">References</h3> </div></div>
</div>
    </div>
    <footer>
        <div>created by buildstarted &copy; 2019 <a href="/about">about</a></div>
        <div>Share this page on social media: copy and paste this url https://linksfor.dev/</div>
    </footer>
    
    <script>
        (function() {
            var COLLECT_URL = "https://dna.buildstarted.com/t";
            var SITE_ID = "linksfor.devs";
            var GLOBAL_VAR_NAME = "__DNA__";

            window[GLOBAL_VAR_NAME] = {};

            window[GLOBAL_VAR_NAME].sendPageView = function() {
                var path = location.pathname;
                var referrer = document.referrer;

                var url = COLLECT_URL + "?siteid=" + SITE_ID + "&p=" + encodeURIComponent(path) + "&r=" + encodeURIComponent(referrer);

                try { fetch(url, { method: "GET" }); } catch (e) {}
            };

            window[GLOBAL_VAR_NAME].sendPageView();
        })();
    </script>
</body>
</html>