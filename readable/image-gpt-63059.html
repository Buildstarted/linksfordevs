<!DOCTYPE html>
<html lang="en">
<head>
    <title>
Image GPT - linksfor.dev(s)    </title>
    <meta charset="utf-8">
    <link rel="alternate" type="application/rss+xml" title="Linksfor.dev(s) feed" href="https://linksfor.dev/feed.rss" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta name="google" value="notranslate">
    <link rel="stylesheet" type="text/css" href="/style.css" />
    <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon">
    <link rel="icon" href="/favicon.ico" type="image/x-icon">
    <meta property="og:title" content="Image GPT - linksfor.dev(s)"/>
    <meta property="og:description" content="We find that, just as a large transformer model trained on language can generate coherent text, the same exact model trained on pixel sequences can generate coherent image completions and samples"/>
    <meta property="og:type" content="website"/>
    <meta property="og:url" content="https://openai.com/blog/image-gpt/"/>

<meta property="og:site_name" content="linksfor.dev(s)" />
</head>
<body>
	<div class="devring" style="background: #222">
		<div class="grid">
			<div style="display: grid; grid-template-columns: .5fr 1fr 1fr 1fr; text-align: center;">
				<span class="devring-title">devring.club</span>
				<a href="https://devring.club/site/1/previous" class="devring-previous">Previous</a>
				<a href="https://devring.club/random" class="devring-random">Random</a>
				<a href="https://devring.club/sites/1/next" class="devring-next">Next</a>
			</div>
		</div>
	</div>
    <div class="grid">
        <h1>
<a href="/" style="color:inherit">linksfor.dev(s)</a>
        </h1>
        <title>linksfor.dev(s) - Image GPT</title>
<div class="readable">
        <h1>Image GPT</h1>
            <div>Reading time: 17-21 minutes</div>
        <div>Posted here: 18 Jun 2020</div>
        <p><a href="https://openai.com/blog/image-gpt/">https://openai.com/blog/image-gpt/</a></p>
        <hr/>
<div id="readability-page-1" class="page"><div>
    
<article id="post-image-gpt">
  
  

  <section>
  <div>
    <section>
      <!--kg-card-begin: markdown-->

<hr>
<h2 id="introduction">Introduction</h2>
<p>Unsupervised and self-supervised learning,<span data-id="lecun-2017-predictive"><sup><a href="#rf1" id="rfref1">1</a></sup></span> or learning without human-labeled data, is a longstanding challenge of machine learning. Recently, it has seen incredible success in language, as transformer<span data-id="vaswani-2017-attention"><sup><a href="#rf2" id="rfref2">2</a></sup></span> models like BERT,<span data-id="devlin-2018-bert"><sup><a href="#rf3" id="rfref3">3</a></sup></span> GPT-2,<span data-id="radford-2019-language"><sup><a href="#rf4" id="rfref4">4</a></sup></span> RoBERTa,<span data-id="liu-2019-roberta"><sup><a href="#rf5" id="rfref5">5</a></sup></span> T5,<span data-id="raffel-2019-exploring"><sup><a href="#rf6" id="rfref6">6</a></sup></span> and other variants<span data-id="dai-2015-semi"><sup><a href="#rf7" id="rfref7">7</a></sup></span><span data-id="peters-2018-deep"><sup><a href="#rf8" id="rfref8">8</a></sup></span><span data-id="howard-2018-universal"><sup><a href="#rf9" id="rfref9">9</a></sup></span><span data-id="radford-2018-improving"><sup><a href="#rf10" id="rfref10">10</a></sup></span> have achieved top performance on a wide array of language tasks. However, the same broad class of models has not been successful in producing strong features for image classification.<span data-id="ke-2018-sparse"><sup><a href="#rf11" id="rfref11">11</a></sup></span> Our work aims to understand and bridge this&nbsp;gap.</p>
<p>Transformer models like BERT and GPT-2 are domain agnostic, meaning that they can be directly applied to 1-D sequences of any form. When we train GPT-2 on images unrolled into long sequences of pixels, which we call iGPT, we find that the model appears to understand 2-D image characteristics such as object appearance and category. This is evidenced by the diverse range of coherent image samples it generates, even without the guidance of human provided labels. As further proof, features from the model achieve state-of-the-art performance on a number of classification datasets and near state-of-the-art unsupervised accuracy<sup><a href="#fn1" id="fnref1">[1]</a></sup> on&nbsp;ImageNet.</p>
<!-- table: results summary -->
<table>
<thead>
  <tr>
    <th>Evaluation</th>
    <th>Dataset</th>
    <th>Our Result</th>
    <th>Best non-<span>iGPT</span> Result</th>
  </tr>
</thead>
<tbody>
  <tr>
    <td rowspan="4">Logistic regression on learned features (linear&nbsp;probe)</td>
    <td>CIFAR-10</td>
    <td><div><div><p><strong>96.3</strong></p><p>iGPT-L 32x32 w/ 1536 features</p></div></div></td>
    <td><div><div><p>95.3</p><p>SimCLR<span data-id="chen-2020-simple"><sup><a href="#rf12" id="rfref12a">12</a></sup></span> w/ 8192 features</p></div></div></td>
  </tr>
  <tr>
    <td>CIFAR-100</td>
    <td><div><div><p><strong>82.8</strong></p><p>iGPT-L 32x32 w/ 1536 features</p></div></div></td>
    <td><div><div><p>80.2</p><p>SimCLR w/ 8192 features</p></div></div></td>
  </tr>
  <tr>
    <td>STL-10</td>
    <td><div><div><p><strong>95.5</strong></p><p>iGPT-L 32x32 w/ 1536 features</p></div></div></td>
    <td><div><div><p>94.2</p><p>AMDIM<span data-id="bachman-2019-learning"><sup><a href="#rf13" id="rfref13a">13</a></sup></span> w/ 8192 features</p></div></div></td>
  </tr>
  <tr>
    <td>ImageNet</td>
    <td><div><div><p>72.0</p><p>iGPT-XL<sup><a href="#igpt-xl">a</a></sup> 64x64 w/ 15360 features</p></div></div></td>
    <td><div><div><p><strong>76.5</strong></p><p>SimCLR w/ 8192 features</p></div></div></td>
  </tr>
  <tr>
    <td rowspan="3">Full fine-tune</td>
    <td>CIFAR-10</td>
    <td><div><div><p><strong>99.0</strong></p><p>iGPT-L 32x32, trained on ImageNet</p></div></div></td>
    <td><div><div><p><strong>99.0</strong><sup><a href="#bit-l">b</a></sup></p><p>GPipe,<span data-id="huang-2019-gpipe"><sup><a href="#rf15" id="rfref15a">15</a></sup></span> trained on ImageNet</p></div></div></td>
  </tr>
  <tr>
    <td>ImageNet 32x32</td>
    <td><div><div><p>66.3</p><p>iGPT-L 32x32</p></div></div></td>
    <td><div><div><p><strong>70.2</strong></p><p>Isometric Nets<span data-id="sandler-2019-nondiscriminative"><sup><a href="#rf16" id="rfref16">16</a></sup></span></p></div></div></td>
  </tr>
</tbody>
</table>
<div>
  <ol>
    <li id="igpt-xl">We only show ImageNet linear probe accuracy for iGPT-XL since other experiments did not finish before we needed to transition to different supercomputing&nbsp;facilities.</li>
    <li id="bit-l">Bit-L,<span data-id="kolesnikov-2019-big"><sup><a href="#rf14" id="rfref14">14</a></sup></span> trained on JFT (300M images with 18K classes), achieved a result of <span>99.3</span>.</li>
  </ol>
</div>
<!-- end table: results summary -->
<p>To highlight the potential of generative<span data-id="lasserre-2006-principled"><sup><a href="#rf17" id="rfref17">17</a></sup></span><span data-id="erhan-2010-why"><sup><a href="#rf18" id="rfref18">18</a></sup></span> sequence modeling<span data-id="elman-1990-finding"><sup><a href="#rf19" id="rfref19">19</a></sup></span><span data-id="mikolov-2010-recurrent"><sup><a href="#rf20" id="rfref20">20</a></sup></span><span data-id="larochelle-2011-neural"><sup><a href="#rf21" id="rfref21">21</a></sup></span><span data-id="graves-2013-generating"><sup><a href="#rf22" id="rfref22">22</a></sup></span> as a general purpose unsupervised learning algorithm, we deliberately use the same transformer architecture as GPT-2 in language. As a consequence, we require significantly more compute in order to produce features competitive with those from top unsupervised convolutional nets.<span data-id="bachman-2019-learning"><sup><a href="#rf13" id="rfref13b">13</a></sup></span><span data-id="tian-2019-contrastive"><sup><a href="#rf23" id="rfref23a">23</a></sup></span><span data-id="he-2019-momentum"><sup><a href="#rf24" id="rfref24a">24</a></sup></span><span data-id="henaff-2019-data"><sup><a href="#rf25" id="rfref25a">25</a></sup></span><span data-id="chen-2020-simple"><sup><a href="#rf12" id="rfref12b">12</a></sup></span> However, our results suggest that when faced with a new domain where the correct model priors are unknown, a large GPT-2 can learn excellent features without the need for domain-specific<span data-id="oord-2016-pixel"><sup><a href="#rf26" id="rfref26">26</a></sup></span><span data-id="parmar-2018-image"><sup><a href="#rf27" id="rfref27a">27</a></sup></span><span data-id="menick-2018-generating"><sup><a href="#rf28" id="rfref28">28</a></sup></span> architectural design&nbsp;choices.</p>
<!-- start #samples-wrap -->
<div id="samples-wrap" data-theme="dark"><div><div><div>
<h2 id="completions">Completions</h2>
<div>
  <!-- categories -->
  
  <!-- legend -->
  
  
  <!-- aside caption -->
  
  <!-- completions -->
  
  <!-- inline caption -->
  <p>Model-generated completions of human-provided half-images. We sample the remaining halves with temperature 1 and without tricks like beam search or nucleus sampling. While we showcase our favorite completions in the first panel, we do not cherry-pick images or completions in all following panels.</p>
</div>
<h2 id="samples">Samples</h2>
<div>
  <!-- aside caption -->
  
  <!-- samples -->
  
  <!-- view more samples -->
  
  <!-- inline caption -->
  <p>Model-generated image samples. We sample these images with temperature 1 and without tricks like beam search or nucleus sampling. All of our samples are shown, with no cherry-picking. Nearly all generated images contain clearly recognizable objects.</p>
</div>
</div></div><!-- end .content .row -->
<!-- scale and theme switcher -->

</div><!-- end .container -->
</div><!-- end #samples-wrap -->
<h2 id="fromlanguagegpttoimagegpt">From language GPT to image GPT</h2>
<p>In language, unsupervised learning algorithms that rely on word prediction (like GPT-2 and BERT) have been extremely successful, achieving top performance on a wide array of language tasks. One possible reason for this success is that instances of downstream language tasks appear naturally in text: questions are often followed by answers (which could help with question-answering) and passages are often followed by summaries (which could help with summarization). In contrast, sequences of pixels do not clearly contain labels for the images they belong&nbsp;to.</p>
<p>Even without this explicit supervision, there is still a reason why GPT-2 on images might work: a sufficiently large transformer trained on next pixel prediction might eventually learn to generate diverse<sup><a href="#fn2" id="fnref2">[2]</a></sup> samples with clearly recognizable objects. Once it learns to do so, an idea known as “Analysis by Synthesis”<span data-id="mumford-1992-computational"><sup><a href="#rf29" id="rfref29">29</a></sup></span><span data-id="rao-1999-predictive"><sup><a href="#rf30" id="rfref30">30</a></sup></span><sup><a href="#fn3" id="fnref3">[3]</a></sup> suggests that the model will also know about object categories. Many early generative models<span data-id="smolensky-1986-information"><sup><a href="#rf31" id="rfref31">31</a></sup></span><span data-id="hinton-2002-training"><sup><a href="#rf32" id="rfref32">32</a></sup></span><span data-id="hinton-2006-fast"><sup><a href="#rf33" id="rfref33">33</a></sup></span><span data-id="vincent-2008-extracting"><sup><a href="#rf34" id="rfref34">34</a></sup></span><span data-id="coates-2011-analysis"><sup><a href="#rf35" id="rfref35">35</a></sup></span><span data-id="le-2012-building"><sup><a href="#rf36" id="rfref36">36</a></sup></span> were motivated by this idea, and more recently, BigBiGAN<span data-id="donahue-2019-large"><sup><a href="#rf37" id="rfref37a">37</a></sup></span> was an example which produced encouraging samples and features. In our work, we first show that better generative models achieve stronger classification performance. Then, through optimizing GPT-2 for generative capabilities, we achieve top-level classification performance in many settings, providing further evidence for analysis by&nbsp;synthesis.</p>
<h2 id="towardsgeneralunsupervisedlearning">Towards general unsupervised learning</h2>
<p>Generative sequence modeling is a universal unsupervised learning algorithm: since all data types can be represented as sequences of bytes, a transformer can be directly applied to any data type without additional engineering. Our work tests the power of this generality by directly applying the architecture used to train GPT-2 on natural language to image generation. We deliberately chose to forgo hand coding any image specific knowledge in the form of convolutions<span data-id="ciresan-2010-deep"><sup><a href="#rf38" id="rfref38">38</a></sup></span> or techniques like relative attention,<span data-id="shaw-2018-self"><sup><a href="#rf39" id="rfref39">39</a></sup></span> sparse attention,<span data-id="child-2019-generating"><sup><a href="#rf40" id="rfref40">40</a></sup></span> and 2-D position embeddings.<span data-id="parmar-2018-image"><sup><a href="#rf27" id="rfref27b">27</a></sup></span></p>
<p>As a consequence of its generality, our method requires significantly more compute to achieve competitive performance in the unsupervised setting. Indeed, contrastive methods<span data-id="becker-1991-self"><sup><a href="#rf41" id="rfref41">41</a></sup></span><span data-id="bromley-1994-signature"><sup><a href="#rf42" id="rfref42">42</a></sup></span><span data-id="mikolov-2013-distributed"><sup><a href="#rf43" id="rfref43">43</a></sup></span><span data-id="oord-2018-representation"><sup><a href="#rf44" id="rfref44">44</a></sup></span><span data-id="hjelm-2018-learning"><sup><a href="#rf45" id="rfref45">45</a></sup></span><span data-id="bachman-2019-learning"><sup><a href="#rf13" id="rfref13c">13</a></sup></span><span data-id="tian-2019-contrastive"><sup><a href="#rf23" id="rfref23b">23</a></sup></span><span data-id="he-2019-momentum"><sup><a href="#rf24" id="rfref24b">24</a></sup></span><span data-id="henaff-2019-data"><sup><a href="#rf25" id="rfref25b">25</a></sup></span><span data-id="chen-2020-simple"><sup><a href="#rf12" id="rfref12c">12</a></sup></span> are still the most computationally efficient methods for producing high quality features from images. However, in showing that an unsupervised transformer model is competitive with the best unsupervised convolutional nets,<span data-id="he-2019-momentum"><sup><a href="#rf24" id="rfref24c">24</a></sup></span><span data-id="henaff-2019-data"><sup><a href="#rf25" id="rfref25c">25</a></sup></span><span data-id="chen-2020-simple"><sup><a href="#rf12" id="rfref12d">12</a></sup></span> we provide evidence that it is possible to trade off hand coded domain knowledge for compute. In new domains,<span data-id="alley-2019-unified"><sup><a href="#rf46" id="rfref46">46</a></sup></span><span data-id="rives-2019-biological"><sup><a href="#rf47" id="rfref47">47</a></sup></span> where there isn’t much knowledge to hand code, scaling compute seems an appropriate technique to&nbsp;test.</p>
<h2 id="approach">Approach</h2>
<p>We train iGPT-S, iGPT-M, and iGPT-L, transformers containing 76M, 455M, and 1.4B parameters respectively, on ImageNet. We also train iGPT-XL<sup><a href="#fn4" id="fnref4">[4]</a></sup>, a 6.8 billion parameter transformer, on a mix of ImageNet and images from the web. Due to the large computational cost of modeling long sequences with dense attention, we train at the low resolutions of 32x32, 48x48, and&nbsp;64x64.</p>
<p>While it is tempting to work at even lower resolutions to further reduce compute cost, prior work has demonstrated that human performance on image classification begins to drop rapidly below these sizes.<span data-id="torralba-2008-80"><sup><a href="#rf48" id="rfref48">48</a></sup></span> Instead, motivated by early color display palettes,<span data-id="wiki-8bit"><sup><a href="#rf49" id="rfref49">49</a></sup></span> we create our own 9-bit color palette to represent pixels. Using this palette yields an input sequence length 3 times shorter than the standard (R, G, B) palette, while still encoding color&nbsp;faithfully.</p>
<h2 id="experimentalresults">Experimental results</h2>
<p>There are two methods we use to assess model performance, both of which involve a downstream classification task. The first, which we refer to as a linear probe, uses the trained model to extract features<sup><a href="#fn5" id="fnref5">[5]</a></sup> from the images in the downstream dataset, and then fits a logistic regression to the labels. The second method fine-tunes<sup><a href="#fn6" id="fnref6">[6]</a></sup> the entire model on the downstream&nbsp;dataset.</p>
<p>Since next pixel prediction is not obviously relevant to image classification, features from the final layer may not be the most predictive of the object category. Our first result shows that feature quality is a sharply increasing, then mildly decreasing function of depth. This behavior suggests that a transformer generative model operates in two phases: in the first phase, each position gathers information from its surrounding context in order to build a contextualized image feature. In the second phase, this contextualized feature is used to solve the conditional next pixel prediction task.  The observed two stage performance of our linear probes is reminiscent of another unsupervised neural net, the bottleneck autoencoder, which is manually designed so that features in the middle are&nbsp;used.</p>
<div>
<p id="chart-depth"><svg viewBox="0,0,720,475"><g><clipPath id="clip"><rect x="0" y="70" width="720" height="360"></rect></clipPath></g><g transform="translate(0,430)" fill="none" font-size="10" font-family="sans-serif" text-anchor="middle"><g opacity="1" transform="translate(98.20212765957447,0)"><line stroke="currentColor" y2="6"></line><text fill="currentColor" y="9" dy="0.71em">5</text></g><g opacity="1" transform="translate(170.32978723404256,0)"><line stroke="currentColor" y2="6"></line><text fill="currentColor" y="9" dy="0.71em">10</text></g><g opacity="1" transform="translate(242.45744680851064,0)"><line stroke="currentColor" y2="6"></line><text fill="currentColor" y="9" dy="0.71em">15</text></g><g opacity="1" transform="translate(314.5851063829787,0)"><line stroke="currentColor" y2="6"></line><text fill="currentColor" y="9" dy="0.71em">20</text></g><g opacity="1" transform="translate(386.7127659574468,0)"><line stroke="currentColor" y2="6"></line><text fill="currentColor" y="9" dy="0.71em">25</text></g><g opacity="1" transform="translate(458.8404255319149,0)"><line stroke="currentColor" y2="6"></line><text fill="currentColor" y="9" dy="0.71em">30</text></g><g opacity="1" transform="translate(530.968085106383,0)"><line stroke="currentColor" y2="6"></line><text fill="currentColor" y="9" dy="0.71em">35</text></g><g opacity="1" transform="translate(603.0957446808511,0)"><line stroke="currentColor" y2="6"></line><text fill="currentColor" y="9" dy="0.71em">40</text></g><g opacity="1" transform="translate(675.2234042553191,0)"><line stroke="currentColor" y2="6"></line><text fill="currentColor" y="9" dy="0.71em">45</text></g><text x="360" dy="2.6em" fill="currentColor" text-anchor="middle">Layer</text></g><g transform="translate(0,0)" fill="none" font-size="10" font-family="sans-serif" text-anchor="start"><g opacity="1" transform="translate(0,430.5)"><line stroke="currentColor" x2="720"></line><text fill="currentColor" x="0" dy="-0.3em">0%</text></g><g opacity="1" transform="translate(0,358.5)"><line stroke="#c5c5d2" x2="720" stroke-opacity="0.2"></line><text fill="currentColor" x="0" dy="-0.3em">20%</text></g><g opacity="1" transform="translate(0,286.5)"><line stroke="#c5c5d2" x2="720" stroke-opacity="0.2"></line><text fill="currentColor" x="0" dy="-0.3em">40%</text></g><g opacity="1" transform="translate(0,214.5)"><line stroke="#c5c5d2" x2="720" stroke-opacity="0.2"></line><text fill="currentColor" x="0" dy="-0.3em">60%</text></g><g opacity="1" transform="translate(0,142.5)"><line stroke="#c5c5d2" x2="720" stroke-opacity="0.2"></line><text fill="currentColor" x="0" dy="-0.3em">80%</text></g><g opacity="1" transform="translate(0,70.5)"><line stroke="#c5c5d2" x2="720" stroke-opacity="0.2"></line><text fill="currentColor" x="0" dy="-0.3em">100%</text><text fill="currentColor" x="0" dy="-1.8em">Linear Probe Accuracy</text></g></g><g clip-path="url(#clip)"><path d="M40,308.716L54.42553191489361,262.132L68.85106382978724,228.22000000000003L83.27659574468085,204.46L97.70212765957447,180.88000000000002L112.12765957446808,160.54L126.5531914893617,147.652L140.97872340425533,138.76L155.40425531914894,131.452L169.82978723404256,122.27199999999999L184.25531914893617,114.74799999999999L198.68085106382978,111.03999999999999L213.1063829787234,104.23599999999999L227.53191489361703,98.332L241.95744680851064,94.76799999999999L256.3829787234043,90.01599999999999L270.8085106382979,88.32399999999997L285.2340425531915,85.22800000000001L299.6595744680851,84.22L314.0851063829787,83.464L328.51063829787233,83.28400000000002L342.93617021276594,83.32000000000001L357.36170212765956,83.536L371.78723404255317,83.68000000000002L386.2127659574468,83.96799999999999L400.63829787234044,84.00400000000002L415.06382978723406,83.71600000000001L429.48936170212767,84.184L443.9148936170213,84.32799999999997L458.3404255319149,84.32799999999997L472.76595744680856,84.58L487.19148936170205,85.04799999999997L501.6170212765957,85.51600000000002L516.0425531914893,85.51600000000002L530.468085106383,85.696L544.8936170212766,86.09199999999998L559.3191489361702,86.12799999999999L573.7446808510639,86.416L588.1702127659574,87.28000000000002L602.5957446808511,87.49599999999998L617.0212765957447,88.18L631.4468085106383,88.46800000000002L645.8723404255319,88.79200000000002L660.2978723404254,89.25999999999999L674.7234042553191,89.72799999999998L689.1489361702128,90.52000000000001L703.5744680851063,91.09599999999999L718,92.03200000000001" data-name="CIFAR-10" fill="none" stroke="#00B3F5" stroke-width="2" stroke-linejoin="round" stroke-linecap="round"></path><path d="M40,380.752L54.42553191489361,352.74399999999997L68.85106382978724,314.98L83.27659574468085,291.97600000000006L97.70212765957447,264.29200000000003L112.12765957446808,248.16400000000002L126.5531914893617,232.32399999999998L140.97872340425533,220.48000000000002L155.40425531914894,210.50799999999998L169.82978723404256,200.86L184.25531914893617,190.06L198.68085106382978,180.44799999999998L213.1063829787234,167.308L227.53191489361703,160.93600000000004L241.95744680851064,153.51999999999998L256.3829787234043,144.772L270.8085106382979,139.51600000000002L285.2340425531915,136.096L299.6595744680851,134.116L314.0851063829787,131.84799999999998L328.51063829787233,133.50400000000002L342.93617021276594,133.68399999999997L357.36170212765956,135.052L371.78723404255317,136.56399999999996L386.2127659574468,136.132L400.63829787234044,137.284L415.06382978723406,137.608L429.48936170212767,137.968L443.9148936170213,138.292L458.3404255319149,138.54399999999998L472.76595744680856,140.02L487.19148936170205,140.164L501.6170212765957,140.38L516.0425531914893,141.82L530.468085106383,142.32399999999998L544.8936170212766,143.728L559.3191489361702,145.492L573.7446808510639,146.14000000000001L588.1702127659574,147.22000000000003L602.5957446808511,148.83999999999997L617.0212765957447,149.74L631.4468085106383,151.82799999999997L645.8723404255319,153.196L660.2978723404254,155.03199999999998L674.7234042553191,157.66000000000003L689.1489361702128,159.60399999999998L703.5744680851063,162.59199999999998L718,165.04000000000002" data-name="CIFAR-100" fill="none" stroke="#EC5D2A" stroke-width="2" stroke-linejoin="round" stroke-linecap="round"></path><path d="M40,301.79499999999996L54.42553191489361,268.675L68.85106382978724,236.275L83.27659574468085,220.25500000000002L97.70212765957447,200.95L112.12765957446808,189.79L126.5531914893617,178.36L140.97872340425533,171.385L155.40425531914894,162.79000000000002L169.82978723404256,155.90499999999997L184.25531914893617,145.15L198.68085106382978,135.25L213.1063829787234,123.95499999999998L227.53191489361703,115.72L241.95744680851064,110.50000000000003L256.3829787234043,99.97000000000003L270.8085106382979,95.11L285.2340425531915,90.11500000000001L299.6595744680851,88.18L314.0851063829787,86.155L328.51063829787233,86.02L342.93617021276594,86.56000000000002L357.36170212765956,86.96499999999999L371.78723404255317,87.68499999999999L386.2127659574468,87.37L400.63829787234044,87.50500000000001L415.06382978723406,87.54999999999998L429.48936170212767,87.90999999999998L443.9148936170213,88.045L458.3404255319149,88.40499999999999L472.76595744680856,88.62999999999998L487.19148936170205,88.85499999999999L501.6170212765957,88.36000000000001L516.0425531914893,88.98999999999998L530.468085106383,89.89L544.8936170212766,90.11500000000001L559.3191489361702,90.29499999999999L573.7446808510639,90.52000000000001L588.1702127659574,90.925L602.5957446808511,91.555L617.0212765957447,92.13999999999999L631.4468085106383,93.22L645.8723404255319,94.25499999999998L660.2978723404254,95.60499999999999L674.7234042553191,96.82L689.1489361702128,97.85499999999999L703.5744680851063,98.98L718,102.08500000000001" data-name="STL-10" fill="none" stroke="#50DE66" stroke-width="2" stroke-linejoin="round" stroke-linecap="round"></path></g><g><text paint-order="stroke" stroke="#fff" stroke-width="1.5" fill="#00A3DF" x="617.0212765957447" y="88.18" alignment-baseline="middle" data-orient="t" text-anchor="middle" dx="0" dy="-0.8em">CIFAR-10</text><text paint-order="stroke" stroke="#fff" stroke-width="1.5" fill="#E2521E" x="617.0212765957447" y="149.74" alignment-baseline="middle" data-orient="bl" text-anchor="end" dx="-0.4em" dy="0.8em">CIFAR-100</text><text paint-order="stroke" stroke="#fff" stroke-width="1.5" fill="#00C21E" x="617.0212765957447" y="92.13999999999999" alignment-baseline="middle" data-orient="bl" text-anchor="end" dx="-0.4em" dy="0.8em">STL-10</text></g><defs><marker id="arrowhead" markerWidth="8" markerHeight="5" refX="7" refY="2.5" orient="auto"><polygon points="0 0, 8 2.5, 0 5" fill="currentColor"></polygon></marker></defs><g><text fill="currentColor" x="364.0851063829787" y="30.464" alignment-baseline="middle" data-orient="r" text-anchor="start" dx="0.5em" dy="0" style="cursor: default;"><tspan text-anchor="start" alignment-baseline="middle" x="364.0851063829787" y="30.464" dx="0.5em" dy="-0.55em">The best features lie in the</tspan><tspan text-anchor="start" alignment-baseline="middle" x="364.0851063829787" y="30.464" dx="0.5em" dy="0.55em">middle of the network</tspan></text><path d="M364.0851063829787, 30.464
    Q314.0851063829787, 30.464000000000002
    314.0851063829787, 80.464" fill="none" stroke="currentColor" stroke-width="1" stroke-linecap="round" marker-end="url(#arrowhead)"></path></g></svg></p>
<p>Feature quality depends heavily on the layer we choose to evaluate. In contrast with supervised models, the best features for these generative models lie in the middle of the network.</p>
</div>
<p>Our next result establishes the link between generative performance and feature quality. We find that both increasing the scale of our models and training for more iterations result in better generative performance, which directly translates into better feature&nbsp;quality.</p>
<div>

<p id="chart-val"><svg viewBox="0,0,720,445" style="overflow: visible;"><g transform="translate(0,400)" fill="none" font-size="10" font-family="sans-serif" text-anchor="middle"><g opacity="1" transform="translate(575.8381343312898,0)"><line stroke="currentColor" y2="6"></line><text fill="currentColor" y="9" dy="0.71em">2.06</text></g><g opacity="1" transform="translate(406.19986220277696,0)"><line stroke="currentColor" y2="6"></line><text fill="currentColor" y="9" dy="0.71em">2.08</text></g><g opacity="1" transform="translate(236.56159007426425,0)"><line stroke="currentColor" y2="6"></line><text fill="currentColor" y="9" dy="0.71em">2.10</text></g><g opacity="1" transform="translate(66.9233179457515,0)"><line stroke="currentColor" y2="6"></line><text fill="currentColor" y="9" dy="0.71em">2.12</text></g><text x="360" dy="2.6em" fill="currentColor" text-anchor="middle">Validation Loss</text></g><g transform="translate(0,0)" fill="none" font-size="10" font-family="sans-serif" text-anchor="start"><g opacity="1" transform="translate(0,400.5)"><line stroke="currentColor" x2="720"></line><text fill="currentColor" x="0" dy="-0.3em">87%</text></g><g opacity="1" transform="translate(0,328.49999999999994)"><line stroke="#c5c5d2" x2="720" stroke-opacity="0.2"></line><text fill="currentColor" x="0" dy="-0.3em">89%</text></g><g opacity="1" transform="translate(0,256.49999999999983)"><line stroke="#c5c5d2" x2="720" stroke-opacity="0.2"></line><text fill="currentColor" x="0" dy="-0.3em">91%</text></g><g opacity="1" transform="translate(0,184.49999999999977)"><line stroke="#c5c5d2" x2="720" stroke-opacity="0.2"></line><text fill="currentColor" x="0" dy="-0.3em">93%</text></g><g opacity="1" transform="translate(0,112.50000000000006)"><line stroke="#c5c5d2" x2="720" stroke-opacity="0.2"></line><text fill="currentColor" x="0" dy="-0.3em">95%</text></g><g opacity="1" transform="translate(0,40.5)"><line stroke="#c5c5d2" x2="720" stroke-opacity="0.2"></line><text fill="currentColor" x="0" dy="-0.3em">97%</text><text fill="currentColor" x="0" dy="-1.8em">CIFAR-10 Linear Probe Accuracy</text></g></g><g><path d="M62,381.28000000000003L158.78033063203992,331.23999999999995L253.96690969743042,276.88L324.55339473010207,270.0399999999999" data-size="s" data-hover="0" fill="none" stroke="#00B3F5" stroke-width="2" stroke-linejoin="round" stroke-linecap="round"></path><path d="M229.99702184566976,241.23999999999987L361.4861911465624,174.28000000000003L481.9675329690349,128.91999999999982L563.1021257626613,105.51999999999998" data-size="m" data-hover="0" fill="none" stroke="#EC5D2A" stroke-width="2" stroke-linejoin="round" stroke-linecap="round"></path><path d="M366.59399952035096,150.51999999999987L518.0936994015221,103.35999999999981L639.751482641209,72.75999999999999L688,64.84000000000006" data-size="l" data-hover="0" fill="none" stroke="#50DE66" stroke-width="2" stroke-linejoin="round" stroke-linecap="round"></path></g><g><g fill="#00B3F5" stroke="transparent" stroke-width="1" data-size="s"><circle r="3.5" data-hover="0" data-iteration="0" cx="62" cy="381.28000000000003"></circle><circle r="3.5" data-hover="0" data-iteration="1" cx="158.78033063203992" cy="331.23999999999995"></circle><circle r="3.5" data-hover="0" data-iteration="2" cx="253.96690969743042" cy="276.88"></circle><circle r="3.5" data-hover="0" data-iteration="3" cx="324.55339473010207" cy="270.0399999999999"></circle></g><g fill="#EC5D2A" stroke="transparent" stroke-width="1" data-size="m"><circle r="3.5" data-hover="0" data-iteration="0" cx="229.99702184566976" cy="241.23999999999987"></circle><circle r="3.5" data-hover="0" data-iteration="1" cx="361.4861911465624" cy="174.28000000000003"></circle><circle r="3.5" data-hover="0" data-iteration="2" cx="481.9675329690349" cy="128.91999999999982"></circle><circle r="3.5" data-hover="0" data-iteration="3" cx="563.1021257626613" cy="105.51999999999998"></circle></g><g fill="#50DE66" stroke="transparent" stroke-width="1" data-size="l"><circle r="3.5" data-hover="0" data-iteration="0" cx="366.59399952035096" cy="150.51999999999987"></circle><circle r="3.5" data-hover="0" data-iteration="1" cx="518.0936994015221" cy="103.35999999999981"></circle><circle r="3.5" data-hover="0" data-iteration="2" cx="639.751482641209" cy="72.75999999999999"></circle><circle r="3.5" data-hover="0" data-iteration="3" cx="688" cy="64.84000000000006"></circle></g></g><g><text paint-order="stroke" stroke="#fff" stroke-width="1.5" fill="#00A3DF" x="158.78033063203992" y="331.23999999999995" alignment-baseline="middle" data-orient="br" text-anchor="start" dx="0.4em" dy="1.2em">iGPT-S</text><text paint-order="stroke" stroke="#fff" stroke-width="1.5" fill="#E2521E" x="361.4861911465624" y="174.28000000000003" alignment-baseline="middle" data-orient="br" text-anchor="start" dx="0.4em" dy="1.2em">iGPT-M</text><text paint-order="stroke" stroke="#fff" stroke-width="1.5" fill="#00C21E" x="639.751482641209" y="72.75999999999999" alignment-baseline="middle" data-orient="tl" text-anchor="end" dx="-0.4em" dy="-1em">iGPT-L</text></g><defs><filter id="shadow"><feDropShadow dx="0" dy="0" stdDeviation="3" flood-color="#050526" flood-opacity="0.1"></feDropShadow></filter></defs><g><image xlink:href="https://cdn.openai.com/image-gpt/validation-loss/igpt-s-ckpt131000.png" width="64" height="64" transform="translate(30, 306.78000000000003)" data-iteration="0" data-size="s" data-hover="0" style="filter: url(&quot;#shadow&quot;); cursor: default;"></image><image xlink:href="https://cdn.openai.com/image-gpt/validation-loss/igpt-s-ckpt262000.png" width="64" height="64" transform="translate(126.78033063203992, 256.73999999999995)" data-iteration="1" data-size="s" data-hover="0" style="filter: url(&quot;#shadow&quot;); cursor: default;"></image><image xlink:href="https://cdn.openai.com/image-gpt/validation-loss/igpt-s-ckpt524000.png" width="64" height="64" transform="translate(221.96690969743042, 287.38)" data-iteration="2" data-size="s" data-hover="0" style="filter: url(&quot;#shadow&quot;); cursor: default;"></image><image xlink:href="https://cdn.openai.com/image-gpt/validation-loss/igpt-s-ckpt1000000.png" width="64" height="64" transform="translate(292.55339473010207, 280.5399999999999)" data-iteration="3" data-size="s" data-hover="0" style="filter: url(&quot;#shadow&quot;); cursor: default;"></image><image xlink:href="https://cdn.openai.com/image-gpt/validation-loss/igpt-m-ckpt131000.png" width="64" height="64" transform="translate(197.99702184566976, 166.73999999999987)" data-iteration="0" data-size="m" data-hover="0" style="filter: url(&quot;#shadow&quot;); cursor: default;"></image><image xlink:href="https://cdn.openai.com/image-gpt/validation-loss/igpt-m-ckpt262000.png" width="64" height="64" transform="translate(329.4861911465624, 184.78000000000003)" data-iteration="1" data-size="m" data-hover="0" style="filter: url(&quot;#shadow&quot;); cursor: default;"></image><image xlink:href="https://cdn.openai.com/image-gpt/validation-loss/igpt-m-ckpt524000.png" width="64" height="64" transform="translate(449.9675329690349, 139.41999999999982)" data-iteration="2" data-size="m" data-hover="0" style="filter: url(&quot;#shadow&quot;); cursor: default;"></image><image xlink:href="https://cdn.openai.com/image-gpt/validation-loss/igpt-m-ckpt1000000.png" width="64" height="64" transform="translate(531.1021257626613, 116.01999999999998)" data-iteration="3" data-size="m" data-hover="0" style="filter: url(&quot;#shadow&quot;); cursor: default;"></image><image xlink:href="https://cdn.openai.com/image-gpt/validation-loss/igpt-l-ckpt131000.png" width="64" height="64" transform="translate(334.59399952035096, 76.01999999999987)" data-iteration="0" data-size="l" data-hover="0" style="filter: url(&quot;#shadow&quot;); cursor: default;"></image><image xlink:href="https://cdn.openai.com/image-gpt/validation-loss/igpt-l-ckpt262000.png" width="64" height="64" transform="translate(486.09369940152214, 28.859999999999815)" data-iteration="1" data-size="l" data-hover="0" style="filter: url(&quot;#shadow&quot;); cursor: default;"></image><image xlink:href="https://cdn.openai.com/image-gpt/validation-loss/igpt-l-ckpt524000.png" width="64" height="64" transform="translate(607.751482641209, 83.25999999999999)" data-iteration="2" data-size="l" data-hover="0" style="filter: url(&quot;#shadow&quot;); cursor: default;"></image><image xlink:href="https://cdn.openai.com/image-gpt/validation-loss/igpt-l-ckpt1000000.png" width="64" height="64" transform="translate(656, 75.34000000000006)" data-iteration="3" data-size="l" data-hover="0" style="filter: url(&quot;#shadow&quot;); cursor: default;"></image></g><g><path stroke="transparent" fill="none" d="M0,400L0,142.75886695024082L133.00581031026644,400L0,400Z"></path><path stroke="transparent" fill="none" d="M261.1638905075451,400L133.0058103102664,400L0,142.75886695024084L0,132.42090743190238L197.68760306323836,288.8504294776497L261.1638905075451,400Z"></path><path stroke="transparent" fill="none" d="M285.05743169108297,230.08934659782435L301.5221822553442,400L261.1638905075451,400L197.68760306323836,288.8504294776497L285.05743169108297,230.08934659782435Z"></path><path stroke="transparent" fill="none" d="M301.5221822553442,400L285.05743169108297,230.08934659782435L293.3117900775632,202.98857612859632L466.17467875072305,269.658478753148L583.0243828760065,400L301.5221822553442,400Z"></path><path stroke="transparent" fill="none" d="M0,40L194.76881601719447,40L261.4065500554704,140.33634335006306L293.3117900775632,202.98857612859632L285.05743169108297,230.08934659782435L197.68760306323836,288.8504294776497L0,132.42090743190238L0,40Z"></path><path stroke="transparent" fill="none" d="M431.2311209837213,176.84439745201428L466.17467875072305,269.658478753148L293.3117900775632,202.98857612859632L261.4065500554704,140.33634335006306L431.2311209837213,176.84439745201428Z"></path><path stroke="transparent" fill="none" d="M431.2311209837213,176.84439745201428L405.6113891734079,40L446.1600123492211,40L537.4952000654806,169.09194806651232L604.0913118989034,400L583.0243828760065,400L466.17467875072305,269.658478753148L431.2311209837213,176.84439745201428Z"></path><path stroke="transparent" fill="none" d="M604.0913118989034,400L537.4952000654806,169.09194806651232L543.6904534961681,40L580.4243266136261,40L708.2365397904439,339.04529735124913L720,375.16209120201313L720,400L604.0913118989034,400Z"></path><path stroke="transparent" fill="none" d="M405.6113891734079,40L431.2311209837213,176.84439745201428L261.4065500554704,140.33634335006306L194.76881601719447,40L405.6113891734079,40Z"></path><path stroke="transparent" fill="none" d="M543.6904534961681,40L537.4952000654806,169.09194806651232L446.1600123492211,40L543.6904534961681,40Z"></path><path stroke="transparent" fill="none" d="M659.14821781343,40L708.2365397904439,339.04529735124913L580.4243266136261,40L659.14821781343,40Z"></path><path stroke="transparent" fill="none" d="M720,40L720,375.16209120201313L708.2365397904439,339.04529735124913L659.14821781343,40L720,40Z"></path></g></svg></p>
<p>Hover to see sample images <span>up</span></p>
<p>Each line tracks a model throughout generative pre-training: the dotted markers denote checkpoints at steps 131K, 262K, 524K, and 1000K. The positive slopes suggest a link between improved generative performance and improved feature quality. Larger models also produce better features than smaller models. iGPT-XL is not included because it was trained on a different dataset.</p>
</div>
<p>When we evaluate our features using linear probes on CIFAR-10, CIFAR-100, and STL-10, we outperform features from all supervised and unsupervised transfer algorithms. Our results are also compelling in the full fine-tuning&nbsp;setting.</p>
<!-- table: eval -->
<table>
<thead>
  <tr>
    <th></th>
    <th></th>
    <th></th>
    <th colspan="2">Pre-trained on ImageNet</th>
  </tr>
  <tr>
    <th>Evaluation</th>
    <th>Model</th>
    <th>Accuracy</th>
    <th>w/o labels</th>
    <th>w/ labels</th>
  </tr>
</thead>
<tbody>
  <tr>
    <td rowspan="3">CIFAR-10<br>Linear Probe</td>
    <td>ResNet-152<span data-id="kornblith-2019-better"><sup><a href="#rf50" id="rfref50">50</a></sup></span></td>
    <td>94.0</td>
    <td></td>
    <td><span>check</span></td>
  </tr>
  <tr>
    <td>SimCLR<span data-id="chen-2020-simple"><sup><a href="#rf12" id="rfref12e">12</a></sup></span></td>
    <td>95.3</td>
    <td><span>check</span></td>
    <td></td>
  </tr>
  <tr>
    <td>iGPT-L 32x32</td>
    <td><strong>96.3</strong></td>
    <td><span>check</span></td>
    <td></td>
  </tr>
  <tr>
    <td rowspan="3">CIFAR-100<br>Linear Probe</td>
    <td>ResNet-152</td>
    <td>78.0</td>
    <td></td>
    <td><span>check</span></td>
  </tr>
  <tr>
    <td>SimCLR</td>
    <td>80.2</td>
    <td><span>check</span></td>
    <td></td>
  </tr>
  <tr>
    <td>iGPT-L 32x32</td>
    <td><strong>82.8</strong></td>
    <td><span>check</span></td>
    <td></td>
  </tr>
  <tr>
    <td rowspan="2">STL-10<br>Linear Probe</td>
    <td>AMDIM-L<span data-id="bachman-2019-learning"><sup><a href="#rf13" id="rfref13d">13</a></sup></span></td>
    <td>94.2</td>
    <td><span>check</span></td>
    <td></td>
  </tr>
  <tr>
    <td>iGPT-L 32x32</td>
    <td><strong>95.5</strong></td>
    <td><span>check</span></td>
    <td></td>
  </tr>
  <tr>
    <td rowspan="4">CIFAR-10<br>Fine-tune</td>
    <td>AutoAugment<span data-id="cubuk-2019-autoaugment"><sup><a href="#rf51" id="rfref51">51</a></sup></span></td>
    <td>98.5</td>
    <td></td>
    <td></td>
  </tr>
  <tr>
    <td>SimCLR</td>
    <td>98.6</td>
    <td><span>check</span></td>
    <td></td>
  </tr>
  <tr>
    <td>GPipe<span data-id="huang-2019-gpipe"><sup><a href="#rf15" id="rfref15b">15</a></sup></span></td>
    <td><strong>99.0</strong></td>
    <td></td>
    <td><span>check</span></td>
  </tr>
  <tr>
    <td>iGPT-L</td>
    <td><strong>99.0</strong></td>
    <td><span>check</span></td>
    <td></td>
  </tr>
  <tr>
    <td rowspan="4">CIFAR-100<br>Fine-tune</td>
    <td>iGPT-L</td>
    <td>88.5</td>
    <td><span>check</span></td>
    <td></td>
  </tr>
  <tr>
    <td>SimCLR</td>
    <td>89.0</td>
    <td><span>check</span></td>
    <td></td>
  </tr>
  <tr>
    <td>AutoAugment</td>
    <td>89.3</td>
    <td></td>
    <td></td>
  </tr>
  <tr>
    <td>EfficientNet<span data-id="tan-2019-efficientnet"><sup><a href="#rf52" id="rfref52">52</a></sup></span></td>
    <td><strong>91.7</strong></td>
    <td></td>
    <td><span>check</span></td>
  </tr>
</tbody>
</table>
<p>A comparison of linear probe and fine-tune accuracies between our models and top performing models which utilize either unsupervised or supervised ImageNet transfer. We also include AutoAugment, the best performing model trained end-to-end on CIFAR.</p>
<p>Given the resurgence of interest in unsupervised and self-supervised learning on ImageNet, we also evaluate the performance of our models using linear probes on ImageNet. This is an especially difficult setting, as we do not train at the standard ImageNet input resolution. Nevertheless, a linear probe on the 1536 features from the best layer of iGPT-L trained on 48x48 images yields 65.2% top-1 accuracy, outperforming&nbsp;AlexNet.</p>
<p>Contrastive methods typically report their best results on 8192 features, so we would ideally evaluate iGPT with an embedding dimension of 8192 for comparison. However, training such a model is prohibitively expensive, so we instead concatenate features from multiple layers as an approximation. Unfortunately, our features tend to be correlated across layers, so we need more of them to be competitive. Taking 15360 features from 5 layers in iGPT-XL yields 72.0% top-1 accuracy, outperforming AMDIM, MoCo, and CPC v2, but still underperforming SimCLR by a decent&nbsp;margin.</p>
<!-- table: features, parameters, accuracy -->
<table>
<thead>
  <tr>
    <th>Method</th>
    <th>Input Resolution</th>
    <th>Features</th>
    <th>Parameters</th>
    <th>Accuracy</th>
  </tr>
</thead>
<tbody>
  <tr>
    <td>Rotation<span data-id="gidaris-2018-unsupervised"><sup><a href="#rf53" id="rfref53">53</a></sup></span></td>
    <td>original</td>
    <td>8192</td>
    <td>86M</td>
    <td>55.4</td>
  </tr>
  <tr>
    <td>iGPT-L</td>
    <td>32x32</td>
    <td>1536</td>
    <td>1362M</td>
    <td>60.3</td>
  </tr>
  <tr>
    <td>BigBiGAN<span data-id="donahue-2019-large"><sup><a href="#rf37" id="rfref37b">37</a></sup></span></td>
    <td>original</td>
    <td>8192</td>
    <td>86M</td>
    <td>61.3</td>
  </tr>
  <tr>
    <td>iGPT-L</td>
    <td>48x48</td>
    <td>1536</td>
    <td>1362M</td>
    <td>65.2</td>
  </tr>
  <tr>
    <td>AMDIM<span data-id="bachman-2019-learning"><sup><a href="#rf13" id="rfref13e">13</a></sup></span></td>
    <td>original</td>
    <td>8192</td>
    <td>626M</td>
    <td>68.1</td>
  </tr>
  <tr>
    <td>MoCo<span data-id="he-2019-momentum"><sup><a href="#rf24" id="rfref24d">24</a></sup></span></td>
    <td>original</td>
    <td>8192</td>
    <td>375M</td>
    <td>68.6</td>
  </tr>
  <tr>
    <td>iGPT-XL</td>
    <td>64x64</td>
    <td>3072</td>
    <td>6801M</td>
    <td>68.7</td>
  </tr>
  <tr>
    <td>SimCLR<span data-id="chen-2020-simple"><sup><a href="#rf12" id="rfref12f">12</a></sup></span></td>
    <td>original</td>
    <td>2048</td>
    <td>24M</td>
    <td>69.3</td>
  </tr>
  <tr>
    <td>CPC v2<span data-id="henaff-2019-data"><sup><a href="#rf25" id="rfref25d">25</a></sup></span></td>
    <td>original</td>
    <td>8192</td>
    <td>303M</td>
    <td>71.5</td>
  </tr>
  <tr>
    <td>iGPT-XL</td>
    <td>64x64</td>
    <td>3072 x 5</td>
    <td>6801M</td>
    <td>72.0</td>
  </tr>
  <tr>
    <td>SimCLR</td>
    <td>original</td>
    <td>8192</td>
    <td>375M</td>
    <td><strong>76.5</strong></td>
  </tr>
</tbody>
</table>
<p>A comparison of linear probe accuracies between our models and state-of-the-art self-supervised models. We achieve competitive performance while training at much lower input resolutions, though our method requires more parameters and compute.</p>
<p>Because masked language models like BERT have outperformed generative models on most language tasks, we also evaluate the performance of BERT on our image models. Instead of training our model to predict the next pixel given all preceding pixels, we mask out 15% of the pixels and train our model to predict them from the unmasked ones. We find that though linear probe performance on BERT models is significantly worse, they excel during&nbsp;fine-tuning:</p>
<div>
<h5>CIFAR-10</h5>

<p id="chart-cifar-10"><svg viewBox="0,0,720,103"><g transform="translate(0,58)" fill="none" font-size="10" font-family="sans-serif" text-anchor="middle"><g opacity="1" transform="translate(100.5,0)"><line stroke="currentColor" y2="6"></line><text fill="currentColor" y="9" dy="0.71em">90%</text></g><g opacity="1" transform="translate(220.5,0)"><line stroke="currentColor" y2="6"></line><text fill="currentColor" y="9" dy="0.71em">92%</text></g><g opacity="1" transform="translate(340.5,0)"><line stroke="currentColor" y2="6"></line><text fill="currentColor" y="9" dy="0.71em">94%</text></g><g opacity="1" transform="translate(460.5,0)"><line stroke="currentColor" y2="6"></line><text fill="currentColor" y="9" dy="0.71em">96%</text></g><g opacity="1" transform="translate(580.5,0)"><line stroke="currentColor" y2="6"></line><text fill="currentColor" y="9" dy="0.71em">98%</text></g><g opacity="1" transform="translate(700.5,0)"><line stroke="currentColor" y2="6"></line><text fill="currentColor" y="9" dy="0.71em">100%</text></g><text x="360" dy="2.6em" fill="currentColor" text-anchor="middle">Accuracy</text></g><g transform="translate(100,0)" fill="none" font-size="10" font-family="sans-serif" text-anchor="end"><g opacity="1" transform="translate(0,44.11904761904762)"><text fill="currentColor" x="-9" dy="0.32em">Generative</text></g><g opacity="1" transform="translate(0,18.880952380952383)"><text fill="currentColor" x="-9" dy="0.32em">BERT</text></g></g><g><g fill="#CC3B08"><rect x="100" y="32.76190476190476" width="0" height="22.714285714285715"></rect><rect x="100" y="7.5238095238095255" width="519.0000000000003" height="22.714285714285715"></rect></g><g fill="#EC5D2A"><rect x="100" y="32.76190476190476" width="540" height="22.714285714285715"></rect><rect x="100" y="7.5238095238095255" width="515.9999999999995" height="22.714285714285715"></rect></g><g fill="#008EC2"><rect x="100" y="32.76190476190476" width="0" height="22.714285714285715"></rect><rect x="100" y="7.5238095238095255" width="317.9999999999998" height="22.714285714285715"></rect></g><g fill="#00B3F5"><rect x="100" y="32.76190476190476" width="377.99999999999983" height="22.714285714285715"></rect><rect x="100" y="7.5238095238095255" width="305.99999999999966" height="22.714285714285715"></rect></g></g></svg></p>
<h5>ImageNet</h5>

<p id="chart-imagenet"><svg viewBox="0,0,720,103"><g transform="translate(0,58)" fill="none" font-size="10" font-family="sans-serif" text-anchor="middle"><g opacity="1" transform="translate(100.5,0)"><line stroke="currentColor" y2="6"></line><text fill="currentColor" y="9" dy="0.71em">50%</text></g><g opacity="1" transform="translate(250.5,0)"><line stroke="currentColor" y2="6"></line><text fill="currentColor" y="9" dy="0.71em">55%</text></g><g opacity="1" transform="translate(400.5,0)"><line stroke="currentColor" y2="6"></line><text fill="currentColor" y="9" dy="0.71em">60%</text></g><g opacity="1" transform="translate(550.5,0)"><line stroke="currentColor" y2="6"></line><text fill="currentColor" y="9" dy="0.71em">65%</text></g><g opacity="1" transform="translate(700.5,0)"><line stroke="currentColor" y2="6"></line><text fill="currentColor" y="9" dy="0.71em">70%</text></g><text x="360" dy="2.6em" fill="currentColor" text-anchor="middle">Accuracy</text></g><g transform="translate(100,0)" fill="none" font-size="10" font-family="sans-serif" text-anchor="end"><g opacity="1" transform="translate(0,44.11904761904762)"><text fill="currentColor" x="-9" dy="0.32em">Generative</text></g><g opacity="1" transform="translate(0,18.880952380952383)"><text fill="currentColor" x="-9" dy="0.32em">BERT</text></g></g><g><g fill="#CC3B08"><rect x="100" y="32.76190476190476" width="0" height="22.714285714285715"></rect><rect x="100" y="7.5238095238095255" width="522.0000000000002" height="22.714285714285715"></rect></g><g fill="#EC5D2A"><rect x="100" y="32.76190476190476" width="488.9999999999999" height="22.714285714285715"></rect><rect x="100" y="7.5238095238095255" width="495" height="22.714285714285715"></rect></g><g fill="#008EC2"><rect x="100" y="32.76190476190476" width="0" height="22.714285714285715"></rect><rect x="100" y="7.5238095238095255" width="150" height="22.714285714285715"></rect></g><g fill="#00B3F5"><rect x="100" y="32.76190476190476" width="308.99999999999994" height="22.714285714285715"></rect><rect x="100" y="7.5238095238095255" width="128.9999999999999" height="22.714285714285715"></rect></g></g></svg></p>
<p>Comparison of generative pre-training with BERT pre-training using iGPT-L at an input resolution of 32<sup>2</sup> × 3. Bold colors show the performance boost from ensembling BERT masks. We see that generative models produce much better features than BERT models after pre-training, but BERT models catch up after fine-tuning.
</p>
</div>
<p>While unsupervised learning promises excellent features without the need for human-labeled data, significant recent progress has been made under the more forgiving framework of semi-supervised learning, which allows for limited amounts of human-labeled data. Successful semi-supervised methods often rely on clever techniques such as consistency regularization, data augmentation, or pseudo-labeling, and purely generative-based approaches<span data-id="welling-2014-semi"><sup><a href="#rf54" id="rfref54">54</a></sup></span><span data-id="salimans-2016-improved"><sup><a href="#rf55" id="rfref55a">55</a></sup></span> have not been competitive for years. We evaluate iGPT-L<sup><a href="#fn7" id="fnref7">[7]</a></sup> on a competitive benchmark for this sub-field and find that a simple linear probe on features from non-augmented images outperforms Mean Teacher<span data-id="tarvainen-2017-mean"><sup><a href="#rf56" id="rfref56a">56</a></sup></span> and MixMatch,<span data-id="berthelot-2019-mixmatch"><sup><a href="#rf57" id="rfref57a">57</a></sup></span> though it underperforms FixMatch.<span data-id="sohn-2020-fixmatch"><sup><a href="#rf59" id="rfref59a">59</a></sup></span></p>
<!-- table: models, labels -->
<div>
<table>
<thead>
  <tr>
    <th>Model</th>
    <th>40 labels</th>
    <th>250 labels</th>
    <th>4000 labels</th>
  </tr>
</thead>
<tbody>
  <tr>
    <td>Improved GAN<span data-id="salimans-2016-improved"><sup><a href="#rf55" id="rfref55b">55</a></sup></span></td>
    <td>—</td>
    <td>—</td>
    <td>81.4 ± 2.3</td>
  </tr>
  <tr>
    <td>Mean Teacher<span data-id="tarvainen-2017-mean"><sup><a href="#rf56" id="rfref56b">56</a></sup></span></td>
    <td>—</td>
    <td>67.7 ± 2.3</td>
    <td>90.8 ± 0.2</td>
  </tr>
  <tr>
    <td>MixMatch<span data-id="berthelot-2019-mixmatch"><sup><a href="#rf57" id="rfref57b">57</a></sup></span></td>
    <td>52.5 ± 11.5</td>
    <td>89.0 ± 0.9</td>
    <td>93.6 ± 0.1</td>
  </tr>
  <tr>
    <td>iGPT-L</td>
    <td>73.2 ± <span>0</span>1.5</td>
    <td>87.6 ± 0.6</td>
    <td>94.3 ± 0.1</td>
  </tr>
  <tr>
    <td>UDA<span data-id="xie-2019-unsupervised"><sup><a href="#rf58" id="rfref58">58</a></sup></span></td>
    <td>71.0 ± <span>0</span>5.9</td>
    <td>91.2 ± 1.1</td>
    <td>95.1 ± 0.2</td>
  </tr>
  <tr>
    <td>FixMatch<span data-id="sohn-2020-fixmatch"><sup><a href="#rf59" id="rfref59b">59</a></sup></span> RA</td>
    <td>86.2 ± <span>0</span>3.4</td>
    <td>94.9 ± 0.7</td>
    <td><strong>95.7 ± 0.1</strong></td>
  </tr>
  <tr>
    <td>FixMatch CTA</td>
    <td><strong>88.6 ± <span>0</span>3.4</strong></td>
    <td><strong>94.9 ± 0.3</strong></td>
    <td>95.7 ± 0.2</td>
  </tr>
</tbody>
</table>
</div><!-- end table: models, labels -->
<p>A comparison of performance on low-data CIFAR-10. By leveraging many unlabeled ImageNet images, iGPT-L is able to outperform methods such as Mean Teacher and MixMatch but still underperforms the state of the art methods. Our approach to semi-supervised learning is very simple since we only fit a logistic regression classifier on iGPT-L’s features without any data augmentation or fine-tuning—a significant difference from specially designed semi-supervised approaches.</p>
<h2 id="limitations">Limitations</h2>
<p>While we have shown that iGPT is capable of learning powerful image features, there are still significant limitations to our approach. Because we use the generic sequence transformer used for GPT-2 in language, our method requires large amounts of compute: iGPT-L was trained for roughly 2500 V100-days while a similarly performing MoCo<span data-id="he-2019-momentum"><sup><a href="#rf24" id="rfref24e">24</a></sup></span> model can be trained in roughly 70&nbsp;V100-days.</p>
<p>Relatedly, we model low resolution inputs using a transformer, while most self-supervised results use convolutional-based encoders which can easily consume inputs at high resolution. A new architecture, such as a domain-agnostic multiscale transformer, might be needed to scale further. Given these limitations, our work primarily serves as a proof-of-concept demonstration of the ability of large transformer-based language models to learn excellent unsupervised representations in novel domains, without the need for hardcoded domain knowledge. However, the significant resource cost to train these models and the greater accuracy of convolutional-neural-network based methods precludes these representations from practical real-world applications in the vision&nbsp;domain.</p>
<p>Finally, generative models can exhibit biases that are a consequence of the data they’ve been trained on. Many of these biases are useful, like assuming that a combination of brown and green pixels represents a branch covered in leaves, then using this bias to continue the image. But some of these biases will be harmful, when considered through a lens of fairness and representation. For instance, if the model develops a visual notion of a scientist that skews male, then it might consistently complete images of scientists with male-presenting people, rather than a mix of genders. We expect that developers will need to pay increasing attention to the data that they feed into their systems and to better understand how it relates to biases in trained&nbsp;models.</p>
<h2 id="conclusion">Conclusion</h2>
<p>We have shown that by trading off 2-D knowledge for scale<span data-id="sutton-2019-bitter"><sup><a href="#rf60" id="rfref60">60</a></sup></span> and by choosing predictive features from the middle of the network, a sequence transformer can be competitive with top convolutional nets for unsupervised image classification. Notably, we achieved our results by directly applying the GPT-2 language model to image generation. Our results suggest that due to its simplicity and generality, a sequence transformer given sufficient compute might ultimately be an effective way to learn excellent features in many&nbsp;domains.</p>
<p>If you’re excited to work with us on this area of research, <a href="https://openai.com/jobs/">we’re&nbsp;hiring</a>!</p>
</section>
  </div>
</section>
  

</article>
  

  </div></div></div>
    </div>
    <footer>
        <div>created by buildstarted &copy; 2020 <a href="/about">about</a></div>
        <div>Share this page on social media: copy and paste this url https://linksfor.dev/</div>
        <div>If you prefer RSS: <a href="https://linksfor.dev/feed.xml">https://linksfor.dev/feed.xml</a></div>
		<div>Customer satisfaction guaranteed to be optional.</div>
    </footer>
    
    <script async defer>
        _dna = window._dna || {};
        _dna.siteId = "linksfor.devs";
        _dna.outlink = true;

        (function () {
            let dna = document.createElement('script');
            dna.type = 'text/javascript';
            dna.async = true;
            dna.src = '//dna.buildstarted.com/t.js';
            let s = document.getElementsByTagName('script')[0];
            s.parentNode.insertBefore(dna, s);
        })();
    </script>
    <noscript><img src="//dna.buildstarted.com/g?siteId=linksfor.devs"/></noscript>
</body>
</html>