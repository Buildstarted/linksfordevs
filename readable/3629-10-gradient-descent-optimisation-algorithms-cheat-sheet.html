<!DOCTYPE html>
<html lang="en">
<head>
    <title>
10 Gradient Descent Optimisation Algorithms &#x2B; Cheat Sheet -
linksfor.dev(s)
    </title>
    <link rel="alternate" type="application/rss+xml" title="Linksfor.dev(s) feed" href="https://linksfor.dev/feed.rss" />
    <meta charset="utf-8">
    <meta name="Description" content="A curated source of links that devs might find interesting. Updated around the clock." />
    <meta name="google" value="notranslate">
    <link rel="stylesheet" type="text/css" href="/style.css" />
    <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon">
    <link rel="icon" href="/favicon.ico" type="image/x-icon">
</head>
<body>
    <div class="grid">
        
<div class="readable">
    <div id="readOverlay" class="style-ebook"><div id="readInner" class="margin-medium size-medium"><h1>10 Gradient Descent Optimisation Algorithms + Cheat Sheet</h1><div><div class="ac ae af ag ah ft aj ak"><h2 id="89a8" class="ju jv fw at as jw jx jy jz ka kb kc kd ke kf kg kh">1. Stochastic Gradient Descent</h2><p id="538f" class="ix iy fw at iz b gp ki gr kj jc kk je kl jg km ji dx">The vanilla gradient descent updates the current weight <em class="jj">w </em>using the current gradient <em class="jj">‚àÇL/‚àÇw </em>multiplied by some factor called the learning rate, <em class="jj">Œ±.</em></p><figure class="kp kq kr ks kt ec do dp paragraph-image"><h2 id="e802" class="ju jv fw at as jw jx jy jz ka kb kc kd ke kf kg kh">2. Momentum</h2><p id="74af" class="ix iy fw at iz b gp ki gr kj jc kk je kl jg km ji dx">Instead of depending only on the current gradient to update the weight, gradient descent with momentum (<a href="https://www.researchgate.net/publication/243648538_Some_methods_of_speeding_up_the_convergence_of_iteration_methods" class="dc by fp fq fr fs" target="_blank" rel="noopener nofollow">Polyak, 1964</a>) replaces the current gradient with <em class="jj">V </em>(which stands for velocity), the exponential moving average of current and past gradients (i.e. up to time <em class="jj">t</em>). Later in this post you will see that this momentum update becomes the standard update for the gradient component.</p><figure class="kp kq kr ks kt ec do dp paragraph-image"><p id="7d64" class="ix iy fw at iz b gp ja gr jb jc jd je jf jg jh ji dx">where</p><figure class="kp kq kr ks kt ec do dp paragraph-image"><p id="32ca" class="ix iy fw at iz b gp ja gr jb jc jd je jf jg jh ji dx">and <em class="jj">V</em> initialised to 0.</p><p id="74ec" class="ix iy fw at iz b gp ja gr jb jc jd je jf jg jh ji dx">Common default value:</p><blockquote class="kv kw kx"><p id="23ff" class="ix iy fw jj iz b gp ja gr jb jc jd je jf jg jh ji dx">Note that many articles reference the momentum method to the publication by <a href="https://www.semanticscholar.org/paper/On-the-momentum-term-in-gradient-descent-learning-Qian/735d4220d5579cc6afe956d9f6ea501a96ae99e2" class="dc by fp fq fr fs" target="_blank" rel="noopener nofollow">Ning Qian, 1999</a>. However, the paper titled <a href="http://proceedings.mlr.press/v28/sutskever13.pdf" class="dc by fp fq fr fs" target="_blank" rel="noopener nofollow">Sutskever et al.</a> attributed the classical momentum to a much earlier publication by Polyak in 1964, as cited above. (Thank you to <a href="https://news.ycombinator.com/item?id=18525494#18528682" class="dc by fp fq fr fs" target="_blank" rel="noopener nofollow">James</a> for pointing this out.)</p></blockquote><h2 id="1c7a" class="ju jv fw at as jw jx jy jz ka kb kc kd ke kf kg kh">3. AdaGrad</h2><p id="9eb2" class="ix iy fw at iz b gp ki gr kj jc kk je kl jg km ji dx">Adaptive gradient, or AdaGrad (<a href="http://jmlr.org/papers/v12/duchi11a.html" class="dc by fp fq fr fs" target="_blank" rel="noopener nofollow">Duchi et al., 2011</a>), works on the learning rate component by dividing the learning rate by the square root of <em class="jj">S</em>, which is the cumulative sum of current and past squared gradients (i.e. up to time <em class="jj">t</em>). Note that the gradient component remains unchanged like in SGD.</p><figure class="kp kq kr ks kt ec do dp paragraph-image"><p id="d11b" class="ix iy fw at iz b gp ja gr jb jc jd je jf jg jh ji dx">where</p><figure class="kp kq kr ks kt ec do dp paragraph-image"><p id="be16" class="ix iy fw at iz b gp ja gr jb jc jd je jf jg jh ji dx">and <em class="jj">S</em> initialised to 0.</p><p id="b525" class="ix iy fw at iz b gp ja gr jb jc jd je jf jg jh ji dx">Notice that <em class="jj">Œµ </em>is added to the denominator. Keras calls this the <em class="jj">fuzz factor</em>, a small floating point value to ensure that we will never have to come across division by zero.</p><p id="484a" class="ix iy fw at iz b gp ja gr jb jc jd je jf jg jh ji dx">Default values (from <a href="https://keras.io/optimizers/#adagrad" class="dc by fp fq fr fs" target="_blank" rel="noopener nofollow">Keras</a>):</p><h2 id="ef85" class="ju jv fw at as jw jx jy jz ka kb kc kd ke kf kg kh">4. RMSprop</h2><p id="9bbc" class="ix iy fw at iz b gp ki gr kj jc kk je kl jg km ji dx">Root mean square prop or RMSprop (<a href="http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf" class="dc by fp fq fr fs" target="_blank" rel="noopener nofollow">Hinton et al., 2012</a>) is another adaptive learning rate that is an improvement of AdaGrad. Instead of taking cumulative sum of squared gradients like in AdaGrad, we take the exponential moving average of these gradients.</p><figure class="kp kq kr ks kt ec do dp paragraph-image"><p id="3ea0" class="ix iy fw at iz b gp ja gr jb jc jd je jf jg jh ji dx">where</p><figure class="kp kq kr ks kt ec do dp paragraph-image"><p id="9b73" class="ix iy fw at iz b gp ja gr jb jc jd je jf jg jh ji dx">and <em class="jj">S</em> initialised to 0.</p><p id="17e0" class="ix iy fw at iz b gp ja gr jb jc jd je jf jg jh ji dx">Default values (from <a href="https://keras.io/optimizers/#rmsprop" class="dc by fp fq fr fs" target="_blank" rel="noopener nofollow">Keras</a>):</p><ul class=""><li id="9507" class="ix iy fw at iz b gp ja gr jb jc jd je jf jg jh ji la lb lc"><em class="jj">Œ±</em> = 0.001</li><li id="1bc7" class="ix iy fw at iz b gp ld gr le jc lf je lg jg lh ji la lb lc"><em class="jj">Œ≤</em> = 0.9 (recommended by the authors of the paper)</li><li id="9332" class="ix iy fw at iz b gp ld gr le jc lf je lg jg lh ji la lb lc"><em class="jj">Œµ = </em>10‚Åª‚Å∂</li></ul><h2 id="8320" class="ju jv fw at as jw jx jy jz ka kb kc kd ke kf kg kh">5. Adadelta</h2><p id="adaa" class="ix iy fw at iz b gp ki gr kj jc kk je kl jg km ji dx">Like RMSprop, Adadelta (<a href="https://arxiv.org/abs/1212.5701" class="dc by fp fq fr fs" target="_blank" rel="noopener nofollow">Zeiler, 2012</a>) is also another improvement from AdaGrad, focusing on the learning rate component. Adadelta is probably short for ‚Äòadaptive delta‚Äô, where <em class="jj">delta</em> here refers to the difference between the current weight and the newly updated weight.</p><p id="0183" class="ix iy fw at iz b gp ja gr jb jc jd je jf jg jh ji dx">The difference between Adadelta and RMSprop is that Adadelta removes the use of the learning rate parameter completely by replacing it with <em class="jj">D,</em> the exponential moving average of squared <em class="jj">deltas</em>.</p><figure class="kp kq kr ks kt ec do dp paragraph-image"><p id="afac" class="ix iy fw at iz b gp ja gr jb jc jd je jf jg jh ji dx">where</p><figure class="kp kq kr ks kt ec do dp paragraph-image"><p id="6158" class="ix iy fw at iz b gp ja gr jb jc jd je jf jg jh ji dx">with <em class="jj">D </em>and <em class="jj">S</em> initialised to 0, and</p><figure class="kp kq kr ks kt ec do dp paragraph-image"><p id="7669" class="ix iy fw at iz b gp ja gr jb jc jd je jf jg jh ji dx">Default values (from <a href="https://keras.io/optimizers/#adadelta" class="dc by fp fq fr fs" target="_blank" rel="noopener nofollow">Keras</a>):</p><h2 id="ab07" class="ju jv fw at as jw jx jy jz ka kb kc kd ke kf kg kh">6. Nesterov Accelerated Gradient (NAG)</h2><p id="809a" class="ix iy fw at iz b gp ki gr kj jc kk je kl jg km ji dx">After Polyak had gained his momentum (pun intended üò¨), a similar update was implemented using Nesterov Accelerated Gradient (<a href="http://proceedings.mlr.press/v28/sutskever13.html" class="dc by fp fq fr fs" target="_blank" rel="noopener nofollow">Sutskever et al., 2013</a>). This update utilises <em class="jj">V</em>, the exponential moving average of what I would call <em class="jj">projected gradients.</em></p><figure class="kp kq kr ks kt ec do dp paragraph-image"><p id="c4b7" class="ix iy fw at iz b gp ja gr jb jc jd je jf jg jh ji dx">where</p><figure class="kp kq kr ks kt ec do dp paragraph-image"><p id="ae98" class="ix iy fw at iz b gp ja gr jb jc jd je jf jg jh ji dx">and <em class="jj">V</em> initialised to 0.</p><p id="acb2" class="ix iy fw at iz b gp ja gr jb jc jd je jf jg jh ji dx">The last term in the second equation is a <em class="jj">projected gradient</em>. This value can be obtained by going ‚Äòone step ahead‚Äô using the previous velocity (Eqn. 4). This means that for this time step <em class="jj">t</em>, we have to carry out another forward propagation before we can finally execute the backpropagation. Here‚Äôs how it goes:</p><ol class=""><li id="002c" class="ix iy fw at iz b gp ja gr jb jc jd je jf jg jh ji lr lb lc">Update the current weight <em class="jj">w </em>to a <em class="jj">projected weight w* </em>using the previous velocity.</li></ol><figure class="kp kq kr ks kt ec do dp paragraph-image"><figcaption class="ax fj fk fl fm dq do dp fn fo as cx">Eqn. 4</figcaption></figure><p id="4290" class="ix iy fw at iz b gp ja gr jb jc jd je jf jg jh ji dx">2. Carry out forward propagation, but using this <em class="jj">projected weight</em>.</p><p id="3391" class="ix iy fw at iz b gp ja gr jb jc jd je jf jg jh ji dx"><em class="jj">3. </em>Obtain the <em class="jj">projected gradient</em><em class="jj">‚àÇL/‚àÇw*</em>.</p><p id="cf8c" class="ix iy fw at iz b gp ja gr jb jc jd je jf jg jh ji dx">4. Compute <em class="jj">V</em> and <em class="jj">w </em>accordingly.</p><p id="2fe3" class="ix iy fw at iz b gp ja gr jb jc jd je jf jg jh ji dx">Common default value:</p><blockquote class="kv kw kx"><p id="d1a4" class="ix iy fw jj iz b gp ja gr jb jc jd je jf jg jh ji dx">Note that the original Nesterov Accelerated Gradient paper (<a href="http://www.cis.pku.edu.cn/faculty/vision/zlin/1983-A%20Method%20of%20Solving%20a%20Convex%20Programming%20Problem%20with%20Convergence%20Rate%20O(k%5E(-2))_Nesterov.pdf" class="dc by fp fq fr fs" target="_blank" rel="noopener nofollow">Nesterov, 1983</a>) was not about <em class="at">stochastic</em> gradient descent and did not explicitly use the gradient descent equation. Hence, a more appropriate reference is the above-mentioned publication by Sutskever et al. in 2013, which described NAG‚Äôs application in stochastic gradient descent. (Again, I‚Äôd like to thank James‚Äôs <a href="https://news.ycombinator.com/item?id=18525494#18528682" class="dc by fp fq fr fs" target="_blank" rel="noopener nofollow">comment</a> on HackerNews for pointing this out.)</p></blockquote><h2 id="2027" class="ju jv fw at as jw jx jy jz ka kb kc kd ke kf kg kh">7. Adam</h2><p id="cdec" class="ix iy fw at iz b gp ki gr kj jc kk je kl jg km ji dx">Adaptive moment estimation, or Adam (<a href="https://arxiv.org/abs/1412.6980" class="dc by fp fq fr fs" target="_blank" rel="noopener nofollow">Kingma &amp; Ba, 2014</a>), is a combination of momentum and RMSprop. It acts upon<br>(i) the gradient component by using <em class="jj">V</em>, the exponential moving average of gradients (like in momentum) and <br>(ii) the learning rate component by dividing the learning rate <em class="jj">Œ±</em> by square root of <em class="jj">S</em>, the exponential moving average of squared gradients (like in RMSprop).</p><figure class="kp kq kr ks kt ec do dp paragraph-image"><p id="8a78" class="ix iy fw at iz b gp ja gr jb jc jd je jf jg jh ji dx">where</p><figure class="kp kq kr ks kt ec do dp paragraph-image"><p id="0254" class="ix iy fw at iz b gp ja gr jb jc jd je jf jg jh ji dx">are the bias corrections, and</p><figure class="kp kq kr ks kt ec do dp paragraph-image"><p id="3115" class="ix iy fw at iz b gp ja gr jb jc jd je jf jg jh ji dx">with <em class="jj">V </em>and <em class="jj">S</em> initialised to 0.</p><p id="8c44" class="ix iy fw at iz b gp ja gr jb jc jd je jf jg jh ji dx">Proposed default values by the authors:</p><ul class=""><li id="4fbe" class="ix iy fw at iz b gp ja gr jb jc jd je jf jg jh ji la lb lc"><em class="jj">Œ±</em> = 0.001</li><li id="4dfa" class="ix iy fw at iz b gp ld gr le jc lf je lg jg lh ji la lb lc"><em class="jj">Œ≤</em>‚ÇÅ = 0.9</li><li id="78b0" class="ix iy fw at iz b gp ld gr le jc lf je lg jg lh ji la lb lc"><em class="jj">Œ≤</em>‚ÇÇ = 0.999</li><li id="f8ac" class="ix iy fw at iz b gp ld gr le jc lf je lg jg lh ji la lb lc"><em class="jj">Œµ</em> = 10‚Åª‚Å∏</li></ul><h2 id="2b3f" class="ju jv fw at as jw jx jy jz ka kb kc kd ke kf kg kh">8. AdaMax</h2><p id="d728" class="ix iy fw at iz b gp ki gr kj jc kk je kl jg km ji dx">AdaMax (<a href="https://arxiv.org/abs/1412.6980" class="dc by fp fq fr fs" target="_blank" rel="noopener nofollow">Kingma &amp; Ba, 2015</a>) is an adaptation of the Adam optimiser by the same authors using infinity norms (hence ‚Äòmax‚Äô). <em class="jj">V </em>is the exponential moving average of gradients, and <em class="jj">S </em>is the exponential moving average of past <em class="jj">p</em>-norm of gradients, approximated to the max function as seen below (see paper for convergence proof).</p><figure class="kp kq kr ks kt ec do dp paragraph-image"><p id="bcae" class="ix iy fw at iz b gp ja gr jb jc jd je jf jg jh ji dx">where</p><figure class="kp kq kr ks kt ec do dp paragraph-image"><p id="3cd0" class="ix iy fw at iz b gp ja gr jb jc jd je jf jg jh ji dx">is the bias correction for <em class="jj">V</em> and</p><figure class="kp kq kr ks kt ec do dp paragraph-image"><p id="211e" class="ix iy fw at iz b gp ja gr jb jc jd je jf jg jh ji dx">with <em class="jj">V </em>and <em class="jj">S</em> initialised to 0.</p><p id="5394" class="ix iy fw at iz b gp ja gr jb jc jd je jf jg jh ji dx">Proposed default values by the authors:</p><ul class=""><li id="d30c" class="ix iy fw at iz b gp ja gr jb jc jd je jf jg jh ji la lb lc"><em class="jj">Œ±</em> = 0.002</li><li id="4143" class="ix iy fw at iz b gp ld gr le jc lf je lg jg lh ji la lb lc"><em class="jj">Œ≤</em>‚ÇÅ = 0.9</li><li id="4d6e" class="ix iy fw at iz b gp ld gr le jc lf je lg jg lh ji la lb lc"><em class="jj">Œ≤</em>‚ÇÇ = 0.999</li></ul><h2 id="6d4c" class="ju jv fw at as jw jx jy jz ka kb kc kd ke kf kg kh">9. Nadam</h2><p id="dddb" class="ix iy fw at iz b gp ki gr kj jc kk je kl jg km ji dx">Nadam (<a href="http://cs229.stanford.edu/proj2015/054_report.pdf" class="dc by fp fq fr fs" target="_blank" rel="noopener nofollow">Dozat, 2015</a>) is an acronym for Nesterov and Adam optimiser. The Nesterov component, however, is a more efficient modification than its original implementation.</p><p id="1892" class="ix iy fw at iz b gp ja gr jb jc jd je jf jg jh ji dx">First we‚Äôd like to show that the Adam optimiser can also be written as:</p><figure class="kp kq kr ks kt ec do dp paragraph-image"><figcaption class="ax fj fk fl fm dq do dp fn fo as cx">Eqn. 5: Weight update for Adam optimiser</figcaption></figure><p id="4242" class="ix iy fw at iz b gp ja gr jb jc jd je jf jg jh ji dx">Nadam makes use of Nesterov to update the gradient one step ahead by replacing the previous <em class="jj">V_hat </em>in the above equation to the current <em class="jj">V_hat</em>:</p><figure class="kp kq kr ks kt ec do dp paragraph-image"><p id="cc9b" class="ix iy fw at iz b gp ja gr jb jc jd je jf jg jh ji dx">where</p><figure class="kp kq kr ks kt ec do dp paragraph-image"><p id="a1ff" class="ix iy fw at iz b gp ja gr jb jc jd je jf jg jh ji dx">and</p><figure class="kp kq kr ks kt ec do dp paragraph-image"><p id="6395" class="ix iy fw at iz b gp ja gr jb jc jd je jf jg jh ji dx">with <em class="jj">V </em>and <em class="jj">S</em> initialised to 0.</p><p id="2dd5" class="ix iy fw at iz b gp ja gr jb jc jd je jf jg jh ji dx">Default values (taken from <a href="https://keras.io/optimizers/#nadam" class="dc by fp fq fr fs" target="_blank" rel="noopener nofollow">Keras</a>):</p><ul class=""><li id="fd45" class="ix iy fw at iz b gp ja gr jb jc jd je jf jg jh ji la lb lc"><em class="jj">Œ±</em> = 0.002</li><li id="7f02" class="ix iy fw at iz b gp ld gr le jc lf je lg jg lh ji la lb lc"><em class="jj">Œ≤</em>‚ÇÅ = 0.9</li><li id="f03d" class="ix iy fw at iz b gp ld gr le jc lf je lg jg lh ji la lb lc"><em class="jj">Œ≤</em>‚ÇÇ = 0.999</li><li id="cbe3" class="ix iy fw at iz b gp ld gr le jc lf je lg jg lh ji la lb lc"><em class="jj">Œµ</em> = 10‚Åª‚Å∑</li></ul><h2 id="de85" class="ju jv fw at as jw jx jy jz ka kb kc kd ke kf kg kh">10. AMSGrad</h2><p id="3606" class="ix iy fw at iz b gp ki gr kj jc kk je kl jg km ji dx">Another variant of Adam is the AMSGrad (<a href="https://openreview.net/pdf?id=ryQu7f-RZ" class="dc by fp fq fr fs" target="_blank" rel="noopener nofollow">Reddi et al., 2018</a>). This variant revisits the adaptive learning rate component in Adam and changes it to ensure that the current <em class="jj">S </em>is always larger than the previous time step<em class="jj">.</em></p><figure class="kp kq kr ks kt ec do dp paragraph-image"><p id="29b9" class="ix iy fw at iz b gp ja gr jb jc jd je jf jg jh ji dx">where</p><figure class="kp kq kr ks kt ec do dp paragraph-image"><p id="5fc6" class="ix iy fw at iz b gp ja gr jb jc jd je jf jg jh ji dx">and</p><figure class="kp kq kr ks kt ec do dp paragraph-image"><p id="5523" class="ix iy fw at iz b gp ja gr jb jc jd je jf jg jh ji dx">with <em class="jj">V </em>and <em class="jj">S</em> initialised to 0.</p><p id="aea7" class="ix iy fw at iz b gp ja gr jb jc jd je jf jg jh ji dx">Default values (taken from <a href="https://keras.io/optimizers/#adam" class="dc by fp fq fr fs" target="_blank" rel="noopener nofollow">Keras</a>):</p><ul class=""><li id="025d" class="ix iy fw at iz b gp ja gr jb jc jd je jf jg jh ji la lb lc"><em class="jj">Œ±</em> = 0.001</li><li id="00a1" class="ix iy fw at iz b gp ld gr le jc lf je lg jg lh ji la lb lc"><em class="jj">Œ≤</em>‚ÇÅ = 0.9</li><li id="ab3a" class="ix iy fw at iz b gp ld gr le jc lf je lg jg lh ji la lb lc"><em class="jj">Œ≤</em>‚ÇÇ = 0.999</li><li id="647b" class="ix iy fw at iz b gp ld gr le jc lf je lg jg lh ji la lb lc"><em class="jj">Œµ</em> = 10‚Åª‚Å∑</li></ul></figure></figure></figure></figure></figure></figure></figure></figure></figure></figure></figure></figure></figure></figure></figure></figure></figure></figure></figure></figure></figure></figure></figure></figure></div></div></div></div>
</div>
    </div>
    <footer>
        <div>created by buildstarted &copy; 2020 <a href="/about">about</a></div>
        <div>Share this page on social media: copy and paste this url https://linksfor.dev/</div>
        <div>If you prefer RSS: <a href="https://linksfor.dev/feed.xml">https://linksfor.dev/feed.xml</a></div>
    </footer>
    
    <script>
        (function () {
            var COLLECT_URL = "https://dna.buildstarted.com/t";
            var SITE_ID = "linksfor.devs";
            var GLOBAL_VAR_NAME = "__DNA__";

            window[GLOBAL_VAR_NAME] = {};

            window[GLOBAL_VAR_NAME].sendPageView = function () {
                var path = location.pathname;
                var referrer = document.referrer;

                var url = COLLECT_URL + "?siteid=" + SITE_ID + "&p=" + encodeURIComponent(path) + "&r=" + encodeURIComponent(referrer);

                try { fetch(url, { method: "GET" }); } catch (e) { }
            };

            window[GLOBAL_VAR_NAME].sendPageView();
        })();
    </script>
</body>
</html>