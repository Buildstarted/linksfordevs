<!DOCTYPE html>
<html lang="en">
<head>
    <title>
The Promise of Hierarchical Reinforcement Learning -
linksfor.dev(s)
    </title>
    <link rel="alternate" type="application/rss+xml" title="Linksfor.dev(s) feed" href="https://linksfor.dev/feed.rss" />
    <meta charset="utf-8">
    <meta name="Description" content="A curated source of links that devs might find interesting. Updated around the clock." />
    <meta name="google" value="notranslate">
    <link rel="stylesheet" type="text/css" href="/style.css" />
    <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon">
    <link rel="icon" href="/favicon.ico" type="image/x-icon">
</head>
<body>
    <div class="grid">
        
<div class="readable">
    <div id="readOverlay" class="style-ebook"><div id="readInner" class="margin-medium size-medium"><h1>The Promise of Hierarchical Reinforcement Learning</h1><div><div class="kg-card-markdown"><h5 id="updatejrgenschmidhuberkindlysuggestedsomecorrectionsconcerningtheearlyworkonintrinsicmotivationsubgoaldiscoveryandartificialcuriositysince1990whichihaveincorporatedandexpanded"><em>Update: Jürgen Schmidhuber kindly suggested some corrections concerning the early work on intrinsic motivation, subgoal discovery and artificial curiosity since 1990, which I have incorporated and expanded.</em></h5><p>Suppose your friend just baked and shared  an excellent cake with you, and you would like to know its recipe. It might seem that it should be very easy for your friend to just tell you how to cook the cake — that it should be easy for him to get across the recipe. But this is a subtler task than you might think; <strong>how detailed should the instructions be?</strong> Does the friend have to explain in detail each of the tiny tasks to be followed?</p><p>Probably not.</p><p>At some point into the recipe of, let’s say, beef bourguignon, one needs to "cut 4 carrots into slices." To humans, there is no need to say: "take a knife; in case it doesn’t cut properly, sharpen it; take a wooden board and put the 4 carrots on it; hold the knife in your dominant hand; contract muscle X to cut the first slice of carrot."</p><p>So, there is a pertinent level of <em>granularity</em> to be adopted when sketching an action for a system to follow. This granularity can be very difficult to mathematically integrate into complex self-learning systems.</p><figure><img src="/content/images/2019/03/image58.png" width="800"><figcaption>Example of different levels of granularity in a recipe</figcaption></figure><p>In addition, there is converging evidence in developmental psychology  that newborns, primates, children, and adults rely on the same cognitive systems for their basic knowledge. These cognitive systems include entities, agents, actions, space, social structures and intuitive theories. During open-ended games such as stacking up physically stable block structures, toddlers will use this knowledge to set sub-goals.</p><p align="center"><a href="https://giphy.com/gifs/lcyOND2tlMObGT6Fp2">via GIPHY</a></p><p>To achieve these goals, toddlers seem to generate sub-goals within the space of their basic knowledge, engaging in <em>temporal abstraction</em>. If we use the recipe for beef bourguignon as an example, the cutting process of an onion is a temporally extended action and can take different numbers of steps to complete depending on the required cutting fineness. This idea of temporal abstraction, once incorporated into reinforcement learning (RL), converts it into <em>hierarchical</em> reinforcement learning (HRL).</p><p>The following was motivated by a modest attempt to <a href="https://distill.pub/2017/research-debt/">distill</a> research on the subject of HRL. We will start by reviewing the fundamentals of RL before elaborating on its current limitations. We will then see how HRL can be an attractive way to counter the limits of RL, including its motivations, main frameworks and own limitations. Finally, we will discuss active and future research in this area.</p><hr><h1 id="theifyouknowrlyoucanskipthissection">The “If-You-Know-RL-You-Can-Skip-This” Section</h1><p>Reinforcement learning (RL) methods have recently shown a wide range of positive results, including beating humanity's best at <a href="https://deepmind.com/research/alphago/">Go</a>, learning to play Atari games just from the raw pixels, and teaching computers to control robots in <a href="https://storage.googleapis.com/joschu-public/knocked-over-stand-up.mp4">simulations</a> or in the <a href="https://youtu.be/jwSbzNHGflM">real world</a>. These achievements are the culmination of research on trial and error learning and optimal control since the 1950s. From these two domains was born the field of RL, which has since then been constantly evolving and remains incredibly stimulating.</p><p>Csaba Szepesvári puts it well in his book : “reinforcement learning refers to both a learning problem and a subfield of machine learning.”</p><p>In short: the learning problem is concerned with software agents that learn goal-oriented behavior by trial and error in an environment that provides rewards in response to the agent’s actions towards achieving that goal.</p><figure><center><img src="/content/images/2019/03/image57.png" width="900"><figcaption>RL learning problem</figcaption></center></figure><p>The learning problem setup is quite simple.</p><p>There are two protagonists: an agent and an environment. The environment is where the agent ‘lives’ and what it interacts with. At each point of the interaction, the agent sees an observation of the state of the world, then decides on an action to be taken. The environment changes when the agent acts on it, but it can also change on its own. The agent also receives an environmental reward signal, a number (or a distribution ) that tells it how good or bad the effect of the action was with respect to the agent’s goal.</p><p>At this point, you could ask: why do RL at all and not directly supervised learning (eg. with data samples (state, action) → reward)? Alex Graves pointed it in his NeurIPS 2018 <a href="https://www.youtube.com/watch?v=rjZCjosEFpI">talk</a> about Unsupervised Learning:</p><ol><li>With a supervised learning setup, we would need huge amounts of data that is difficult to obtain and can be complex to define</li><li>From what we understand from the way toddlers learn when they discover the world , learning without relying on a massive amount of data feels more human</li><li>RL could allow for better generalisation in a variety of complex real-world environments with eg. intrinsic motivation and auxiliary tasks </li></ol><p>The third argument makes even more sense when considering HRL, whose ambition is to be very effective, particularly in terms of generalisation and transfer of learning.</p><hr><details><summary>Click here for a summary of Markov Decision Processes and RL</summary><p>Formally speaking, a Markov Decision Processes (MDP) is used to describe an environment for reinforcement learning where the environment is fully observable. Under the well-known Markov property “the future is independent of the past given the present” we define a finite MDP as a tuple $&lt;S, A, p, r&gt;$ where $S$ is a finite set of states, $A$ is a finite set of actions, $p(s'|s,a)$ is the probability of transition from one state to another $s'$ when action $a$ is taken, $r(s,a,s')$ is a distribution on the rewards obtained when action $a$ is taken from $s$ and the following state $s'$. A stationary deterministic policy $\pi:S\rightarrow A$ maps states to actions.</p><p>In short, in a traditional RL problem, the agent aims to maximise its expected discounted return $$R_t = \sum_{k = 0}^\infty \gamma^kr_{t+k+1}$$ where $r_t$ is the reward the agent receives at time $t$ and $\gamma\in[0,1)$ is the discount factor .</p><p>In a fully observable setting, the agent observes the true state of the environment $s_t\in S$ and chooses an action $a_t\in A$ according to policy $\pi(a|s)$.</p><p>One way to address the RL problem is to define what is called the action-value function $Q$ of a policy $\pi$ : $$Q_{\pi}(s,a) = E[R_{t} | s_{t}=s, a_{t}=a]$$</p><p>The Bellman optimality equation $$ Q^\ast (s,a) = r(s,a) + \gamma \sum_{s' \in S} P(s'|s,a) \max_{a'} Q^\ast (s',a') $$</p><p>recursively represents the optimal Q-function $$Q^\ast (s,a) = \max_{\pi} Q^{\pi}(s,a)$$ as a function of the expected immediate reward $r(s,a)$ and the transition function $P(s'|s,a)$, which in turn yields an optimal greedy policy $\pi^\ast (s) = \arg\max_{a} Q^\ast (s,a)$.</p><p>Q-learning  uses a sample-based approximation of the Bellman optimality equation to iteratively improve the Q-function. Q-learning has been shown to converge in the limit, with probability 1, to the optimal value function<br>$Q^\ast$ under standard stochastic approximation assumptions. It is a RL solution for a MDP.</p><p>In deep Q-learning , the Q-function is represented by a neural network parameterised by $\theta$.</p></details><hr><p>What emerges from the above is really what the main question in RL is: <strong>How do we maximize future rewards?</strong></p><p>Answering this question actually requires answering to other sub-questions, including:</p><ul><li>What should we learn (models, state utilities, policies, etc.)?</li><li>How should we learn (TD learning, Monte Carlo, etc.)?</li><li>How do we represent what we have learned (deep neural networks, big tables, etc.)?</li><li>How to use what we have learnt: really often the first question that needs to be answered...</li></ul><hr><h1 id="sowhatishardinrl">So, what is hard in RL?</h1><p align="center"><a href="https://giphy.com/gifs/iYnmrnrBynUyhwvG0H">via GIPHY</a></p><p>In this famous experiment from Warneken and Tomasello (full video <a href="https://www.youtube.com/watch?v=Z-eU5xZW7cU">here</a>), an 18-month-old child can understand what is happening and how to interact with the situation without having seen what to do before. The kid must have some common sense to be able to do that: understanding the physics, the action, the constraints, the plan. If you look to the end of the experiment, the child even takes a quick look at the man's hands to deduce how the plan will be completed.</p><p>We are still a long way from setting up such a capable system with current RL methods. One reason may be because RL suffers from a variety of defects that hinder learning and prevent it from being applied to more complex environments. HRL aims at alleviating precisely this learning complexity by breaking down specific parts of learning. The question is, therefore, whether this is enough. Strongly in line with Andrey Kurenkov's <a href="https://thegradient.pub/why-rl-is-flawed/">essay</a> in a previous Gradient article, the main weaknesses of RL, in comparison to the promises of HRL, can be broken down as follows.</p><ul><li>Sample efficiency: data generation is often a bottleneck and current RL methods are data inefficient. With HRL, sub-tasks and abstract actions can be used in different tasks on the same domain (transfer learning)</li><li>Scaling up: the application of classic RL to the problems with large action and/or state space is infeasible (curse of dimensionality). HRL aims to decompose large problems into smaller ones (efficient learning)</li><li>Generalization: trained agents can solve complex tasks, but if we want them to transfer their experience to new (even similar) environments, most state of the art RL algorithms will fail (brittleness due to overspecialization)</li><li>Abstraction: state and temporal abstractions allow to simplify the problem since resulting sub-tasks can effectively be solved by RL approaches (better knowledge representation)</li></ul><p>In addition, all the basic algorithms for reinforcement learning are so-called “flat” methods. They treat the state space as a huge, flat search space, meaning that the paths from the starting state to the target state are very long. If we look at this with the example of the recipe, <strong>it would give us a sequence of actions solely composed of a series of muscular micro-contractions</strong>. In addition, the length of these paths dictates the cost of learning, as information on future rewards must be disseminated backwards along these paths. In short, the reward signal is weak and delayed.</p><p>Perhaps we could step back and look at what we have learnt so far: in the 1970s, research in the field of planning showed that hierarchical methods such as hierarchical task networks , macro actions  and state abstraction methods  can provide exponential reductions in compute costs to find the right plans. There is also a large literature on subgoal discovery, intrinsic motivation and artificial curiosity . Nevertheless, we still lack a fully acceptable method for integrating hierarchies into the effective RL algorithms introduced so far.</p><hr><h1 id="hierarchicalreinforcementlearning">Hierarchical Reinforcement Learning</h1><p>As we just saw, the reinforcement learning problem suffers from serious scaling issues. Hierarchical reinforcement learning (HRL) is a computational approach intended to address these issues by learning to operate on different levels of temporal abstraction .</p><p>To really understand the need for a hierarchical structure in the learning algorithm and in order to make the bridge between RL and HRL, we need to remember what we are trying to solve: MDPs. HRL methods learn  a policy made up of multiple layers, each of which is responsible for control at a different level of temporal abstraction. Indeed, the key innovation of the HRL is to extend the set of available actions so that the agent can now choose to perform not only elementary actions, but also macro-actions, i.e. sequences of lower-level actions. Hence, with actions that are extended over time, we must take into account the time  elapsed between decision-making moments. Luckily, MDP planning and learning algorithms can easily be extended to accommodate HRL.</p><p>In order to do that, let us welcome the <em>semi-Markov decision process</em> (SMDP). In this setup, $p(s'|s,a)$ turns into $p(s',\tau|s,a)$.</p><figure><img src="/content/images/2019/03/image52.png" width="600"><figcaption>Hierarchical learning dynamics. <a href="https://www.princeton.edu/~yael/Publications/RibasFernandesSolwayEtAl2011.pdf">Source</a></figcaption></figure><p>The figure above clearly illustrates the paradigm: $a$ is a primitive action, $\sigma$ is a subroutine or macro-action, $\pi$ is the action policy, $\pi_\sigma$ is the subroutine-specific action polcy, and $V$ and $V_a$ are the state values.</p><p>The promise of HRL is to have:</p><ol><li>Long-term credit assignment: faster learning and better generalisation</li><li>Structured exploration: explore with sub-policies rather than primitive actions</li><li>Transfer learning: different levels of hierarchy can encompass different knowledge and allow for better transfer</li></ol><p>In the following and in the aforementioned setting, we outline the foundational methods that have emerged since 1993.</p><hr><h1 id="buthow">But, How?</h1><h2 id="feudallearning">Feudal Learning</h2><figure><img src="/content/images/2019/03/image48.png" width="800"><figcaption>Feudal hierarchy. <a href="https://shogunvseurope.weebly.com/feudal-society.html">Source</a></figcaption></figure><p>Inspired by Medieval Europe's Feudal system, this HRL method demonstrates how to create a managerial learning hierarchy in which lords (or managers) learn to assign tasks (or sub-goals) to their serfs (or sub-managers) who, in turn, learn to satisfy them. Sub-managers learn to maximize their reinforcement in the context of the command as pictured in the illustration below with the black circle.</p><figure><img src="/content/images/2019/03/image44.png" width="300"><figcaption>Illustration of the feudal system in a standard maze task. <a href="https://papers.nips.cc/paper/714-feudal-reinforcement-learning.pdf">Source</a></figcaption></figure><p>In practice, Feudal learning  takes advantage of two notions:</p><ul><li>Information hiding: the managerial hierarchy observes the environment at different resolutions</li><li>Reward hiding: communication is made between managers and "workers" through goals - a reward is given for reaching them</li></ul><p>A noteworthy effect of information and reward hiding is that the managers only need to know the state of the system at the granularity of their own choices of tasks. They also don’t know what choices their workers have made to satisfy their command, since it is not needed for the system setup to learn.</p><p>Unfortunately, the Feudal Q-learning algorithm introduced in  is tailored to a specific kind of problem, and does not converge to any well-defined optimal policy. But it has paved the way for many other contributions.</p><h2 id="optionsframework">Options Framework</h2><p>The most well-known formulation for HRL is probably the Options framework . A <em>(Markov) option</em> is a triple $o = &lt;I_o, \pi_o, \beta_o&gt;$ with:</p><ul><li>$I_o$: the initiation set</li><li>$\pi_o: S\times A\rightarrow[0,1]$: the option's policy</li><li>$\beta_o:S\rightarrow[0,1]$: the termination condition</li></ul><figure><img src="/content/images/2019/03/image64.png" width="600"><figcaption>Understanding the difference between primitive actions and options. <a href="http://www-anw.cs.umass.edu/~barto/courses/cs687/Sutton-Precup-Singh-AIJ99.pdf">Source</a></figcaption></figure><p>One can grasp the idea of this framework with the self-explanatory example above where the options can be summed up as “going to hallways” and the actions as “going N, S, W, or E.” The options can be considered as individual actions at a higher level of abstraction (ie. each state can be used as a subgoal ) and can, as a result, be abstracted into skills.</p><p>Unlike Feudal learning, if the action space consists of both primitive actions and options, then an algorithm following the Options framework is proven  to converge to an optimal policy. Otherwise, it will still converge, but to a hierarchically optimal policy.</p><figure><img src="/content/images/2019/03/image65.png" width="400"><figcaption>Understanding the Options in the context of a SMDP. <a href="http://www-anw.cs.umass.edu/~barto/courses/cs687/Sutton-Precup-Singh-AIJ99.pdf">Source</a></figcaption></figure><p>The resulting idea is that an Options framework is composed of two levels:</p><ul><li>The bottom level is a sub-policy:
<ul><li>takes environment observations</li><li>outputs actions</li><li>runs until termination</li></ul></li><li>The top level is a policy-over-options:
<ul><li>takes environment observations</li><li>outputs sub-policies</li><li>runs until termination</li></ul></li></ul><p>Options are quite easy to implement, and effective in defining high-level competencies which in turn improves convergence speed. Moreover, options themselves can be used to define option hierarchies. However, and as a natural consequence, options increase the complexity of the MDP. They also do not explicitly address the problem of task segmentation.</p><h2 id="hierarchicalabstractmachines">Hierarchical Abstract Machines</h2><p>HAMs consist of non-deterministic finite state machines whose transitions may invoke lower-level machines (the optimal action is yet to be decided or learnt). A machine is a partial policy represented by a Finite State Automaton (FSA). There are four machine states:</p><ul><li>Action states execute an action in the environment</li><li>Call states execute another machine as a subroutine</li><li>Choice states non-deterministically select a next machine state</li><li>Stop states halt execution of the machine and return control to the previous call state</li></ul><p>We can view policies as programs. For HAMs, the learning occurs within machines, since machines are only partially defined. The approach is to flatten all machines out and consider the state space of the problem $&lt;s,m&gt;$ where $m$ is the machine state and $s$is the state of the underlying MDP.</p><figure><img src="/content/images/2019/03/image63.png" width="500"><figcaption>The grid-world environment that was used throughout the paper. <a href="https://arxiv.org/abs/cs/9905014">Source</a></figcaption></figure><p>When the machine encounters a <em>Call state</em>, it executes the machine it is supposed to execute in a deterministic way. When it encounters a <em>Stop state</em>, it simply sends the command back to the parent machine. Unlike the case where learning takes place directly on the MDP where actions are learnt in each state, in the HAM framework learning only takes place in the <em>Choice states</em>. Thus, the state space on which the learning takes place may be smaller than the actual state space.</p><figure><img src="/content/images/2019/03/image67.png" width="500"><figcaption>Example of FSA for the grid-world environment</figcaption></figure><p>In the above example, each time an obstacle is encountered, the machine enters a Choice state where either it chooses the follow-wall machine (which just continuously follows the wall in a certain direction), or it chooses the back-off machine (which moves back and execution continues).</p><p>The learnt machine’s policy is therefore to decide which machine to call and with what probability.</p><p>For all the above reasons, the HAM framework offers us the ability to simplify the MDP by restricting the class of realizable policies. Similar to the Options framework, it also has theoretical guarantees of optimality . The main problems are that HAMs are complex to design and implement and that there are not many significant applications available.</p><h2 id="maxq">MAXQ</h2><figure><img src="/content/images/2019/03/image50.png" width="500"><figcaption>Example of a MAXQ hierarchy. <a href="https://arxiv.org/pdf/cs/9905014.pdf">Source</a></figcaption></figure><p>MAXQ is a hierarchical learning algorithm in which the hierarchy of a task is obtained by decomposing the Q value of state-action pair into the sum of two components $Q(p,s,a) = V(a,s) + C(p,s,a)$ where $V(a,s)$ is the total expected reward received when executing the action $a$ in state $s$ (classic $Q$) and $C(p,s,a)$ is the total reward expected from the performance of the parent-task, noted by $p$, after taking the action $a$. In fact, the action $a$ may not only contain a primitive action, but also a sequence of actions.</p><p>In essence, one can understand the MAXQ framework  as decomposing the value function of an MDP into combinations of value functions of smaller constituent MDPs, a finite set of sub-tasks where each sub-task is formalized as 1. a termination predicate, 2. a set of actions and 3. a pseudo reward. For this particular aspect, the MAXQ framework is related to the Feudal Q-learning.</p><p>Nevertheless, MAXQ’s advantage over the other frameworks is that it learns a <em>recursively optimal policy</em>, meaning that the policy for a parent task is optimal given the learnt policies of its children. Namely, the task’s policy is context-free: each subtask is optimally solved without reference to the context in which it is executed. While this does not mean it will find an optimal policy, it opens the door to state abstraction and better transfer learning, and can provide common macro actions to many other tasks.</p><p>State abstraction helps to reduce memory. Think about it: when you want to reach a door, no matter what colour the door is or whether it is made of wood or metal. State abstraction should help to represent similar states and reject irrelevant state variables. Moreover, with state abstraction, the necessary exploration is reduced and their reusability increased (because they do not depend on their higher parents). In fact, an abstract state is a state with fewer state variables: different states in the world correspond to the same abstract state. Therefore, if we can reduce some state variables (only a few variables are relevant to the task), then we can significantly reduce the learning time. Ultimately, we will use different abstract states for different macro-actions.</p><p>In short, the MAXQ framework proposes a real hierarchical decomposition of tasks (contrary to Options), it facilitates the reuse of sub-policies and allows temporal and spatial abstraction. Although one of the concerns is that MAXQ involves a very complex structure and that recursively optimal policies can be highly suboptimal policies.</p><hr><h2 id="recentworks">Recent works</h2><p>Inspired (or interpretable) by these founding elements (Feudal, Options, HAM, MAXQ) of HRL, interesting articles have been published more recently with rather encouraging results.</p><p>FeUdal Networks  present a modular architecture. Inspired by Dayan’s seminal idea of Feudal RL, the manager chooses a direction to go in a latent state space, and the worker learns to achieve that direction through actions in the environment. This means that FuN represents sub-goals as directions in latent state space which then translate into meaningful behavioural primitives. The paper introduces a method that allows better long-term credit assignment and makes memorisation more tractable.</p><figure><img src="/content/images/2019/03/image47.png" width="800"><figcaption>FuN Architecture. <a href="https://arxiv.org/abs/1703.01161">Source</a></figcaption></figure><p>While earlier works used pre-specified option policies, there has been recent success in discovering options such as this paper  which showcases an end-to-end trainable system that can scale to very large domains with sub-policies with theoretical possibility of learning options jointly with a policy-over-options by extending the policy gradient theorem to options. Contrary to FuN, here the Managers’ output is trained with gradients coming directly from the Worker and no intrinsic reward is used.</p><figure><img src="/content/images/2019/03/image55.png" width="400"><figcaption>Termination probabilities learnt by the option-critic agent with 4 options. <a href="https://arxiv.org/abs/1609.05140">Source</a></figcaption></figure><p>As you can see by the figure above, termination events are more likely to occur near the doors, intuitively this means that reaching those doors are learnt as being meaningful sub-goals.</p><p>The main contribution of  is that the method is very sample efficient compared to previous works thanks to a novel off-policy correction and the fact that the learning algorithm directly uses the state observation as the goal. There is no goal representation, hence no goal representation training needed. This means that the higher-level policy receives a meaningful supervision signal from the task reward at the outset.</p><figure><img src="/content/images/2019/03/image46.png" width="800"><figcaption>Design and basic training of HIRO. <a href="https://arxiv.org/abs/1805.08296">Source</a></figcaption></figure><figure><img src="/content/images/2019/03/image56.png" width="1000"><figcaption>Comparison between HAC and HIRO on 3 continuous tasks. <a href="https://openreview.net/forum?id=ryzECoAcY7">Source</a></figcaption></figure><p>“We introduce a new HRL framework that can significantly accelerate learning by enabling hierarchical agents to jointly learn a hierarchy of policies. Our framework is primarily comprised of two components: (i) a particular hierarchical architecture and (ii) a method for learning the multiple levels of policies in parallel given sparse rewards. The hierarchies produced by our framework have a specific architecture consisting of a set of nested, goal-conditioned policies that use the state space as the mechanism for breaking down a task into subtasks. [...] HIRO, which was developed simultaneously and independently to our approach, uses the same hierarchical architecture, but does not use either form of hindsight and is therefore not as efficient at learning multiple levels of policies in sparse reward tasks.”</p><p>“We study a novel architecture and training procedure for locomotion tasks. A high-frequency, low-level "spinal" network with access to proprioceptive sensors learns sensorimotor primitives by training on simple tasks. This pre-trained module is fixed and connected to a low-frequency, high-level "cortical" network, with access to all sensors, which drives behavior by modulating the inputs to the spinal network. Where a monolithic end-to-end architecture fails completely, learning with a pre-trained spinal module succeeds at multiple high-level tasks, and enables the effective exploration required to learn from sparse rewards.”</p><p>This paper introduces a HRL method for training locomotive controllers that effectively improves sample efficiency and achieves transfer among different tasks. The idea of the authors is to obtain low-level policies that are invariable according to the tasks. Then by recycling and re-training the meta-policy that schedules over the low-level policies, different skills can be obtained with fewer samples than by training from scratch.</p><figure><img src="/content/images/2019/03/image53.png" width="600"><figcaption>Screenshot of the humanoid approaching a virtual gate in the transfer task. <a href="https://arxiv.org/abs/1610.05182">Source</a></figcaption></figure><p>In this paper, the authors focused on SC2LE, the StarCraft research learning environment introduced by DeepMind. They developed a method for full-length game learning where a controller chooses a sub-policy based on current observations at each relatively large time interval (8 seconds). Then, at each relatively short time interval (one second), a sub-policy chooses a macro-action that is mastered before learning from the repetitions of human expert games. StarCraft is a very challenging playground where the state and action spaces are very large. The approach takes advantage of the hierarchical structure to reduce those spaces. In addition, the size of the execution steps of strategic movements is reduced with the temporal abstraction provided by the controller. Finally, each sub-policy can have its own specific reward function which help divide the complex problem into several easier sub-problems.</p><figure><img src="/content/images/2019/03/image62.png" width="800"><figcaption>Screenshot of the StarCraft II environment. <a href="https://arxiv.org/abs/1809.09095">Source</a></figcaption></figure><p>“We present hierarchical-DQN (h-DQN), a framework to integrate hierarchical value functions, operating at different temporal scales, with intrinsically motivated deep reinforcement learning. A top-level value function learns a policy over intrinsic goals, and a lower-level function learns a policy over atomic actions to satisfy the given goals. h-DQN allows for flexible goal specifications, such as functions over entities and relations. This provides an efficient space for exploration in complicated environments. We demonstrate the strength of our approach on two problems with very sparse, delayed feedback: (1) a complex discrete stochastic decision process, and (2) the classic ATARI game ‘Montezuma's Revenge’.”</p><figure><img src="/content/images/2019/03/image43.png" width="800"><figcaption>The meta-controller chooses the goal (illustrated in red) which the controller tries to satisfy by taking a series of low-level actions. <a href="https://arxiv.org/abs/1604.06057">Source</a></figcaption></figure><p>“In this work, we formulate an approach for the end-to-end meta learning of hierarchical policies. We present a model for representing shared information as a set of sub-policies. We then provide a framework for training these models over distributions of environments. Even though we do not optimize towards the true objective, we achieve significant speedups in learning. In addition, we naturally discover diverse sub-policies without the need for hand engineering.”</p><figure><img src="/content/images/2019/03/image51.png" width="800"><figcaption>Sub-policies learned from mazes to move down, right, and up. <a href="https://arxiv.org/abs/1710.09767">Source</a></figcaption></figure><p>“We introduced Modulated Policy Hierarchies (MPHs) to address environments with sparse rewards that can be decomposed into sub-tasks. By combining rich modulation signals, temporal abstraction, and intrinsic motivation, MPH benefits from better exploration and increased stability of training. Moreover, in contrast to many state-of-the-art approaches, MPH does not require pre-training, multiple training phases or manual reward shaping. We evaluated MPH on two simulated robot manipulation tasks: pushing and block stacking. In both cases, MPH outperformed baselines and the recently proposed MLSH algorithm, suggesting that our approach may be a fertile direction for further investigation.”</p><figure><img src="/content/images/2019/03/image61.png" width="800"><figcaption>Modulation signals for a trained options baseline and MPH on the stacking task. <a href="https://arxiv.org/abs/1812.00025">Source</a></figcaption></figure><p>“We present a novel deep recurrent neural network architecture that learns to build implicit plans in an end-to-end manner by purely interacting with an environment in reinforcement learning setting. The network builds an internal plan, which is continuously updated upon observation of the next input from the environment. It can also partition this internal representation into contiguous sub-sequences by learning for how long the plan can be committed to - i.e. followed without re-planning. Combining these properties, the proposed model, dubbed STRategic Attentive Writer (STRAW) can learn high-level, temporally abstracted macro- actions of varying lengths that are solely learnt from data without any prior information. These macro-actions enable both structured exploration and economic computation. We experimentally demonstrate that STRAW delivers strong improvements on several ATARI games by employing temporally extended planning strategies (e.g. Ms. Pacman and Frostbite). It is at the same time a general algorithm that can be applied on any sequence data.”</p><figure><img src="/content/images/2019/03/image57-1.png" width="800"><figcaption>Schematic illustration of STRAW playing a maze navigation game. <a href="https://arxiv.org/abs/1606.04695">Source</a></figcaption></figure><p>“We propose a lifelong learning system that has the ability to reuse and transfer knowledge from one task to another while efficiently retaining the previously learned knowledge-base. Knowledge is transferred by learning reusable skills to solve tasks in Minecraft. [...] These reusable skills, which we refer to as Deep Skill Networks, are then incorporated into our novel Hierarchical Deep Reinforcement Learning Network (H-DRLN) architecture using two techniques: (1) a deep skill array and (2) skill distillation, our novel variation of policy distillation (Rusu et. al. 2015) for learning skills.”</p><p>“We propose Abstract Markov Decision Process (AMDP) hierarchies as a method for reasoning about a network of subgoals. [...] An AMDP is an MDP whose states are abstract representations of the states of an underlying environment (the ground MDP). The actions of the AMDP are either primitive actions from the environment MDP or subgoals to be solved. [...] A major limitation of MAXQ is that value functions over the hierarchy are found by processing the state–action space at the lowest level and backing up values to the abstract subtask nodes (bottom-up process). [...] AMDPs model each subtask’s transition and reward functions locally, resulting in faster planning, since backup across multiple levels of the hierarchy is unnecessary. This top-down planning approach decides what a good subgoal is before planning to achieve it.”</p><figure><img src="/content/images/2019/03/image45.png" width="800"><figcaption>Left: starting position for the turtlebot. Middle: the turtlebot gets to the block. Right: it pushes the block into the goal room. <a href="https://aaai.org/ocs/index.php/ICAPS/ICAPS17/paper/view/15759">Source</a></figcaption></figure><p>“For complex, high-dimensional Markov Decision Processes (MDPs), it may be necessary to represent the policy with function approximation. A problem is mis- specified whenever, the representation cannot express any policy with acceptable performance. [...] We introduce a meta-algorithm, Iterative Hierarchical Optimization for Misspecified Problems (IHOMP), that uses an RL algorithm as a “black box” to iteratively learn options that repair MPs. To force the options to specialize, IHOMP uses a partition of the state-space and trains one option for each class in the partition.”</p><figure><img src="/content/images/2019/03/image54.png" width="400"><figcaption>An episodic MDP with S-shaped state-space and goal region G. (i) Flat approach, vs. (ii) Hierarchical approach. <a href="https://arxiv.org/abs/1602.03348">Source</a></figcaption></figure><p>“Our approach uses unsupervised asymmetric self-play [15] as a pre-training phase for the low-level policy, prior to training the hierarchical model. In self-play, the agent devises tasks for itself via the goal embedding and then attempts to solve them. [...] A high-level policy can then direct the lower one by generating a sequence of continuous sub-goal vectors. [...] These can then be utilized in a hierarchical RL framework to speed exploration on complex tasks with sparse reward. Experiments on AntGather demonstrate the ability of the resulting hierarchical controller to move the Ant long distances to obtain reward, unlike non-hierarchical policy gradient methods. One limitation of our self-play approach is that the choice of D (the distance function used to decide if the self-play task has been completed successfully or not) requires some domain knowledge.”</p><figure><img src="/content/images/2019/03/image62-1.png" width="800"><figcaption>HSP method architecture. <a href="https://arxiv.org/abs/1811.09083">Source</a></figcaption></figure><p>“We propose and implement a novel model-free method for subgoal discovery using incremental unsupervised learning over a small memory of the most recent experiences of the agent. When combined with an intrinsic motivation learning mechanism, this method learns subgoals and skills together, based on experiences in the environment. Thus, we offer an original approach to HRL that does not require the acquisition of a model of the environment, suitable for large-scale applications. We conducted experiments using our method on large-scale RL problems, such as portions of the difficult Atari 2600 game Montezuma’s Revenge.”</p><figure><img src="/content/images/2019/03/image49.png" width="800"><figcaption>From left to right: A sample screen from the ATARI 2600 game Montezuma’s Revenge; The CNN architecture for the controller’s value function; The CNN architecture for the meta- controller’s value function; The results of the unsupervised subgoal discovery algorithm. <a href="https://arxiv.org/abs/1810.10096">Source</a></figcaption></figure><hr><h1 id="thefutureofhrl">The Future of HRL</h1><p>In the field of cognitive science, research  has long suggested that human and animal behaviour is based on a hierarchical structure. There certainly is a shift that real-world and complex environments will require us to adopt. This could be found in one of the main appealing aspects of HRL: the use of skills to reduce the search complexity of the problem.</p><blockquote><p><em>"Stop learning tasks, start learning skills." -Satinder Singh, NeurIPS 2018</em></p></blockquote><p>However, depending on the framework used, specifying a good hierarchy by hand requires domain-specific knowledge and careful engineering, motivating the need for learning skills automatically. Essentially, to choose an appropriate hierarchy framework one needs to look how available the domain knowledge is (a friendly combination of the three is also conceivable ):</p><ul><li>If the behaviours are completely specified → Options</li><li>If the behaviours are partially specified → HAM</li><li>If less domain knowledge is available → MAXQ, Learned Options </li></ul><p>In the table below, the columns imply:</p><ul><li><em>Temporal abstraction</em> allows representing knowledge about courses of action that take place at different time scales. We talk about temporally extended actions</li><li><em>State abstraction</em> occurs when a sub-task ignores some aspects of the state of the environment, it requires that sub-tasks be specified in terms of termination predicates as opposed to using the option or partial policy methods</li><li><em>"Sub-tasks: fixed policy provided by the programmer"</em> means given a set of options, the system learns a policy over those options</li><li><em>"Sub-tasks: non-deterministic finite-state controller"</em> means given a hierarchy of partial policies the system learns a policy for the entire problem</li><li><em>“Sub-tasks: termination predicate and a local reward function"</em> means given a set of sub-tasks the system learns policies for entire problem</li><li>A <em>hierarchical optimal policy</em> is a policy that is optimal among all the policies that can be expressed given the hierarchical structure</li><li>A <em>recursively optimal policy</em> is a policy optimal for each SMDP corresponding to each of the sub-tasks in the decomposition</li></ul><p><img src="/content/images/2019/03/image66.png" alt=""></p><p>Comparison of the foundational HRL frameworks characteristics</p></div></div></div></div>
</div>
    </div>
    <footer>
        <div>created by buildstarted &copy; 2020 <a href="/about">about</a></div>
        <div>Share this page on social media: copy and paste this url https://linksfor.dev/</div>
        <div>If you prefer RSS: <a href="https://linksfor.dev/feed.xml">https://linksfor.dev/feed.xml</a></div>
    </footer>
    
    <script>
        (function () {
            var COLLECT_URL = "https://dna.buildstarted.com/t";
            var SITE_ID = "linksfor.devs";
            var GLOBAL_VAR_NAME = "__DNA__";

            window[GLOBAL_VAR_NAME] = {};

            window[GLOBAL_VAR_NAME].sendPageView = function () {
                var path = location.pathname;
                var referrer = document.referrer;

                var url = COLLECT_URL + "?siteid=" + SITE_ID + "&p=" + encodeURIComponent(path) + "&r=" + encodeURIComponent(referrer);

                try { fetch(url, { method: "GET" }); } catch (e) { }
            };

            window[GLOBAL_VAR_NAME].sendPageView();
        })();
    </script>
</body>
</html>