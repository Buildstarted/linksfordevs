<!DOCTYPE html>
<html lang="en">
<head>
    <title>linksfor.dev(s)</title>
    <meta charset="utf-8">
    <link rel="alternate" type="application/rss+xml" title="Linksfor.dev(s) feed" href="https://linksfor.dev/feed.rss" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta name="google" value="notranslate">
    <link rel="stylesheet" type="text/css" href="/style.css" />
    <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon">
    <link rel="icon" href="/favicon.ico" type="image/x-icon">
    
    <meta property="article:author" content="Buildstarted"/>
    <meta property="og:site_name" content="linksfor.dev(s)" />
    <meta property="og:title" content="linksfor.dev(s)" />
    <meta property="og:type" content="website" />
    <meta property="og:description" content="A curated list of sources of development information including c#, c++, and other dev related links." />
</head>
<body>
    <div class="grid">
        <h1>
                <span style="cursor: default" title="linksfor.dev(s) has been running for 1 year! :partypopper:">üéâ</span>
<a href="/" style="color:inherit">linksfor.dev(s)</a>
        </h1>
        <title>linksfor.dev(s) - Breaking Down Barriers &#x2013; Part 5: Back To The Real World</title>
<div class="readable">
        <h1>Breaking Down Barriers &#x2013; Part 5: Back To The Real World</h1>
        <p>
Reading time: 36-46 minutes        </p>
        <p><a href="https://mynameismjp.wordpress.com/2018/09/08/breaking-down-barriers-part-5-back-to-the-real-world/">https://mynameismjp.wordpress.com/2018/09/08/breaking-down-barriers-part-5-back-to-the-real-world/</a></p>
        <hr/>
<div id="readability-page-1" class="page"><article id="post-7866">

		<!-- .entry-header -->

		
		<div>

			<div>
				<p data-adtags-visited="true"><em>This is Part 5 of a series about GPU synchronization and preemption. You can find the other articles here:</em></p>
<p data-adtags-visited="true"><a href="https://mynameismjp.wordpress.com/2018/03/06/breaking-down-barriers-part-1-whats-a-barrier/">Part 1 ‚Äì What‚Äôs a Barrier?</a><br>
<a href="https://mynameismjp.wordpress.com/2018/04/01/breaking-down-barriers-part-2-synchronizing-gpu-threads/">Part 2 ‚Äì Synchronizing GPU Threads</a><br>
<a href="https://mynameismjp.wordpress.com/2018/06/17/breaking-down-barriers-part-3-multiple-command-processors/">Part 3 ‚Äì Multiple Command Processors</a><br>
<a href="https://mynameismjp.wordpress.com/2018/07/03/breaking-down-barriers-part-4-gpu-preemption/">Part 4 ‚Äì GPU Preemption</a><br>
<a href="https://mynameismjp.wordpress.com/2018/09/08/breaking-down-barriers-part-5-back-to-the-real-world/">Part 5 ‚Äì Back To The Real World</a><br>
<a href="https://mynameismjp.wordpress.com/2018/12/09/breaking-down-barriers-part-6-experimenting-with-overlap-and-preemption/">Part 6 ‚Äì Experimenting With Overlap and Preemption</a></p>
<p data-adtags-visited="true">Welcome to part 5 of the series! If you‚Äôve read all of the articles so far, thanks for hanging in there! In the <a href="https://mynameismjp.wordpress.com/2018/07/03/breaking-down-barriers-part-4-gpu-preemption/">last article</a>, we spent a good deal of time talking about how GPU preemption might work for an imaginary GPU. We also talked about how having multiple command processors capable of kicking off GPU threads could potentially be helpful for reducing preemption latency, which in turn can improve the overall responsiveness of a GPU that needs to serve multiple applications.&nbsp; For this article, I want to get back to discussing real-world GPU‚Äôs, and how they deal with preemption in the context of an actual OS and driver infrastructure. I‚Äôm mostly going to restrict myself to discussing Microsoft Windows and its associated Windows Display Driver Model (WDDM) and native 3D graphics API‚Äôs (D3D11 and D3D12), since these are what I know best and have the most experience in. So I‚Äôm afraid you‚Äôll have to look elsewhere if you need comprehensive information about Linux, MacOS, OpenGL, or Metal. I will talk just a bit about Vulkan in a few places, but I‚Äôm pretty far from an expert on that topic. I‚Äôm also going to only talk about desktop GPU‚Äôs, since these are what I deal with almost exclusively in my line of work.</p>
<p data-adtags-visited="true">The first part of this article is something of a retrospective on pre-Windows 10 technology, so feel free to skip that if you‚Äôre not interested. However I think that it provides a lot of valuable context for understanding&nbsp;<em>why</em> D3D12 is designed the way it is (which is of course one of the central themes of this series!), which is why I included it in the first place. It‚Äôs also possible that my understanding of the Windows driver model is incomplete or inaccurate in a few ways, since I‚Äôve only ever been on the application side of things. If you‚Äôre a seasoned driver developer or OS engineer and you read something that causes you to make a face, please feel free to reach out and/or correct me in the comments.</p>
<h3>The Past: XDDM and WDDM 1.x</h3>
<p data-adtags-visited="true">When I first started learning the very basics of graphics programming, I was in my junior year of college back in 2005/6. Naturally I used the very latest in 3D graphics API‚Äôs to draw my excessively-shiny spheres, which at the time was D3D9 running on Windows XP. Things were rather‚Ä¶different back then. 3D rendering on the GPU still wasn‚Äôt really mainstream in terms of general application/OS usage, and was generally only used by games that expected to run in exclusive fullscreen modes. Technically it was possible to have two different programs simultaneously use D3D9 to render to two different windows, but the extent to which those two programs would cooperate and play nicely with each other could vary depending on the particular video card you were using, as well as the implementation details of its driver<sup><a href="#LostDevice">1</a></sup>. This was due to the fact that the Windows 2000/XP display driver model (known as <a href="https://docs.microsoft.com/en-us/windows-hardware/drivers/display/windows-2000-display-driver-model-design-guide">XDDM</a>, or XPDM) mostly let display drivers handle complex issues like scheduling command buffers for execution on the GPU, or managing GPU resource memory from multiple processes. Things were particularly rough on the memory/resource side of things, since the OS and drivers were working through the aging <a href="https://docs.microsoft.com/en-us/windows-hardware/drivers/display/directdraw-architecture">DirectDraw infrastructure</a>.&nbsp; On top of that you still had the entire legacy GDI pipeline with its own <a href="https://docs.microsoft.com/en-us/windows-hardware/drivers/display/graphics-system-overview">driver stack</a>, which was supposed to interoperate with D3D and DirectDraw.</p>
<p data-adtags-visited="true">All of this makes sense when you consider that it all grew organically from a world where 3D GPU‚Äôs were still a novelty intended for playing games, and 2D graphics accelerators were ubiquitous. However it was also quite clear that the landscape was rapidly changing by the time that Windows Vista was nearing release. 3D-capable video processors were now found even in low-end machines (thanks to ‚Äúintegrated‚Äù GPU‚Äôs such as the <a href="https://en.wikipedia.org/wiki/Intel_GMA">GMA series from Intel</a>), and the growing programmability of GPU‚Äôs was causing intrepid programmers to explore the possibility of using those GPU‚Äôs for things outside the realm of 3D graphics for games. OS developers like Microsoft were also considering the idea of using the GPU‚Äôs 3D rendering capabilities as a core component of how they displayed UI on their desktops. In other words, GPU‚Äôs were really starting to grow up, which meant it was time for the OS/driver stack to grow up along with them. This is where <a href="https://en.wikipedia.org/wiki/Windows_Display_Driver_Model">WDDM</a> comes in.</p>
<p data-adtags-visited="true">WDDM, which is short for ‚Äú<a href="https://docs.microsoft.com/en-us/windows-hardware/drivers/display/windows-vista-display-driver-model-design-guide">Windows Display Driver Model</a>‚Äú, was the new driver stack that replaced XDDM/XPDM in all Windows versions starting with Vista. WDDM was quite a departure from the earlier models, in that it really started to treat the GPU and its on-board memory as a shared resource arbitrated by the OS itself. For instance, with WDDM the OS was now in charge of submitting command buffers to a GPU, while the driver merely provided <a href="https://docs.microsoft.com/en-us/windows-hardware/drivers/display/submitting-a-command-buffer">hooks</a> for building command buffers in its hardware-specific format, and then executing a command buffer scheduled by the OS. This allows the OS internal scheduler to decide when a particular program‚Äôs packet of work should be executed, potentially deciding that one program‚Äôs commands are more important than another program‚Äôs commands. In a similar vein, a global video memory manager now ‚Äúowned‚Äù the on-board memory of the video card, with the driver again providing the <a href="https://docs.microsoft.com/en-us/windows-hardware/drivers/display/paging-video-memory-resources">necessary hooks</a> that would allow the OS to move data from system memory to video memory (or vice versa). This helped to unify GPU memory management across all vendors, and removed the need for band-aids like the old <a href="https://docs.microsoft.com/en-us/windows/desktop/direct3d9/managing-resources">MANAGED resource pool</a> from D3D9 by effectively virtualizing all resources. The <a href="https://docs.microsoft.com/en-us/windows-hardware/drivers/display/windows-vista-and-later-display-driver-model-operation-flow">general flow</a> of commands through the WDDM driver stack looked something like this:</p>
<p data-adtags-visited="true"><img data-attachment-id="7867" data-permalink="https://mynameismjp.wordpress.com/2018/09/08/breaking-down-barriers-part-5-back-to-the-real-world/wddm_flow/" data-orig-file="https://mynameismjp.files.wordpress.com/2018/08/wddm_flow.png" data-orig-size="1102,988" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="WDDM_Flow" data-image-description="" data-medium-file="https://mynameismjp.files.wordpress.com/2018/08/wddm_flow.png?w=300" data-large-file="https://mynameismjp.files.wordpress.com/2018/08/wddm_flow.png?w=756" src="https://mynameismjp.files.wordpress.com/2018/08/wddm_flow.png?w=756" alt="WDDM_Flow" srcset="https://mynameismjp.files.wordpress.com/2018/08/wddm_flow.png?w=756 756w, https://mynameismjp.files.wordpress.com/2018/08/wddm_flow.png?w=150 150w, https://mynameismjp.files.wordpress.com/2018/08/wddm_flow.png?w=300 300w, https://mynameismjp.files.wordpress.com/2018/08/wddm_flow.png?w=768 768w, https://mynameismjp.files.wordpress.com/2018/08/wddm_flow.png?w=1024 1024w, https://mynameismjp.files.wordpress.com/2018/08/wddm_flow.png 1102w" sizes="(max-width: 756px) 100vw, 756px"></p>
<p data-adtags-visited="true">If you haven‚Äôt been a windows driver developer for the past 10 years, there‚Äôs probably a few steps in there that would make you raise an eyebrow. For instance, what the heck is a ‚Äú<a href="https://docs.microsoft.com/en-us/windows-hardware/drivers/display/introduction-to-command-and-dma-buffers">DMA buffer</a>‚Äú? It sounds like something fancy that might be related to hardware DMA units, but in reality it‚Äôs just what the OS calls command buffers on the kernel side of the user-mode/kernel-mode boundary. I suspect that the ‚ÄúDMA‚Äù part of that name comes from the fact that the GPU can directly read from a DMA buffer through a physical address, but I don‚Äôt actually know for sure. The step that you might find confusing is the one that‚Äôs labeled ‚ÄúPatch DMA Buffer with Physical Addresses‚Äù. At the time of WDDM 1.0, GPU‚Äôs had simple memory controllers that could only access their memory using physical addresses. This is rather unlike CPU‚Äôs, which have the capability to work with virtual addresses that are mapped to physical addresses using a page table. Having the vendor-supplied drivers work directly with physical addresses when dealing with resources like buffers or textures would generally be a Bad Idea for a few reasons, but under WDDM it‚Äôs a non-starter due to the fact that the OS‚Äôs video memory manager has the ability to move resources in and out of device memory as it sees fit. So to make things work, WDDM requires that the driver submit allocation and patch lists alongside DMA buffers. The allocation list tells the OS all of the resources that referenced by the commands inside the DMA buffer, which lets the memory manager decide which resources need to be resident in device memory. The patch list then tells the OS where all of the resources are actually referenced within the DMA buffer, which the OS then uses to <a href="https://docs.microsoft.com/en-us/windows-hardware/drivers/ddi/content/d3dkmddi/nc-d3dkmddi-dxgkddi_patch">patch the DMA buffer with physical addresses</a> right before that buffer is executed on the GPU.</p>
<p data-adtags-visited="true">It should be obvious at this point that there was a whole lot going on behind the scenes whenever an app made a few draw calls! Fortunately D3D application developers only really had to worry about that left column, since everything in the other columns was generally hidden from you. In fact, the way that the D3D10/D3D11 API‚Äôs were written didn‚Äôt even directly expose the fact that GPU‚Äôs were parallel processors being fed by buffers full of command streams. For the most part you could pretend that your Draws and Dispatches were happening synchronously with your CPU commands, and you would still get correct results. The only time the abstraction really leaked through was if you looked at the actual timings of your calls, particularly if you hit a case that required the CPU to wait for the GPU to catch up. All things considered, writing D3D programs really got <em>easier</em> for application developers in the transition from D3D9 to D3D10/11, with some of the reasons why getting spelled out in this <a href="https://docs.microsoft.com/en-us/windows/desktop/direct3darticles/graphics-apis-in-windows-vista">document</a>. <sup><a href="#UserKernelMode">2</a></sup> Driver and OS developers weren‚Äôt so lucky, and things were certainly quite rocky back during the initial transition from XDDM to WDDM 1.0. But after years of refinements it seems to have proved its worth, as we now take it for granted that the <a href="https://docs.microsoft.com/en-us/windows/desktop/dwm/dwm-overview">desktop composites its windows using the GPU</a>, or that <a href="https://www.chromium.org/developers/design-documents/gpu-accelerated-compositing-in-chrome">Chrome can use GPU acceleration</a> for rendering webpages while you‚Äôre simultaneously playing Battlefield 1 on highest settings.</p>
<p data-adtags-visited="true">While there were a lot of minor updates to WDDM over various Windows versions and service packs, there was one major improvement in Windows 8 that‚Äôs worth calling out. Starting in WDDM 1.2, drivers could now <a href="https://docs.microsoft.com/en-us/windows-hardware/drivers/display/gpu-preemption">specify whether or not they supported preemption</a> of their command buffers, and also what level of granularity they could prempt at. Drivers indicating their supporting by returning two enum values, <a href="https://docs.microsoft.com/en-us/windows-hardware/drivers/ddi/content/d3dkmdt/ne-d3dkmdt-_d3dkmdt_graphics_preemption_granularity">one for graphics</a> and <a href="https://docs.microsoft.com/en-us/windows-hardware/drivers/ddi/content/d3dkmdt/ne-d3dkmdt-_d3dkmdt_compute_preemption_granularity">one for compute</a>. Looking at the compute enum, the possible values seem quite similar to the premption granularities that we discussed in the earlier articles:</p>
<div><div id="highlighter_400414"><table><tbody><tr><td><p>1</p><p>2</p><p>3</p><p>4</p><p>5</p><p>6</p><p>7</p><p>8</p></td><td><div><p><code>typedef enum _D3DKMDT_COMPUTE_PREEMPTION_GRANULARITY {</code></p><p><code>D3DKMDT_COMPUTE_PREEMPTION_NONE ,</code></p><p><code>D3DKMDT_COMPUTE_PREEMPTION_DMA_BUFFER_BOUNDARY ,</code></p><p><code>D3DKMDT_COMPUTE_PREEMPTION_DISPATCH_BOUNDARY ,</code></p><p><code>D3DKMDT_COMPUTE_PREEMPTION_THREAD_GROUP_BOUNDARY ,</code></p><p><code>D3DKMDT_COMPUTE_PREEMPTION_THREAD_BOUNDARY ,</code></p><p><code>D3DKMDT_COMPUTE_PREEMPTION_SHADER_BOUNDARY</code></p><p><code>} D3DKMDT_COMPUTE_PREEMPTION_GRANULARITY;</code></p></div></td></tr></tbody></table></div></div>
<p data-adtags-visited="true">As we discussed in earlier articles, finer-grained preemption can make for much nicer user experience when multitasking, especially when a particular program is really trying to hog the GPU.</p>
<p data-adtags-visited="true">UPDATE: it turns out that you can actually view your video card‚Äôs reported premption granularity by using the old DirectX Caps Viewer! These days its included as part of the Windows 10 SDK, so you can typically find it in C:\Program Files (x86)\Windows Kits\10\bin\&lt;SDKVersion&gt;\x64\dxcapsviewer.exe. An Nvidia Titan V reports pixel-level for graphics and dispatch-level for compute, while an AMD RX Vega reports draw-level for graphics and DMA buffer-level for compute. Many thanks to <a href="https://twitter.com/Locuza_">Locuza</a> for pointing this out to me!</p>
<h3>Problems With D3D11 and WDDM 1.x</h3>
<p data-adtags-visited="true">I think it‚Äôs fair to say that from a D3D application developer‚Äôs point of view, things generally got better and easier when comparing the Windows XP era to the Windows 7 era. I‚Äôm sure driver and OS developers had plenty to complain about, but certainly in my day job as a game developer I appreciated the increased orthogonality of the API‚Äôs as well as the fact that multi-tasking generally ‚Äújust worked‚Äù across a variety of hardware (including multiple GPU‚Äôs in the same machine!). But you should never underestimate the ability of a game programmer to complain about something (especially when it comes to Windows!), and we certainly found some things to gripe about. Pretty much all of them revolved around performance and multithreading.</p>
<p data-adtags-visited="true">Back when D3D11 was first announced, there was a lot of hype around the fact that it allowed for multi-threaded rendering via so-called <a href="https://docs.microsoft.com/en-us/windows/desktop/direct3d11/overviews-direct3d-11-render-multi-thread-render">deferred contexts</a>. While this turned out to be technically true (it‚Äôs totally possible to have multiple threads issuing Draw and Dispatch calls in stock D3D11), in practice it wasn‚Äôt as useful as everyone hoped it would be. Probably the biggest roadblock is something that we discussed way back in <a href="https://mynameismjp.wordpress.com/2018/03/06/breaking-down-barriers-part-1-whats-a-barrier/">part 1</a>: it‚Äôs really tough (or perhaps impossible) to make implicit dependencies work in an efficient way when multiple threads are generating the rendering commands. If you‚Äôve got to run through the entire set of commands for a frame to determine when a depth buffer transitions from being writable to being readable as a texture, it quickly pushes you down the path of serializing your command lists before final generation of the hardware command buffer. This is definitely&nbsp;<em>not</em> great if your goal was to distribute the cost of command buffer generation across multiple cores, which is really the only reason to use deferred command lists in the first place. So that was a pretty big bummer, and led to games (mostly) sticking to single-threaded rendering on Windows.</p>
<p data-adtags-visited="true">Another issue that has sprung up in recent years is D3D11‚Äôs inability to leverage multiple command processors/hardware queues on modern GPU‚Äôs. As we learned in <a href="https://mynameismjp.wordpress.com/2018/06/17/breaking-down-barriers-part-3-multiple-command-processors/">Part 3</a>, submitting commands into multiple front-ends can potentially lead to higher shader core utilization, which in turn leads to higher overall throughput. We also saw that effectively utilizing these additional command processors requires identifying and submitting multiple chains of commands that aren‚Äôt dependent on one another. Having the driver do this automatically is a big ask, especially when data dependencies are implicit rather than explicit. If you think about it, it‚Äôs very similar to utilizing multiple CPU cores via threads: the compiler can handle detecting dependencies within a single thread, but it doesn‚Äôt extract out independent sequences of instructions that would long enough to justify having the OS spin up a whole new thread. The problem is made even more difficult by the fact that command buffer building and submission is also implicit in D3D11/WDDM 1.x, which means there‚Äôs no place for the app to give hints or more explicit input about which front-end a stream of commands should be submitted to. On top of that GPU‚Äôs typically have restrictions on which commands can be executed on which command processors (for instance, some can only consume compute-oriented commands), and D3D11 has no way of expressing these restrictions through its API‚Äôs.</p>
<p data-adtags-visited="true">One last issue that I‚Äôll discuss is less technical in nature, and has to do with the abstraction presented by the API and how it relates to deferred command lists. As I mentioned earlier, D3D11 adopts the model where command buffer recording and submission to the hardware is implicit based on an app‚Äôs sequence of D3D11 commands, and is completely hidden behind the driver. This also effectively hides the asynchronous/parallel nature of the GPU, and lets you assume that draw/dispatch/copy commands happen synchronously and still get correct results. Or at least, that‚Äôs the case if you‚Äôre using the immediate context. Deferred contexts break this abstraction, since their semantics imply that the commands aren‚Äôt actually executed right away and are instead executed when <a href="https://docs.microsoft.com/en-us/windows/desktop/api/d3d11/nf-d3d11-id3d11devicecontext-executecommandlist">ExecuteCommandList</a> is called on the immediate context. This basically works, but it‚Äôs also pretty odd when you consider that the immediate context isn‚Äôt really ‚Äúimmediate‚Äù, and is actually ‚Äúdeferred‚Äù with implicit submission. So really a deferred context was more of a double-deferred context! On top of that weirdness, deferred command lists also had some odd restrictions as a result of their user-facing execution model. In particular, the <a href="https://docs.microsoft.com/en-us/windows/desktop/api/d3d11/nf-d3d11-id3d11devicecontext-map">Map</a>&nbsp;operation was restricted to write-only operations, and disallowed anything involving reading. This stems from the fact that commands on a deferred command list can‚Äôt be expected to execute (from an app point of view) until ExecuteCommandList was called, which in turn meant that you couldn‚Äôt expect to read the results of prior commands. This is actually similar to how things work now in D3D12 (more on that in a minute), but it‚Äôs a bit strange that those semantics only applied when doing multi-threading in D3D11. Finally, I suspect that the automatic memory management/versioning happening behind the scenes when using&nbsp;D3D11_MAP_WRITE_DISCARD really stretched things to their breaking point when having to support that in a multi-threaded, deferred context scenario. At some point it just makes more sense to let apps manage their memory and submission using the app‚Äôs own knowledge of its particular needs, which leads us to‚Ä¶</p>
<h3>The Present: Windows 10, D3D12, and WDDM 2.0</h3>
<p data-adtags-visited="true">With D3D12 and <a href="https://docs.microsoft.com/en-us/windows-hardware/drivers/display/wddm-2-0-and-windows-10">WDDM 2.0</a>, the API and driver stack has pulled back much of the curtain around how work actually gets submitted to the GPU. Instead of abstracting away the fact that commands are getting recorded to buffers and executed asynchronously by a parallel processor, D3D12 (almost) directly exposes those details to applications. The API gives you an interface for recording commands (<a href="https://docs.microsoft.com/en-us/windows/desktop/api/d3d12/nn-d3d12-id3d12graphicscommandlist">ID3D12GraphicsCommandList</a>), another interface for storing the memory into which those commands are recorded (<a href="https://docs.microsoft.com/en-us/windows/desktop/api/d3d12/nn-d3d12-id3d12commandallocator">ID3D12CommandAllocator</a>), and a function that allows you to queue a completed command list for execution (<a href="https://docs.microsoft.com/en-us/windows/desktop/api/d3d12/nf-d3d12-id3d12commandqueue-executecommandlists">ExecuteCommandLists</a>). Of course with this newfound power comes new responsibilities. Since the app is in the driver seat now, it means it has to now be aware of when those command buffers are in-flight so that it can avoid writing to them. This leads to a typical double-buffered ‚Äúsubmission loop‚Äù where the application waits for the GPU to finish processing an old command buffer before re-using it:</p>
<p data-adtags-visited="true"><img data-attachment-id="7870" data-permalink="https://mynameismjp.wordpress.com/2018/09/08/breaking-down-barriers-part-5-back-to-the-real-world/command_buffer_submission/" data-orig-file="https://mynameismjp.files.wordpress.com/2018/08/command_buffer_submission1.png" data-orig-size="1200,564" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Command_Buffer_Submission" data-image-description="" data-medium-file="https://mynameismjp.files.wordpress.com/2018/08/command_buffer_submission1.png?w=300" data-large-file="https://mynameismjp.files.wordpress.com/2018/08/command_buffer_submission1.png?w=756" src="https://mynameismjp.files.wordpress.com/2018/08/command_buffer_submission1.png?w=756" alt="Command_Buffer_Submission" srcset="https://mynameismjp.files.wordpress.com/2018/08/command_buffer_submission1.png?w=756 756w, https://mynameismjp.files.wordpress.com/2018/08/command_buffer_submission1.png?w=150 150w, https://mynameismjp.files.wordpress.com/2018/08/command_buffer_submission1.png?w=300 300w, https://mynameismjp.files.wordpress.com/2018/08/command_buffer_submission1.png?w=768 768w, https://mynameismjp.files.wordpress.com/2018/08/command_buffer_submission1.png?w=1024 1024w, https://mynameismjp.files.wordpress.com/2018/08/command_buffer_submission1.png 1200w" sizes="(max-width: 756px) 100vw, 756px"></p>
<p data-adtags-visited="true">You generally deal with similar patterns all over the place when working with D3D12, since you‚Äôre now responsible for all lifetime management when it comes to memory and resources. No more MAP_DISCARD for you when update a constant buffer, instead you have to craft your own mechanisms for grabbing a safe bit of memory. And if you want to read back results from the GPU, you had best make sure that you‚Äôve submitted the relevant command buffer and waited for it to complete before attempting to read that memory on the CPU. Like the barriers that we <a href="https://mynameismjp.wordpress.com/2018/03/06/breaking-down-barriers-part-1-whats-a-barrier/">discussed in the first article,</a> it‚Äôs quite a large burden that‚Äôs been shifted from the driver to the application. And the shift was mostly done for the same exact reason: to enable better efficiency and multithreading. The natural way to divide up command generation is to have each thread or task generate its own command buffer, and then submit them all as a ‚Äúchain‚Äù when they‚Äôre all been recorded. When you combine this with explicit barriers and a single API for specifying pipeline state via PSO‚Äôs, you get an API that truly allows you parallelize command buffer generation across multiple CPU cores.</p>
<p data-adtags-visited="true">Providing more explicit control over submission also gives apps the ability to take advantage of GPU‚Äôs that have more than one front-end for queuing or processing commands. In <a href="https://mynameismjp.wordpress.com/2018/06/17/breaking-down-barriers-part-3-multiple-command-processors/">Part 3</a>, we talked about how adding an additional command processor to a GPU can help improve overall utilization for cases where an application submits multiple distinct dependency chains (commonly referred to asynchronous compute). If we were stuck in the old D3D11 model where dependencies and submission were implicit, it would be very difficult (or impossible) for a driver to be able to extract out a non-dependent chain of dispatches to execute on a second command processor. Fortunately with D3D12 we‚Äôre actually in a position to explicitly say ‚Äúthis list of commands can be executed separately, and depends on this other list of commands‚Äù, which can be done via command queues and fences. Command queues, like their name suggest, represent a single queue into which completed command buffers can be submitted for running on the GPU. When you create a command queue, you have to specify 1 of 3 different command list types to run on that queue:</p>
<ol>
<li>COPY ‚Äì can execute CopyResource commands</li>
<li>COMPUTE ‚Äì can execute CopyResource and Dispatch commands</li>
<li>DIRECT ‚Äì can execute CopyResource, Dispatch, and Draw commands</li>
</ol>
<p data-adtags-visited="true">These command list types can potentially correspond to different ‚Äúengines‚Äù on the GPU, which is what D3D/WDDM like to call a command processor. Each engine can have its own distinct set of commands it can run, hence the 3 types of command lists. Typically a discrete GPU will have at least 1 graphics command processor capable of executing all possible commands, and at least 1 DMA unit that can issue copy commands that are optimized for transferring over the PCI-e bus. Many recent GPU‚Äôs also have additional compute-only command processors, but we‚Äôll get into that in more detail later in this article.</p>
<p data-adtags-visited="true">As an example, let‚Äôs return to the post-processing case that we were discussing in Part 3:</p>
<p data-adtags-visited="true"><img data-attachment-id="7837" data-permalink="https://mynameismjp.wordpress.com/2018/06/17/breaking-down-barriers-part-3-multiple-command-processors/bloom_dof_combined2/" data-orig-file="https://mynameismjp.files.wordpress.com/2018/06/bloom_dof_combined2.png" data-orig-size="1200,466" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Bloom_DOF_Combined2" data-image-description="" data-medium-file="https://mynameismjp.files.wordpress.com/2018/06/bloom_dof_combined2.png?w=300" data-large-file="https://mynameismjp.files.wordpress.com/2018/06/bloom_dof_combined2.png?w=756" src="https://mynameismjp.files.wordpress.com/2018/06/bloom_dof_combined2.png?w=756" alt="Bloom_DOF_Combined2" srcset="https://mynameismjp.files.wordpress.com/2018/06/bloom_dof_combined2.png?w=756 756w, https://mynameismjp.files.wordpress.com/2018/06/bloom_dof_combined2.png?w=150 150w, https://mynameismjp.files.wordpress.com/2018/06/bloom_dof_combined2.png?w=300 300w, https://mynameismjp.files.wordpress.com/2018/06/bloom_dof_combined2.png?w=768 768w, https://mynameismjp.files.wordpress.com/2018/06/bloom_dof_combined2.png?w=1024 1024w, https://mynameismjp.files.wordpress.com/2018/06/bloom_dof_combined2.png 1200w" sizes="(max-width: 756px) 100vw, 756px"></p>
<p data-adtags-visited="true">To submit our work in such a way that the two dependency chains could be executed on both command processors, we would do something like this in D3D12:</p>
<ul>
<li>Create a DIRECT command queue, which we‚Äôll call <strong>GfxQueue</strong></li>
<li>Create 3 DIRECT command lists, which we‚Äôll call <strong>GfxCmdListA</strong>, <strong>GfxCmdListB</strong>, and <strong>GfxCmdListC</strong></li>
<li>Create a COMPUTE command queue and command list, which we‚Äôll call <strong>ComputeQueue</strong> and <strong>ComputeCmdList</strong></li>
<li>Create two fences, <strong>FenceA</strong> and <strong>FenceB</strong></li>
<li>Each frame:
<ul>
<li>Record all rendering commands for the Main Pass to <strong>GfxCmdListA</strong></li>
<li>Record all bloom rendering commands to <strong>GfxCmdListB</strong></li>
<li>Record all tone mapping and subsequent rendering commands to <strong>GfxCmdListC</strong></li>
<li>Record all DOF dispatches to <strong>ComputeCmdList</strong></li>
<li>Submit <strong>GfxCmdListA</strong> to <strong>GfxQueue</strong></li>
<li>Signal <strong>FenceA</strong> from&nbsp;<strong>GfxQueue</strong> on completion of <strong>GfxCmdListA</strong></li>
<li>Submit <strong>GfxCmdListB</strong> to <strong>GfxQueue</strong></li>
<li>Tell <strong>ComputeQueue</strong> to wait for <strong>FenceA</strong></li>
<li>Submit <strong>ComputeCmdList</strong> to <strong>ComputeQueue</strong></li>
<li>Signal <strong>FenceB</strong> from <strong>ComputeQueue</strong> on completion of <strong>ComputeCmdList</strong></li>
<li>Tell <strong>GfxQueue</strong> to wait for <strong>FenceB</strong></li>
<li>Submit <strong>GfxCmdListC</strong> to <strong>GfxQueue</strong></li>
</ul>
</li>
</ul>
<p data-adtags-visited="true"><img data-attachment-id="7871" data-permalink="https://mynameismjp.wordpress.com/2018/09/08/breaking-down-barriers-part-5-back-to-the-real-world/bloom_dof_annotated/" data-orig-file="https://mynameismjp.files.wordpress.com/2018/08/bloom_dof_annotated.png" data-orig-size="1400,543" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Bloom_DOF_Annotated" data-image-description="" data-medium-file="https://mynameismjp.files.wordpress.com/2018/08/bloom_dof_annotated.png?w=300" data-large-file="https://mynameismjp.files.wordpress.com/2018/08/bloom_dof_annotated.png?w=756" src="https://mynameismjp.files.wordpress.com/2018/08/bloom_dof_annotated.png?w=756" alt="Bloom_DOF_Annotated" srcset="https://mynameismjp.files.wordpress.com/2018/08/bloom_dof_annotated.png?w=756 756w, https://mynameismjp.files.wordpress.com/2018/08/bloom_dof_annotated.png?w=150 150w, https://mynameismjp.files.wordpress.com/2018/08/bloom_dof_annotated.png?w=300 300w, https://mynameismjp.files.wordpress.com/2018/08/bloom_dof_annotated.png?w=768 768w, https://mynameismjp.files.wordpress.com/2018/08/bloom_dof_annotated.png?w=1024 1024w, https://mynameismjp.files.wordpress.com/2018/08/bloom_dof_annotated.png 1400w" sizes="(max-width: 756px) 100vw, 756px"></p>
<p data-adtags-visited="true">Since these D3D12 commands are very explicit about the dependencies between command lists as well as the kind of queue that the command lists should be submitted to, the OS and driver now have enough information to actually schedule the command lists on separate command processors. This is a major improvement over D3D11, whose implicit submission model didn‚Äôt really allow for this to occur.</p>
<p data-adtags-visited="true">Based on how the queue and submission API‚Äôs work, you may be tempted into thinking that submitting a command list to an ID3D12CommandQueue will directly submit the corresponding command buffer(s) to a hardware queue for a particular GPU front-end. But this isn‚Äôt really true in D3D12 and WDDM 2.0. When we were going over the details of WDDM 1.x, we discussed how the OS actually had its own software scheduler that was responsible for deciding when a command buffer should actually run on a GPU, and potentially arbitrating submissions from multiple applications. This is very much still the case in D3D12/WDDM2.0. Consequently you‚Äôre not submitting command buffers to a hardware queue, you‚Äôre instead submitting them to the OS‚Äôs scheduler. In addition to letting the OS handle shared resources between multiple applications, it also allows the OS to ‚Äúvirtualize‚Äù the queues for hardware that doesn‚Äôt actually support concurrent execution on multiple front ends (in D3D/WDDM terminology we would say that the GPU only has a single engine). If you look at the descriptions of the queue types that I listed above, you may have noticed that they‚Äôre set up such that each queue type‚Äôs list of supported commands is actually a subset of the next queue type‚Äôs supported commands. In other words, the DIRECT type can do everything you can do on a COMPUTE or COPY queue, and COMPUTE can do everything that you can do on a COPY queue:</p>
<p data-adtags-visited="true"><img data-attachment-id="7875" data-permalink="https://mynameismjp.wordpress.com/2018/09/08/breaking-down-barriers-part-5-back-to-the-real-world/queue_types_venn/" data-orig-file="https://mynameismjp.files.wordpress.com/2018/09/queue_types_venn.png" data-orig-size="700,600" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Queue_Types_Venn" data-image-description="" data-medium-file="https://mynameismjp.files.wordpress.com/2018/09/queue_types_venn.png?w=300" data-large-file="https://mynameismjp.files.wordpress.com/2018/09/queue_types_venn.png?w=700" src="https://mynameismjp.files.wordpress.com/2018/09/queue_types_venn.png?w=300&amp;h=257" alt="Queue_Types_Venn" width="300" height="257" srcset="https://mynameismjp.files.wordpress.com/2018/09/queue_types_venn.png?w=300&amp;h=257 300w, https://mynameismjp.files.wordpress.com/2018/09/queue_types_venn.png?w=600&amp;h=514 600w, https://mynameismjp.files.wordpress.com/2018/09/queue_types_venn.png?w=150&amp;h=129 150w" sizes="(max-width: 300px) 100vw, 300px"></p>
<p data-adtags-visited="true">With the queue/engine functionality being specified as subsets of each other, the OS can take a submission from a COPY queue and redirect it to a DIRECT engine if it needs to. This saves applications from having to change how they submit their command buffers based on the supported functionality of the end-user‚Äôs hardware and drivers. However to pull this off, the OS has to be able to take command buffers that were intended to be concurrent and ‚Äúflatten‚Äô them down to a serial stream of command buffers. As an example, let‚Äôs return to the DOF/bloom case study that we used earlier. If we were to submit that sequence of command buffers on a GPU that only has a single DIRECT engine, the OS will have to flatten the submissions into something like this:</p>
<p data-adtags-visited="true"><img data-attachment-id="7876" data-permalink="https://mynameismjp.wordpress.com/2018/09/08/breaking-down-barriers-part-5-back-to-the-real-world/bloom_dof_single_engine/" data-orig-file="https://mynameismjp.files.wordpress.com/2018/09/bloom_dof_single_engine.png" data-orig-size="1151,800" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Bloom_DOF_Single_Engine" data-image-description="" data-medium-file="https://mynameismjp.files.wordpress.com/2018/09/bloom_dof_single_engine.png?w=300" data-large-file="https://mynameismjp.files.wordpress.com/2018/09/bloom_dof_single_engine.png?w=756" src="https://mynameismjp.files.wordpress.com/2018/09/bloom_dof_single_engine.png?w=756" alt="Bloom_DOF_Single_Engine" srcset="https://mynameismjp.files.wordpress.com/2018/09/bloom_dof_single_engine.png?w=756 756w, https://mynameismjp.files.wordpress.com/2018/09/bloom_dof_single_engine.png?w=150 150w, https://mynameismjp.files.wordpress.com/2018/09/bloom_dof_single_engine.png?w=300 300w, https://mynameismjp.files.wordpress.com/2018/09/bloom_dof_single_engine.png?w=768 768w, https://mynameismjp.files.wordpress.com/2018/09/bloom_dof_single_engine.png?w=1024 1024w, https://mynameismjp.files.wordpress.com/2018/09/bloom_dof_single_engine.png 1151w" sizes="(max-width: 756px) 100vw, 756px"></p>
<p data-adtags-visited="true">Since the bloom and DOF command buffers are independent of each other, they can be executed in any order with regards to other and still give the same results. However you can tell the OS scheduler to favor one queue‚Äôs submissions over the other by creating the queue with a <a href="https://docs.microsoft.com/en-us/windows/desktop/api/d3d12/ne-d3d12-d3d12_command_queue_priority">higher-than-normal priority</a>. We should also note here that the OS needs to be fully aware of inter-queue dependencies in order to do this kind of flattening operation, which is why fences are heavyweight OS-level constructs instead of being lightweight sync operations. This is also why you can‚Äôt perform cross-queue synchronization within a command list, which would be convenient for applications but would break the ability of the scheduler to flatten multi-queue submissions. One benefit gained from giving the OS global knowledge of submission dependencies is that it can detect and prevent certain deadlocks in advance., which is nicer than allowing the system to choke.</p>
<p data-adtags-visited="true">If you‚Äôre ever curious to see what the OS and scheduler are actually doing with your command buffers behind the scenes,&nbsp;the best way to do so is to use <a href="https://graphics.stanford.edu/~mdfisher/GPUView.html">GPUView</a>&nbsp;(which is most easily done by using <a href="https://randomascii.wordpress.com/">Bruce Dawson</a>‚Äòs invaluable <a href="https://github.com/google/UIforETW">UIforETW tool</a>). Here‚Äôs an example capture from an submitting&nbsp;a bunch of work to both a DIRECT and COMPUTE queue running on an AMD RX 460:</p>
<p data-adtags-visited="true"><img data-attachment-id="7445" data-permalink="https://mynameismjp.wordpress.com/?attachment_id=7445" data-orig-file="https://mynameismjp.files.wordpress.com/2017/02/gpuview.png" data-orig-size="851,840" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="gpuview" data-image-description="" data-medium-file="https://mynameismjp.files.wordpress.com/2017/02/gpuview.png?w=300" data-large-file="https://mynameismjp.files.wordpress.com/2017/02/gpuview.png?w=756" src="https://mynameismjp.files.wordpress.com/2017/02/gpuview.png?w=756" alt="gpuview" srcset="https://mynameismjp.files.wordpress.com/2017/02/gpuview.png?w=756 756w, https://mynameismjp.files.wordpress.com/2017/02/gpuview.png?w=150 150w, https://mynameismjp.files.wordpress.com/2017/02/gpuview.png?w=300 300w, https://mynameismjp.files.wordpress.com/2017/02/gpuview.png?w=768 768w, https://mynameismjp.files.wordpress.com/2017/02/gpuview.png 851w" sizes="(max-width: 756px) 100vw, 756px"></p>
<p data-adtags-visited="true">This screenshot is showing about 4 frame‚Äôs worth of capture data, which you can tell by looking at the activity in the flip queue. The darker area shows where the GPU is idle while waiting for VSYNC, so we can use that denote where the frame begins and ends. Towards the bottom we have the ‚Äúdevice contexts‚Äù, which is showing command buffers that have been submitted on the CPU side of things by a single process. The top one shows the compute submissions, while the bottom one is the graphics/direct submissions. All of these command buffers end up in the hardware queues at the top of the image, which is what we‚Äôre interested in. Here we see there‚Äôs two hardware queues, one called ‚Äú3D‚Äù and the other called ‚ÄúCOMPUTE_0‚Äù. By looking at the activity on these queues we can see that they‚Äôre both processing submissions simultaneously, which suggests that the work from each queue will overlap. Ultimately what we‚Äôre seeing here is pretty similar what I was illustrating in <a href="https://mynameismjp.wordpress.com/2018/07/03/breaking-down-barriers-part-4-gpu-preemption/">Part 4</a> with the GPU/Driver timelines, which means we can also use this tool to inspect preemption behavior from multiple applications.</p>
<p data-adtags-visited="true">For those of you using Vulkan, be aware that their queue/submission model does not work like D3D12‚Äôs! In Vulkan, every physical device can be queried to discover which queue ‚Äúfamilies‚Äù it supports (a queue family is analogous to an engine type in D3D12), as well as how many of each queue family is available. You can then pass handles to those exposed queues when <a href="https://www.khronos.org/registry/vulkan/specs/1.1-extensions/html/vkspec.html#devsandqueues-queue-creation">creating your logical device</a>,&nbsp;which then allows you to submit command buffers to those queues at runtime. This means that if a GPU only has a single front-end, you will probably only see a single queue exposed on the physical device.</p>
<p data-adtags-visited="true">There‚Äôs one more improvement in WDDM 2.0 that I wanted to quickly mention: remember how I described the process for patching command buffers with physical addresses in WDDM 1.x? That‚Äôs no longer necessary in WDDM 2.0 thanks to its added support for <a href="https://docs.microsoft.com/en-us/windows-hardware/drivers/display/gpu-virtual-memory-in-wddm-2-0">GPU virtual memory</a>. With the new driver model, each process gets its own virtual address space for GPU memory, just like they do for CPU memory. The app and driver can freely embed these virtual addresses directly into command buffers or <a href="https://docs.microsoft.com/en-us/windows/desktop/direct3d12/indirect-drawing">ExecuteIndirect argument buffers</a> without having to go through an expensive patching process right before execution. This change also allows the user-mode driver to directly build and <a href="https://docs.microsoft.com/en-us/windows-hardware/drivers/ddi/content/d3dumddi/nc-d3dumddi-pfnd3dddi_submitcommandcb">submit</a> a GPU-accessible command buffer instead of requiring the kernel-mode driver to create a DMA buffer. This is a nice performance improvement, and I would imagine it‚Äôs also an improvement from a security point of view. When using the D3D12 API you often work directly with GPU virtual address for things like creating vertex buffer views or setting root CBV‚Äôs/SRV‚Äôs using a single pointer, which is pretty convenient for apps. Vulkan on the other hand doesn‚Äôt expose GPU virtual addresses (likely because it has to run on platforms and hardware without full virtual memory support), so it will instead work with <a href="https://www.khronos.org/registry/vulkan/specs/1.1-extensions/man/html/vkBindBufferMemory.html">allocation/offset pairs</a>.</p>
<h3>A Quick Look at Real GPU‚Äôs</h3>
<p data-adtags-visited="true">Before we wrap up this article, I wanted to also spend just a bit of timing looking at real-world GPU‚Äôs and their support for multiple command processors. In Parts 2, 3, and 4 we used my made-up GPU to look at how the basics of preemption and overlapped processing worked. But I wanted to also go over some real-world examples to see how they compare to my overly-simplified example, and also how they relate to D3D12‚Äôs support for multi-queue submission.</p>
<p data-adtags-visited="true">Let‚Äôs start out by looking at GPU‚Äôs from AMD‚Äôs. Take a look at this slide from one of <a href="http://developer.amd.com/wordpress/media/2013/06/2620_final.pdf">AMD‚Äôs presentations</a>:</p>
<p data-adtags-visited="true"><img data-attachment-id="7304" data-permalink="https://mynameismjp.wordpress.com/?attachment_id=7304" data-orig-file="https://mynameismjp.files.wordpress.com/2017/01/amd_command_processors.png" data-orig-size="960,542" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="amd_command_processors" data-image-description="" data-medium-file="https://mynameismjp.files.wordpress.com/2017/01/amd_command_processors.png?w=300" data-large-file="https://mynameismjp.files.wordpress.com/2017/01/amd_command_processors.png?w=756" src="https://mynameismjp.files.wordpress.com/2017/01/amd_command_processors.png?w=756" alt="amd_command_processors" srcset="https://mynameismjp.files.wordpress.com/2017/01/amd_command_processors.png?w=756 756w, https://mynameismjp.files.wordpress.com/2017/01/amd_command_processors.png?w=150 150w, https://mynameismjp.files.wordpress.com/2017/01/amd_command_processors.png?w=300 300w, https://mynameismjp.files.wordpress.com/2017/01/amd_command_processors.png?w=768 768w, https://mynameismjp.files.wordpress.com/2017/01/amd_command_processors.png 960w" sizes="(max-width: 756px) 100vw, 756px"></p>
<p data-adtags-visited="true">In this diagram there‚Äôs one ‚ÄúGFX‚Äù command processor, which is is the only command processor capable&nbsp;of&nbsp;issuing Draw commands that use the primitive and pixel pipes. It can also issue Dispatch calls that use the CS pipeline to access the shader cores.&nbsp;In the top left there are a few units labeled&nbsp;‚ÄúACE‚Äù, which&nbsp;stands for ‚ÄúAsynchronous Compute Engine‚Äù. These guys are basically an array of simplified, independent command processors that can only process compute-related commands. This means that they can issue Dispatch calls and perform synchronization-related commands, but they can‚Äôt execute Draw commands that utilize the graphics pipeline. In other words, they exactly fit the required feature set of a COMPUTE engine in D3D12, which is obviously not a coincidence. In&nbsp;terms of functionality they‚Äôre also rather similar to the dual command processors of my fictional MJP-4000, and they can provide the same benefits for preemption, multitasking, and increased utilization from overlapping multiple workloads. In fact, if you read some of <a href="http://amd-dev.wpengine.netdna-cdn.com/wordpress/media/2012/10/Asynchronous-Shaders-White-Paper-FINAL.pdf">AMD‚Äôs documentation</a> you‚Äôll see them explain some of the same concepts that I was discussing in my earlier articles.</p>
<p data-adtags-visited="true">In practice the ACE‚Äôs are&nbsp;a bit more sophisticated than what was discussed in my examples,&nbsp;particularly due to the fact that each ACE contains up to 8&nbsp;hardware queues for feeding command buffers to the ACE‚Äôs command processor. AMD‚Äôs processors support up to 8 ACE‚Äôs on a single chip, potentially allowing for 64 different command streams to be in-flight simultaneously! The separate queues support various scheduling and synchronization operations, effectively allowing them to serve as a very simple hardware task scheduler. These ACE‚Äôs have been present through every revision of the GCN architecture, including the recent RX Vega series. AMD has also mentioned a few improvements over the course of 4 architecture revisions. Most notably for their semi-recent <a href="http://radeon.com/_downloads/polaris-whitepaper-4.8.16.pdf">Polaris architecture</a>, the ACE‚Äôs were augmented with what they call a ‚ÄúQuick Response Queue‚Äù. &nbsp;Their documentation suggests that this allows ACE‚Äôs to submit high-priority workloads that take priority over work submitted from other command processors, which should allow for the&nbsp;kind of thread-level preemption that I showed in own preemption examples. Polaris also introduced what they called a ‚ÄúHardware Scheduler‚Äù (HWS), which is described as additional independent processors that can be used to create a layer of virtualization around the physical hardware queues.</p>
<p data-adtags-visited="true">UPDATE: It turns out that the Quick Response Queue has <a href="https://www.anandtech.com/show/10195/oculus-rift-launch-day-news">been around</a> since GCN2, and HWS has been around since Fiji/GCN3. Thanks again to Locuza for correcting me on that.</p>
<p data-adtags-visited="true">This information suggests that we should see at least 1 compute engine exposed to Windows scheduler, as well as at least 1 queue with&nbsp;COMPUTE_BIT set being exposed on the physical device through Vulkan. The D3D12 side can be verified through experimentation by using ETW captures and GPUView (see the <a href="https://mynameismjp.files.wordpress.com/2017/02/gpuview.png?w=680">image</a> that I posted above showing a COMPUTE submission being executed on a hardware compute queue), but for Vulkan we can verify very easily using the <a href="https://vulkan.gpuinfo.org/listreports.php">Vulkan Hardware Database</a>. Here‚Äôs <a href="https://vulkan.gpuinfo.org/displayreport.php?id=3966#queuefamilies">what we get</a> when looking at the ‚ÄúQueue families‚Äù tab of an <a href="https://radeon.com/_downloads/vega-whitepaper-11.6.17.pdf">RX Vega</a>:</p>
<p data-adtags-visited="true"><img data-attachment-id="7877" data-permalink="https://mynameismjp.wordpress.com/2018/09/08/breaking-down-barriers-part-5-back-to-the-real-world/vega_vulkan_db/" data-orig-file="https://mynameismjp.files.wordpress.com/2018/09/vega_vulkan_db.png" data-orig-size="631,774" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Vega_Vulkan_DB" data-image-description="" data-medium-file="https://mynameismjp.files.wordpress.com/2018/09/vega_vulkan_db.png?w=245" data-large-file="https://mynameismjp.files.wordpress.com/2018/09/vega_vulkan_db.png?w=631" src="https://mynameismjp.files.wordpress.com/2018/09/vega_vulkan_db.png?w=756" alt="Vega_Vulkan_DB" srcset="https://mynameismjp.files.wordpress.com/2018/09/vega_vulkan_db.png 631w, https://mynameismjp.files.wordpress.com/2018/09/vega_vulkan_db.png?w=122 122w, https://mynameismjp.files.wordpress.com/2018/09/vega_vulkan_db.png?w=245 245w" sizes="(max-width: 631px) 100vw, 631px"></p>
<p data-adtags-visited="true">It looks like AMD chose to expose their ACE‚Äôs to Vulkan as 8 compute queues, which is plenty for an individual application. We can also see 2 queues with TRANSFER_BIT specified, which is Vulkan‚Äôs version of a COPY queue. These correspond to the hardware DMA units that are present on an AMD Vega, as well as on virtually every discrete PC video card in the wild. Like I mentioned earlier, DMA units on GPU‚Äôs are specifically optimized for transferring batches of data across the PCI-e bus into on-board GPU memory. The typical use case is read-only textures and buffers, which need to live in on-board memory in order for the GPU to be able to read from them with full bandwidth. Drivers from the pre-D3D12 days would use the DMA to perform initialization of resources with app-specified memory, since the DMA unit is optimized for transferring from system memory, and also because it can execute concurrently with graphics operations. DMA units can also transform textures into hardware-specific layouts, which typically involves using some variant of a <a href="https://en.wikipedia.org/wiki/Z-order_curve">Z-order curve</a> to obtain better 2D cache locality. With D3D12 and Vulkan you‚Äôre of course on your own when initializing resources, and a COPY queue is the natural choice for doing this on discrete GPU‚Äôs.</p>
<p data-adtags-visited="true">As for Nvidia, their hardware also appears to feature functionality that allows for simultaneously submitting multiple workloads to their GPUs. Their Kepler architecture introduced what they refer to as <a href="https://blogs.nvidia.com/blog/2012/08/23/unleash-legacy-mpi-codes-with-keplers-hyper-q/">Hyper-Q</a>, which sounds pretty similar to the ACE‚Äôs on AMD GPUs:</p>
<p data-adtags-visited="true"><img data-attachment-id="7882" data-permalink="https://mynameismjp.wordpress.com/2018/09/08/breaking-down-barriers-part-5-back-to-the-real-world/hyper-q/" data-orig-file="https://mynameismjp.files.wordpress.com/2018/09/hyper-q.png" data-orig-size="633,313" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="hyper-q" data-image-description="" data-medium-file="https://mynameismjp.files.wordpress.com/2018/09/hyper-q.png?w=300" data-large-file="https://mynameismjp.files.wordpress.com/2018/09/hyper-q.png?w=633" src="https://mynameismjp.files.wordpress.com/2018/09/hyper-q.png?w=756" alt="hyper-q" srcset="https://mynameismjp.files.wordpress.com/2018/09/hyper-q.png 633w, https://mynameismjp.files.wordpress.com/2018/09/hyper-q.png?w=150 150w, https://mynameismjp.files.wordpress.com/2018/09/hyper-q.png?w=300 300w" sizes="(max-width: 633px) 100vw, 633px"></p>
<p data-adtags-visited="true">Their (very limited) documentation describes 32 hardware queues, and mentions that they can be used to submit multiple kernels from the same application or to allow multiple CUDA applications to submit workloads concurrently. Unfortunately it seems this functionality was limited only to CUDA, since <a href="http://www.anandtech.com/show/9124/amd-dives-deep-on-asynchronous-shading">comments from Nvidia</a> indicated that their GPU‚Äôs could not simultaneously execute&nbsp;Hyper-Q commands alongside graphics commands. According to Nvidia, only their then-recent Maxwell 2.0 architecture was capable of running in a ‚Äúmixed mode‚Äù, where 1 hardware queue could process graphics commands while the other 31 queues could simultaneously process compute commands. In theory this would allow for similar capabilities to AMD‚Äôs hardware, however it appears that Nvidia <a href="https://benchmarks.ul.com/news/a-closer-look-at-asynchronous-compute-in-3dmark-time-spy">never exposed</a> the additional compute queues through either D3D12 or <a href="https://vulkan.gpuinfo.org/displayreport.php?id=3829#queuefamilies">Vulkan</a>, and instead allowed the OS to merge compute submissions onto the graphics queue. It wasn‚Äôt until&nbsp;Nvidia released their more recent Pascal architecture that they decided to finally offer a bit of information on the subject. Their <a href="http://international.download.nvidia.com/geforce-com/international/pdfs/GeForce_GTX_1080_Whitepaper_FINAL.pdf">whitepaper</a> has a section describing a new feature that they refer to as ‚Äúdynamic load balancing‚Äù, which contains the following diagram and text on page 14:</p>
<p data-adtags-visited="true"><img data-attachment-id="7883" data-permalink="https://mynameismjp.wordpress.com/2018/09/08/breaking-down-barriers-part-5-back-to-the-real-world/nv_dynamic_load_balancing/" data-orig-file="https://mynameismjp.files.wordpress.com/2018/09/nv_dynamic_load_balancing.png" data-orig-size="1315,736" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="NV_Dynamic_Load_Balancing" data-image-description="" data-medium-file="https://mynameismjp.files.wordpress.com/2018/09/nv_dynamic_load_balancing.png?w=300" data-large-file="https://mynameismjp.files.wordpress.com/2018/09/nv_dynamic_load_balancing.png?w=756" src="https://mynameismjp.files.wordpress.com/2018/09/nv_dynamic_load_balancing.png?w=756" alt="NV_Dynamic_Load_Balancing" srcset="https://mynameismjp.files.wordpress.com/2018/09/nv_dynamic_load_balancing.png?w=756 756w, https://mynameismjp.files.wordpress.com/2018/09/nv_dynamic_load_balancing.png?w=150 150w, https://mynameismjp.files.wordpress.com/2018/09/nv_dynamic_load_balancing.png?w=300 300w, https://mynameismjp.files.wordpress.com/2018/09/nv_dynamic_load_balancing.png?w=768 768w, https://mynameismjp.files.wordpress.com/2018/09/nv_dynamic_load_balancing.png?w=1024 1024w, https://mynameismjp.files.wordpress.com/2018/09/nv_dynamic_load_balancing.png 1315w" sizes="(max-width: 756px) 100vw, 756px"></p>
<p data-adtags-visited="true"><em>‚ÄúIn Maxwell generation GPUs, overlapping workloads were implemented with static partitioning of the GPU into a subset that runs graphics, and a subset that runs compute. This is efficient provided that the balance of work between the two loads roughly matches the partitioning ratio. However, if the compute workload takes longer than the graphics workload, and both need to complete before new work can be done, and the portion of the GPU configured to run graphics will go idle. This can cause reduced performance that may exceed any performance benefit that would have been provided from running the workloads overlapped.‚Äù</em></p>
<p data-adtags-visited="true">This suggests that the issue wasn‚Äôt in their queues, but perhaps in the actual functional units that would execute the compute shaders. From their description, it sounds&nbsp;as though their hardware&nbsp;units needed to be configured as either ‚Äúgraphics‚Äù or ‚Äúcompute‚Äù mode (perhaps on the SM level), which meant that for compute to overlap with graphics the hardware needed to be statically partitioned. This isn‚Äôt optimal,&nbsp;and it explains why they may have decided it was better off to just serialize compute queue submissions into the graphics submissions. Meanwhile the Pascal architecture sounds as though it can re-partition dynamically, which would make concurrent compute execution more feasible. This can be confirmed by noting that Pascal GPU‚Äôs will execute COMPUTE submissions on a compute hardware queue when <a href="http://www.futuremark.com/static/images/news/nvidia-gtx-1080-async.png">viewed in GPUView</a>, or by looking at the <a href="https://vulkan.gpuinfo.org/displayreport.php?id=3970#queuefamilies">exposed queues in Vulkan:</a></p>
<p data-adtags-visited="true"><img data-attachment-id="7879" data-permalink="https://mynameismjp.wordpress.com/2018/09/08/breaking-down-barriers-part-5-back-to-the-real-world/pascal_vulkan_db-2/" data-orig-file="https://mynameismjp.files.wordpress.com/2018/09/pascal_vulkan_db1.png" data-orig-size="637,731" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Pascal_Vulkan_DB" data-image-description="" data-medium-file="https://mynameismjp.files.wordpress.com/2018/09/pascal_vulkan_db1.png?w=261" data-large-file="https://mynameismjp.files.wordpress.com/2018/09/pascal_vulkan_db1.png?w=637" src="https://mynameismjp.files.wordpress.com/2018/09/pascal_vulkan_db1.png?w=756" alt="Pascal_Vulkan_DB" srcset="https://mynameismjp.files.wordpress.com/2018/09/pascal_vulkan_db1.png 637w, https://mynameismjp.files.wordpress.com/2018/09/pascal_vulkan_db1.png?w=131 131w, https://mynameismjp.files.wordpress.com/2018/09/pascal_vulkan_db1.png?w=261 261w" sizes="(max-width: 637px) 100vw, 637px"></p>
<p data-adtags-visited="true">We see 8 compute queues exposed here, which matches the RX Vega. Interestingly we also see 16(!) queues with GRAPHICS_BIT set, which is also very intriguing! Unfortunately Nvidia‚Äôs hasn‚Äôt said much on the hardware that‚Äôs backing these queues, so we‚Äôll have to wait and see if we learn more in the future.</p>
<p data-adtags-visited="true">There are also pages later in the Pascal whitepaper that provide some details on its thread-level preemption capability,&nbsp;which is a big improvement on the draw-level preemption supported by the earlier Maxwell 2.0 architecture. Interestingly it seems their hardware also features instruction-level preemption, but only for CUDA. As far as I know there hasn‚Äôt been any information released about the queues on their newer Volta or Turing architectures, so we‚Äôll probably have to wait a while to see if there have been any improvements.</p>
<p data-adtags-visited="true">Intel‚Äôs Gen9 architecture that was used in their Skylake CPU‚Äôs does not appear to have anything beyond a single DIRECT/graphics engine exposed through either D3D12 or Vulkan.&nbsp;GPUView only shows 1 hardware queue processing commands, and the Vulkan DB only shows 1 queue being exposed on the physical device. Unfortunately I don‚Äôt have access to a Gen 9.5 GPU (used on Kaby Lake), so I haven‚Äôt been able to personally test it. However it appears that these GPU‚Äôs still only <a href="https://vulkan.gpuinfo.org/displayreport.php?id=3955#queuefamilies">expose a single queue in Vulkan</a>, so I would assume that nothing has changed on that front. Not having even a COPY engine on these GPU‚Äôs might seem strange at first, but remember that DMA units are mainly a feature of discrete video cards. Integrated GPU‚Äôs like Intel‚Äôs utilize CPU memory instead of dedicated video memory, which means that the CPU can write that memory instead of requiring a dedicated hardware unit.</p>
<h3>Next Up</h3>
<p data-adtags-visited="true">For the sixth (and final!) part of this series, I‚Äôm going to share some results that I‚Äôve gathered from experimenting with overlapped GPU commands and GPU preemption. Stay tuned!</p>
<ol>
<li id="LostDevice">Successful cooperation and stability also depended on how well the programs themselves handled the infamous <a href="https://docs.microsoft.com/en-us/windows/desktop/direct3d9/lost-devices">lost device state</a> that occurred in certain scenarios, most notably when alt-tabbing from fullscreen.</li>
<li id="UserKernelMode">That document briefly mentions one of biggest changes brought about by WDDM, which is the fact that IHV‚Äôs now had to provide a user-mode driver component alongside their kernel-mode driver. This was a really big deal at the time, since under XDDM the IHV only provided a kernel-mode driver. This meant that anytime the driver had to process a draw call or do some state management, the application had to undergo an expensive user-mode -&gt; kernel mode context switch. The lack of a user-mode driver component was commonly cited as a reason as to why OpenGL could ‚Äúhandle more draw calls‚Äù than D3D9, and why D3D10 would be a major improvement for CPU performance. The concept of a user-mode driver also came just in time for everyone to fully adopt programmable shaders for everything, since drivers now frequently host complex optimizing compilers in their DLL‚Äôs so that they can JIT-compile shader bytecode.</li>
</ol>
			
			
										</div><!-- .entry-content -->

			<!-- .entry-footer -->

			
<!-- #comments -->

		</div>

		<!-- .sidebar -->

	</article></div></div>
    </div>
    <footer>
        <div>created by buildstarted &copy; 2020 <a href="/about">about</a></div>
        <div>Share this page on social media: copy and paste this url https://linksfor.dev/</div>
        <div>If you prefer RSS: <a href="https://linksfor.dev/feed.xml">https://linksfor.dev/feed.xml</a></div>
    </footer>
    
    <script async defer>
        _dna = window._dna || {};
        _dna.siteId = "linksfor.devs";
        _dna.outlink = true;

        (function () {
            let dna = document.createElement('script');
            dna.type = 'text/javascript';
            dna.async = true;
            dna.src = '//dna.buildstarted.com/t.js';
            let s = document.getElementsByTagName('script')[0];
            s.parentNode.insertBefore(dna, s);
        })();
    </script>
    <noscript><img src="//dna.buildstarted.com/g?siteId=linksfor.devs"/></noscript>
</body>
</html>