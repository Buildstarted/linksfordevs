<!DOCTYPE html>
<html lang="en">
<head>
    <title>
.NET for Apache Spark Helps Makes Big Data Accessible -
linksfor.dev(s)
    </title>
    <link rel="alternate" type="application/rss+xml" title="Linksfor.dev(s) feed" href="https://linksfor.dev/feed.rss" />
    <meta charset="utf-8">
    <meta name="Description" content="A curated source of links that devs might find interesting. Updated around the clock." />
    <meta name="google" value="notranslate">
    <link rel="stylesheet" type="text/css" href="/style.css" />
    <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon">
    <link rel="icon" href="/favicon.ico" type="image/x-icon">
</head>
<body>
    <div class="grid">
            <h1>
                    <span style="cursor: default" title="linksfor.dev(s) has been running for 1 year! :partypopper:">ðŸŽ‰</span>
<a href="/" style="color:inherit">linksfor.dev(s)</a>
            </h1>
        
<div class="readable">
    <h1>.NET for Apache Spark Helps Makes Big Data Accessible</h1>
    <p><a href="https://devblogs.microsoft.com/dotnet/using-net-for-apache-spark-to-analyze-log-data/">https://devblogs.microsoft.com/dotnet/using-net-for-apache-spark-to-analyze-log-data/</a></p>
    <hr/>
    <div id="readOverlay" class="style-ebook"><div id="readInner" class="margin-medium size-medium"><h1>.NET for Apache Spark Helps Makes Big Data Accessible</h1><div><div class="entry-content col-12 sharepostcontent"><div class="row justify-content-center"><div class="col-md-4"><div><img src="https://devblogs.microsoft.com/dotnet/wp-content/uploads/sites/10/2020/01/Profile-Pic-150x150.jpg" width="58" height="58" alt="Brigit Murtaugh" class="avatar avatar-58 wp-user-avatar wp-user-avatar-58 alignnone photo"><p>Brigit</p></div></div></div><p>February 10th, 2020</p><p>At Spark + AI Summit in <a href="https://devblogs.microsoft.com/dotnet/introducing-net-for-apache-spark/">May 2019</a>, we released <a href="https://dot.net/spark">.NET for Apache Spark</a>. .NET for Apache Spark is aimed at making <a href="https://spark.apache.org/">ApacheÂ® Sparkâ„¢</a>, and thus the exciting world of big data analytics, accessible to .NET developers.</p><p>.NET for Spark can be used for processing batches of data, real-time streams, machine learning, and ad-hoc query. In this blog post, weâ€™ll explore how to use .NET for Spark to perform a very popular big data task known as <strong>log analysis</strong>.</p><p>The remainder of this post describes the following topics:</p><h3><a id="whatislog"></a>What is log analysis?</h3><p>Log analysis, also known as <em>log processing</em>, is the process of analyzing computer-generated records called logs. Logs tell us whatâ€™s happening on a tool like a computer or web server, such as what applications are being used or the top websites users visit.</p><p>The goal of log analysis is to gain meaningful insights from these logs about activity and performance of our tools or services. .NET for Spark enables us to analyze anywhere from megabytes to petabytes of log data with blazing fast and efficient processing!</p><p>In this blog post, weâ€™ll be analyzing a set of <a href="https://httpd.apache.org/docs/1.3/logs.html">Apache log entries</a> that express how users are interacting with content on a web server. You can view a sample of Apache log entries <a href="https://raw.githubusercontent.com/elastic/examples/master/Common%20Data%20Formats/apache_logs/apache_logs">here</a>.</p><h3><a id="sparkforlog"></a>Writing a .NET for Spark log analysis app</h3><p>Log analysis is an example of <a href="https://docs.microsoft.com/en-us/dotnet/spark/tutorials/batch-processing">batch processing</a> with Spark. Batch processing is the transformation of data at rest, meaning that the source data has already been loaded into data storage. In our case, the input text file is already populated with logs and wonâ€™t be receiving new or updated logs as we process it.</p><p>When creating a new .NET for Spark application, there are just a few steps we need to follow to start getting those interesting insights from our data:</p><ol><li>Create a Spark Session.</li><li>Read input data, typically using a DataFrame.</li><li>Manipulate and analyze input data, typically using Spark SQL.</li></ol><h4><a id="createsession"></a> Create a Spark Session</h4><p>In any Spark application, we start off by establishing a new <a href="https://docs.microsoft.com/en-us/dotnet/api/microsoft.spark.sql.sparksession?view=spark-dotnet">SparkSession</a>, which is the entry point to programming with Spark:</p><pre class="prettyprint">SparkSession spark = SparkSession
    .Builder()
    .AppName("Apache User Log Processing")
    .GetOrCreate();</pre><p>By calling on the <code>spark</code> object created above, we can now access Spark and DataFrame functionality throughout our program â€“ great! But what is a DataFrame? Letâ€™s learn about it in the next step.</p><h4><a id="readindata"></a> Read input data</h4><p>Now that we have access to Spark functionality, we can read in the log data weâ€™ll be analyzing. We store input data in a <a href="https://docs.microsoft.com/en-us/dotnet/api/microsoft.spark.sql.dataframe?view=spark-dotnet">DataFrame</a>, which is a distributed collection of data organized into named columns:</p><pre class="prettyprint">DataFrame generalDf = spark.Read().Text("&lt;path to input data set&gt;");</pre><p>When our input is contained in a <em>.txt</em> file, we use the <code>.Text()</code> method, as shown above. There are <a href="https://docs.microsoft.com/en-us/dotnet/api/microsoft.spark.sql.dataframereader?view=spark-dotnet#methods">other methods</a> to read in data from other sources, such as <code>.Csv()</code> to read in comma-separated values files.</p><h4><a id="manipulatedata"></a> Manipulate and analyze input data</h4><p>With our input logs stored in a DataFrame, we can start analyzing them â€“ now things are getting exciting!</p><p>An important first step is <strong>data preparation</strong>. Data prep involves cleaning up our data in some way. This could include removing incomplete entries to avoid error in later calculations or removing irrelevant input to improve performance.</p><p>In our example, we should first ensure all of our entries are complete logs. We can do this by comparing each log entry to a <a href="https://docs.microsoft.com/en-us/dotnet/standard/base-types/regular-expression-language-quick-reference">regular expression</a> (AKA a regex), which is a sequence of characters that defines a pattern.</p><p>Letâ€™s define a regex expressing a pattern all valid Apache log entries should follow:</p><pre class="prettyprint">string s_apacheRx = "^(\S+) (\S+) (\S+) [([\w:/]+\s[+-]\d{4})] \"(\S+) (\S+) (\S+)\" (\d{3}) (\d+)";</pre><p>How do we perform a calculation on each row of a DataFrame, like comparing each log entry to the above regex? The answer is <em>Spark SQL</em>.</p><h4>Spark SQL</h4><p>Spark SQL provides many great functions for working with the structured data stored in a DataFrame. One of the most popular features of Spark SQL is <em>UDFs</em>, or user-defined functions. We define the type of input they take and the type of output they produce, and then the actual calculation or filtering they perform.</p><p>Letâ€™s define a new UDF <code>GeneralReg</code> to compare each log entry to the <code>s_apacheRx</code> regex. Our UDF requires an Apache log entry, which is a string, and will return a true or false depending upon if the log matches the regex:</p><pre class="prettyprint">spark.Udf().Register&lt;string, bool&gt;("GeneralReg", log =&gt; Regex.IsMatch(log, s_apacheRx));</pre><p>So how do we call <code>GeneralReg</code>?</p><p>In addition to UDFs, Spark SQL provides the ability to write <strong>SQL calls</strong> to analyze our data â€“ how convenient! Itâ€™s common to write a SQL call to apply a UDF to each row of data.</p><p>To call <code>GeneralReg</code> from above, letâ€™s use the following SQL call:</p><pre class="prettyprint">DataFrame generalDf = spark.Sql("SELECT logs.value, GeneralReg(logs.value) FROM Logs");</pre><p>This SQL call tests each row of <code>generalDf</code> to determine if itâ€™s a valid and complete log.</p><p>We can use <a href="https://docs.microsoft.com/en-us/dotnet/api/microsoft.spark.sql.dataframe.filter?view=spark-dotnet">.Filter()</a> to only keep the complete log entries in our data, and then <a href="https://docs.microsoft.com/en-us/dotnet/api/microsoft.spark.sql.dataframe.show?view=spark-dotnet">.Show()</a> to display our newly filtered DataFrame:</p><pre class="prettyprint">generalDf = generalDf.Filter(generalDf["GeneralReg(value)"]);
generalDf.Show();</pre><p>Now that weâ€™ve performed some initial data prep, we can continue filtering and analyzing our data. Letâ€™s find log entries from IP addresses starting with 10 and related to spam in some way:</p><pre class="prettyprint">// Choose valid log entries that start with 10
spark.Udf().Register&lt;string, bool&gt;(
    "IPReg",
    log =&gt; Regex.IsMatch(log, "^(?=10)"));

generalDf.CreateOrReplaceTempView("IPLogs");

// Apply UDF to get valid log entries starting with 10
DataFrame ipDf = spark.Sql(
    "SELECT iplogs.value FROM IPLogs WHERE IPReg(iplogs.value)");
ipDf.Show();

// Choose valid log entries that start with 10 and deal with spam
spark.Udf().Register&lt;string, bool&gt;(
    "SpamRegEx",
    log =&gt; Regex.IsMatch(log, "\\b(?=spam)\\b"));

ipDf.CreateOrReplaceTempView("SpamLogs");

// Apply UDF to get valid, start with 10, spam entries
DataFrame spamDF = spark.Sql(
    "SELECT spamlogs.value FROM SpamLogs WHERE SpamRegEx(spamlogs.value)");</pre><p>Finally, letâ€™s count the number of GET requests in our final cleaned dataset. The magic of .NET for Spark is that we can combine it with other popular .NET features to write our apps. Weâ€™ll use <a href="https://docs.microsoft.com/en-us/dotnet/csharp/programming-guide/concepts/linq/">LINQ</a> to analyze the data in our Spark app one last time:</p><pre class="prettyprint">int numGetRequests = spamDF 
    .Collect() 
    .Where(r =&gt; ContainsGet(r.GetAs&lt;string&gt;("value"))) 
    .Count();</pre><p>In the above code, <code>ContainsGet()</code> checks for GET requests using <a href="https://docs.microsoft.com/en-us/dotnet/api/system.text.regularexpressions.regex.match?view=netframework-4.8">regex matching</a>:</p><pre class="prettyprint">// Use regex matching to group data 
// Each group matches a column in our log schema 
// i.e. first group = first column = IP
public static bool ContainsGet(string logLine) 
{ 
    Match match = Regex.Match(logLine, s_apacheRx);

    // Determine if valid log entry is a GET request
    if (match.Success)
    {
        Console.WriteLine("Full log entry: '{0}'", match.Groups[0].Value);
    
        // 5th column/group in schema is "method"
        if (match.Groups[5].Value == "GET")
        {
            return true;
        }
    }

    return false;

} </pre><p>As a final step in our Spark apps, we call <code>spark.Stop()</code> to shut down the underlying Spark Session and Spark Context.</p><p>You can view the <a href="https://github.com/dotnet/spark/blob/master/examples/Microsoft.Spark.CSharp.Examples/Sql/Batch/Logging.cs">complete log processing example</a> in our GitHub repo.</p><h3><a id="running"></a> Running your app</h3><p><a href="https://docs.microsoft.com/en-us/dotnet/spark/tutorials/get-started">To run a .NET for Apache Spark app</a>, you need to use the <code>spark-submit</code> command, which will submit your application to run on Apache Spark.</p><p>The main parts of <code>spark-submit</code> include:</p><ul><li>â€“class, to call the DotnetRunner.</li><li>â€“master, to determine if this is a local or cloud Spark submission.</li><li>Path to the Microsoft.Spark jar file.</li><li>Any arguments or dependencies for your app, such as the path to your input file or the dll containing UDF definitions.</li></ul><p>Youâ€™ll also need to download and setup some dependencies before running a .NET for Spark app locally, such as Java and Apache Spark.</p><p>A sample Windows command for running your app is as follows:</p><p><code>spark-submit --class org.apache.spark.deploy.dotnet.DotnetRunner --master local /path/to/microsoft-spark-&lt;version&gt;.jar dotnet /path/to/netcoreapp&lt;version&gt;/LoggingApp.dll</code></p><h3><a id="wrapup"></a>.NET for Apache Spark Wrap Up</h3><p>Weâ€™d love to help you get started with .NET for Apache Spark and hear your feedback.</p><p>You can <a href="https://dot.net/spark">Request a Demo</a> from our landing page and check out the <a href="https://github.com/dotnet/spark">.NET for Spark GitHub repo</a> to learn more about how you can apply .NET for Spark in your apps and get involved with our effort to make .NET a great tech stack for building big data applications!</p><div class="authorinfoarea"><div><p>Program Manager,&nbsp;.NET</p><p><strong>Follow Brigit</strong>&nbsp;&nbsp;&nbsp;<a class="no-underline stayinformed" aria-label="Brigit Murtaugh Twitter profile" target="_blank" href="https://twitter.com/BrigitMurtaugh"></a><a class="no-underline stayinformed" aria-label="Brigit Murtaugh LinkedIn profile" target="_blank" href="https://www.linkedin.com/in/brigit-murtaugh/"><i class="fa fa-linkedin"></i></a><a class="no-underline stayinformed" aria-label="Brigit Murtaugh GitHub profile" target="_blank" href="https://github.com/bamurtaugh"><i class="fa fa-github"></i></a><a class="no-underline stayinformed hvr-pop" aria-label="Brigit Murtaugh RSS Feed" target="_blank" href="https://devblogs.microsoft.com/dotnet/author/brigitmurtaugh/feed/"></a></p></div></div></div></div></div></div>
</div>
    </div>
    <footer>
        <div>created by buildstarted &copy; 2020 <a href="/about">about</a></div>
        <div>Share this page on social media: copy and paste this url https://linksfor.dev/</div>
        <div>If you prefer RSS: <a href="https://linksfor.dev/feed.xml">https://linksfor.dev/feed.xml</a></div>
    </footer>
    
    <script>
        (function () {
            var COLLECT_URL = "https://dna.buildstarted.com/t";
            var SITE_ID = "linksfor.devs";
            var GLOBAL_VAR_NAME = "__DNA__";

            window[GLOBAL_VAR_NAME] = {};

            window[GLOBAL_VAR_NAME].sendPageView = function () {
                var path = location.pathname;
                var referrer = document.referrer;

                var url = COLLECT_URL + "?siteid=" + SITE_ID + "&p=" + encodeURIComponent(path) + "&r=" + encodeURIComponent(referrer);

                try { fetch(url, { method: "GET" }); } catch (e) { }
            };

            window[GLOBAL_VAR_NAME].sendPageView();
        })();
    </script>
</body>
</html>