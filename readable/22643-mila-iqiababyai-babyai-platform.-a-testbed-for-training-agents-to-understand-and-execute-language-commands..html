<!DOCTYPE html>
<html lang="en">
<head>
    <title>
mila-iqia/babyai: BabyAI platform. A testbed for training agents to understand and execute language commands. -
linksfor.dev(s)
    </title>
    <link rel="alternate" type="application/rss+xml" title="Linksfor.dev(s) feed" href="https://linksfor.dev/feed.rss" />
    <meta charset="utf-8">
    <meta name="Description" content="A curated source of links that devs might find interesting. Updated around the clock." />
    <meta name="google" value="notranslate">
    <link rel="stylesheet" type="text/css" href="/style.css" />
    <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon">
    <link rel="icon" href="/favicon.ico" type="image/x-icon">
</head>
<body>
    <div class="grid">
        
<div class="readable">
    <div id="readOverlay" class="style-ebook"><div id="readInner" class="margin-medium size-medium"><h1>mila-iqia/babyai: BabyAI platform. A testbed for training agents to understand and execute language commands.</h1><div><div id="" class="markdown-body entry-content p-5"><p><a href="https://travis-ci.org/mila-iqia/babyai" rel="nofollow"><img src="https://camo.githubusercontent.com/57b203bf714918c5313a7ae8eba0f9724062a9af/68747470733a2f2f7472617669732d63692e6f72672f6d696c612d697169612f6261627961692e7376673f6272616e63683d6d6173746572" alt="Build Status" data-canonical-src="https://travis-ci.org/mila-iqia/babyai.svg?branch=master"></a></p><p>A platform for simulating language learning with a human in the loop. This is an ongoing research project based at <a href="https://mila.quebec/en/" rel="nofollow">Mila</a>.</p><p>Contents:</p><h2><a id="user-content-citation" class="anchor" aria-hidden="true" href="#citation"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Citation</h2><p>If you use this platform in your research, please cite:</p><pre><code>@inproceedings{
  babyai_iclr19,
  title={Baby{AI}: First Steps Towards Grounded Language Learning With a Human In the Loop},
  author={Maxime Chevalier-Boisvert and Dzmitry Bahdanau and Salem Lahlou and Lucas Willems and Chitwan Saharia and Thien Huu Nguyen and Yoshua Bengio},
  booktitle={International Conference on Learning Representations},
  year={2019},
  url={https://openreview.net/forum?id=rJeXCo0cYX},
}
</code></pre><h2><a id="user-content-replicating-iclr19-results" class="anchor" aria-hidden="true" href="#replicating-iclr19-results"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Replicating ICLR19 Results</h2><p>The master branch of this repository is updated frequently. If you are looking to replicate or compare against the results from the <a href="https://openreview.net/forum?id=rJeXCo0cYX" rel="nofollow">ICLR19 BabyAI paper</a>, please use the docker image, demonstration dataset and source code from the <a href="https://github.com/mila-iqia/babyai/tree/iclr19">iclr19 branch</a> of this repository.</p><h2><a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Installation</h2><p>Requirements:</p><ul><li>Python 3.5+</li><li>OpenAI Gym</li><li>NumPy</li><li>PyTorch 0.4.1+</li><li>blosc</li></ul><p>Start by manually installing PyTorch. See the <a href="http://pytorch.org/" rel="nofollow">PyTorch website</a>
for installation instructions specific to your platform.</p><p>Then, clone this repository and install the other dependencies with <code>pip3</code>:</p><pre><code>git clone https://github.com/mila-iqia/babyai.git
cd babyai
pip3 install --editable .
</code></pre><h3><a id="user-content-installation-using-conda-alternative-method" class="anchor" aria-hidden="true" href="#installation-using-conda-alternative-method"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Installation using Conda (Alternative Method)</h3><p>If you are using conda, you can create a <code>babyai</code> environment with all the dependencies by running:</p><pre><code>git clone https://github.com/mila-iqia/babyai.git
cd babyai
conda env create -f environment.yaml
source activate babyai
</code></pre><p>After that, execute the following commands to setup the environment.</p><pre><code>cd ..
git clone https://github.com/maximecb/gym-minigrid.git
cd gym-minigrid
pip install --editable .
</code></pre><p>The last command installs the repository in editable mode. Move back to the <code>babyai</code> repository and install that in editable mode as well.</p><pre><code>cd ../babyai
pip install --editable .
</code></pre><h3><a id="user-content-babyai-storage-path" class="anchor" aria-hidden="true" href="#babyai-storage-path"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>BabyAI Storage Path</h3><p>Add this line to <code>.bashrc</code> (Linux), or <code>.bash_profile</code> (Mac).</p><pre><code>export BABYAI_STORAGE='/&lt;PATH&gt;/&lt;TO&gt;/&lt;BABYAI&gt;/&lt;REPOSITORY&gt;/&lt;PARENT&gt;'
</code></pre><p>where <code>/&lt;PATH&gt;/&lt;TO&gt;/&lt;BABYAI&gt;/&lt;REPOSITORY&gt;/&lt;PARENT&gt;</code> is the folder where you typed <code>git clone https://github.com/mila-iqia/babyai.git</code> earlier.</p><p>Models, logs and demos will be produced in this directory, in the folders <code>models</code>, <code>logs</code> and <code>demos</code> respectively.</p><h2><a id="user-content-usage" class="anchor" aria-hidden="true" href="#usage"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Usage</h2><p>To run the interactive GUI application that illustrates the platform:</p><pre><code>scripts/manual_control.py
</code></pre><p>The level being run can be selected with the <code>--env</code> option, eg:</p><pre><code>scripts/manual_control.py --env BabyAI-UnlockPickup-v0
</code></pre><h3><a id="user-content-the-levels" class="anchor" aria-hidden="true" href="#the-levels"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>The Levels</h3><p>Documentation for the ICLR19 levels can be found in
<a href="/mila-iqia/babyai/blob/master/docs/iclr19_levels.md">docs/iclr19_levels.md</a>.
There are also older levels documented in
<a href="/mila-iqia/babyai/blob/master/docs/bonus_levels.md">docs/bonus_levels.md</a>.</p><h3><a id="user-content-pixel-observations" class="anchor" aria-hidden="true" href="#pixel-observations"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Pixel Observations</h3><p>Please note that the default observation format is a partially observable view of the environment using a compact encoding, with 3 input values per visible grid cell, 7x7x3 values total. These values are <strong>not pixels</strong>. If you want to obtain an array of RGB pixels as observations instead, use the <code>RGBImgPartialObsWrapper</code>. You can use it as follows:</p><pre><code>import babyai
from gym_minigrid.wrappers import *
env = gym.make('BabyAI-GoToRedBall-v0')
env = RGBImgPartialObsWrapper(env)
</code></pre><p>This wrapper, as well as other wrappers to change the observation format can be <a href="https://github.com/maximecb/gym-minigrid/blob/master/gym_minigrid/wrappers.py">found here</a>.</p><h2><a id="user-content-about-this-project" class="anchor" aria-hidden="true" href="#about-this-project"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>About this Project</h2><p>BabyAI is an open-ended grounded language acquisition effort at <a href="https://mila.quebec/en/" rel="nofollow">Mila</a>. The current BabyAI platform was designed to study data-effiency of existing methods under the assumption that a human provides all teaching signals
(i.e. demonstrations, rewards, etc.). For more information, see the <a href="https://openreview.net/forum?id=rJeXCo0cYX" rel="nofollow">ICLR19 paper</a>.</p></div></div></div></div>
</div>
    </div>
    <footer>
        <div>created by buildstarted &copy; 2020 <a href="/about">about</a></div>
        <div>Share this page on social media: copy and paste this url https://linksfor.dev/</div>
        <div>If you prefer RSS: <a href="https://linksfor.dev/feed.xml">https://linksfor.dev/feed.xml</a></div>
    </footer>
    
    <script>
        (function () {
            var COLLECT_URL = "https://dna.buildstarted.com/t";
            var SITE_ID = "linksfor.devs";
            var GLOBAL_VAR_NAME = "__DNA__";

            window[GLOBAL_VAR_NAME] = {};

            window[GLOBAL_VAR_NAME].sendPageView = function () {
                var path = location.pathname;
                var referrer = document.referrer;

                var url = COLLECT_URL + "?siteid=" + SITE_ID + "&p=" + encodeURIComponent(path) + "&r=" + encodeURIComponent(referrer);

                try { fetch(url, { method: "GET" }); } catch (e) { }
            };

            window[GLOBAL_VAR_NAME].sendPageView();
        })();
    </script>
</body>
</html>