<!DOCTYPE html>
<html lang="en">
<head>
    <title>
The tyranny of I/O and the rise of distributed systems - linksfor.dev(s)    </title>
    <meta charset="utf-8">
    <link rel="alternate" type="application/rss+xml" title="Linksfor.dev(s) feed" href="https://linksfor.dev/feed.rss" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta name="google" value="notranslate">
    <link rel="stylesheet" type="text/css" href="/style.css" />
    <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon">
    <link rel="icon" href="/favicon.ico" type="image/x-icon">
    <meta property="og:title" content="The tyranny of I/O and the rise of distributed systems - linksfor.dev(s)"/>
    <meta property="og:description" content="A system that runs on a single machine is an order of magnitude simpler than one that reside on multiple machines. The complexity involved in maintaining con..."/>
    <meta property="og:type" content="website"/>
    <meta property="og:url" content="https://ayende.com/blog/190209-C/the-tyranny-of-i-o-and-the-rise-of-distributed-systems?Key=35588803-7158-41b6-86b2-a2ae15063843"/>

<meta property="og:site_name" content="linksfor.dev(s)" />
</head>
<body>
    <div class="grid">
        <h1>
<a href="/" style="color:inherit">linksfor.dev(s)</a>
        </h1>
        <title>linksfor.dev(s) - The tyranny of I/O and the rise of distributed systems</title>
<div class="readable">
        <h1>The tyranny of I/O and the rise of distributed systems</h1>
            <div>Reading time: 7-8 minutes</div>
        <div>Posted here: 05 Mar 2020</div>
        <p><a href="https://ayende.com/blog/190209-C/the-tyranny-of-i-o-and-the-rise-of-distributed-systems?Key=35588803-7158-41b6-86b2-a2ae15063843">https://ayende.com/blog/190209-C/the-tyranny-of-i-o-and-the-rise-of-distributed-systems?Key=35588803-7158-41b6-86b2-a2ae15063843</a></p>
        <hr/>
<div id="readability-page-1" class="page"><div>
        <p>A system that runs on a single machine is an order of magnitude simpler than one that reside on multiple machines. The complexity involved in maintaining consistency across multiple machines is huge. I have been dealing with this for the past 15 years and I can confidently tell you that no sane person would go for multi machine setup in favor of a single machine if they can get away with it. So what was the root cause for the push toward multiple machines and distributed architecture across the board for the past 20 years? And why are we see a backlash against that today?</p><p>You’ll hear people talking about the need for high availability and the desire to avoid a single point of failure. And that is true, to a degree. But there are other ways to handle that (primary / secondary model) rather than the full blown multi node setup. </p><p>Some users simply have too much data to go around and have to make use of a distributed architecture. If you are gathering a TB / day of data, no single system is going to help you, for example. However, most users aren’t there. A growth rate of GB / day (fully retained) is quite respectable and will take over a decade to start becoming a problem on a single machine. </p><p>What I think people don’t remember so well is that the landscape has <em>changed</em> quite significantly in the past 10 – 15 years. I’m not talking about Moore’s law, I’m talking about something that is actually quite a bit more significant. The <em>dramatic</em> boost in speed that we saw in storage.</p><p>Here are some numbers from <a href="https://docplayer.net/3287897-Overview-of-i-o-performance-and-raid-in-an-rdbms-environment-by-edward-whalen-performance-tuning-corporation.html">the start of the last decade</a> a <em>top of the line</em> 32GB SCSI drive with 15K RPM could hit 161 IOPS. Looking at something more <a href="https://www.storagereview.com/review/wd-red-14tb-nas-hdd-review">modern disk with 14 TB</a> will have 938 IOPS. That is a speed increase of over 500%, which is amazing, but not enough to matter. These two numbers are from <em>hard disks</em>. But we have had a major disruption in storage at the start of the millennium. The advent of SSD drives. </p><p>It turns out that SSDs aren’t nearly as new as one would expect them. They were just <em>horribly</em> expensive. Here are <a href="https://www.digchip.com/datasheets/download_datasheet.php?id=317259&amp;part-number=FFD-350-256-T">the specs for such a drive around 2003.</a> The cost would be <a href="https://www.pcworld.com/article/246617/evolution-of-the-solid-state-drive.html#slide11">tens of thousands</a> (USD) per <em>drive</em>. To be fair, this was meant to be used in rugged environment (think military tech, missiles and such), but there wasn’t much else in the market. In 2003 the first new commodity SSD started to appear, with sizes that topped at 512MB. </p><p>All of this is to say, in the early 2000s, if you wanted to store non trivial amount of data, you had to face the fact that you had to deal with hard disks. And you could expect some pretty harsh limitations on the number of IOPS available. And that, in turn, meant that the deciding factor for scale out wasn’t really the processing speed. Remember that the <a href="https://en.wikipedia.org/wiki/C10k_problem">C10K problem was still a challenge</a>, but reasonable one, in 1999. That is, handling 10K concurrent connections on a single server (to compare, millions of connections per server isn’t out of the ordinary).</p><p>Given 10K connections per server, with each one of them needing a single IO per 5 seconds, what would happen? That means that we need to handle 2,000 IOPS. That is over ten times what you can get from a top of the line disk at that time. So even if you had a RAID0 with ten drives and was able to get perfect distribution of IO to drive, you would still be about 20% short. And I don’t think you’ll want to get 10 drives at RAID0 in production. Given the I/O limits, you could reasonably expect to serve 100 – 300 operations per second per server. And that is assuming that you were able to cache some portion of the data in memory and avoid disk hits. The only way to handle this kind of limitation was to scale out, to have more servers (and more disks) to handle the load. </p><p>The rise of commodity SSDs changed the picture significantly and NVMe drives are the icing on the cake. SSD can do tens of thousands of IOPS and NVMe can do hundreds of thousands IOPS (and some exceed the million IOPS with comfortable margin). </p><p>Going back to the C10K issue? A <a href="https://www.crucial.com/ssd/mx500/ct250mx500ssd4#">49.99$</a> drive with 256GB has specs that exceed 90,000 IOPS. Those 2000 IOPS we had to get 10 machines for? That isn’t really noticeable at all today. In fact, let’s go higher. Let’s say we have 50,000 concurrent connections each one issuing an operation once a second. This is a hundred times more work than the previous example. But the environment in which we are running is very different.</p><p>Given an operating budget of 150$, I will use the hardware from <a href="https://www.tomshardware.com/news/raspberry-pi-4-ssd-test,39811.html">this article</a>, which is basically a Raspberry PI with SSD drive (and fully 50$ of the budget go for the adapter to plug the SSD to the PI). That gives me the ability to handle 4,412 requests per second using Apache, which isn’t the best server in the world. Given that the disk used in the article can handle more than 250,000 IOPS, we can run a <em>lot</em> on a “server” that fits into a lunch box and cost significantly less than the <em>monthly </em>cost of a similar machine on the cloud. This factor totally change the way you would architect your systems. </p><p>The much better hardware means that you can select a simpler architecture and avoid a lot of complexities along the way. Although… we need to talk about the cloud, where the old costs are still very much a factor.</p><p>Using AWS as the baseline, I can get a 250GB gp2 SSD drive for 25$ / month. That would give me 750 IOPS for 25$. That is nice, I guess, but it puts me at less than what I can get from a modern HDD today. There is the burst capability on the cloud, which can smooth out some spikes, but I’ll ignore that for now. Let’s say that I wanted to get higher speed, I can increase the disk size (and hence the IOPS) at linear rate. The max I can get from gp2 is 16,000 IOPS at a cost of 533$.&nbsp; Moving to io1 SSD, we can get 500GB drive with 3,000 IOPS for 257$ per month, and exceeding 20,000 IOPS on a 1TB drive would cost 1,425$. </p><p>In contrast, 242$ / month will get us a r5ad.2xlarge machine with 8 cores, 64 GB and 300 GB NVMe drive. A 1,453$ will get us a r5ad.12xlarge with 48 cores, 384 GB and 1.8TB NVMe drive. You are better off upgrading the machine entirely and running on top of the local NVMe drive and handling the persistency yourself than paying the storage costs associated with having it out as a block storage. </p><p>This tyranny of I/O costs and performance has had a huge impact on the overall system architecture of many systems. Scale out was not, as usually discussed, a reaction to the limits of handling the number of users. It was a limit on how fast the I/O systems could handle concurrent load. With SSD and NVMe drives, we are in a totally different field and need to consider how that affect our systems.</p><p>In many cases, you’ll want to have just enough data distribution to ensure high availability. Otherwise, it make more sense to get larger but fewer machines. The reduction in management overhead alone is usually worth it, but the key aspect is reducing the number of moving pieces in your architecture.</p>
    </div></div></div>
    </div>
    <footer>
        <div>created by buildstarted &copy; 2020 <a href="/about">about</a></div>
        <div>Share this page on social media: copy and paste this url https://linksfor.dev/</div>
        <div>If you prefer RSS: <a href="https://linksfor.dev/feed.xml">https://linksfor.dev/feed.xml</a></div>
		<div>Customer satisfaction guaranteed to be optional.</div>
    </footer>
    
    <script async defer>
        _dna = window._dna || {};
        _dna.siteId = "linksfor.devs";
        _dna.outlink = true;

        (function () {
            let dna = document.createElement('script');
            dna.type = 'text/javascript';
            dna.async = true;
            dna.src = '//dna.buildstarted.com/t.js';
            let s = document.getElementsByTagName('script')[0];
            s.parentNode.insertBefore(dna, s);
        })();
    </script>
    <noscript><img src="//dna.buildstarted.com/g?siteId=linksfor.devs"/></noscript>
</body>
</html>