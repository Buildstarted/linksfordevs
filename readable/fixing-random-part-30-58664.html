<!DOCTYPE html>
<html lang="en">
<head>
    <title>
Fixing Random, part 30 - linksfor.dev(s)    </title>
    <meta charset="utf-8">
    <link rel="alternate" type="application/rss+xml" title="Linksfor.dev(s) feed" href="https://linksfor.dev/feed.rss" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta name="google" value="notranslate">
    <link rel="stylesheet" type="text/css" href="/style.css" />
    <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon">
    <link rel="icon" href="/favicon.ico" type="image/x-icon">
    <meta property="og:title" content="Fixing Random, part 30 - linksfor.dev(s)"/>
    <meta property="og:description" content="Last time on FAIC I posed and solved a problem in Bayesian reasoning involving only discrete distributions, and then proposed a variation on the problem whereby we change the prior distribution to &#x2026;"/>
    <meta property="og:type" content="website"/>
    <meta property="og:url" content="https://ericlippert.com/2019/05/13/fixing-random-part-30/"/>

<meta property="og:site_name" content="linksfor.dev(s)" />
</head>
<body>
    <div class="devring" style="background: #222">
        <div style="text-align:center">Explore other dev related sites in this ring. If you would like to join this ring <a href="https://devring.club">click here</a>.</div>
        <div class="grid">
            <div style="display: grid; grid-template-columns: .5fr 1fr 1fr 1fr; text-align: center;">
                <span class="devring-title"><a href="https://devring.club/">devring.club</a></span>
                <a href="https://devring.club/sites/1/prev" class="devring-previous">Previous</a>
                <a href="https://devring.club/random" class="devring-random">Random</a>
                <a href="https://devring.club/sites/1/next" class="devring-next">Next</a>
            </div>
        </div>
    </div>
    <div class="grid">
        <h1>
<a href="/" style="color:inherit">linksfor.dev(s)</a>
        </h1>
        <title>linksfor.dev(s) - Fixing Random, part 30</title>
<div class="readable">
        <h1>Fixing Random, part 30</h1>
            <div>Reading time: 12-15 minutes</div>
        <div>Posted here: 21 Feb 2020</div>
        <p><a href="https://ericlippert.com/2019/05/13/fixing-random-part-30/">https://ericlippert.com/2019/05/13/fixing-random-part-30/</a></p>
        <hr/>
<div id="readability-page-1" class="page"><div>
		<p data-adtags-visited="true"><a href="https://ericlippert.com/2019/05/10/fixing-random-part-29/">Last time on FAIC</a> I posed and solved a problem in Bayesian reasoning involving only discrete distributions, and then proposed a variation on the problem whereby we change the prior distribution to a continuous distribution, while preserving that the likelihood function produced a discrete distribution.</p>
<p data-adtags-visited="true">The question is: <em>what is the continuous posterior when we are given an observation of the discrete result?</em></p>
<p data-adtags-visited="true">More specifically, the problem I gave was: suppose we have a prior in the form of a process which produces random values between 0 and 1. We sample from that process and produce a coin that is heads with the given probability. We flip the coin; it comes up heads. <em>What is the posterior distribution of coin probabilities?</em></p>
<p data-adtags-visited="true">I find this question a bit hard to wrap my head around. Here’s one way to think about it: Suppose we stamp the probability of the coin coming up heads onto the coin. We mint and then flip a million of those coins <em>once each</em>. We discard all the coins that came up tails. The question is:<em> what is the distribution of probabilities stamped on the coins that came up heads?</em></p>
<p data-adtags-visited="true">Let’s remind ourselves of Bayes’ Theorem. For prior <strong>P(A)</strong> and likelihood function <strong>P(B|A)</strong>, that the posterior is:</p>
<p data-adtags-visited="true"><strong>P(A|B) = P(A) P(B|A) / P(B)</strong></p>
<p data-adtags-visited="true">Remembering of course that <strong>P(A|B)</strong> is logically a function that takes a&nbsp;<strong>B</strong> and returns a distribution of <strong>A</strong> and similarly for <strong>P(B|A)</strong>.</p>
<p data-adtags-visited="true">But so far we’ve only seen examples of Bayes’ Theorem for discrete distributions. Fortunately, it turns out that we can do almost the exact same arithmetic in our weighted non-normalized distributions and get the correct result.</p>
<hr>
<p data-adtags-visited="true"><strong>Aside:</strong> A formal presentation of the continuous version of Bayes Theorem and proving that it is correct would require some calculus and distract from where I want to go in this episode, so I’m just going to wave my hands here. Rest assured that we could put this on a solid theoretical foundation if we chose to.</p>
<hr>
<p data-adtags-visited="true">Let’s think about this in terms of our type system. If we have a prior:</p>
<p data-adtags-visited="true"><code>IWeightedDistribution&lt;double&gt; prior = whatever;</code></p>
<p data-adtags-visited="true">and a likelihood:</p>
<p data-adtags-visited="true"><code>Func&lt;double, IWeightedDistribution&lt;Result&gt;&gt; likelihood = whatever;</code></p>
<p data-adtags-visited="true">then what we want is a function:</p>
<p data-adtags-visited="true"><code>Func&lt;Result, IWeightedDistribution&lt;double&gt;&gt; posterior = ???</code></p>
<p data-adtags-visited="true">Let’s suppose our result is <code>Heads</code>. The question is: what is <code>posterior(Heads).Weight(d)</code>&nbsp;equal to, for any <code>d</code>&nbsp;we care to choose? We just apply Bayes’ Theorem <em>on the weights</em>. That is, this expression should be equal to:</p>
<p data-adtags-visited="true"><code>prior.Weight(d) * likelihood(d).Weight(Heads) / ???.Weight(Heads)</code></p>
<p data-adtags-visited="true">We have a problem; we do not have an <code>IWeightedDistribution&lt;Result&gt;</code>&nbsp;to get <code>Weight(Heads)</code>&nbsp;from to divide through. That is: we need to know what the probability is of getting heads if we sample a coin from the mint, flip it, and do <em>not</em> discard heads.</p>
<p data-adtags-visited="true">We could estimate it by repeated computation. We could call</p>
<p data-adtags-visited="true"><code>likelihood(prior.Sample()).Sample()</code></p>
<p data-adtags-visited="true">a billion times; the fraction of them that are <code>Heads</code>&nbsp;is the weight of <code>Heads</code>&nbsp;overall.</p>
<p data-adtags-visited="true">That sounds expensive though. Let’s give this some more thought.</p>
<p data-adtags-visited="true">Whatever the <code>Weight(Heads)</code>&nbsp;is, it is a <em>positive&nbsp;constant,&nbsp;</em>right? And we have already abandoned the requirement that weights have to be normalized so that the area under the curve is exactly 1.0. <strong>Positive constants do not affect proportionality.&nbsp;</strong>We do not actually <em>need</em> to compute the denominator at all in order to solve a continuous Bayesian inference problem; we just assume that the denominator is a positive constant, <em>and so we can ignore it.</em></p>
<p data-adtags-visited="true">So <code>posterior(Heads)</code>&nbsp;must produce a distribution such that <code>posterior(Heads).Weight(d)</code> is&nbsp;<em>proportional</em>&nbsp;<em>to:</em></p>
<p data-adtags-visited="true"><code>prior.Weight(d) * likelihood(d).Weight(Heads)</code></p>
<p data-adtags-visited="true">But that is just a non-normalized weight function, and we already have the gear to produce a distribution when we are given a non-normalized weight function; we can use our <code>Metropolis</code> class from two episodes ago. It can take a non-normalized weight function, an initial distribution and a proposal distribution, and produce a weighted distribution from it.</p>
<hr>
<p data-adtags-visited="true"><strong>Aside:</strong>&nbsp;Notice that we don’t even need <code>prior</code> to be a distribution that we can sample from; all we need is its weight function.</p>
<hr>
<p data-adtags-visited="true">That was all very abstract, so let’s look at the example I proposed last time: a mint with poor quality control produces coins with a particular probability of coming up heads; that’s our prior.</p>
<p data-adtags-visited="true">Therefore we’ll need a PDF that has zero weight for all values less than 0 and greater than 1. We don’t care if it is normalized or not.</p>
<p data-adtags-visited="true">Remember, this distribution represents the quality of coins that come from the mint, and the value produced by sampling this distribution is the bias of the coin towards heads, where 0 is “double tailed” and 1 is “double headed”.</p>
<p data-adtags-visited="true">Let’s choose a beta distribution.</p>
<hr>
<p data-adtags-visited="true"><strong>Aside:</strong> I’ve implemented the very useful gamma and beta distributions, but I won’t bother to show them in the blog. The code is tedious and explaining the mathematics would get us pretty off-topic. The short version is that the beta distribution is a distribution on numbers between 0 and 1 that can take on a variety of shapes. See the source code repository for the details of the implementation, and <a href="https://en.wikipedia.org/wiki/Beta_distribution">Wikipedia</a> for an explanation of the distribution; various possible shapes for its two parameters are given below:</p>
<p data-adtags-visited="true"><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/f/f3/Beta_distribution_pdf.svg/325px-Beta_distribution_pdf.svg.png" alt="Probability density function for the Beta distribution"></p>
<p data-adtags-visited="true">The code for the Beta and Gamma distributions and the rest of the code in this episode can be found <a href="https://github.com/ericlippert/probability/tree/episode30">here</a>.</p>
<hr>
<p data-adtags-visited="true">Let’s take <code>Beta.Distribution(5, 5)</code> as our prior distribution. That is, the “fairness” of the coins produced by our terrible mint is distributed like this:</p>
<p data-adtags-visited="true"><span><span>var</span><span>&nbsp;</span><span>prior</span><span>&nbsp;</span><span>=</span><span>&nbsp;</span><span>Beta</span><span>.</span><span>Distribution</span><span>(</span><span>5</span><span>,</span><span>&nbsp;</span><span>5</span><span>);</span><br>
<span>Console</span><span>.</span><span>WriteLine</span><span>(</span><span>prior</span><span>.</span><span>Histogram</span><span>(</span><span>0</span><span>,</span><span>&nbsp;</span><span>1</span><span>));</span></span></p>
<pre>&nbsp;                 ****                   
                 ******                 
                ********                
               **********               
               **********               
              ************              
              ************              
             **************             
             ***************            
            ****************            
            ****************            
           ******************           
          ********************          
          ********************          
         **********************         
        ***********************         
        ************************        
       **************************       
     ******************************     
----------------------------------------</pre>
<p data-adtags-visited="true">Most of the coins are pretty fair, but there is still a significant fraction of coins that produce 90% heads or 90% tails.</p>
<p data-adtags-visited="true">We need a likelihood function, but that is easy enough. Let’s make an enumerated type for coins called <code>Result</code>&nbsp;with two values, <code>Heads</code>&nbsp;and <code>Tails</code>:</p>
<p data-adtags-visited="true"><span><span>IWeightedDistribution</span><span>&lt;</span><span>Result</span><span>&gt;</span><span>&nbsp;</span><span>likelihood</span><span>(</span><span>double</span><span>&nbsp;</span><span>d</span><span>)</span><span>&nbsp;</span><span>=&gt;</span><span>&nbsp;</span><br>
<span><span>&nbsp;&nbsp;</span></span><span>Flip</span><span>&lt;</span><span>Result</span><span>&gt;</span><span>.</span><span>Distribution</span><span>(</span><span>Heads</span><span>,</span><span>&nbsp;</span><span>Tails</span><span>,</span><span>&nbsp;</span><span>d</span><span>);</span></span></p>
<p data-adtags-visited="true">We can use <code>Metropolis</code> to compute the posterior, but what should we use for the initial and proposal distributions? The prior seems like a perfectly reasonable guess as to the initial distribution, and we can use a normal distribution centered on the previous sample for the proposal.</p>
<p data-adtags-visited="true"><span><span><span>IWeightedDistribution</span><span>&lt;</span><span>double</span><span>&gt;&nbsp;posterior(</span><span>Result</span><span>&nbsp;r)&nbsp;=&gt;</span><br>
<span><span>&nbsp;&nbsp;</span></span><span>Metropolis</span><span>&lt;</span><span>double</span><span>&gt;.Distribution(</span><br>
<span>&nbsp; &nbsp; d&nbsp;=&gt;&nbsp;prior.Weight(d)&nbsp;*&nbsp;likelihood(d).Weight(r), // weight</span><br>
<span>&nbsp;&nbsp;&nbsp;&nbsp;prior, // initial</span><br>
<span>&nbsp;&nbsp;&nbsp;&nbsp;d&nbsp;=&gt;&nbsp;</span><span>Normal</span><span>.Distribution(d,&nbsp;</span><span>1</span><span>)); // proposal</span></span></span></p>
<p data-adtags-visited="true">But you know what, l<span>et’s once again make a generic helper function:</span></p>
<p data-adtags-visited="true"><span><span>public</span><span>&nbsp;</span><span>static</span><span>&nbsp;</span><span>Func</span><span>&lt;</span><span>B</span><span>,</span><span>&nbsp;</span><span>IWeightedDistribution</span><span>&lt;</span><span>double</span><span>&gt;&gt;</span><span>&nbsp;</span><span>Posterior</span><span>&lt;</span><span>B</span><span>&gt;(</span><br>
<span>&nbsp; </span><span>this</span><span>&nbsp;</span><span>IWeightedDistribution</span><span>&lt;</span><span>double</span><span>&gt;</span><span>&nbsp;</span><span>prior</span><span>,</span><br>
<span>&nbsp;&nbsp;</span><span>Func</span><span>&lt;</span><span>double</span><span>,</span><span>&nbsp;</span><span>IWeightedDistribution</span><span>&lt;</span><span>B</span><span>&gt;&gt;</span><span>&nbsp;</span><span>likelihood</span><span>)</span><span>&nbsp;</span><span>=&gt;</span><br>
<span>&nbsp; &nbsp;&nbsp;</span><span>b</span><span>&nbsp;</span><span>=&gt;</span><span>&nbsp;</span><br>
<span>&nbsp; &nbsp; &nbsp;&nbsp;</span><span>Metropolis</span><span>&lt;</span><span>double</span><span>&gt;</span><span>.</span><span>Distribution</span><span>(</span><br>
<span>&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;</span><span>d</span><span>&nbsp;</span><span>=&gt;</span><span>&nbsp;</span><span>prior</span><span>.</span><span>Weight</span><span>(</span><span>d</span><span>)</span><span>&nbsp;</span><span>*</span><span>&nbsp;</span><span>likelihood</span><span>(</span><span>d</span><span>)</span><span>.</span><span>Weight</span><span>(</span><span>b</span><span>),</span><br>
<span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span>prior</span><span>,</span><br>
<span>&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;</span><span>d</span><span>&nbsp;</span><span>=&gt;</span><span>&nbsp;</span><span>Normal</span><span>.</span><span>Distribution</span><span>(</span><span>d</span><span>,</span><span>&nbsp;</span><span>1</span><span>));</span></span></p>
<p data-adtags-visited="true">And now we’re all set:</p>
<p data-adtags-visited="true"><span><span>var</span><span>&nbsp;</span><span>posterior</span><span>&nbsp;</span><span>=</span><span>&nbsp;</span><span>prior</span><span>.</span><span>Posterior</span><span>(</span><span>likelihood</span><span>);</span><br>
<span>Console</span><span>.</span><span>WriteLine</span><span>(</span><span>posterior</span><span>(</span><span>Heads</span><span>)</span><span>.</span><span>Histogram</span><span>(</span><span>0</span><span>,</span><span>&nbsp;</span><span>1</span><span>));</span></span></p>
<pre>&nbsp;                    ****               
                    ******              
                   *******              
                  *********             
                 ***********            
                ************            
                ************            
               *************            
               **************           
               ***************          
              ****************          
             ******************         
             ******************         
            *******************         
            ********************        
           **********************       
          ************************      
         *************************      
       *****************************    
----------------------------------------</pre>
<p data-adtags-visited="true">It is subtle, but if you compare the prior to the posterior you can see that the posterior is slightly shifted to the right, which is the “more heads” end of the distribution.</p>
<hr>
<p data-adtags-visited="true"><strong>Aside:</strong>&nbsp;In this specific case we could have computed the posterior exactly. It turns out that the posterior of a beta distribution in this scenario is another beta distribution, just with different shape parameters. But the point is that we don’t <em>need</em> to know any special facts about the prior in order to create a distribution that estimates the posterior.</p>
<hr>
<p data-adtags-visited="true">Does this result make sense?</p>
<p data-adtags-visited="true">The prior is that a coin randomly pulled from this mint is biased, but equally likely to be biased towards heads or tails. But if we get a head, that is evidence, however slight, that the coin is not biased towards tails. We don’t know if this coin is close to fair, or very biased towards heads, or very biased towards tails, but we should expect based on this single observation that the first and second possibilities are <em>more probable</em> than the prior, and the third possibility is <em>less probable</em>&nbsp;than the prior. We see that reflected in the histogram.</p>
<p data-adtags-visited="true">Or, another way to think about this is: suppose we sampled from the prior many times, we flipped each coin exactly once, and we threw away the ones that came up tails, and then we did a histogram of the <em>fairness</em> of those that came up heads. Unsurprisingly, <em>the coins more likely to turn up tails got thrown away more often than the coins that came up heads, so the posterior histogram is biased towards heads.</em></p>
<hr>
<p data-adtags-visited="true"><strong>Exercise:</strong> Suppose we changed the scenario up so that we were computing the posterior probability of <em>two</em> flips. If we get two heads, obviously the posterior will be even more on the “heads” side. <em>How do you think the posterior looks if we observe one head, one tail?</em>&nbsp;Is it different than the prior, and if so, how? Try implementing it and see if your conjecture was correct.</p>
<hr>
<p data-adtags-visited="true">What have we learned? That given a continuous prior, a discrete likelihood, and an observation, we can simulate sampling from the continuous posterior.</p>
<p data-adtags-visited="true">Why is this useful? The coin-flipping example is not particularly interesting, but of course coin-flipping is a stand-in for more interesting examples. Perhaps we have a prior on how strongly a typical member of a population believes in the dangers of smoking; if we observe one of them smoking, what can we deduce is the posterior strength of their beliefs? Or perhaps we have a prior on whether a web site user will take an action on the site, like buying a product or changing their password, or whatever; if we observe them taking that action, what posterior deductions can we make about their beliefs?</p>
<p data-adtags-visited="true">As we’ve seen a number of times in this series, it’s hard for humans to make correct deductions about posterior probabilities even when given good priors; if we can make tools available to software developers that help us make accurate deductions, that has a potentially powerful effect on everything from the design of developer tools to the design of public policy.</p>
<hr>
<p data-adtags-visited="true"><strong>Next time on FAIC:</strong> Thus far in this series we’ve been very interested in the question “given a description of a distribution, how can I sample from it efficiently?” But there are other things we want to do with distributions; for example, suppose we have a random process that produces outcomes, and each outcome is associated with a gain or loss of some amount of money. What is the <em>expected amount</em> we will gain or lose in the long run? We’ll look at some of these “expected value” problems, and see how the gear we’ve developed so far can help compute the answers.</p>

			
			
						</div></div></div>
    </div>
    <footer>
        <div>created by buildstarted &copy; 2020 <a href="/about">about</a></div>
        <div>Share this page on social media: copy and paste this url https://linksfor.dev/</div>
        <div>If you prefer RSS: <a href="https://linksfor.dev/feed.xml">https://linksfor.dev/feed.xml</a></div>
        <div>Customer satisfaction guaranteed to be optional.</div>
    </footer>
    
    <script async defer>
        _dna = window._dna || {};
        _dna.siteId = "linksfor.devs";
        _dna.outlink = true;

        (function() {
            let dna = document.createElement('script');
            dna.type = 'text/javascript';
            dna.async = true;
            dna.src = '//dna.buildstarted.com/t.js';
            let s = document.getElementsByTagName('script')[0];
            s.parentNode.insertBefore(dna, s);
        })();
    </script>
    <noscript><img src="//dna.buildstarted.com/g?siteId=linksfor.devs"/></noscript>
</body>
</html>