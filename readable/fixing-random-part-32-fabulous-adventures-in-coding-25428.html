<!DOCTYPE html>
<html lang="en">
<head>
    <title>linksfor.dev(s)</title>
    <meta charset="utf-8">
    <link rel="alternate" type="application/rss+xml" title="Linksfor.dev(s) feed" href="https://linksfor.dev/feed.rss" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta name="google" value="notranslate">
    <link rel="stylesheet" type="text/css" href="/style.css" />
    <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon">
    <link rel="icon" href="/favicon.ico" type="image/x-icon">
</head>
<body>
    <div class="grid">
        <h1>
                <span style="cursor: default" title="linksfor.dev(s) has been running for 1 year! :partypopper:">🎉</span>
<a href="/" style="color:inherit">linksfor.dev(s)</a>
        </h1>
        <title>linksfor.dev(s) - Fixing Random, part 32</title>
<div class="readable">
        <h1>Fixing Random, part 32</h1>
        <p>
Reading time: 10-13 minutes        </p>
        <p><a href="https://ericlippert.com/2019/05/28/fixing-random-part-32/">https://ericlippert.com/2019/05/28/fixing-random-part-32/</a></p>
        <hr/>
<div id="readability-page-1" class="page"><div>
		<p data-adtags-visited="true"><a href="https://ericlippert.com/2019/05/20/fixing-random-part-31/">Last time on FAIC</a> we reviewed the meaning of “expected value”: when you get a whole bunch of samples from a distribution, and a function on those samples, what is the average value of the function’s value as the number of samples gets large?</p>
<p data-adtags-visited="true">I gave a naive implementation:</p>
<p data-adtags-visited="true"><span><span>public</span><span>&nbsp;</span><span>static</span><span>&nbsp;</span><span>double</span><span>&nbsp;</span><span>ExpectedValue&lt;T&gt;</span><span>(</span><br>
<span>&nbsp; &nbsp;&nbsp;</span><span>this</span><span>&nbsp;</span><span>IDistribution</span><span>&lt;</span><span>T</span><span>&gt;</span><span>&nbsp;</span><span>d</span><span>,</span><br>
<span>&nbsp; &nbsp;&nbsp;</span><span>Func</span><span>&lt;</span><span>T</span><span>,</span><span>&nbsp;</span><span>double</span><span>&gt;</span><span>&nbsp;</span><span>f</span><span>)</span><span>&nbsp;</span><span>=&gt;</span><br>
<span>&nbsp;&nbsp;</span><span>d</span><span>.</span><span>Samples</span><span>()</span><span>.</span><span>Take</span><span>(</span><span>1000</span><span>)</span><span>.</span><span>Select</span><span>(</span><span>f</span><span>)</span><span>.</span><span>Average</span><span>();</span></span></p>
<p data-adtags-visited="true"><span><span>public</span><span>&nbsp;</span><span>static</span><span>&nbsp;</span><span>double</span><span>&nbsp;</span><span>ExpectedValue</span><span>(</span><br>
<span>&nbsp; &nbsp;&nbsp;</span><span>this</span><span>&nbsp;</span><span>IDistribution</span><span>&lt;double</span><span>&gt;</span><span>&nbsp;</span><span>d</span><span>)</span><span>&nbsp;</span><span>=&gt;</span><br>
<span>&nbsp;&nbsp;</span><span>d</span><span>.ExpectedValue(x=&gt;x);</span></span></p>
<p data-adtags-visited="true">Though short and sweet, this implementation has some problems; the most obvious one is that hard-coded 1000 in there; where did it come from? Nowhere in particular, that’s where.</p>
<p data-adtags-visited="true">It seems highly likely that this is either too big, and we can compute a good estimate in fewer samples, or that it is too small, and we’re missing some important values.</p>
<p data-adtags-visited="true">Let’s explore a scenario where the number of samples is too small. The scenario will be <em>contrived</em>, but not <em>entirely</em> unrealistic.</p>
<p data-adtags-visited="true">Let’s suppose we have an investment strategy; we invest a certain amount of money with this strategy, and when the strategy is complete, we have either more or less money than we started with. To simplify things, let’s say that the “outcome” of this strategy is just a number between 0.0 and 1.0; 0.0 indicates that the strategy has completely failed, resulting in a loss, and 1.0 indicates that the strategy has completely succeeded, resulting in the maximum possible return.</p>
<p data-adtags-visited="true">Before we go on, I want to talk a bit about that “resulting in a loss” part. If you’re a normal, sensible investor like me and you have $100 to invest, you buy a stock or a mutual fund for $100 because you believe it will increase in value. If it goes up to $110 you sell it and pocket the $10 profit. If it goes down to $90, you sell it and take the $10 loss. But in no case do you ever lose more than the $100 you invested. (Though of course you do pay fees on each trade whether it goes up or down; let’s suppose those are negligible.) Our goal is to get that 10% return on investment.</p>
<p data-adtags-visited="true">Now consider the following much riskier strategy for spending $100 to speculate in the market: suppose there is a stock currently at $100 which we believe will go down, not up. We borrow a hundred shares from someone willing to lend them to us, and sell them for $10000. We pay the lender $100 interest for their trouble. Now if the stock goes down to $90, we buy the hundred shares back for $9000, return them to the owner, and we’ve spent $100 but received $1000; we’ve made a <strong>900%</strong> return on the $100 we “invested”. This is a “short sale”, and as you can see, you get a 900% return instead of a 10% return.</p>
<p data-adtags-visited="true">(I say “invested” in scare quotes because this isn’t really investing; it’s speculation. Which is a genteel word for “gambling”.)</p>
<p data-adtags-visited="true">But perhaps you also see the danger here. Suppose the stock goes down but only to $99.50. We buy back the shares for $9950, and we’ve only gained $50 on the trade, so we’ve lost half of our $100; in the “long” strategy, we would only have lost fifty cents; your losses can easily be bigger with the short strategy than with the long strategy.</p>
<p data-adtags-visited="true">But worse, what if the stock goes <strong>up</strong> to $110. We have to buy back those shares for $11000, so we’ve “invested” $100 and gotten a “return” of -$1000, for a total loss of $1100 on a $100 investment. In a short sale if things go catastrophically wrong you can end up losing <em>more</em> than your original investment. A lot more!</p>
<p data-adtags-visited="true">As I often say, foreshadowing is the sign of a quality blog; let’s continue with our example.</p>
<p data-adtags-visited="true">We have a process that produces values from 0.0 to 1.0 that indicates the “success level” of the strategy. What is the distribution of success level? Let’s suppose it’s a straightforward bell-shaped curve with the mean on the “success” side:</p>
<p data-adtags-visited="true"><img data-attachment-id="6285" data-permalink="https://ericlippert.com/2019/05/28/fixing-random-part-32/screen-shot-2019-05-20-at-1-14-22-pm/" data-orig-file="https://ericlippert.files.wordpress.com/2019/05/screen-shot-2019-05-20-at-1.14.22-pm.png" data-orig-size="1344,732" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Screen Shot 2019-05-20 at 1.14.22 PM" data-image-description="" data-medium-file="https://ericlippert.files.wordpress.com/2019/05/screen-shot-2019-05-20-at-1.14.22-pm.png?w=300" data-large-file="https://ericlippert.files.wordpress.com/2019/05/screen-shot-2019-05-20-at-1.14.22-pm.png?w=584" src="https://ericlippert.files.wordpress.com/2019/05/screen-shot-2019-05-20-at-1.14.22-pm.png?w=584" alt="Screen Shot 2019-05-20 at 1.14.22 PM.png" srcset="https://ericlippert.files.wordpress.com/2019/05/screen-shot-2019-05-20-at-1.14.22-pm.png?w=584 584w, https://ericlippert.files.wordpress.com/2019/05/screen-shot-2019-05-20-at-1.14.22-pm.png?w=1168 1168w, https://ericlippert.files.wordpress.com/2019/05/screen-shot-2019-05-20-at-1.14.22-pm.png?w=150 150w, https://ericlippert.files.wordpress.com/2019/05/screen-shot-2019-05-20-at-1.14.22-pm.png?w=300 300w, https://ericlippert.files.wordpress.com/2019/05/screen-shot-2019-05-20-at-1.14.22-pm.png?w=768 768w, https://ericlippert.files.wordpress.com/2019/05/screen-shot-2019-05-20-at-1.14.22-pm.png?w=1024 1024w" sizes="(max-width: 584px) 100vw, 584px"></p>
<p data-adtags-visited="true">This is a PDF, and extra bonus, it is normalized: the total area under the curve is 1.0. <strong>The vast majority of the time — 99.9% — our investment strategy produces an outcome between 0.48 and 1.0, so that’s the area of the graph I’m going to focus on.</strong></p>
<hr>
<p data-adtags-visited="true"><strong>Aside:</strong> I know it looks a little weird seeing a bell-shaped curve that just cuts off abruptly at 1.0, but let’s not stress about it. Again, this is a contrived example, and we’ll see why it is irrelevant later.</p>
<hr>
<p data-adtags-visited="true">Now, I don’t intend to imply that the “success level” is the <em>return</em>: I’m not saying that most of the time we get a 75% return. Let’s suppose that we’ve worked out a function that translates “strategy outcome level” to the percentage gain on our investment. That is, if the function is 0.0, we’ve not gained anything but we’ve not lost anything either. If it is 0.5 then our profits are 50% of our investment; if it’s -0.25 then we’ve taken a net loss of 25% of our investment, and so on.</p>
<p data-adtags-visited="true">Let’s propose such a function, and zoom in on the right side of the diagram where the 99.9% of cases are found: from 0.46 to 1.0. Here’s our function:</p>
<p data-adtags-visited="true"><img data-attachment-id="6286" data-permalink="https://ericlippert.com/2019/05/28/fixing-random-part-32/screen-shot-2019-05-20-at-1-34-22-pm/" data-orig-file="https://ericlippert.files.wordpress.com/2019/05/screen-shot-2019-05-20-at-1.34.22-pm.png" data-orig-size="1458,810" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Screen Shot 2019-05-20 at 1.34.22 PM" data-image-description="" data-medium-file="https://ericlippert.files.wordpress.com/2019/05/screen-shot-2019-05-20-at-1.34.22-pm.png?w=300" data-large-file="https://ericlippert.files.wordpress.com/2019/05/screen-shot-2019-05-20-at-1.34.22-pm.png?w=584" src="https://ericlippert.files.wordpress.com/2019/05/screen-shot-2019-05-20-at-1.34.22-pm.png?w=584" alt="Screen Shot 2019-05-20 at 1.34.22 PM.png" srcset="https://ericlippert.files.wordpress.com/2019/05/screen-shot-2019-05-20-at-1.34.22-pm.png?w=584 584w, https://ericlippert.files.wordpress.com/2019/05/screen-shot-2019-05-20-at-1.34.22-pm.png?w=1166 1166w, https://ericlippert.files.wordpress.com/2019/05/screen-shot-2019-05-20-at-1.34.22-pm.png?w=150 150w, https://ericlippert.files.wordpress.com/2019/05/screen-shot-2019-05-20-at-1.34.22-pm.png?w=300 300w, https://ericlippert.files.wordpress.com/2019/05/screen-shot-2019-05-20-at-1.34.22-pm.png?w=768 768w, https://ericlippert.files.wordpress.com/2019/05/screen-shot-2019-05-20-at-1.34.22-pm.png?w=1024 1024w" sizes="(max-width: 584px) 100vw, 584px"></p>
<p data-adtags-visited="true">If our process produces an outcome of 0.54 or larger, we make between a 0% and an 18% return, which seems pretty good; after all the vast majority of the bulk of the distribution is to the right of 0.54. Now, in the unlikely case where we get an outcome from 0.48 to 0.54, we lose money; on the left hand end, we’re losing almost 200%; that is, if we put in $100 we not only fail to make it back, we end up <em>owing</em> $100.</p>
<p data-adtags-visited="true">The question at hand, of course, is <strong>what is the average value of the “profit function” given the distribution of outcomes?</strong> That is, if we ran this strategy a thousand times, on average what return would we get?</p>
<p data-adtags-visited="true">Well, we already wrote the code, so let’s run it: (Code can be found <a href="https://github.com/ericlippert/probability/tree/episode32">here</a>.)</p>
<p data-adtags-visited="true"><span><span>var</span> <span>p </span><span>=</span><span>&nbsp;</span><span>Normal</span><span>.</span><span>Distribution</span><span>(</span><span>0.75</span><span>,</span><span>&nbsp;</span><span>0.09</span><span>);</span><br>
<span>double</span> <span>f</span><span>(</span><span>double</span><span>&nbsp;</span><span>x</span><span>)</span><span>&nbsp;</span><span>=&gt;</span><span><br>
</span><span>&nbsp; </span><span>Atan</span><span>(</span><span>1000</span><span>&nbsp;</span><span>*</span><span>&nbsp;</span><span>(</span><span>x</span><span>&nbsp;</span><span>–</span><span>&nbsp;</span><span>.45</span><span>))</span><span>&nbsp;</span><span>*</span><span>&nbsp;</span><span>20</span><span>&nbsp;</span><span>–</span><span>&nbsp;</span><span>31.2</span><span>;</span><br>
<span>Console</span><span>.</span><span>WriteLine</span><span>(</span><span>$”</span><span>{</span><span>p</span><span>.</span><span>ExpectedValue</span><span>(</span><span>f</span><span>):</span><span>0.##</span><span>}</span><span>“</span><span>);</span><br>
</span></p>
<hr>
<p data-adtags-visited="true"><strong>Aside:</strong> I’m ignoring the fact that if we get a sample out of the normal distribution that is greater than 1.0 or less than 0.0 we do not discard it. The region to the left of 0.0 is absurdly unlikely, and the region to the right of 1.0 has low probability and the value of the profit function is very flat. Writing a lot of fiddly code to correct for this problem would not change the estimated expected value much and would be a distraction from the far deeper problem we’re about to discover.</p>
<p data-adtags-visited="true">Also, I’m limiting the distribution to a support of 0.0 to 1.0 for pedagogical reasons; as we’ll see in later episodes, we will eventually remove this restriction, so let’s just ignore it now.</p>
<hr>
<p data-adtags-visited="true">We get the answer:</p>
<pre>0.14</pre>
<p data-adtags-visited="true">Super, we will get on average a 14% return on our investment with this strategy. Seems like a good investment; though we have a small chance of losing big, the much larger chance of getting a solid 0% to 18% return outweighs it. <em>On average.</em></p>
<hr>
<p data-adtags-visited="true"><strong>Aside:</strong> It is important to realize that this 14% figure does not <em>necessarily</em> imply that <em>typically</em> we get a 14% return when we run this strategy, any more than the expected value of 3.5 implies that we’ll <em>typically</em> get 3.5 when we roll a d6. The expected value only tells you what the average behaviour is; it doesn’t tell you, for example, what the probability is that you’ll lose everything. It tells you that if you ran this strategy a million times, on average you’d get a 14% return, and that’s <em>all</em> it tells you. Be careful when making decisions relying upon expected values!</p>
<hr>
<p data-adtags-visited="true">This 14% expected result totally makes sense, right? We know most of the time the result will be between 0% and 18%, so 14% seems like a totally reasonable expected value.</p>
<p data-adtags-visited="true">As one of my managers at Microsoft used to say: <em>would you bet your car on it?</em></p>
<p data-adtags-visited="true">Let’s just run the code again to be sure.</p>
<pre>0.14</pre>
<p data-adtags-visited="true">Whew. That’s a relief.</p>
<p data-adtags-visited="true">You know what they say: <em>doing the same thing twice and expecting different results is the definition of practicing the piano</em>, but I’m still not satisfied. Let’s run it another hundred times:</p>
<pre>0.14, 0.08, 0.09, 0.14, 0.08, 0.14, 0.14, 0.07, 0.07, 0.14, ...</pre>
<p data-adtags-visited="true">Oh dear me.</p>
<p data-adtags-visited="true">The expected return of our strategy is usually 14%; why it is sometimes <em>half</em> that? Yes, there is randomness in our algorithm, but surely it should not cause the value computed to vary so widely!</p>
<p data-adtags-visited="true">It seems like we cannot rely on this implementation, but why not? The code looks right.</p>
<p data-adtags-visited="true">As it turns out, I showed you <em>exactly the wrong part of the graphs</em> above. (Because I am devious.) I showed you the parts that make up 99.9% of the likely cases. Let’s look at the graph of the profit-or-loss function over the whole range of outcomes from 0.0 to 1.0:</p>
<p data-adtags-visited="true"><img data-attachment-id="6292" data-permalink="https://ericlippert.com/2019/05/28/fixing-random-part-32/screen-shot-2019-05-20-at-5-09-29-pm/" data-orig-file="https://ericlippert.files.wordpress.com/2019/05/screen-shot-2019-05-20-at-5.09.29-pm.png" data-orig-size="731,388" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Screen Shot 2019-05-20 at 5.09.29 PM" data-image-description="" data-medium-file="https://ericlippert.files.wordpress.com/2019/05/screen-shot-2019-05-20-at-5.09.29-pm.png?w=300" data-large-file="https://ericlippert.files.wordpress.com/2019/05/screen-shot-2019-05-20-at-5.09.29-pm.png?w=584" src="https://ericlippert.files.wordpress.com/2019/05/screen-shot-2019-05-20-at-5.09.29-pm.png?w=584" alt="Screen Shot 2019-05-20 at 5.09.29 PM.png" srcset="https://ericlippert.files.wordpress.com/2019/05/screen-shot-2019-05-20-at-5.09.29-pm.png?w=584 584w, https://ericlippert.files.wordpress.com/2019/05/screen-shot-2019-05-20-at-5.09.29-pm.png?w=150 150w, https://ericlippert.files.wordpress.com/2019/05/screen-shot-2019-05-20-at-5.09.29-pm.png?w=300 300w, https://ericlippert.files.wordpress.com/2019/05/screen-shot-2019-05-20-at-5.09.29-pm.png 731w" sizes="(max-width: 584px) 100vw, 584px"></p>
<p data-adtags-visited="true">OUCH. It’s an approximation of a step function; in the original graph that started at 0.46, we should have noticed that the extremely steep part of the curve was trending downwards to the left at an alarming rate. If you have an outcome of 0.48 from our process, you’re going to lose half your money; 0.46, you lose all your money and a little bit more. But by the time we get to 0.44, we’re down to losing over 60 times your original investment; this is a catastrophic outcome.</p>
<p data-adtags-visited="true"><strong>How likely is that catastrophic outcome?</strong> Let’s zoom in on just that part of the probability distribution. I’ll re-scale the profit function so that they fit on the same graph, so you can see where exactly the step happens:</p>
<p data-adtags-visited="true"><img data-attachment-id="6294" data-permalink="https://ericlippert.com/2019/05/28/fixing-random-part-32/2019-05-20-3/" data-orig-file="https://ericlippert.files.wordpress.com/2019/05/2019-05-20-3.png" data-orig-size="1727,937" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="2019-05-20 (3)" data-image-description="" data-medium-file="https://ericlippert.files.wordpress.com/2019/05/2019-05-20-3.png?w=300" data-large-file="https://ericlippert.files.wordpress.com/2019/05/2019-05-20-3.png?w=584" src="https://ericlippert.files.wordpress.com/2019/05/2019-05-20-3.png?w=584" alt="2019-05-20 (3)" srcset="https://ericlippert.files.wordpress.com/2019/05/2019-05-20-3.png?w=584 584w, https://ericlippert.files.wordpress.com/2019/05/2019-05-20-3.png?w=1168 1168w, https://ericlippert.files.wordpress.com/2019/05/2019-05-20-3.png?w=150 150w, https://ericlippert.files.wordpress.com/2019/05/2019-05-20-3.png?w=300 300w, https://ericlippert.files.wordpress.com/2019/05/2019-05-20-3.png?w=768 768w, https://ericlippert.files.wordpress.com/2019/05/2019-05-20-3.png?w=1024 1024w" sizes="(max-width: 584px) 100vw, 584px"></p>
<p data-adtags-visited="true">This is a normalized PDF, so the probability of getting a particular outcome is equal to the area of the region under the curve. The region under the curve from 0.38 to 0.45 is a little less than a triangle with base 0.07 and height 0.02, so its area is in the ballpark of 0.0007. We will generate a sample in that region roughly once every 1400 samples.</p>
<p data-adtags-visited="true">But… we’re computing the expected value by <em>taking only a thousand samples</em>, so it is quite likely that we don’t hit this region at all. The expected losses in that tiny area are so enormous that it changes the average every time we do sample from it. <strong>Whether you include a -60 value or not in the average is a huge influence as to whether the average is 0.14 or 0.07.&nbsp;</strong></p>
<hr>
<p data-adtags-visited="true"><strong>Aside:</strong> if you run the expected value estimate a few thousand times or more it just gets crazier; sometimes the expected value is actually negative if we just by luck get more than one sample in this critical region.</p>
<hr>
<p data-adtags-visited="true">I contrived this example to produce occasional “<a href="https://en.wikipedia.org/wiki/Black_swan_theory">black swan events</a>“; obviously, our naïve algorithm for computing expected value does not take into account the difficulty of sampling a black swan event, and therefore produces inaccurate and inconsistent results.</p>
<hr>
<p data-adtags-visited="true"><strong>Next time on FAIC:</strong> We know how to solve this problem exactly for discrete distributions; can we use some of those ideas to improve our estimate of the expected value in this scenario?</p>
			
			
						</div></div></div>
    </div>
    <footer>
        <div>created by buildstarted &copy; 2020 <a href="/about">about</a></div>
        <div>Share this page on social media: copy and paste this url https://linksfor.dev/</div>
        <div>If you prefer RSS: <a href="https://linksfor.dev/feed.xml">https://linksfor.dev/feed.xml</a></div>
    </footer>
    
    <script async defer>
        _dna = window._dna || {};
        _dna.siteId = "linksfor.devs";
        _dna.outlink = true;

        (function () {
            let dna = document.createElement('script');
            dna.type = 'text/javascript';
            dna.async = true;
            dna.src = '//dna.buildstarted.com/t.js';
            let s = document.getElementsByTagName('script')[0];
            s.parentNode.insertBefore(dna, s);
        })();
    </script>
    <noscript><img src="//dna.buildstarted.com/g?siteId=linksfor.devs"/></noscript>
</body>
</html>