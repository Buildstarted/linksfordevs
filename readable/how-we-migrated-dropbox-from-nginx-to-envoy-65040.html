<!DOCTYPE html>
<html lang="en">
<head>
    <title>
How we migrated Dropbox from Nginx to Envoy - linksfor.dev(s)    </title>
    <meta charset="utf-8">
    <link rel="alternate" type="application/rss+xml" title="Linksfor.dev(s) feed" href="https://linksfor.dev/feed.rss" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta name="google" value="notranslate">
    <link rel="stylesheet" type="text/css" href="/style.css" />
    <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon">
    <link rel="icon" href="/favicon.ico" type="image/x-icon">
    <meta property="og:title" content="How we migrated Dropbox from Nginx to Envoy - linksfor.dev(s)"/>
    <meta property="article:author" content="Alexey Ivanov"/>
    <meta property="og:description" content="In this blogpost we&#x2019;ll talk about the old Nginx-based traffic infrastructure, its pain points, and the benefits we gained by migrating to Envoy. We&#x2019;ll compare Nginx to Envoy across many software engineering and operational dimensions. We&#x2019;ll also briefly touch on the migration process, its current state, and some of the problems encountered on the way."/>
    <meta property="og:type" content="website"/>
    <meta property="og:url" content="https://dropbox.tech/infrastructure/how-we-migrated-dropbox-from-nginx-to-envoy"/>

<meta property="og:site_name" content="linksfor.dev(s)" />
</head>
<body>
    <div class="devring" style="background: #222">
        <div style="text-align:center">Explore other dev related sites in this ring. If you would like to join this ring <a href="https://devring.club">click here</a>.</div>
        <div class="grid">
            <div style="display: grid; grid-template-columns: .5fr 1fr 1fr 1fr; text-align: center;">
                <span class="devring-title"><a href="https://devring.club/">devring.club</a></span>
                <a href="https://devring.club/sites/1/prev" class="devring-previous">Previous</a>
                <a href="https://devring.club/random" class="devring-random">Random</a>
                <a href="https://devring.club/sites/1/next" class="devring-next">Next</a>
            </div>
        </div>
    </div>
    <div class="grid">
        <h1>
<a href="/" style="color:inherit">linksfor.dev(s)</a>
        </h1>
        <title>linksfor.dev(s) - How we migrated Dropbox from Nginx to Envoy</title>
<div class="readable">
        <h1>How we migrated Dropbox from Nginx to Envoy</h1>
            <div>by Alexey Ivanov</div>
            <div>Reading time: 41-52 minutes</div>
        <div>Posted here: 31 Jul 2020</div>
        <p><a href="https://dropbox.tech/infrastructure/how-we-migrated-dropbox-from-nginx-to-envoy">https://dropbox.tech/infrastructure/how-we-migrated-dropbox-from-nginx-to-envoy</a></p>
        <hr/>
<div id="readability-page-1" class="page"><div>
            


<div>
    
    <div>
<p>In this blogpost we’ll talk about the old Nginx-based traffic infrastructure, its pain points, and the benefits we gained by migrating to <a href="https://www.envoyproxy.io/">Envoy</a>. We’ll compare Nginx to Envoy across many software engineering and operational dimensions. We’ll also briefly touch on the migration process, its current state, and some of the problems encountered on the way.</p>
<p>When we moved most of Dropbox traffic to Envoy, we had to seamlessly migrate a system that already handles tens of millions of open connections, millions of requests per second, and terabits of bandwidth. This effectively made us into one of the biggest Envoy users in the world.&nbsp;</p>
<p>Disclaimer: although we’ve tried to remain objective, quite a few of these comparisons are specific to Dropbox and the way our software development works: making bets on Bazel, gRPC, and C++/Golang.</p>
<p>Also note that we’ll cover the open source version of the Nginx, not its commercial version with additional features.</p>

</div>
<div>
<p id="-our-legacy-nginx-based-traffic-infrastructure">
    <h2 data-index="1"> Our legacy Nginx-based traffic infrastructure</h2>
</p>
</div>
<div>
<p>Our Nginx configuration was mostly static and rendered with a combination of Python2, Jinja2, and YAML. Any change to it required a full re-deployment. All dynamic parts, such as upstream management and a stats exporter, were written in Lua. Any sufficiently complex logic was moved to <a href="https://dropbox.tech/infrastructure/meet-bandaid-the-dropbox-service-proxy">the next proxy layer, written in Go</a>.</p>
<p>Our post, “<a href="https://dropbox.tech/infrastructure/dropbox-traffic-infrastructure-edge-network">Dropbox</a><a href="https://dropbox.tech/infrastructure/dropbox-traffic-infrastructure-edge-network"> traffic infrastructure: Edge network</a>,” has a section about our legacy Nginx-based infrastructure.</p>
<p>Nginx served us well for almost a decade. But it didn’t adapt to our current development best-practices:</p>
<ul>
<li>Our internal and (private) external APIs are gradually migrating from REST to gRPC which requires all sorts of transcoding features from proxies.</li>
<li>Protocol buffers became <i>de facto</i> standard for service definitions and configurations.</li>
<li>All software, regardless of the language, is built and tested with Bazel.</li>
<li>Heavy involvement of our engineers on essential infrastructure projects in the open source community.</li>
</ul>
<p>Also, operationally Nginx was quite expensive to maintain:<br>
</p>
<ul>
<li>Config generation logic was too flexible and split between YAML, Jinja2, and Python.</li>
<li>Monitoring was a mix of Lua, log parsing, and system-based monitoring.</li>
<li>An increased reliance on third party modules affected stability, performance, and the cost of subsequent upgrades.</li>
<li>Nginx deployment and process management was quite different from the rest of the services. It relied a lot on other systems’ configurations: syslog, logrotate, etc, as opposed to being fully separate from the base system.</li>
</ul>
<p>With all of that, for the first time in 10 years, we started looking for a potential replacement for Nginx.<br>
</p>

</div>

<div>
<p>As we frequently mention, internally we rely heavily on the Golang-based proxy called <a href="https://dropbox.tech/infrastructure/meet-bandaid-the-dropbox-service-proxy">Bandaid</a>. It has a great integration with Dropbox infrastructure, because it has access to the vast ecosystem of internal Golang libraries: monitoring, service discoveries, rate limiting, etc. We considered migrating from Nginx to Bandaid but there are a couple of issues that prevent us from doing that:</p>
<ul>
<li>Golang is more resource intensive than C/C++. Low resource usage is especially important for us on the Edge since we can’t easily “auto-scale” our deployments there.<ul>
<li>CPU overhead mostly comes from GC, HTTP parser and TLS, with the latter being less optimized than BoringSSL used by Nginx/Envoy.</li>
<li>The “goroutine-per-request” model and GC overhead greatly increase memory requirements in high-connection services like ours.</li>
</ul>
</li>
<li>No FIPS support for Go’s TLS stack.</li>
<li>Bandaid does not have a community outside of Dropbox, which means that we can only rely on ourself for feature development.</li>
</ul>
<p>With all that we’ve decided to start migrating our traffic infrastructure to Envoy instead.<br>
</p>

</div>
<div>
<p id="-our-new-envoy-based-traffic-infrastructure">
    <h2 data-index="3"> Our new Envoy-based traffic infrastructure</h2>
</p>
</div>
<div>
<p>Let’s look into the main development and operational dimensions one by one, to see why we think Envoy is a better choice for us and what we gained by moving from Nginx to Envoy.</p>
<h3>Performance</h3>
<p>Nginx’s<a href="https://www.nginx.com/blog/inside-nginx-how-we-designed-for-performance-scale/"> architecture</a> is event-driven and multi-process. It has support for <a href="https://www.nginx.com/blog/socket-sharding-nginx-release-1-9-1/"><span>SO_REUSEPORT</span></a>, <a href="https://lore.kernel.org/patchwork/cover/543309/"><span>EPOLLEXCLUSIVE</span></a>, and worker-to-CPU pinning. Although it is event-loop based, is it not fully non-blocking. This means some operations, like <a href="https://blog.cloudflare.com/how-we-scaled-nginx-and-saved-the-world-54-years-every-day/">opening a file</a> or access/error logging, can potentially cause an event-loop stall (<a href="http://nginx.org/en/docs/http/ngx_http_core_module.html#aio">even</a><a href="http://nginx.org/en/docs/http/ngx_http_core_module.html#aio"> with </a><a href="http://nginx.org/en/docs/http/ngx_http_core_module.html#aio"><span>aio</span></a><a href="http://nginx.org/en/docs/http/ngx_http_core_module.html#aio">, aio_write, and thread pools enabled</a>.) This leads to increased tail latencies, which can cause multi-second delays on spinning disk drives.</p>
<p>Envoy has a similar event-driven architecture, except it uses threads instead of processes. It also has <span>SO_REUSEPORT</span> support (with a BPF filter support) and relies on libevent for event loop implementation (in other words, no fancy <span>epoll(2)</span> features like <span>EPOLLEXCLUSIVE</span>.) Envoy does not have any blocking IO operations in the event loop. Even logging is implemented in a non-blocking way, so that it does not cause stalls.</p>
<p>It looks like in theory Nginx and Envoy should have similar performance characteristics. But hope is not our strategy, so our first step was to run a diverse set of workload tests against similarly tuned Nginx and Envoy setups.</p>
<p>If you are interested in performance tuning, we describe our standard tuning guidelines in “<a href="https://dropbox.tech/infrastructure/optimizing-web-servers-for-high-throughput-and-low-latency">Optimizing</a><a href="https://dropbox.tech/infrastructure/optimizing-web-servers-for-high-throughput-and-low-latency"> web servers for high throughput and low latency</a>.” It involves everything from picking the hardware, to OS tunables, to library choices and web server configuration.</p>
<p>Our test results showed similar performance between Nginx and Envoy under most of our test workloads: high requests per second (RPS), high bandwidth, and a mixed low-latency/high-bandwidth gRPC proxying.<br>
</p>
<p>It is arguably very hard to make a good performance test. Nginx has <a href="https://www.nginx.com/blog/testing-the-performance-of-nginx-and-nginx-plus-web-servers/">guidelines for performance testing</a>, but these are not codified. Envoy also has <a href="https://www.envoyproxy.io/docs/envoy/latest/faq/performance/how_to_benchmark_envoy">a guideline for benchmarking</a>, and even some tooling under the <a href="https://github.com/envoyproxy/envoy-perf">envoy</a><a href="https://github.com/envoyproxy/envoy-perf">-perf</a> project, but sadly the latter looks unmaintained.&nbsp;</p>
<p>We resorted to using our internal testing tool. It’s called “hulk” because of its reputation for smashing our services.</p>
<p>That said, there were a couple of notable differences in results:</p>
<ul>
<li>Nginx showed higher long tail latencies. This was mostly due to event loops stalls under heavy I/O, especially if used together with <span>SO_REUSEPORT</span> since in that case <a href="https://blog.cloudflare.com/the-sad-state-of-linux-socket-balancing/#so_reuseporttotherescue">connections can be accepted on behalf of a currently blocked worker</a>.</li>
<li>Nginx performance without stats collections is on part with Envoy, but our Lua stats collection slowed Nginx on the high-RPS test by a factor of 3. This was expected given our reliance on <span>lua_shared_dict</span>, which is synchronized across workers with a mutex.</li>
</ul>
<p>We do understand how inefficient our stats collection was. We considered implementing something akin to FreeBSD’s <a href="https://www.freebsd.org/cgi/man.cgi?query=counter&amp;sektion=9&amp;manpath=freebsd-release-ports#IMPLEMENTATION_DETAILS"><span>counter(9)</span></a> in userspace: CPU pinning, per-worker lockless counters with a fetching routine that loops through all workers aggregating their individual stats. But we gave up on this idea, because if we wanted to instrument Nginx internals (e.g. all error conditions), it would mean supporting an enormous patch that would make subsequent upgrades a true hell.</p>
<p>Since Envoy does not suffer from either of these issues, after migrating to it we were able to release up to 60% of servers previously exclusively occupied by Nginx.</p>
<h3>Observability</h3>
<p>Observability is the <a href="https://landing.google.com/sre/sre-book/chapters/part3/#fig_part-practices_reliability-hierarchy">most fundamental operational need</a> for any product, but especially for such a foundational piece of infrastructure as a proxy. It is even more important during the migration period, so that any issue can be detected by the monitoring system rather than reported by frustrated users.</p>
<p>Non-commercial Nginx comes with a “<a href="http://nginx.org/en/docs/http/ngx_http_stub_status_module.html">stub</a><a href="http://nginx.org/en/docs/http/ngx_http_stub_status_module.html"> status</a>” module that has 7 stats:</p>

</div>
<div>




    <pre><code>Active connections: 291 <span>
server </span>accepts handled requests
 16630948 16630948 31070465 
Reading: 6 Writing: 179 Waiting: 106 
</code></pre>
</div>
<p>This was definitely not enough, so we’ve added a simple <span>log_by_lua</span> handler that adds per-request stats based on headers and variables that are available in Lua: status codes, sizes, cache hits, etc. Here is an example of a simple stats-emitting function:</p>
<div>




    <pre><code><span>function</span> <span><span><span>_M</span>.</span></span>cache<span>_hit_stats(<span>stat</span>)</span>
    <span>if</span> <span><span><span>_var</span>.</span></span>upstream_cache_status <span>then</span>
        <span>if</span> <span><span><span>_var</span>.</span></span>upstream_cache_status<span> == </span><span>"HIT"</span> <span>then</span>
            stat:add(<span>"upstream_cache_hit"</span>)
        <span>else</span>
            stat:add(<span>"upstream_cache_miss"</span>)
        <span>end</span>
    <span>end</span>
<span>end</span>
</code></pre>
</div>
<div>
<p>In addition to the per-request Lua stats, we also had a very brittle <span>error.log</span> parser that was responsible for upstream, http, Lua, and TLS error classification.</p>
<p>On top of all that, we had a separate exporter for gathering Nginx internal state: time since the last reload, number of workers, RSS/VMS sizes, TLS certificate ages, etc.</p>
<p>A typical Envoy setup provides us thousands of distinct metrics (in <a href="https://prometheus.io/docs/instrumenting/exposition_formats/#text-based-format">prometheus format</a>) describing both proxied traffic and server’s internal state:</p>

</div>
<div>




    <pre><code>$ curl -s http:<span>//</span>localhost:<span>3990</span><span>/stats/</span>prometheus | wc -l
<span>14819</span>
</code></pre>
</div>
<div>
<p>This includes a myriad of stats with different aggregations:</p>
<ul>
<li>Per-cluster/per-upstream/per-vhost HTTP stats, including connection pool info and various timing histograms.</li>
<li>Per-listener TCP/HTTP/TLS downstream connection stats.</li>
<li>Various internal/runtime stats from basic version info and uptime to memory allocator stats and deprecated feature usage counters.</li>
</ul>
<p>A special shoutout is needed for Envoy’s <a href="https://www.envoyproxy.io/docs/envoy/latest/operations/admin">admin interface</a>. Not only does it provide additional structured stats through <span>/certs</span>, <span>/clusters</span>, and <span>/config_dump</span> endpoints, but there are also very important operational features:</p>
<ul>
<li>The ability to change error logging on the fly through <a href="https://www.envoyproxy.io/docs/envoy/latest/operations/admin#post--logging"><span>/logging</span></a>. This allowed us to troubleshoot fairly obscure problems in a matter of minutes.</li>
<li><span>/cpuprofiler</span>, <span>/heapprofiler</span>, <span>/contention</span> which would surely be quite useful during the inevitable performance troubleshooting.</li>
<li><span>/runtime_modify</span> &nbsp;endpoint allows us to change set of configuration parameters without pushing new configuration, which could be used in feature gating, etc.</li>
</ul>
<p>In addition to stats, Envoy also supports <a href="https://www.envoyproxy.io/docs/envoy/latest/intro/arch_overview/observability/tracing">pluggable tracing providers</a>. This is useful not only to our Traffic team, who own multiple load-balancing tiers, but also for application developers who want to track request latencies end-to-end from the edge to app servers.</p>
<p>Technically, Nginx also supports tracing through a third-party <a href="https://github.com/opentracing-contrib/nginx-opentracing">OpenTracing integration</a><a href="https://github.com/opentracing-contrib/nginx-opentracing">,</a><a href="https://github.com/opentracing-contrib/nginx-opentracing"> </a>but it is not under heavy development.</p>
<p>And last but not least, Envoy has the ability to <a href="https://www.envoyproxy.io/docs/envoy/latest/api-v3/data/accesslog/v3/accesslog.proto">stream access logs over gRPC</a>. This removes the burden of supporting syslog-to-hive bridges from our Traffic team. Besides, it’s way easier (and secure!) to spin up a generic gRPC service in Dropbox production than to add a custom TCP/UDP listener.</p>
<p>Configuration of access logging in Envoy, like everything else, happens through a gRPC management service, the <a href="https://www.envoyproxy.io/docs/envoy/latest/api-v2/config/accesslog/v2/als.proto">Access Log Service</a> (ALS). Management services are the standard way of integrating the Envoy data plane with various services in production. This brings us to our next topic.</p>
<h3>Integration</h3>
<p>Nginx’s approach to integration is best described as “Unix-ish.” Configuration is very static. It heavily relies on files (e.g. the config file itself, TLS certificates and tickets, allowlists/blocklists, etc.) and well-known industry protocols (<a href="http://nginx.org/en/docs/syslog.html">logging</a><a href="http://nginx.org/en/docs/syslog.html"> to syslog</a> and auth sub-requests through HTTP). Such simplicity and backwards compatibility is a good thing for small setups, since Nginx can be easily automated with a couple of shell scripts. But as the system’s scale increases, testability and standardization become more important.</p>
<p>Envoy is far more opinionated in how the traffic dataplane should be integrated with its control plane, and hence with the rest of infrastructure. It encourages the use of <a href="https://developers.google.com/protocol-buffers">protobufs</a> and gRPC by providing a stable API commonly referred as <a href="https://docs.google.com/document/d/1xeVvJ6KjFBkNjVspPbY_PwEDHC7XPi0J5p1SqUXcCl8/edit#heading=h.c0uts5ftkk58">xDS</a>. Envoy discovers its dynamic resources by <a href="https://www.envoyproxy.io/docs/envoy/latest/intro/arch_overview/operations/dynamic_configuration#arch-overview-dynamic-config">querying one or more of these xDS services</a>.</p>
<p>Nowadays, the xDS APIs are evolving beyond Envoy: <a href="https://blog.envoyproxy.io/the-universal-data-plane-api-d15cec7a">Universal </a><a href="https://blog.envoyproxy.io/the-universal-data-plane-api-d15cec7a">D</a><a href="https://blog.envoyproxy.io/the-universal-data-plane-api-d15cec7a">ata Plane API</a><a href="https://blog.envoyproxy.io/the-universal-data-plane-api-d15cec7a"> </a><a href="https://blog.envoyproxy.io/the-universal-data-plane-api-d15cec7a">(UDPA)</a> has the ambitious goal of “becoming de facto standard of L4/L7 loadbalancers.”</p>
<p>From our experience, this ambition works out well. We already use <a href="https://docs.google.com/document/d/1NSnK3346BkBo1JUU3I9I5NYYnaJZQPt8_Z_XCBCI3uA/edit#">Open Request Cost Aggregation</a><a href="https://docs.google.com/document/d/1NSnK3346BkBo1JUU3I9I5NYYnaJZQPt8_Z_XCBCI3uA/edit#"> </a><a href="https://docs.google.com/document/d/1NSnK3346BkBo1JUU3I9I5NYYnaJZQPt8_Z_XCBCI3uA/edit#">(ORCA)</a> for our internal load testing, and are considering using UDPA for our non-Envoy loadbalancers e.g. our <a href="https://github.com/facebookincubator/katran">Katran-based eBPF/XDP Layer-4 Load Balancer</a>.</p>
<p>This is especially good for Dropbox, where all services internally already interact through gRPC-based APIs. We’ve implemented our own version of xDS control plane that integrates Envoy with our configuration management, service discovery, secret management, and route information.</p>
<p>For more information about Dropbox RPC, please read “<a href="https://dropbox.tech/infrastructure/courier-dropbox-migration-to-grpc">Courier:</a><a href="https://dropbox.tech/infrastructure/courier-dropbox-migration-to-grpc"> Dropbox migration to gRPC</a>.” There we describe in detail how we integrated service discovery, secret management, stats, tracing, circuit breaking, etc, with gRPC.</p>
<p>Here are <b>some</b> of the available xDS services, their Nginx alternatives, and our examples of how we use them:</p>
<ul>
<li><a href="https://www.envoyproxy.io/docs/envoy/latest/api-v2/config/accesslog/v2/als.proto">Access Log Service</a><a href="https://www.envoyproxy.io/docs/envoy/latest/api-v2/config/accesslog/v2/als.proto"> </a><a href="https://www.envoyproxy.io/docs/envoy/latest/api-v2/config/accesslog/v2/als.proto">(ALS)</a>, as mentioned above, lets us dynamically configure access log destinations, encodings, and formats. Imagine a dynamic version of Nginx’s <span>log_format</span> and <span>access_log</span>.</li>
<li><a href="https://www.envoyproxy.io/docs/envoy/latest/intro/arch_overview/upstream/service_discovery#arch-overview-service-discovery-types-eds">Endpoint discovery service</a><a href="https://www.envoyproxy.io/docs/envoy/latest/intro/arch_overview/upstream/service_discovery#arch-overview-service-discovery-types-eds"> </a><a href="https://www.envoyproxy.io/docs/envoy/latest/intro/arch_overview/upstream/service_discovery#arch-overview-service-discovery-types-eds">(EDS)</a> provides information about cluster members. This is analogous to a dynamically updated list of <span>upstream</span> block’s <span>server</span> entries (e.g. for Lua that would be a &nbsp;<a href="https://github.com/openresty/lua-nginx-module#balancer_by_lua_block"><span>balancer_by_lua_block</span></a>) in the Nginx config. In our case we proxied this to our internal service discovery.</li>
<li><a href="https://www.envoyproxy.io/docs/envoy/latest/configuration/security/secret">Secret discovery service</a><a href="https://www.envoyproxy.io/docs/envoy/latest/configuration/security/secret"> </a><a href="https://www.envoyproxy.io/docs/envoy/latest/configuration/security/secret">(SDS)</a> provides various TLS-related information that would cover various <span>ssl_*</span> directives (and respectively <a href="https://github.com/openresty/lua-nginx-module#ssl_certificate_by_lua_block"><span>ssl_*_by_lua_block</span></a>.) &nbsp;We adapted this interface to our secret distribution service.</li>
<li><a href="https://www.envoyproxy.io/docs/envoy/latest/configuration/operations/runtime#config-runtime-rtds">Runtime Discovery Service</a><a href="https://www.envoyproxy.io/docs/envoy/latest/configuration/operations/runtime#config-runtime-rtds"> </a><a href="https://www.envoyproxy.io/docs/envoy/latest/configuration/operations/runtime#config-runtime-rtds">(RTDS)</a> is providing runtime flags. Our implementation of this functionality in Nginx was quite hacky, based on checking the existence of various files from Lua. This approach can quickly become inconsistent between the individual servers. Envoy’s default implementation is also filesystem-based, but we instead pointed our RTDS xDS API to our distributed configuration storage. That way we can control whole clusters at once (through a tool with a <span>sysctl</span>-like interface) and there are no accidental inconsistencies between different servers.</li>
<li><a href="https://www.envoyproxy.io/docs/envoy/latest/configuration/http/http_conn_man/rds#config-http-conn-man-rds">Route discovery service</a><a href="https://www.envoyproxy.io/docs/envoy/latest/configuration/http/http_conn_man/rds#config-http-conn-man-rds"> </a><a href="https://www.envoyproxy.io/docs/envoy/latest/configuration/http/http_conn_man/rds#config-http-conn-man-rds">(RDS)</a> maps routes to virtual hosts, and allows additional configuration for headers and filters. In Nginx terms, these would be analogous to a dynamic location block with <span>set_header</span>/<span>proxy_set_header</span> and a <span>proxy_pass</span>. On lower proxy tiers we autogenerate these directly from our service definition configs.</li>
</ul>
<p>For an example of Envoy’s integration with an existing production system, here is a canonical example of how to <a href="https://www.envoyproxy.io/learn/service-discovery">integrate </a><a href="https://www.envoyproxy.io/learn/service-discovery">Envoy</a><a href="https://www.envoyproxy.io/learn/service-discovery"> with a custom service discovery</a>. There are also a couple of open source Envoy control-plane implementations, such as <a href="https://istio.io/">Istio</a> and the less complex <a href="https://github.com/envoyproxy/go-control-plane">go-control-plane</a>.</p>
<p>Our homegrown Envoy control plane implements an increasing number of xDS APIs. It is deployed as a normal gRPC service in production, and acts as an adapter for our infrastructure building blocks. It does this through a set of common Golang libraries to talk to internal services and expose them through a stable xDS APIs to Envoy. The whole process does not involve any filesystem calls, signals, cron, logrotate, syslog, log parsers, etc.</p>
<h3>Configuration</h3>
<p>Nginx has the undeniable advantage of a simple human-readable configuration. But this win gets lost as config gets more complex and begins to be code-generated.</p>
<p>As mentioned above, our Nginx config is generated through a mix of Python2, Jinja2, and YAML. Some of you may have seen or even written a variation of this in erb, pug, Text::Template, or maybe even m4:</p>

</div>
<div>




    <pre><code><span>{% <span><span>for</span></span> server <span>in</span> servers %}</span><span>
server {
    </span><span>{% <span><span>for</span></span> error_page <span>in</span> server.error_pages %}</span><span>
    error_page </span><span>{{ error_page.statuses|<span>join</span>(' ') }}</span><span> </span><span>{{ error_page.file }}</span><span>;
    </span><span>{% <span><span>endfor</span></span> %}</span><span>
    ...
    </span><span>{% <span><span>for</span></span> route <span>in</span> service.routes %}</span><span>
    </span><span>{% <span><span>if</span></span> route.regex or route.prefix or route.exact_path %}</span><span>
    location </span><span>{% <span><span>if</span></span> route.regex %}</span><span>~ </span><span>{{route.regex}}</span><span>{%
            <span><span>elif</span></span> route.exact_path %}</span><span>= </span><span>{{ route.exact_path }}</span><span>{%
            <span><span>else</span></span> %}</span><span>{{ route.prefix }}</span><span>{% <span><span>endif</span></span> %}</span><span> {
        </span><span>{% <span><span>if</span></span> route.brotli_level %}</span><span>
        brotli on;
        brotli_comp_level </span><span>{{ route.brotli_level }}</span><span>;
        </span><span>{% <span><span>endif</span></span> %}</span><span>
        ...
</span></code></pre>
</div>
<div>
<p>Our approach to Nginx config generation had a huge issue: all of the languages involved in config generation allowed substitution and/or logic. YAML has anchors, Jinja2 has loops/ifs/macroses, and of course Python is Turing-complete. Without a clean data model, complexity quickly spread across all three of them.</p>
<p>This problem is arguably fixable, but there were a couple of foundational ones:</p>
<ul>
<li>There is no declarative description for the config format. If we wanted to programmatically generate and validate configuration, we would need to invent it ourselves.</li>
<li>Config that is syntactically valid could still be invalid from a C code standpoint. For example, some of the buffer-related variables have limitations on values, restrictions on alignment, and interdependencies with other variables. To semantically validate a config we needed to run it through <span>nginx -t</span>.</li>
</ul>
<p>Envoy, on the other hand, has a unified data-model for configs: all of its configuration is defined in Protocol Buffers. This not only solves the data modeling problem, but also adds typing information to the config values. Given that protobufs are first class citizens in Dropbox production, and a common way of describing/configuring services, this makes integration <i>so</i> much easier.&nbsp;</p>
<p>Our new config generator for Envoy is based on protobufs and Python3. All data modeling is done in proto files, while all the logic is in Python. Here’s an example:</p>

</div>
<div>




    <pre><code><span>from</span> dropbox.proto.envoy.extensions.filters.http.gzip.v3.gzip_pb2 <span>import</span> Gzip
<span>from</span> dropbox.proto.envoy.extensions.filters.http.compressor.v3.compressor_pb2 <span>import</span> Compressor
    
<span>def</span> default_gzip_config(
    compression_level: <span>Gzip</span>.<span>CompressionLevel</span>.<span>Enum</span> = <span>Gzip</span>.<span>CompressionLevel</span>.<span>DEFAULT</span>,
    ) -&gt; <span>Gzip</span>:
        return <span>Gzip</span>(
            # <span>Envoy's</span> <span>default</span> is 6 (<span>Z_DEFAULT_COMPRESSION</span>).
            compression_level=compression_level,
            # <span>Envoy's</span> <span>default</span> is 4k (12 <span>bits</span>). <span>Nginx</span> uses 32k (<span>MAX_WBITS</span>, 15 <span>bits</span>).
            window_bits=<span>UInt32Value</span>(value=<span>12</span>),
            # <span>Envoy's</span> <span>default</span> is 5. <span>Nginx</span> uses 8 (<span>MAX_MEM_LEVEL</span> - 1).
            memory_level=<span>UInt32Value</span>(value=<span>5</span>),
            compressor=<span>Compressor</span>(
                content_length=<span>UInt32Value</span>(value=<span>1024</span>),
                remove_accept_encoding_header=<span>True</span>,
                content_type=default_compressible_mime_types(),
            ),
        )</code></pre>
</div>
<div>
<ul>
<li>Note the <a href="https://www.python.org/dev/peps/pep-0484/">Python3 type annotations</a> in that code! &nbsp;Coupled with <a href="https://github.com/dropbox/mypy-protobuf">mypy-protobuf protoc plugin</a>, these provide end-to-end typing inside the config generator. IDEs capable of checking them will immediately highlight typing mismatches.</li>
</ul>
<p>There are still cases where a type-checked protobuf can be logically invalid. In the example above, gzip <span>window_bits</span> can only take values between 9 and 15. This kind of restriction can be easily defined with a help of <a href="https://github.com/envoyproxy/protoc-gen-validate">protoc-gen-validate protoc plugin</a>:</p>

</div>
<div>




    <pre><code>google.protobuf.UInt32Value window_bits = <span>9</span> [(validate.rules).<span>uint</span>32 = {lte: <span>15</span> gte: <span>9</span>}];                      
</code></pre>
</div>
<p>Finally, an implicit benefit of using a formally defined configuration model is that it organically leads to the documentation being collocated with the configuration definitions. <a href="https://github.com/envoyproxy/envoy/blob/master/api/envoy/extensions/filters/http/gzip/v3/gzip.proto#L50">Here</a><a href="https://github.com/envoyproxy/envoy/blob/master/api/envoy/extensions/filters/http/gzip/v3/gzip.proto#L50">’</a><a href="https://github.com/envoyproxy/envoy/blob/master/api/envoy/extensions/filters/http/gzip/v3/gzip.proto#L50">s an example from </a><a href="https://github.com/envoyproxy/envoy/blob/master/api/envoy/extensions/filters/http/gzip/v3/gzip.proto#L50"><span>gzip.proto</span></a>:</p>
<div>




    <pre><code>

google.protobuf.UInt32Value memory_level = <span>1</span> [(validate.rules).<span>uint</span>32 = {lte: <span>9</span> gte: <span>1</span>}];
</code></pre>
</div>
<div>
<p>For those of you thinking about using protobufs in your production systems, but worried you may lack a schema-less representation, here’s a good article from Envoy core developer Harvey Tuch about how to work around this using <span>google.protobuf.Struct</span> and <span>google.protobuf.Any</span>: “<a href="https://blog.envoyproxy.io/dynamic-extensibility-and-protocol-buffers-dcd0bf0b8801">Dynamic</a><a href="https://blog.envoyproxy.io/dynamic-extensibility-and-protocol-buffers-dcd0bf0b8801"> extensibility and Protocol Buffers</a>.”</p>
<h3>Extensibility</h3>
<p>Extending Nginx beyond what’s possible with standard configuration usually requires writing a C module. Nginx’s <a href="http://nginx.org/en/docs/dev/development_guide.html">development guide</a> provides a solid introduction to the available building blocks. That said, this approach is relatively heavyweight. In practice, it takes a fairly senior software engineer to safely write an Nginx module.</p>
<p>In terms of infrastructure available for module developers, they can expect basic containers like hash tables/queues/rb-trees, (non-RAII) memory management, and hooks for all phases of request processing. There are also couple of external libraries like pcre, zlib, openssl, and, of course, libc.</p>
<p>For more lightweight feature extension, Nginx provides <a href="http://nginx.org/en/docs/http/ngx_http_perl_module.html#perl">Perl</a> and <a href="http://nginx.org/en/docs/http/ngx_http_js_module.html">Javascript</a> interfaces. Sadly, both are fairly limited in their abilities, mostly restricted to the content phase of request processing.</p>
<p>The most commonly used extension method adopted by the community is based on a third-party l<a href="https://github.com/openresty/lua-nginx-module">ua-</a><a href="https://github.com/openresty/lua-nginx-module">nginx</a><a href="https://github.com/openresty/lua-nginx-module">-module</a> and various <a href="https://github.com/openresty/">OpenResty libraries</a>. This approach can be hooked in at pretty much any phase of request processing. We used <span>log_by_lua</span> for stats collection, and <span>balancer_by_lua</span> for dynamic backend reconfiguration.</p>
<p>In theory, Nginx provides the ability to develop <a href="http://lxr.nginx.org/source/src/misc/ngx_cpp_test_module.cpp">modules in C++</a>. In practice, it lacks proper C++ interfaces/wrappers for all the primitives to make this worthwhile. There are nonetheless some <a href="https://github.com/chronolaw/ngx_cpp_dev">community attempts at it</a>. These are far from ready for production, though.</p>
<p>Envoy’s main extension mechanism is through C++ plugins. The process is <a href="https://blog.envoyproxy.io/how-to-write-envoy-filters-like-a-ninja-part-1-d166e5abec09">not as well documented</a> as in Nginx’s case, but it is simpler. This is partially due to:<br>
</p>
<ul>
<li><b>Clean and well-commented interfaces.</b> C++ classes act as natural extension and documentation points. For example, <a href="https://github.com/envoyproxy/envoy/blob/master/include/envoy/http/filter.h">checkout the HTTP filter interface</a>.</li>
<li><b>C++14 language and standard library. </b>From basic language features like templates and lambda functions, to type-safe containers and algorithms. In general, writing modern C++14 is not much different from using Golang or, with a stretch, one may even say Python.</li>
<li><b>Features beyond C++14 and its stdlib. </b>Provided by the <a href="https://abseil.io/about/intro">abseil</a> library, these include drop-in replacements from newer C++ standards, mutexes with built-in <a href="http://clang.llvm.org/docs/ThreadSafetyAnalysis.html">static deadlock detection</a> and debug support, additional/more efficient containers, <a href="https://abseil.io/about/philosophy">and much more</a>.</li>
</ul>
<p>For specifics, here’s a <a href="https://github.com/envoyproxy/envoy-filter-example/tree/master/http-filter-example">canonical example of an HTTP Filter module</a>.</p>
<p>We were able to integrate Envoy with <a href="https://dropbox.tech/infrastructure/monitoring-server-applications-with-vortex">Vortex2</a><a href="https://dropbox.tech/infrastructure/monitoring-server-applications-with-vortex"> </a><a href="https://dropbox.tech/infrastructure/monitoring-server-applications-with-vortex">(our</a><a href="https://dropbox.tech/infrastructure/monitoring-server-applications-with-vortex"> monitoring framework)</a> with only 200 lines of code by simply implementing the Envoy <a href="https://github.com/envoyproxy/envoy/tree/master/include/envoy/stats">stats</a> interface.</p>
<p>Envoy<a href="https://www.envoyproxy.io/docs/envoy/latest/configuration/http/http_filters/lua_filter"> also has Lua support</a> through <a href="https://github.com/moonjit/moonjit">moonjit</a>, a LuaJIT fork with improved Lua 5.2 support. Compared to Nginx’s 3rd-party Lua integration it has far fewer capabilities and hooks. This makes Lua in Envoy far less attractive due to the cost of additional complexity in developing, testing, and troubleshooting interpreted code. Companies that specialize in Lua development may disagree, but in our case we decided to avoid it and use C++ exclusively for Envoy extensibility.</p>
<p>What distinguishes Envoy from the rest of web servers is its emerging support for <a href="https://developer.mozilla.org/en-US/docs/WebAssembly/Concepts">WebAssembly</a> (WASM) — a fast, portable, and secure extension mechanism. WASM is not meant to be used directly, but as a compilation target for any general-purpose programming language. Envoy implements a <a href="https://github.com/proxy-wasm/spec/blob/master/abi-versions/vNEXT/README.md">WebAssembly for Proxies specification</a> (and also includes reference <a href="https://github.com/proxy-wasm/proxy-wasm-rust-sdk">Rust</a> and <a href="https://github.com/proxy-wasm/proxy-wasm-cpp-sdk">C++</a> SDKs) that describes the boundary between WASM code and a generic L4/L7 proxy. That separation between the proxy and extension code allows for secure sandboxing, while WASM low-level compact binary format allows for near native efficiency. On top of that, in Envoy proxy-wasm extensions are integrated with xDS. This allows dynamic updates and even potential A/B testing.</p>
<p>The “<a href="https://youtu.be/XdWmm_mtVXI?t=779">Extending</a><a href="https://youtu.be/XdWmm_mtVXI?t=779"> </a><a href="https://youtu.be/XdWmm_mtVXI?t=779">Envoy</a><a href="https://youtu.be/XdWmm_mtVXI?t=779"> with WebAssembly</a>” presentation from Kubecon’19 (remember that time when we had non-virtual conferences?) has a nice overview of &nbsp;WASM in Envoy and its potential uses. It also hints at performance levels of 60-70% of native C++ code.</p>
<p>With WASM, service providers get a safe and efficient way of running customers’ code on their edge. Customers get the benefit of portability: Their extensions can run on any cloud that implements the proxy-wasm ABI. Additionally, it allows your users to use any language as long as it can be compiled to WebAssembly. This enables them to use a broader set of non-C++ libraries, securely and efficiently.</p>
<p>Istio is putting a lot of resources into WebAssembly development: they already have an experimental version of the WASM-based telemetry extension and the <a href="https://webassemblyhub.io/">WebAssemblyHub community</a> for sharing extensions. You can read about it in detail in <a href="https://istio.io/latest/blog/2020/wasm-announce/">“Redefining</a><a href="https://istio.io/latest/blog/2020/wasm-announce/"> extensibility in proxies - introducing WebAssembly to </a><a href="https://istio.io/latest/blog/2020/wasm-announce/">Envoy</a><a href="https://istio.io/latest/blog/2020/wasm-announce/"> and Istio</a><a href="https://istio.io/latest/blog/2020/wasm-announce/">.</a><a href="https://istio.io/latest/blog/2020/wasm-announce/">”&nbsp;</a></p>
<p>Currently, we don’t use WebAssembly at Dropbox. But this might change when the Go SDK for proxy-wasm is available.</p>
<h3>Building and Testing</h3>
<p>By default, Nginx is built using a custom <a href="https://github.com/nginx/nginx/tree/master/auto">shell-based configuration system</a> and make-based build system. This is simple and elegant, but it took quite a bit of effort to integrate it into <a href="https://dropbox.tech/infrastructure/continuous-integration-and-deployment-with-bazel">B</a><a href="https://dropbox.tech/infrastructure/continuous-integration-and-deployment-with-bazel">azel-built monorepo</a> to get all the benefits of incremental, distributed, hermetic, and reproducible builds.</p>
<p>Google open<a href="https://nginx.googlesource.com/nginx/">-</a>sourced their <a href="https://nginx.googlesource.com/nginx/">B</a><a href="https://nginx.googlesource.com/nginx/">azel-built </a><a href="https://nginx.googlesource.com/nginx/">Nginx</a><a href="https://nginx.googlesource.com/nginx/"> version</a> which consists of Nginx, BoringSSL, PCRE, ZLIB, and Brotli library/module.</p>
<p>Testing-wise, Nginx has a set of Perl-driven <a href="http://hg.nginx.org/nginx-tests">integration tests</a> in a separate repository and no unit tests.</p>
<p>Given our heavy usage of Lua and absence of a built-in unit testing framework, we resorted to testing using mock configs and a simple Python-based test driver:</p>

</div>
<div>




    <pre><code><span><span>class</span> <span>ProtocolCountersTest</span>(<span>NginxTestCase</span>):</span>
    @classmethod
    <span><span>def</span> <span>setUpClass</span><span>(cls)</span></span>:
        <span>super</span>(ProtocolCountersTest, cls).setUpClass()
        cls.nginx_a = cls.add_nginx(
            nginx_CONFIG_PATH, endpoint=[<span>"in"</span>], upstream=[<span>"out"</span>],
        )
        cls.start_nginxes()

    @assert_delta(lambda <span>d:</span> d == <span>0</span>, get_stat(<span>"request_protocol_http2"</span>))
    @assert_delta(lambda <span>d:</span> d == <span>1</span>, get_stat(<span>"request_protocol_http1"</span>))
    <span><span>def</span> <span>test_http</span><span>(<span>self</span>)</span></span>:
        r = requests.get(<span>self</span>.nginx_a.endpoint[<span>"in"</span>].url(<span>"/"</span>))
        assert r.status_code == requests.codes.ok
</code></pre>
</div>
<div>
<p>On top of that, we verify the syntax-correctness of all generated configs by preprocessing them (e.g. replacing all IP addresses with 127/8 ones, switching to self-signed TLS certs, etc.) and running <span>nginx -c</span> on the result.</p>
<p>On the Envoy side, the main build system is already Bazel. So integrating it with our monorepo was trivial: Bazel easily allows <a href="https://docs.bazel.build/versions/master/external.html">adding external dependencies</a>.</p>
<p>We also use <a href="https://github.com/google/copybara">copybara</a> scripts to sync protobufs for both Envoy and udpa. Copybara is handy when you need to do simple transformations without the need to forever maintain a large patchset.</p>
<p>With Envoy we have the flexibility of using either unit tests (based on gtest/gmock) with a set of <a href="https://github.com/envoyproxy/envoy/tree/master/test/mocks">pre-written mocks</a>, or Envoy’s <a href="https://github.com/envoyproxy/envoy/tree/master/test/integration">integration test framework</a>, or both. There’s no need anymore to rely on slow end-to-end integration tests for every trivial change.</p>
<p><a href="https://github.com/google/googletest">gtest</a> is a fairly well-known unit-test framework used by Chromium and LLVM, among others. If you want to know more about googletest there are good intros for both <a href="https://github.com/google/googletest/blob/master/googletest/docs/primer.md">googletest</a> and <a href="https://chromium.googlesource.com/external/github.com/google/googletest/+/HEAD/googlemock/docs/cook_book.md">googlemock</a>.</p>
<p>Open source Envoy development <a href="https://github.com/envoyproxy/envoy/blob/master/CONTRIBUTING.md#submitting-a-pr">requires changes to have 100% unit test coverage</a>. Tests are automatically triggered for each pull request via the <a href="https://dev.azure.com/cncf/envoy/_build?view=pipelines">Azure CI Pipeline</a>.</p>
<p>It’s also a common practice to micro-benchmark performance-sensitive code with <a href="https://github.com/google/benchmark">google/becnhmark</a>:</p>

</div>
<div>




    <pre><code>$ bazel <span>run</span> <span>--compilation_mode</span>=opt test/common/upstream:load_balancer_benchmark -- <span>--benchmark_filter</span>=<span>".*LeastRequestLoadBalancerChooseHost.*"</span>
BM_LeastRequestLoadBalancerChooseHost/100/1/1000000          848 ms          449 ms            2 <span>mean_hits</span>=10k <span>relative_stddev_hits</span>=0.0102051 <span>stddev_hits</span>=102.051
<span>..</span>.
</code></pre>
</div>
<p>After switching to Envoy, we began to rely exclusively on unit tests for our internal module development:</p>
<div>




    <pre><code>TEST_F(CourierClientIdFilterTest, IdentityParsing) {
  <span><span>struct</span> <span>TestCase</span> {</span>
    <span>std</span>::<span>vector</span>&lt;<span>std</span>::<span>string</span>&gt; uris;
    Identity expected;
  };
  <span>std</span>::<span>vector</span>&lt;TestCase&gt; tests = {
    {{<span>"spiffe://prod.dropbox.com/service/foo"</span>}, {<span>"spiffe://prod.dropbox.com/service/foo"</span>, <span>"foo"</span>}},
    {{<span>"spiffe://prod.dropbox.com/user/boo"</span>}, {<span>"spiffe://prod.dropbox.com/user/boo"</span>, <span>"user.boo"</span>}},
    {{<span>"spiffe://prod.dropbox.com/host/strange"</span>}, {<span>"spiffe://prod.dropbox.com/host/strange"</span>, <span>"host.strange"</span>}},
    {{<span>"spiffe://corp.dropbox.com/user/bad-prefix"</span>}, {<span>""</span>, <span>""</span>}},
  };
  <span>for</span> (<span>auto</span>&amp; test : tests) {
    EXPECT_CALL(*ssl_, uriSanPeerCertificate()).WillOnce(testing::Return(test.uris));
    EXPECT_EQ(GetIdentity(ssl_), test.expected);
  }
}
</code></pre>
</div>
<div>
<p>Having sub-second test roundtrips has a compounding effect on productivity. It empowers us to put more effort into increasing test coverage. And being able to choose between unit and integration tests allows us to balance coverage, speed, and cost of Envoy tests.</p>
<p>Bazel is one of the best things that ever happened to our developer experience. It has a very steep learning curve and is a large upfront investment, but it has a very high return on that investment: <a href="https://docs.bazel.build/versions/master/guide.html#correct-incremental-rebuilds">incremental builds</a>, <a href="https://docs.bazel.build/versions/master/remote-caching.html">remote caching</a>, <a href="https://docs.bazel.build/versions/master/remote-execution.html">distributed builds/tests</a>, etc.</p>
<p>One of the less discussed benefits of Bazel is that it gives us an ability to <a href="https://docs.bazel.build/versions/master/query-how-to.html">query</a> <a href="https://docs.bazel.build/versions/master/skylark/aspects.html">and even augment</a> the dependency graph. A programmatic interface to the dependency graph, coupled with a common build system across all languages, is a very powerful feature. It can be used as a foundational building block for things like linters, code generation, vulnerability tracking, deployment system, etc.</p>
<h3>Security</h3>
<p>Nginx’s code surface is quite small, with minimal external dependencies. It’s typical to see only 3 external dependencies on the resulting binary: zlib (or <a href="https://github.com/cloudflare/zlib">one of its faster variants</a>), a TLS library, and PCRE. Nginx has a custom implementation of all protocol parsers, the event library, and they even went as far as to re-implement some libc functions.</p>
<p>At some point Nginx was considered so secure that it was used as a default web server in OpenBSD. Later two development communities had a falling out, which lead to the creation of &nbsp;<span>httpd</span>. You can read about the motivation behind that move in BSDCon’s “<a href="https://www.openbsd.org/papers/httpd-asiabsdcon2015.pdf">Introducing</a><a href="https://www.openbsd.org/papers/httpd-asiabsdcon2015.pdf"> OpenBSD</a><a href="https://www.openbsd.org/papers/httpd-asiabsdcon2015.pdf"> </a><a href="https://www.openbsd.org/papers/httpd-asiabsdcon2015.pdf">’s</a><a href="https://www.openbsd.org/papers/httpd-asiabsdcon2015.pdf"> new httpd</a>.”</p>
<p>This minimalism paid off in practice. Nginx has only had 30 <a href="https://nginx.org/en/security_advisories.html">vulnerabilities and exposures</a> reported in more than 11 years.</p>
<p>Envoy, on the other hand, has way more code, especially when you consider that that C++ code is far more dense than the basic C used for Nginx. It also incorporates millions of lines of code from external dependencies. Everything from event notification to protocol parsers is offloaded to 3rd party libraries. This increases attack surface and bloats the resulting binary.</p>
<p>To counteract this, Envoy relies heavily on modern security practices. It uses <a href="https://github.com/google/sanitizers/wiki/AddressSanitizer">AddressSanitizer</a>, <a href="https://github.com/google/sanitizers/wiki/ThreadSanitizerCppManual">ThreadSanitizer</a>, and <a href="https://github.com/google/sanitizers/wiki/MemorySanitizer">MemorySanitizer</a>. Its developers even went beyond that and adopted <a href="https://bugs.chromium.org/p/oss-fuzz/issues/list?q=label%3AProj-envoy&amp;sort=-id">fuzzing</a>.</p>
<p>Any opensource project that is critical to the global IT infrastructure can be accepted to the <a href="https://github.com/google/oss-fuzz">OSS-Fuzz</a>—a free platform for automated fuzzing. To learn more about it, see “<a href="https://google.github.io/oss-fuzz/architecture/">OSS-Fuzz</a><a href="https://google.github.io/oss-fuzz/architecture/"> / Architecture</a>.”</p>
<p>In practice, though, all these precautions do not fully counteract the increased code footprint. As a result, Envoy has had <a href="https://github.com/envoyproxy/envoy/security/advisories">22 security advisories in the </a><a href="https://github.com/envoyproxy/envoy/security/advisories">p</a><a href="https://github.com/envoyproxy/envoy/security/advisories">ast 2 years</a>.&nbsp;</p>
<p>Envoy's <a href="https://github.com/envoyproxy/envoy/security/policy">security release policy is described in great detail</a>, and in <a href="https://github.com/envoyproxy/envoy/tree/master/security/postmortems">postmortems</a> for selected vulnerabilities. Envoy is also a participant in <a href="https://www.envoyproxy.io/docs/envoy/latest/intro/arch_overview/security/google_vrp#arch-overview-google-vrp">Google’s Vulnerability Reward Program</a><a href="https://www.envoyproxy.io/docs/envoy/latest/intro/arch_overview/security/google_vrp#arch-overview-google-vrp"> </a><a href="https://www.envoyproxy.io/docs/envoy/latest/intro/arch_overview/security/google_vrp#arch-overview-google-vrp">(VRP)</a>. Open to all security researchers, VRP provides rewards for vulnerabilities discovered and reported according to their rules.</p>
<p>For a practical example of how some of these vulnerabilities can be potentially exploited, see this writeup about CVE-2019–18801: “<a href="https://blog.envoyproxy.io/exploiting-an-envoy-heap-vulnerability-96173d41792">Exploiting</a><a href="https://blog.envoyproxy.io/exploiting-an-envoy-heap-vulnerability-96173d41792"> an </a><a href="https://blog.envoyproxy.io/exploiting-an-envoy-heap-vulnerability-96173d41792">Envoy</a><a href="https://blog.envoyproxy.io/exploiting-an-envoy-heap-vulnerability-96173d41792"> heap vulnerability</a>.”&nbsp;</p>
<p>To counteract the increased vulnerability risk, we use best binary hardening security practices from our upstream OS vendors <a href="https://wiki.ubuntu.com/Security/Features">Ubuntu</a> and <a href="https://wiki.debian.org/Hardening">Debian</a>. We defined a special hardened build profile for all edge-exposed binaries. It includes ASLR, stack protectors, and symbol table hardening:&nbsp;</p>

</div>
<div>




    <pre><code>build:hardened --force_pic
build:hardened <span>--copt</span>=-fstack-clash-protection
build:hardened <span>--copt</span>=-fstack-protector-strong
build:hardened <span>--linkopt</span>=-Wl,-z,relro,-z,now
</code></pre>
</div>
<div>
<p>Forking web-servers, like Nginx, in most environments <a href="http://hmarco.org/renewssp/data/Preventing_brute_force_attacks_against_stack_canary_protection_on_networking_servers-Paper.pdf">have issues with stack protector</a>. Since master and worker processes share the same stack canary, and on canary verification failure worker process is killed, the canary can be brute-forced bit-by-bit in about 1000 tries. Envoy, which uses threads as a concurrency primitive, is not affected by this attack.</p>
<p>We also want to harden third-party dependencies where we can. We use <a href="https://boringssl.googlesource.com/boringssl/+/master/crypto/fipsmodule/FIPS.md">BoringSSL in FIPS mode</a>, which includes startup self-tests and integrity checking of the binary. We’re also considering running ASAN-enabled binaries on some of our edge canary servers.</p>
<h3>Features</h3>
<p>Here comes the most opinionated part of the post, brace yourself.</p>
<p>Nginx began as a web server specialized on serving static files with minimal resource consumption. Its functionality is top of the line there: static serving, caching (including thundering herd protection), and range caching.</p>
<p>On the proxying side, though, Nginx lacks features needed for modern infrastructures. There’s no HTTP/2 to backends. gRPC proxying is available but without connection multiplexing. There’s no support for gRPC transcoding. On top of that, Nginx’s “open-core” model restricts features that can go into an open source version of the proxy. As a result, some of the critical features like statistics are not available in the “community” version.<br>
</p>
<p>Envoy, by contrast, has evolved as an ingress/egress proxy, used frequently for gRPC-heavy environments. Its web-serving functionality is rudimentary: <a href="https://github.com/envoyproxy/envoy/issues/378">no file serving</a>, still <a href="https://www.envoyproxy.io/docs/envoy/latest/api-v3/extensions/filters/http/cache/v3alpha/cache.proto.html">work-in-progress caching</a>, neither <a href="https://github.com/envoyproxy/envoy/issues/4429">brotli</a> nor pre-compression. For these use cases we still have a small fallback Nginx setup that Envoy uses as an upstream cluster.<br>
</p>
<p>When HTTP cache in Envoy becomes production-ready, we could move most of static-serving use cases to it, using S3 instead of filesystem for long-term storage. To read more about eCache design, see “<a href="https://docs.google.com/document/d/1WPuim_GzhfdsnIj_tf-fIeutK0jO4aVQfVrLJFoLN3g/view#heading=h.wjxw6fq7wefi">eCache:</a><a href="https://docs.google.com/document/d/1WPuim_GzhfdsnIj_tf-fIeutK0jO4aVQfVrLJFoLN3g/view#heading=h.wjxw6fq7wefi"> a multi-backend HTTP cache</a><a href="https://docs.google.com/document/d/1WPuim_GzhfdsnIj_tf-fIeutK0jO4aVQfVrLJFoLN3g/view#heading=h.wjxw6fq7wefi"> for Envoy</a>.”&nbsp;</p>
<p>Envoy also has native support for many gRPC-related capabilities:<br>
</p>
<ul>
<li><b>gRPC proxying.</b> This is a basic capability that allowed us to use gRPC end-to-end for our applications (e.g. Dropbox desktop client.)</li>
<li><b>HTTP/2 to backends. </b>This feature allows us to greatly reduce the number of TCP connections between our traffic tiers, reducing memory consumption and keepalive traffic.</li>
<li><a href="https://www.envoyproxy.io/docs/envoy/latest/configuration/http/http_filters/grpc_http1_bridge_filter">gRPC → HTTP bridge</a> (+ <a href="https://www.envoyproxy.io/docs/envoy/latest/configuration/http/http_filters/grpc_http1_reverse_bridge_filter">reverse</a>.) &nbsp;These allowed us to expose legacy HTTP/1 applications using a modern gRPC stack.</li>
<li><a href="https://www.envoyproxy.io/docs/envoy/latest/configuration/http/http_filters/grpc_web_filter">gRPC-WEB</a>. This feature allowed us to use gRPC end-to-end even in the environments where middleboxes (firewalls, IDS, etc) don’t yet support HTTP/2.</li>
<li><a href="https://www.envoyproxy.io/docs/envoy/latest/configuration/http/http_filters/grpc_json_transcoder_filter">gRPC JSON transcoder</a>. This enables us to transcode all inbound traffic, including <a href="https://www.dropbox.com/developers/documentation/http/overview">Dropbox public APIs</a>, from REST into gRPC.</li>
</ul>
<p>In addition, Envoy can also be used as an outbound proxy. We used it to unify a couple of other use cases:<br>
</p>
<ul>
<li>Egress proxy: since Envoy<a href="https://github.com/envoyproxy/envoy/issues/1451"> added support for the HTTP CONNECT method</a>, it can be used as a drop-in replacement for Squid proxies. We’ve begun to replace our outbound Squid installations with Envoy. This not only greatly improves visibility, but also reduces operational toil by unifying the stack with a common dataplane and observability (no more parsing logs for stats.)</li>
<li>Third-party software service discovery: we are relying on the <a href="https://dropbox.tech/infrastructure/courier-dropbox-migration-to-grpc">Courier gRPC libraries</a> in our software instead of using Envoy as a service mesh. But we do use Envoy in one-off cases where we need to connect an open source service with our service discovery with minimal effort. For example, Envoy is used as a service discovery sidecar in our analytics stack. Hadoop can dynamically discover its name and journal nodes. <a href="https://github.com/apache/incubator-superset">Superset</a> can discover airflow, presto, and hive backends. <a href="https://grafana.com/">Grafana</a> can discover its MySQL database.</li>
</ul>
<h2>Community</h2>
<p>Nginx development is quite centralized. Most of its development happens behind closed doors. There’s some external activity on the <a href="http://mailman.nginx.org/pipermail/nginx-devel/">nginx-devel</a> mailing list, and there are occasional development-related discussions on the <a href="https://trac.nginx.org/nginx">official bug tracker</a>.</p>
<p>There is an <span>#nginx</span> channel on FreeNode. Feel free to join it for more interactive <a href="https://www.nginx.com/resources/wiki/community/irc/">community</a> conversations.</p>
<p>Envoy development is open and decentralized: coordinated through GitHub issues/pull requests, <a href="https://groups.google.com/g/envoy-dev">mailing list</a>, and <a href="https://goo.gl/5Cergb">community meetings</a>.</p>
<p>There is also quite a bit of community activity on Slack. You can get your invite <a href="https://envoyslack.cncf.io/">here</a>.</p>
<p>It’s hard to quantify the development styles and engineering community, so let’s look at a specific example of HTTP/3 development.</p>
<p>Nginx <a href="https://hg.nginx.org/nginx-quic/">QUIC and HTTP/3 implementation</a> was <a href="https://www.nginx.com/blog/introducing-technology-preview-nginx-support-for-quic-http-3/">recently presented by F5</a>. The code is clean, with zero external dependencies. But the development process itself was rather opaque. Half a year before that, <a href="https://blog.cloudflare.com/experiment-with-http-3-using-nginx-and-quiche/">Cloudflare came up with their own HTTP/3 implementation for </a><a href="https://blog.cloudflare.com/experiment-with-http-3-using-nginx-and-quiche/">Nginx</a>. As a result, the community now has two separate experimental versions of HTTP/3 for Nginx.</p>
<p>In Envoy’s case, HTTP/3 implementation is also a work in progress, based on chromium’s "<a href="https://docs.google.com/document/d/19qcrwAa8hVYZv2r8zZ7SgkylivAQNQ7E3loMJ-vk9_k/edit">quiche</a>" (QUIC, HTTP, Etc.) library. The project’s status is tracked in the <a href="https://github.com/envoyproxy/envoy/issues/2557">GitHub issue</a>. The <a href="https://docs.google.com/document/d/1dEo19y-trABuW2x6-T564LmK7Ld-BPXZOlnR4df9KVU/edit#heading=h.w2fjl4fs3sex">de</a><a href="https://docs.google.com/document/d/1dEo19y-trABuW2x6-T564LmK7Ld-BPXZOlnR4df9KVU/edit#heading=h.w2fjl4fs3sex">sign doc</a> was publicly available way before patches were completed. Remaining work that would benefit from community involvement is tagged with “<a href="https://github.com/envoyproxy/envoy/issues?q=is%3Aopen+label%3Aarea%2Fquic+label%3A%22help+wanted%22">help</a><a href="https://github.com/envoyproxy/envoy/issues?q=is%3Aopen+label%3Aarea%2Fquic+label%3A%22help+wanted%22"> wanted</a>.”</p>
<p>As you can see, the latter structure is much more transparent and greatly encourages collaboration. For us, this means that we managed to upstream lots of small to medium changes to Envoy–everything from <a href="https://github.com/envoyproxy/envoy/pull/10286/files">operational improvements</a> and <a href="https://github.com/envoyproxy/envoy/pull/9556">performance optimizations</a> to <a href="https://github.com/envoyproxy/envoy/pull/10673">new gRPC transcoding features</a> and <a href="https://github.com/envoyproxy/envoy/pull/11006">load</a><a href="https://github.com/envoyproxy/envoy/pull/11006"> </a><a href="https://github.com/envoyproxy/envoy/pull/11006">balancing changes</a>.</p>

</div>
<div>
<p id="-current-state-of-our-migration">
    <h2 data-index="4"> Current state of our migration</h2>
</p>
</div>
<div>
<p>We’ve been running Nginx and Envoy side-by-side for over half a year and gradually switching traffic from one to another with DNS. By now we have migrated a wide variety of workloads to Envoy:</p>
<ul>
<li><b>Ingress high-throughput services.</b> All file data from Dropbox desktop client is served via end-to-end gRPC through Envoy. By switching to Envoy we’ve also slightly improved users’ performance, due to better connection reuse from the edge.</li>
<li><b>Ingress high-RPS services.</b> This is all file metadata for Dropbox desktop client. We get the same benefits of end-to-end gRPC, plus the removal of the connection pool, which means we are not bounded by one request per connection at a time.</li>
<li><b>Notification and telemetry services.</b> Here we handle all real-time notifications, so these servers have millions of HTTP connections (one for each active client.) Notification services can now be implemented via streaming gRPC instead of an expensive long-poll method.</li>
<li><b>Mixed high-throughput/high-RPS services.</b> API traffic (both metadata and data itself.) This allows us to start thinking about public gRPC APIs. We may even switch to transcoding our existing REST-based APIs right on the Edge.</li>
<li><b>Egress high-throughput proxies.</b> In our case, the Dropbox to AWS communication, mostly S3. This would allow us to eventually remove all Squid proxies from our production network, leaving us with a single L4/L7 data plane.&nbsp;</li>
</ul>
<p>One of the last things to migrate would be www.dropbox.com itself. After this migration, we can start decommissioning our edge Nginx deployments. An epoch would pass.</p>

</div>

<div>
<p>Migration was not flawless, of course. But it didn’t lead to any notable outages. The hardest part of the migration was our API services. A lot of different devices communicate with Dropbox over our public API—everything from <span>curl-</span>/<span>wget</span>-powered shell scripts and embedded devices with custom HTTP/1.0 stacks, to every possible HTTP library out there. Nginx is a battle-tested de-facto industry standard. Understandably, most of the libraries implicitly depend on some of its behaviors. Along with a number of inconsistencies between Nginx and Envoy behaviors on which our api users depend, there were a number of bugs in Envoy and its libraries. All of them were quickly resolved and upstreamed by us with the community help.</p>
<p>Here is just a gist of some the “unusual”/non-RFC behaviors:</p>
<ul>
<li><a href="https://github.com/envoyproxy/envoy/pull/7621"><b>Merge slashes in URLs</b></a>. URL normalization and slash merging is a very common feature for web-proxies. Nginx <a href="http://nginx.org/en/docs/http/ngx_http_core_module.html#merge_slashes">enables slash normalization and slash merging by default</a> but Envoy did not support the latter. We submitted a patch upstream that add that functionality and allows users to opt-in by using the <a href="https://www.envoyproxy.io/docs/envoy/latest/api-v2/config/filter/network/http_connection_manager/v2/http_connection_manager.proto#envoy-api-field-config-filter-network-http-connection-manager-v2-httpconnectionmanager-merge-slashes"><span>merge_slashes</span></a> option.</li>
<li><a href="https://github.com/envoyproxy/envoy/pull/10960"><b>Ports in virtual host names</b></a>. Nginx allows receiving <span>Host</span> header in both forms: either <span>example.com</span> or <span>example.com:port</span>. We had a couple of API users that used to rely on this behavior. First we worked around this by duplicating our vhosts in our configuration (with and without port) but later added an option to ignore the matching port on the Envoy side: <a href="https://www.envoyproxy.io/docs/envoy/latest/api-v3/extensions/filters/network/http_connection_manager/v3/http_connection_manager.proto#envoy-v3-api-field-extensions-filters-network-http-connection-manager-v3-httpconnectionmanager-strip-matching-host-port"><span>strip_matching_host_port</span></a>.</li>
<li><a href="https://github.com/envoyproxy/envoy/issues/10041"><b>Transfer encoding case sensitivity</b></a>. A tiny subset API client for some unknown reason used <span>Transfer-Encoding: Chunked</span> (note the capital “C”) header. This is technically valid, since RFC7230 states that <span>Transfer-Encoding</span>/<span>TE</span> headers are case insensitive. The fix was trivial and submitted to the upstream Envoy.</li>
<li><a href="https://github.com/envoyproxy/envoy/issues/11398"><b>Request that have both </b></a><a href="https://github.com/envoyproxy/envoy/issues/11398"><b><span>Content-Length</span></b></a><a href="https://github.com/envoyproxy/envoy/issues/11398"><b> and </b></a><span><a href="https://github.com/envoyproxy/envoy/issues/11398"><b>Transfer-Encoding: c</b></a><a href="https://github.com/envoyproxy/envoy/issues/11398"><b>hunked</b></a></span>. Requests like that used to work with Nginx, but were broken by Envoy migration. <a href="https://tools.ietf.org/html/rfc7230#section-3.3.3">RFC7230 is a bit tricky there</a>, but general idea is web-servers should error these requests because they are likely “smuggled.” On the other hand, next sentence indicates that proxies should just remove the <span>Content-Length</span> header and forward the request. We’ve <a href="https://github.com/nodejs/http-parser/issues/517">extended http-parse to allow library users to opt-in into supporting such requests</a> and currently working on adding the support to to Envoy itself.</li>
</ul>
<p>It’s also worth mentioning some common configuration issues we’ve encountered:</p>
<ul>
<li><b>Circuit-breaking misconfiguration.</b> In our experience, if you are running Envoy as an inbound proxy, especially in a mixed HTTP/1&amp;HTTP/2 environment, improperly set up circuit breakers can cause unexpected downtimes during traffic spikes or backend outages. Consider relaxing them if you are not using Envoy as a mesh proxy. It’s worth mentioning that by default, circuit-breaking limits in Envoy are pretty tight — be careful there!</li>
<li><b>Buffering.</b> Nginx allows request buffering on disk. This is especially useful in environments where you have legacy HTTP/1.0 backends that don’t understand <span>chunked</span> transfer encoding. Nginx could convert these into requests with <span>Content-Length</span> by buffering them on disk. Envoy has a Buffer filter, but without the ability to store data on disk we are restricted on how much we can buffer in memory.</li>
</ul>
<p>If you’re considering using Envoy as your Edge proxy, you would benefit from reading “<a href="https://www.envoyproxy.io/docs/envoy/latest/configuration/best_practices/edge">Configuring</a><a href="https://www.envoyproxy.io/docs/envoy/latest/configuration/best_practices/edge"> </a><a href="https://www.envoyproxy.io/docs/envoy/latest/configuration/best_practices/edge">Envoy</a><a href="https://www.envoyproxy.io/docs/envoy/latest/configuration/best_practices/edge"> as an edge proxy</a>.” &nbsp;It does have security and resource limits that you would want to have on the most exposed part of your infrastructure.</p>

</div>

<div>
<ul>
<li><a href="https://tools.ietf.org/html/draft-ietf-quic-http">HTTP/3</a> is getting closer for the prime time. Support for it was added to the most popular browsers (for now, <a href="https://caniuse.com/#feat=http3">gated by a flags or command-line options</a>). Envoy support for it is also experimentally available. After we upgrade the <a href="http://vger.kernel.org/lpc_net2018_talks/willemdebruijn-lpc2018-udpgso-paper-DRAFT-1.pdf">Linux kernel to support UDP acceleration</a>, we will experiment with it on our Edge.</li>
<li>Internal xDS-based load balancer and outlier detection. Currently, we are looking at using the combination of <a href="https://www.envoyproxy.io/docs/envoy/latest/api-v3/service/load_stats/v3/lrs.proto">Load Reporting service</a><a href="https://www.envoyproxy.io/docs/envoy/latest/api-v3/service/load_stats/v3/lrs.proto"> </a><a href="https://www.envoyproxy.io/docs/envoy/latest/api-v3/service/load_stats/v3/lrs.proto">(LRS)</a> and <a href="https://www.envoyproxy.io/docs/envoy/latest/intro/arch_overview/upstream/service_discovery#arch-overview-service-discovery-types-eds">Endpoint discovery service</a><a href="https://www.envoyproxy.io/docs/envoy/latest/intro/arch_overview/upstream/service_discovery#arch-overview-service-discovery-types-eds"> </a><a href="https://www.envoyproxy.io/docs/envoy/latest/intro/arch_overview/upstream/service_discovery#arch-overview-service-discovery-types-eds">(EDS)</a> as building blocks for creating a common look-aside, load-aware loadbalancer for both Envoy and gRPC.</li>
<li>WASM-based Envoy extensions. When Golang proxy-wasm SDK is available we can start writing Envoy extensions in Go which will give us access to a wide variety of internal Golang libs.</li>
<li>Replacement for Bandaid. Unifying all Dropbox proxy layers under a single data-plane sounds very compelling. For that to happen we’ll need to migrate a lot of Bandaid features (especially, <a href="https://dropbox.tech/infrastructure/enhancing-bandaid-load-balancing-at-dropbox-by-leveraging-real-time-backend-server-load-information">around loadbalancing</a>) to Envoy. This is a long way but it’s our current plan.</li>
<li>Envoy<a href="https://envoy-mobile.github.io/"> mobile</a>. Eventually, we want to look into using Envoy in our mobile apps. It is very compelling from Traffic perspective to support a single stack with unified monitoring and modern capabilities (HTTP/3, gRPC, TLS1.3, etc) across all mobile platforms.</li>
</ul>

</div>

<div>
<p>This migration was truly a team effort. Traffic and Runtime teams were spearheading it but other teams heavily contributed: Agata Cieplik, Jeffrey Gensler, Konstantin Belyalov, Louis Opter, Naphat Sanguansin, Nikita V. Shirokov, Utsav Shah, Yi-Shu Tai, and of course the awesome Envoy community that helped us throughout that journey.</p>
<p>We also want to explicitly acknowledge the tech lead of the Runtime team <b>Ruslan Nigmatullin </b>whose actions as the Envoy evangelist, the author of the Envoy MVP, and the main driver from the software engineering side enabled this project to happen.</p>

</div>

<p>If you’ve read this far, there’s a good chance that you actually enjoy digging deep into webservers/proxies and may enjoy working on the Dropbox Traffic team! Dropbox has a globally distributed Edge network, terabits of traffic, and millions of requests per second. All of it is managed by a <a href="https://www.dropbox.com/jobs/listing/2034032">small team in Mountain View, CA</a>.</p>

    
</div>

        </div></div></div>
    </div>
    <footer>
        <div>created by <a href="https://buildstarted.com">buildstarted</a> &copy; 2020 <a href="/about">about</a></div>
        <div>Share this page on social media: copy and paste this url https://linksfor.dev/</div>
        <div>If you prefer RSS: <a href="https://linksfor.dev/feed.xml">https://linksfor.dev/feed.xml</a></div>
        <div>Customer satisfaction guaranteed to be optional.</div>
    </footer>
    
    <script async defer>
        _dna = window._dna || {};
        _dna.siteId = "linksfor.devs";
        _dna.outlink = true;

        (function() {
            let dna = document.createElement('script');
            dna.type = 'text/javascript';
            dna.async = true;
            dna.src = '//dna.buildstarted.com/t.js';
            let s = document.getElementsByTagName('script')[0];
            s.parentNode.insertBefore(dna, s);
        })();
    </script>
    <noscript><img src="//dna.buildstarted.com/g?siteId=linksfor.devs"/></noscript>
</body>
</html>