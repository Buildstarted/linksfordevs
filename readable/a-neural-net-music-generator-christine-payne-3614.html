<!DOCTYPE html>
<html lang="en">
<head>
    <title>
Clara: A Neural Net Music Generator - linksfor.dev(s)    </title>
    <meta charset="utf-8">
    <link rel="alternate" type="application/rss+xml" title="Linksfor.dev(s) feed" href="https://linksfor.dev/feed.rss" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta name="google" value="notranslate">
    <link rel="stylesheet" type="text/css" href="/style.css" />
    <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon">
    <link rel="icon" href="/favicon.ico" type="image/x-icon">
    <meta property="og:title" content="Clara: A Neural Net Music Generator - linksfor.dev(s)"/>
    <meta property="article:author" content="By mcleavey"/>
    <meta property="og:description" content="Project Overview Clara is an LSTM that composes piano music and chamber music. It has some parallels to Google&#x2019;s Magenta project, although it&#x2019;s an entirely separate project, and uses PyTorch, MIT&#x2019;s music21, and the FastAI library. I use a 62 note range (instead of the full 88-key piano), and I allow any number of notes [&#x2026;]"/>
    <meta property="og:type" content="website"/>
    <meta property="og:url" content="http://christinemcleavey.com/clara-a-neural-net-music-generator/"/>

<meta property="og:site_name" content="linksfor.dev(s)" />
</head>
<body>
	<div class="devring" style="background: #222">
		<div class="grid">
			<div style="display: grid; grid-template-columns: .5fr 1fr 1fr 1fr; text-align: center;">
				<span class="devring-title"><a href="https://devring.club/">devring.club</a></span>
				<a href="https://devring.club/sites/1/prev" class="devring-previous">Previous</a>
				<a href="https://devring.club/random" class="devring-random">Random</a>
				<a href="https://devring.club/sites/1/next" class="devring-next">Next</a>
			</div>
		</div>
	</div>
    <div class="grid">
        <h1>
<a href="/" style="color:inherit">linksfor.dev(s)</a>
        </h1>
        <title>linksfor.dev(s) - Clara: A Neural Net Music Generator</title>
<div class="readable">
        <h1>Clara: A Neural Net Music Generator</h1>
            <div>by By mcleavey</div>
            <div>Reading time: 9-11 minutes</div>
        <div>Posted here: 27 Feb 2019</div>
        <p><a href="http://christinemcleavey.com/clara-a-neural-net-music-generator/">http://christinemcleavey.com/clara-a-neural-net-music-generator/</a></p>
        <hr/>
<div id="readability-page-1" class="page"><div itemprop="mainEntityOfPage">
	<h2>Project Overview</h2>
<p>Clara is an LSTM that composes piano music and chamber music. It has some parallels to Google’s Magenta project, although it’s an entirely separate project, and uses PyTorch, MIT’s music21, and the FastAI library. I use a 62 note range (instead of the full 88-key piano), and I allow any number of notes to play at each musical time step (in contrast with many generation models which insist on having always exactly 4 notes at a time, or in having much smaller note ranges). I trained my models using midi samples from Classical Archives, although the code should work for any piano midi files, as well as for many chamber music files.</p>
<p>The code is available <a href="https://github.com/mcleavey/musical-neural-net">here</a>. In particular, take a look at <em>BasicIntro.ipynb</em>, a Jupyter Notebook where you can create your own musical generations with my pretrained model.</p>
<p>A more detailed write-up of this project is available <a href="http://www.christinemcleavey.com/files/clara-musical-lstm.pdf">here.</a> Try guessing which samples are human and which are AI <a href="http://christinemcleavey.com/human-or-ai/">here.</a></p>
<h2>Midi to Text Encoding</h2>
<p>One of the most important factors in the success of the musical generations turned out to be my choice of encoding the midi files into text formats. The midi files provide the timing of when each note starts and stops (it also contains information about the instrument, the volume, and tempo markings for the piece). These note events can happen at arbitrary times, there can be any number of notes playing at once. You can read more about midi file format <a href="https://www.csie.ntu.edu.tw/~r92092/ref/midi/" target="_blank" rel="noopener">here</a>.</p>
<h4>Sample Frequency</h4>
<p>Music often has a quarter note as a basic measurement of time. It is very common to see notes played 3 times or 4 times per quarter note (triplets, or 16ths, respectively). Because of this, I sample the midi files either 12 times per quarter note (to allow for both triplets and 16th to be encoded faithfully), or 4 times per quarter note (triplets are distorted slightly, but it makes it easier for a model to learn a longer scale musical pattern). The user can also specify a different sampling frequency.</p>
<h4>Chordwise vs Notewise</h4>
<p>One option would be to ask the model to predict yes/no for each of the 88 piano keys, at every musical time step. However, since each individual key is silent for the great majority of the time, it would be very difficult for a model to learn to ever risk predicting “yes” for a note. I offer two different possible solutions for this problem. In language models, we often work at either the word-level or character-level. Similarly, for music generation, I offer either chordwise or notewise levels.</p>
<p><img src="http://christinemcleavey.com/wp-content/uploads/2018/08/music-generation-presentation.jpg" alt="" width="610" height="343" srcset="http://christinemcleavey.com/wp-content/uploads/2018/08/music-generation-presentation.jpg 960w, http://christinemcleavey.com/wp-content/uploads/2018/08/music-generation-presentation-300x169.jpg 300w, http://christinemcleavey.com/wp-content/uploads/2018/08/music-generation-presentation-768x432.jpg 768w" sizes="(max-width: 610px) 100vw, 610px"></p>
<p>For <strong>chordwise</strong>, I consider each combination of notes that are ever seen in the musical corpus to be a chord. There are usually on the order of 55,000 different chords (depending on the note range and the number of different composers). Then I treat the chord progression exactly as a language. In the same way a language model might predict “cat” when prompted with “The large dog chased the small …”, I ask the musical model to predict the next chord when prompted with a chord sequence. Note that a “chord” is used generally here – it is any combination of notes played all at one time, not necessarily a traditional musical chord (such as “C Major”).</p>
<p>For <strong>notewise</strong>, I consider each note start and stop to be a different word (so the words might be “p28” and “endp28” for the start and stop of note #28 on the piano). I then use “wait” to denote the end of a time step. In this way, the vocab size is around 180.</p>

<h4>Chamber Music</h4>
<p>The notewise representation lends itself relatively easily to adding in multiple instruments. The main difference is in encoding the midi files. Here, I mark violin notes with the prefix “v” and piano notes with the prefix “p”, but otherwise train the music generator in the same way. One problem is there are much fewer violin/piano pieces than pure piano solo pieces. I include any pieces that are a solo instrument and piano (such as cello/piano, horn/piano), and I transpose the solo instrument parts to be within the violin range. In the future I may explore ways to use the piano solo music to pretrain the network.</p>
<h4>Data Augmentation</h4>
<p>For all the encodings, I also transpose all the pieces into every possible key. The test set is kept separate: a piece in the test set cannot be a transposition of a piece in the training set.</p>
<h2>Musical Generator</h2>
<p>I used an AWD-LSTM network from the FastAI library as the generator model. In train.py, one can easily modify the hyperparameters (changing the embedding size, number of LSTM layers, amount of dropout, etc). I train the model by asking it to predict the next note or chord, given an input sequence.</p>
<p>Once the model is trained, I create generations by sampling the output prediction, and then feeding that back into the model, and asking it to predict the next step, and so forth. In this way, generated samples can be of arbitrary length. I achieved much better results by not always sampling the most likely prediction by the model. With an adjustable frequency, I sometimes select the 2nd or 3rd most likely guesses (according to the probabilities predicted by the model). In this way, the generations are more diverse, and a single prompt can produce a large variety of samples.</p>
<h3>Results</h3>
<p>Here are some samples from notewise and chordwise piano solo generations, and from notewise chamber music generation.</p>
<!--[if lt IE 9]><script>document.createElement('audio');</script><![endif]-->
	
	
<p>Here are some benefits and disadvantages I found in using the two methods:</p>
<p><strong>Chordwise representation:</strong></p>
<ul>
<li>Able to recreate 45 seconds of Mozart</li>
<li>Better long scale patterns</li>
<li>Does not generalize well beyond the training set</li>
<li>More variety in quality of generations</li>
<li>Usually most effective to sample top prediction at each timestep</li>
<li>Odd effect where it will occasionally “jump” to a neighboring key, but otherwise continue playing the passage normally</li>
</ul>
<p><strong>Notewise representation:</strong></p>
<ul>
<li>Creates musical sounding samples even when prompted with music it has never seen before</li>
<li>Handles a 12-step sample frequency well, which allows it to generate both 16th and triplets smoothly</li>
<li>Able to create small-scale rhythmic patterns that are musically coherent</li>
<li>Does not create long scale patterns well</li>
<li>Easily extends to chamber music</li>
<li>Easily handles note durations (particularly important for violin, which often holds notes much longer than piano)</li>
<li>Usually most effective to sample from the top 3 predictions at each timestep</li>
</ul>
<h2>Critic and Composer Classifier</h2>
<p>I created a critic model that attempts to classify whether a sample of music is human composed or neural net composed. “Real” samples come from the original human-composed midi files (encoded into text files). “Fake” samples are created by the generator LSTM.</p>
<p>This can be used as a way to score musical generations. (This is quite similar to a GAN, but I don’t use the scores as a target for training the generator.) In this way, I can select the best (or worst!) samples from a batch created by the generator.</p>
<p>I also created a composer model that attempts to classify which human composer created the music sample. Training samples come from the original human-composed midi files (again encoded into text file).</p>
<p>This can be used to create an Inception-like score for the models. Models score highly when each individual generation is very much like a specific human composer, and when on average there are generations that match all of the different human composers. A model would score poorly for generating only pieces like Chopin, and never generating ones like any of the other humans.</p>
<p>Alternately, it is interesting to combine these two model scores. Consider a sample that the Critic considers “real”, but which the Composer Classifier thinks don’t match any human composer: this might sound like a new musical style.</p>
<h2>Future Directions</h2>
<p>There are many possible directions for this project. In particular, I’m interested in the problem of creating pieces that have a longer scale coherency or structure. I suspect there may be a way to combine the chordwise and notewise encodings, or to otherwise to first have a model that predicts a full measure at a time, and then a second model that fleshes that measure prediction encoding into individual notes.</p>
<p>Using the Musical Inception score as a metric, I would like to do a more scientific search for the ideal hyperparameters and encoding choices, training several models in parallel.</p>
<p>Using the Composer Classifier, I’m also working on creating an audio version of Google’s DeepDream, adjusting the input music sample to look more like any of the individual human composers.</p>
<h2>Thanks</h2>
<p>I’ve had an amazing summer as part of OpenAI’s Scholars Program. I want to say a special thanks to my brilliant mentor Karthik Narasimhan (who is starting this fall as an assistant professor at Princeton!), to the other scholars (I’m proud to be part of such a talented, kind, and inspiring group!), to the OpenAI team as a whole, and in particular to Larissa Schiavo who organized the scholars program, and who did a great job keeping us on track and making us feel welcome.</p>
</div></div></div>
    </div>
    <footer>
        <div>created by buildstarted &copy; 2020 <a href="/about">about</a></div>
        <div>Share this page on social media: copy and paste this url https://linksfor.dev/</div>
        <div>If you prefer RSS: <a href="https://linksfor.dev/feed.xml">https://linksfor.dev/feed.xml</a></div>
		<div>Customer satisfaction guaranteed to be optional.</div>
    </footer>
    
    <script async defer>
        _dna = window._dna || {};
        _dna.siteId = "linksfor.devs";
        _dna.outlink = true;

        (function () {
            let dna = document.createElement('script');
            dna.type = 'text/javascript';
            dna.async = true;
            dna.src = '//dna.buildstarted.com/t.js';
            let s = document.getElementsByTagName('script')[0];
            s.parentNode.insertBefore(dna, s);
        })();
    </script>
    <noscript><img src="//dna.buildstarted.com/g?siteId=linksfor.devs"/></noscript>
</body>
</html>