<!DOCTYPE html>
<html lang="en">
<head>
    <title>
How GPT3 Works - Visualizations and Animations - linksfor.dev(s)    </title>
    <meta charset="utf-8">
    <link rel="alternate" type="application/rss+xml" title="Linksfor.dev(s) feed" href="https://linksfor.dev/feed.rss" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta name="google" value="notranslate">
    <link rel="stylesheet" type="text/css" href="/style.css" />
    <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon">
    <link rel="icon" href="/favicon.ico" type="image/x-icon">
    <meta property="og:title" content="How GPT3 Works - Visualizations and Animations - linksfor.dev(s)"/>
    <meta property="article:author" content="Jay Alammar"/>
    <meta property="og:description" content="(Live thread, will be updated with new visuals and polish over the next few days)&#xA;&#xA;A trained language model generates text.&#xA;&#xA;We can optionally pass it some text as input, which influences its output.&#xA;&#xA;The output is generated from what the model &#x201C;learned&#x201D; during its training period where it scanned vast amounts of text.&#xA;&#xA;&#xA;  &#xA;  &#xA;&#xA;&#xA;&#xA;"/>
    <meta property="og:type" content="website"/>
    <meta property="og:url" content="https://jalammar.github.io/how-gpt3-works-visualizations-animations/"/>

<meta property="og:site_name" content="linksfor.dev(s)" />
</head>
<body>
    <div class="devring" style="background: #222">
        <div style="text-align:center">Explore other dev related sites in this ring. If you would like to join this ring <a href="https://devring.club">click here</a>.</div>
        <div class="grid">
            <div style="display: grid; grid-template-columns: .5fr 1fr 1fr 1fr; text-align: center;">
                <span class="devring-title"><a href="https://devring.club/">devring.club</a></span>
                <a href="https://devring.club/sites/1/prev" class="devring-previous">Previous</a>
                <a href="https://devring.club/random" class="devring-random">Random</a>
                <a href="https://devring.club/sites/1/next" class="devring-next">Next</a>
            </div>
        </div>
    </div>
    <div class="grid">
        <h1>
<a href="/" style="color:inherit">linksfor.dev(s)</a>
        </h1>
        <title>linksfor.dev(s) - How GPT3 Works - Visualizations and Animations</title>
<div class="readable">
        <h1>How GPT3 Works - Visualizations and Animations</h1>
            <div>by Jay Alammar</div>
            <div>Reading time: 4-5 minutes</div>
        <div>Posted here: 28 Jul 2020</div>
        <p><a href="https://jalammar.github.io/how-gpt3-works-visualizations-animations/">https://jalammar.github.io/how-gpt3-works-visualizations-animations/</a></p>
        <hr/>
<div id="readability-page-1" class="page"><div>
    <p>(Live thread, will be updated with new visuals and polish over the next few days)</p>

<p>A trained language model generates text.</p>

<p>We can optionally pass it some text as input, which influences its output.</p>

<p>The output is generated from what the model “learned” during its training period where it scanned vast amounts of text.</p>

<p><img src="https://jalammar.github.io/images/gpt3/01-gpt3-language-model-overview.gif">
  <br>

</p>

<!--more-->

<p>Training is the process of exposing the model to lots of text. It has been done once and complete. All the experiments you see now are from that one trained model. It was estimated to cost 355 GPU years and cost $4.6m.</p>

<p><img src="https://jalammar.github.io/images/gpt3/02-gpt3-training-language-model.gif">
  <br>

</p>

<p>The dataset of 300 billion tokens of text is used to generate training examples for the model. For example, these are three training examples generated from the one sentence at the top.</p>

<p>You can see how you can slide a window across all the text and make lots of examples.</p>

<p><img src="https://jalammar.github.io/images/gpt3/gpt3-training-examples-sliding-window.png">
  <br>

</p>

<p>The model is presented with an example. We only show it the features and ask it to predict the next word.</p>

<p>The model’s prediction will be wrong. We calculate the error in its prediction and update the model so next time it makes a better prediction.</p>

<p>Repeat millions of times</p>

<p><img src="https://jalammar.github.io/images/gpt3/03-gpt3-training-step-back-prop.gif">
  <br>

</p>

<p>Now let’s look at these same steps with a bit more detail.</p>

<p>GPT3 actually generates output one token at a time (let’s assume a token is a word for now).</p>

<p><img src="https://jalammar.github.io/images/gpt3/04-gpt3-generate-tokens-output.gif">
  <br>

</p>

<p>Please note: This is a description of how GPT-3 works and not a discussion of what is novel about it (which is mainly the ridiculously large scale). The architecture is a transformer decoder model based on this paper https://arxiv.org/pdf/1801.10198.pdf</p>

<p>GPT3 is MASSIVE. It encodes what it learns from training in 175 billion numbers (called parameters). These numbers are used to calculate which token to generate at each run.</p>

<p>The untrained model starts with random parameters. Training finds values that lead to better predictions.</p>

<p><img src="https://jalammar.github.io/images/gpt3/gpt3-parameters-weights.png">
  <br>

</p>

<p>These numbers are part of hundreds of matrices inside the model. Prediction is mostly a lot of matrix multiplication.</p>

<p>In my <a href="https://youtube.com/watch?v=mSTCzNgDJy4">Intro to AI on YouTube</a>, I showed a simple ML model with one parameter. A good start to unpack this 175B monstrosity.</p>

<p>To shed light on how these parameters are distributed and used, we’ll need to open the model and look inside.</p>

<p>GPT3 is 2048 tokens wide. That is its “context window”. That means it has 2048 tracks along which tokens are processed.</p>

<p><img src="https://jalammar.github.io/images/gpt3/05-gpt3-generate-output-context-window.gif">
  <br>

</p>

<p>Let’s follow the purple track. How does a system process the word “robotics” and produce “A”?</p>

<p>High-level steps:</p>

<ol>
  <li>Convert the word to <a href="https://jalammar.github.io/illustrated-word2vec/">a vector (list of numbers) representing the word</a></li>
  <li>Compute prediction</li>
  <li>Convert resulting vector to word</li>
</ol>

<p><img src="https://jalammar.github.io/images/gpt3/06-gpt3-embedding.gif">
  <br>

</p>

<p>The important calculations of the GPT3 occur inside its stack of 96 transformer decoder layers.</p>

<p>See all these layers? This is the “depth” in “deep learning”.</p>

<p>Each of these layers has its own 1.8B parameter to make its calculations.</p>

<p><img src="https://jalammar.github.io/images/gpt3/07-gpt3-processing-transformer-blocks.gif">
  <br>

</p>

<p>You can see a detailed explanation of everything inside the decoder in my blog post <a href="https://jalammar.github.io/illustrated-gpt2/">The Illustrated GPT2</a>.</p>

<p>The difference with GPT3 is the alternating dense and <a href="https://arxiv.org/pdf/1904.10509.pdf">sparse self-attention layers</a>.</p>

<p>This is an X-ray of an input and response (“Okay human”) within GPT3. Notice how every token flows through the entire layer stack. We don’t care about the output of the first words. When the input is done, we start caring about the output. We feed every word back into the model.</p>

<p><img src="https://jalammar.github.io/images/gpt3/08-gpt3-tokens-transformer-blocks.gif">
  <br>

</p>

<p>In the React code generation example (https://twitter.com/sharifshameem/status/1284421499915403264), the description would be the input prompt (in green), in addition to a couple of examples of description=&gt;code, I believe. And the react code would be generated like the pink tokens here token after token.</p>

<p>My assumption is that the priming examples and the description are appended as input, with specific tokens separating examples and the results. Then fed into the model.</p>

<p><img src="https://jalammar.github.io/images/gpt3/09-gpt3-generating-react-code-example.gif">
  <br>

</p>

<p>It’s impressive that this works like this. Because you just wait until fine-tuning is rolled out for the GPT3. The possibilities will be even more amazing.</p>

<p>Fine-tuning actually updates the model’s weights to make the model better at a certain task.</p>

<p><img src="https://jalammar.github.io/images/gpt3/10-gpt3-fine-tuning.gif">
  <br>

</p>

  </div></div></div>
    </div>
    <footer>
        <div>created by buildstarted &copy; 2020 <a href="/about">about</a></div>
        <div>Share this page on social media: copy and paste this url https://linksfor.dev/</div>
        <div>If you prefer RSS: <a href="https://linksfor.dev/feed.xml">https://linksfor.dev/feed.xml</a></div>
        <div>Customer satisfaction guaranteed to be optional.</div>
    </footer>
    
    <script async defer>
        _dna = window._dna || {};
        _dna.siteId = "linksfor.devs";
        _dna.outlink = true;

        (function() {
            let dna = document.createElement('script');
            dna.type = 'text/javascript';
            dna.async = true;
            dna.src = '//dna.buildstarted.com/t.js';
            let s = document.getElementsByTagName('script')[0];
            s.parentNode.insertBefore(dna, s);
        })();
    </script>
    <noscript><img src="//dna.buildstarted.com/g?siteId=linksfor.devs"/></noscript>
</body>
</html>