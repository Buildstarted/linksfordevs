<!DOCTYPE html>
<html lang="en">
<head>
    <title>
Hardware Intrinsics in .NET Core -
linksfor.dev(s)
    </title>
    <link rel="alternate" type="application/rss+xml" title="Linksfor.dev(s) feed" href="https://linksfor.dev/feed.rss" />
    <meta charset="utf-8">
    <meta name="Description" content="A curated source of links that devs might find interesting. Updated around the clock." />
    <meta name="google" value="notranslate">
    <link rel="stylesheet" type="text/css" href="/style.css" />
    <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon">
    <link rel="icon" href="/favicon.ico" type="image/x-icon">
</head>
<body>
    <div class="grid">
        
<div class="readable">
    <div id="readOverlay" class="style-ebook"><div id="readInner" class="margin-medium size-medium"><h1>Hardware Intrinsics in .NET Core</h1><div><div class="entry-content col-12 sharepostcontent"><div class="row justify-content-center"><div class="col-md-4"><div><img src="https://secure.gravatar.com/avatar/45d46131d8cd64c7fb83cba998b67c6f?s=58&amp;d=mm&amp;r=g" width="58" height="58" alt="Avatar" class="avatar avatar-58 wp-user-avatar wp-user-avatar-58 photo avatar-default"><p>Tanner</p></div></div></div><p>September 3rd, 2019</p><p class="code-line" data-line="2">Several years ago,&nbsp;<a title="https://devblogs.microsoft.com/dotnet/the-jit-finally-proposed-jit-and-simd-are-getting-married/" href="https://devblogs.microsoft.com/dotnet/the-jit-finally-proposed-jit-and-simd-are-getting-married/">we decided that it was time to support SIMD code in .NET</a>. We introduced the System.Numerics namespace with&nbsp;<code>Vector2</code>,&nbsp;<code>Vector3</code>,&nbsp;<code>Vector4</code>,&nbsp;<code>Vector&lt;T&gt;</code>, and related types. These types expose a general-purpose API for creating, accessing, and operating on them using hardware vector instructions (when available). They also provide a software fallback for when the hardware does not provide the appropriate instructions. This enabled a number of common algorithms to be vectorized, often with only minor refactorings. However, the generality of this approach made it difficult for programs to take full advantage of all vector instructions available on modern hardware. Additionally, modern hardware often exposes a number of specialized non-vector instructions that can dramatically improve performance. In this blog post, I’m exploring how we’ve addressed this limitation in .NET Core 3.0.</p><h3 id="what-are-hardware-intrinsics" class="code-line" data-line="4">What are hardware intrinsics?</h3><p class="code-line" data-line="6">In .NET Core 3.0, we added a new feature called&nbsp;<em>hardware intrinsics</em>. Hardware intrinsics provide access to many of these hardware specific instructions that can’t easily be exposed in a more general-purpose mechanism. They differ from the existing SIMD intrinsics in that they are not general-purpose (the new hardware intrinsics are not cross-platform and the architecture does not provide a software fallback) and instead directly expose platform and hardware specific functionality to the .NET developer. The existing SIMD intrinsics, in comparison, are cross-platform, provide a software fallback, and are slightly abstracted from the underlying hardware. That abstraction can come at a cost and prevent certain functionality from being exposed (when said functionality does not exist or is not easily emulated on all target hardware).</p><p class="code-line" data-line="8">The new intrinsics and supporting types are exposed under the&nbsp;<code>System.Runtime.Intrinsics</code>&nbsp;namespace. For .NET Core 3.0 there currently exists one namespace:&nbsp;<code>System.Runtime.Intrinsics.X86</code>. We are working on exposing hardware intrinsics for other platforms, such as&nbsp;<code>System.Runtime.Intrinsics.Arm</code>.</p><p class="code-line" data-line="10">Under the platform specific namespaces, intrinsics are grouped into classes which represent logical hardware instruction groups (frequently referred to as&nbsp;<em>Instruction Set Architectures</em>&nbsp;or ISAs). Each class then exposes an&nbsp;<code>IsSupported</code>&nbsp;property that indicates whether the hardware you are currently executing on supports that instruction set. Each class then also exposes a set of methods that map to the underlying instructions exposed by that instruction set. There is sometimes additionally a subclass that is part of the same instruction set but that may be limited to specific hardware. For example, the&nbsp;<code>Lzcnt</code>&nbsp;class provides access to the leading zero count instructions. There is then a subclass named&nbsp;<code>X64</code>&nbsp;which exposes the forms of the instruction that are only usable on 64-bit machines.</p><p class="code-line" data-line="12">Some of the classes are also hierarchical in nature. For example, if&nbsp;<code>Lzcnt.X64.IsSupported</code>&nbsp;returns true, then&nbsp;<code>Lzcnt.IsSupported</code>&nbsp;must also return true since it is an explicit subclass. Likewise, if&nbsp;<code>Sse2.IsSupported</code>&nbsp;returns true, then&nbsp;<code>Sse.IsSupported</code>&nbsp;must also return true because&nbsp;<code>Sse2</code>&nbsp;explicitly inherits from the&nbsp;<code>Sse</code>class. However, it is worth noting that just because classes have similar names does not mean they are definitely hierarchical. For example,&nbsp;<code>Bmi2</code>&nbsp;does not inherit from&nbsp;<code>Bmi1</code>&nbsp;and so the&nbsp;<code>IsSupported</code>&nbsp;checks for the two instruction sets are distinct from each other. The design philosophy of these types is to truthfully represent the ISA specification. SSE2 requires to support SSE1, so we exposed a subclass and since BMI2 doesn’t require supporting BMI1, we didn’t use inheritance.</p><p class="code-line" data-line="14">An example of the API shape described above is the following:</p><p>You can also see a more complete list by browsing the source code on&nbsp;<a title="https://source.dot.net/#System.Private.CoreLib/shared/System/Runtime/Intrinsics/X86/Sse.PlatformNotSupported.cs" href="https://source.dot.net/#System.Private.CoreLib/shared/System/Runtime/Intrinsics/X86/Sse.PlatformNotSupported.cs">source.dot.net</a>&nbsp;or&nbsp;<a title="https://github.com/dotnet/coreclr/blob/master/src/System.Private.CoreLib/shared/System/Runtime/Intrinsics/X86/Sse.cs" href="https://github.com/dotnet/coreclr/blob/master/src/System.Private.CoreLib/shared/System/Runtime/Intrinsics/X86/Sse.cs">dotnet/coreclr on GitHub</a>.</p><p class="code-line" data-line="58">The&nbsp;<code>IsSupported</code>&nbsp;checks are treated as runtime constants by the JIT (when optimizations are enabled) and so you do not need to cross-compile to support multiple different ISAs, platforms, or architectures. Instead, you just write your code using&nbsp;<code>if</code>-statements and the unused code paths (any code path which is not executed, due to the condition for the branch being&nbsp;<code>false</code>&nbsp;or an earlier branch being taken instead) are dropped from the generated code (the native assembly code generated by the JIT at runtime).</p><p class="code-line" data-line="60">It is essential that you guard usage of hardware intrinsics with the appropriate&nbsp;<code>IsSupported</code>&nbsp;check. If your code is unguarded and runs on a machine or architecture/platform that doesn’t support the intrinsic, a&nbsp;<code>PlatformNotSupportedException</code>&nbsp;is thrown by the runtime.</p><h3 id="what-benefits-do-these-provide-me" class="code-line" data-line="62">What benefits do these provide me?</h3><p class="code-line" data-line="64">Hardware Intrinsics definitely aren’t for everyone, but they can be used to boost perf in some computationally heavy workloads. Frameworks such as CoreFX or&nbsp;<a title="http://ml.net/" href="http://ml.net/">ML.NET</a>&nbsp;take advantage of these methods to help accelerate things like copying memory, searching for the index of an item in an array/string, resizing images, or working with vectors, matrices, and tensors. Manually vectorizing some code that has been identified as a bottleneck can also be easier than it seems. Vectorizing your code is really all about performing multiple operations at once, generally using Single-Instruction Multiple Data (<code>SIMD</code>) instructions.</p><p class="code-line" data-line="66">It is important to profile your code before vectorizing to ensure that the code you are optimizing is part of a hot spot (and therefore the optimization will be impactful). It is also important to profile while you are iterating on the vectorized code, as not all code will benefit from vectorization.</p><h3 id="vectorizing-a-simple-algorithm" class="code-line" data-line="68">Vectorizing a simple algorithm</h3><p class="code-line" data-line="70">Take for example an algorithm which sums all elements in an array or span. This code is a perfect candidate for vectorization because it does the same unconditional operation every iteration of the loop and those operations are fairly trivial in nature.</p><p class="code-line" data-line="72">An example of such an algorithm might look like the following:</p><p class="code-line" data-line="87">The code is simple and understandable, but it is also not particularly fast for large inputs since you are only doing a single trivial operation per loop iteration.</p><table><thead><tr><th>Method</th><th>Count</th><th>Mean</th><th>Error</th><th>StdDev</th></tr></thead><tbody><tr><td>Sum</td><td>1</td><td>2.477 ns</td><td>0.0192 ns</td><td>0.0179 ns</td></tr><tr><td>Sum</td><td>2</td><td>2.164 ns</td><td>0.0265 ns</td><td>0.0235 ns</td></tr><tr><td>Sum</td><td>4</td><td>3.224 ns</td><td>0.0302 ns</td><td>0.0267 ns</td></tr><tr><td>Sum</td><td>8</td><td>4.347 ns</td><td>0.0665 ns</td><td>0.0622 ns</td></tr><tr><td>Sum</td><td>16</td><td>8.444 ns</td><td>0.2042 ns</td><td>0.3734 ns</td></tr><tr><td>Sum</td><td>32</td><td>13.963 ns</td><td>0.2182 ns</td><td>0.2041 ns</td></tr><tr><td>Sum</td><td>64</td><td>50.374 ns</td><td>0.2955 ns</td><td>0.2620 ns</td></tr><tr><td>Sum</td><td>128</td><td>60.139 ns</td><td>0.3890 ns</td><td>0.3639 ns</td></tr><tr><td>Sum</td><td>256</td><td>106.416 ns</td><td>0.6404 ns</td><td>0.5990 ns</td></tr><tr><td>Sum</td><td>512</td><td>291.450 ns</td><td>3.5148 ns</td><td>3.2878 ns</td></tr><tr><td>Sum</td><td>1024</td><td>574.243 ns</td><td>9.5851 ns</td><td>8.4970 ns</td></tr><tr><td>Sum</td><td>2048</td><td>1,137.819 ns</td><td>5.9363 ns</td><td>5.5529 ns</td></tr><tr><td>Sum</td><td>4096</td><td>2,228.341 ns</td><td>22.8882 ns</td><td>21.4097 ns</td></tr><tr><td>Sum</td><td>8192</td><td>2,973.040 ns</td><td>14.2863 ns</td><td>12.6644 ns</td></tr><tr><td>Sum</td><td>16384</td><td>5,883.504 ns</td><td>15.9619 ns</td><td>14.9308 ns</td></tr><tr><td>Sum</td><td>32768</td><td>11,699.237 ns</td><td>104.0970 ns</td><td>97.3724 ns</td></tr></tbody></table><h4 data-line="116"></h4><h4 id="improving-the-perf-by-unrolling-the-loop" class="code-line" data-line="116">Improving the perf by unrolling the loop</h4><p class="code-line" data-line="118">Modern CPUs have many ways of increasing the throughput at which it executes your code. For single-threaded applications, one of the ways it can do this is by executing multiple primitive operations in a single cycle (a cycle is the basic unit of time in a CPU).</p><p class="code-line" data-line="120">Most modern CPUs can execute about 4 add operations in a single cycle (under optimal conditions), so by laying out your code correctly and profiling it, you can sometimes optimize your code to have better performance, even when only executing on a single-thread.</p><p class="code-line" data-line="122">While the JIT can perform loop unrolling itself, it is conservative in deciding when to do so due to the larger codegen it produces. So, it can be beneficial to manually unroll the loop in your source code instead.</p><p class="code-line" data-line="124">You might unroll your code like the following:</p><p class="code-line" data-line="157">The code is slightly more complicated but takes better advantage of your hardware.</p><p class="code-line" data-line="159">For really small loops, the code ends up being slightly slower, but that normalizes itself for inputs that have 8 elements and then starts getting faster for inputs with even more elements (taking 26% less time at 32k elements). It’s also worth noting that this optimization doesn’t always improve performance. For example, when handling&nbsp;<code>float</code>, the unrolled version is practically the same speed as the original version, so it’s important to profile your code accordingly.</p><table><thead><tr><th>Method</th><th>Count</th><th>Mean</th><th>Error</th><th>StdDev</th></tr></thead><tbody><tr><td>SumUnrolled</td><td>1</td><td>2.922 ns</td><td>0.0651 ns</td><td>0.0609 ns</td></tr><tr><td>SumUnrolled</td><td>2</td><td>3.576 ns</td><td>0.0116 ns</td><td>0.0109 ns</td></tr><tr><td>SumUnrolled</td><td>4</td><td>3.708 ns</td><td>0.0157 ns</td><td>0.0139 ns</td></tr><tr><td>SumUnrolled</td><td>8</td><td>4.832 ns</td><td>0.0486 ns</td><td>0.0454 ns</td></tr><tr><td>SumUnrolled</td><td>16</td><td>7.490 ns</td><td>0.1131 ns</td><td>0.1058 ns</td></tr><tr><td>SumUnrolled</td><td>32</td><td>11.277 ns</td><td>0.0910 ns</td><td>0.0851 ns</td></tr><tr><td>SumUnrolled</td><td>64</td><td>19.761 ns</td><td>0.2016 ns</td><td>0.1885 ns</td></tr><tr><td>SumUnrolled</td><td>128</td><td>36.639 ns</td><td>0.3043 ns</td><td>0.2847 ns</td></tr><tr><td>SumUnrolled</td><td>256</td><td>77.969 ns</td><td>0.8409 ns</td><td>0.7866 ns</td></tr><tr><td>SumUnrolled</td><td>512</td><td>146.357 ns</td><td>1.3209 ns</td><td>1.2356 ns</td></tr><tr><td>SumUnrolled</td><td>1024</td><td>287.354 ns</td><td>0.9223 ns</td><td>0.8627 ns</td></tr><tr><td>SumUnrolled</td><td>2048</td><td>566.405 ns</td><td>4.0155 ns</td><td>3.5596 ns</td></tr><tr><td>SumUnrolled</td><td>4096</td><td>1,131.016 ns</td><td>7.3601 ns</td><td>6.5246 ns</td></tr><tr><td>SumUnrolled</td><td>8192</td><td>2,259.836 ns</td><td>8.6539 ns</td><td>8.0949 ns</td></tr><tr><td>SumUnrolled</td><td>16384</td><td>4,501.295 ns</td><td>6.4186 ns</td><td>6.0040 ns</td></tr><tr><td>SumUnrolled</td><td>32768</td><td>8,979.690 ns</td><td>19.5265 ns</td><td>18.2651 ns</td></tr></tbody></table><h4 id="improving-the-perf-by-vectorizing-the-loop" class="code-line" data-line="183"><a href="https://devblogs.microsoft.com/dotnet/wp-content/uploads/sites/10/2019/09/base-unrolled-1024x808.png"><img class="wp-image-24363 size-large aligncenter" src="https://devblogs.microsoft.com/dotnet/wp-content/uploads/sites/10/2019/09/base-unrolled-1024x808.png" alt="" width="640" height="505" srcset="https://devblogs.microsoft.com/dotnet/wp-content/uploads/sites/10/2019/09/base-unrolled-1024x808.png 1024w, https://devblogs.microsoft.com/dotnet/wp-content/uploads/sites/10/2019/09/base-unrolled-300x237.png 300w, https://devblogs.microsoft.com/dotnet/wp-content/uploads/sites/10/2019/09/base-unrolled-768x606.png 768w, https://devblogs.microsoft.com/dotnet/wp-content/uploads/sites/10/2019/09/base-unrolled.png 1026w" sizes="(max-width: 640px) 100vw, 640px"></a></h4><h4 class="code-line" data-line="183">Improving the perf by vectorizing the loop</h4><p class="code-line" data-line="185">However, we can still optimize the code a bit more. SIMD instructions are another way modern CPUs allow you to improve throughput. Using a single instruction they allow you to perform multiple operations in a single cycle. This can be better than the loop unrolling because it performs essentially the same operation, but with smaller generated code.</p><p class="code-line" data-line="187">To elaborate a bit, each one of the add instructions from the unrolled loop is 4 bytes in size, so it takes 16-bytes of space to have all 4 adds in the unrolled form. However, the SIMD add instruction also performs 4 additions, but it only takes 4 bytes to encode. This means there are less instructions for the CPU to decode and execute each iteration of the loop. There are also other things the CPU can assume and optimize around for this single instruction, but those are out of scope for this blog post. What’s even better is that modern CPUs can also execute more than one SIMD instruction per cycle, so in certain cases you can then unroll your vectorized code to improve the performance further.</p><p class="code-line" data-line="189">You should generally start by looking at whether the general-purpose&nbsp;<code>Vector&lt;T&gt;</code>&nbsp;class will suit your needs. It, like the newer hardware intrinsics, will emit SIMD instructions, but given that it is general-purpose you can reduce the amount of code you need to write/maintain.</p><p class="code-line" data-line="191">The code might look like:</p><p class="code-line" data-line="223">The code is faster, but we have to fall back to accessing individual elements when computing the overall sum.&nbsp;<code>Vector&lt;T&gt;</code>&nbsp;also does not have a well-defined size and can vary based on the hardware you are running against. The hardware intrinsics provide some additional functionality that can make this code a bit nicer and faster still (at the cost of additional code complexity and maintainence requirements).</p><table><thead><tr><th>Method</th><th>Count</th><th>Mean</th><th>Error</th><th>StdDev</th></tr></thead><tbody><tr><td>SumVectorT</td><td>1</td><td>4.517 ns</td><td>0.0752 ns</td><td>0.0703 ns</td></tr><tr><td>SumVectorT</td><td>2</td><td>4.853 ns</td><td>0.0609 ns</td><td>0.0570 ns</td></tr><tr><td>SumVectorT</td><td>4</td><td>5.047 ns</td><td>0.0909 ns</td><td>0.0850 ns</td></tr><tr><td>SumVectorT</td><td>8</td><td>5.671 ns</td><td>0.0251 ns</td><td>0.0223 ns</td></tr><tr><td>SumVectorT</td><td>16</td><td>6.579 ns</td><td>0.0330 ns</td><td>0.0276 ns</td></tr><tr><td>SumVectorT</td><td>32</td><td>10.460 ns</td><td>0.0241 ns</td><td>0.0226 ns</td></tr><tr><td>SumVectorT</td><td>64</td><td>17.148 ns</td><td>0.0407 ns</td><td>0.0381 ns</td></tr><tr><td>SumVectorT</td><td>128</td><td>23.239 ns</td><td>0.0853 ns</td><td>0.0756 ns</td></tr><tr><td>SumVectorT</td><td>256</td><td>62.146 ns</td><td>0.8319 ns</td><td>0.7782 ns</td></tr><tr><td>SumVectorT</td><td>512</td><td>114.863 ns</td><td>0.4175 ns</td><td>0.3906 ns</td></tr><tr><td>SumVectorT</td><td>1024</td><td>172.129 ns</td><td>1.8673 ns</td><td>1.7467 ns</td></tr><tr><td>SumVectorT</td><td>2048</td><td>429.722 ns</td><td>1.0461 ns</td><td>0.9786 ns</td></tr><tr><td>SumVectorT</td><td>4096</td><td>654.209 ns</td><td>3.6215 ns</td><td>3.0241 ns</td></tr><tr><td>SumVectorT</td><td>8192</td><td>1,675.046 ns</td><td>14.5231 ns</td><td>13.5849 ns</td></tr><tr><td>SumVectorT</td><td>16384</td><td>2,514.778 ns</td><td>5.3369 ns</td><td>4.9921 ns</td></tr><tr><td>SumVectorT</td><td>32768</td><td>6,689.829 ns</td><td>13.9947 ns</td><td>13.0906 ns</td></tr></tbody></table><p class="code-line" data-line="244"><a href="https://devblogs.microsoft.com/dotnet/wp-content/uploads/sites/10/2019/09/base-unrolled-vectort-1024x808.png"><img class="wp-image-24364 size-large aligncenter" src="https://devblogs.microsoft.com/dotnet/wp-content/uploads/sites/10/2019/09/base-unrolled-vectort-1024x808.png" alt="" width="640" height="505" srcset="https://devblogs.microsoft.com/dotnet/wp-content/uploads/sites/10/2019/09/base-unrolled-vectort-1024x808.png 1024w, https://devblogs.microsoft.com/dotnet/wp-content/uploads/sites/10/2019/09/base-unrolled-vectort-300x237.png 300w, https://devblogs.microsoft.com/dotnet/wp-content/uploads/sites/10/2019/09/base-unrolled-vectort-768x606.png 768w, https://devblogs.microsoft.com/dotnet/wp-content/uploads/sites/10/2019/09/base-unrolled-vectort.png 1026w" sizes="(max-width: 640px) 100vw, 640px"></a></p><p class="code-line" data-line="246"><em>NOTE:</em>&nbsp;For the purposes of this blogpost, I forced the size of&nbsp;<code>Vector&lt;T&gt;</code>&nbsp;to 16-bytes using an internal configuration knob (<code>COMPlus_SIMD16ByteOnly=1</code>). This normalized the results when comparing&nbsp;<code>SumVectorT</code>&nbsp;to&nbsp;<code>SumVectorizedSse</code>&nbsp;and kept the latter code simpler. Namely, it avoided the need to write an&nbsp;<code>if (Avx2.IsSupported) { }</code>&nbsp;code path. Such a code path is nearly identical to the&nbsp;<code>Sse2</code>&nbsp;path, but deals with&nbsp;<code>Vector256&lt;T&gt;</code>&nbsp;(32-bytes) and processes even more elements per loop iteration.</p><p class="code-line" data-line="249">So, you might take advantage of the new hardware intrinsics like so:</p><p class="code-line" data-line="303">The code is again slightly more complicated, but it’s significantly faster for all but the smallest workloads. At 32k elements, it’s taking 75% less time than the unrolled loop and 81% less than the original code.</p><p class="code-line" data-line="305">You’ll notice that we have a few&nbsp;<code>IsSupported</code>&nbsp;checks. The first checks if the hardware intrinsics are supported for the current platform at all and falls back to the unrolled loop if they aren’t. This path will currently be hit for platforms like ARM/ARM64 which don’t have hardware intrinsics or if someone disables them for any reason. The second&nbsp;<code>IsSupported</code>&nbsp;check is in the&nbsp;<code>SumVectorizedSse</code>&nbsp;method and is used to produce slightly better codegen on newer hardware that additionally supports the&nbsp;<code>Ssse3</code>&nbsp;instruction set.</p><p class="code-line" data-line="307">Otherwise, most of the logic is essentially the same as what we had done for the unrolled version.&nbsp;<code>Vector128&lt;T&gt;</code>&nbsp;is a 128-bit type that contains&nbsp;<code>Vector128&lt;T&gt;.Count</code>&nbsp;elements. In the case of&nbsp;<code>uint</code>, which is itself 32-bits, you have&nbsp;<code>4</code>&nbsp;(<code>128 / 32</code>) elements, which is exactly how much we unrolled the loop by.</p><table><thead><tr><th>Method</th><th>Count</th><th>Mean</th><th>Error</th><th>StdDev</th></tr></thead><tbody><tr><td>SumVectorized</td><td>1</td><td>4.555 ns</td><td>0.0192 ns</td><td>0.0179 ns</td></tr><tr><td>SumVectorized</td><td>2</td><td>4.848 ns</td><td>0.0147 ns</td><td>0.0137 ns</td></tr><tr><td>SumVectorized</td><td>4</td><td>5.381 ns</td><td>0.0210 ns</td><td>0.0186 ns</td></tr><tr><td>SumVectorized</td><td>8</td><td>4.838 ns</td><td>0.0209 ns</td><td>0.0186 ns</td></tr><tr><td>SumVectorized</td><td>16</td><td>5.107 ns</td><td>0.0175 ns</td><td>0.0146 ns</td></tr><tr><td>SumVectorized</td><td>32</td><td>5.646 ns</td><td>0.0230 ns</td><td>0.0204 ns</td></tr><tr><td>SumVectorized</td><td>64</td><td>6.763 ns</td><td>0.0338 ns</td><td>0.0316 ns</td></tr><tr><td>SumVectorized</td><td>128</td><td>9.308 ns</td><td>0.1041 ns</td><td>0.0870 ns</td></tr><tr><td>SumVectorized</td><td>256</td><td>15.634 ns</td><td>0.0927 ns</td><td>0.0821 ns</td></tr><tr><td>SumVectorized</td><td>512</td><td>34.706 ns</td><td>0.2851 ns</td><td>0.2381 ns</td></tr><tr><td>SumVectorized</td><td>1024</td><td>68.110 ns</td><td>0.4016 ns</td><td>0.3756 ns</td></tr><tr><td>SumVectorized</td><td>2048</td><td>136.533 ns</td><td>1.3104 ns</td><td>1.2257 ns</td></tr><tr><td>SumVectorized</td><td>4096</td><td>277.930 ns</td><td>0.5913 ns</td><td>0.5531 ns</td></tr><tr><td>SumVectorized</td><td>8192</td><td>554.720 ns</td><td>3.5133 ns</td><td>3.2864 ns</td></tr><tr><td>SumVectorized</td><td>16384</td><td>1,110.730 ns</td><td>3.3043 ns</td><td>3.0909 ns</td></tr><tr><td>SumVectorized</td><td>32768</td><td>2,200.996 ns</td><td>21.0538 ns</td><td>19.6938 ns</td></tr></tbody></table><p class="code-line" data-line="328"><a href="https://devblogs.microsoft.com/dotnet/wp-content/uploads/sites/10/2019/09/base-unrolled-vectort-vectorized-1024x808.png"><img class="wp-image-24365 size-large aligncenter" src="https://devblogs.microsoft.com/dotnet/wp-content/uploads/sites/10/2019/09/base-unrolled-vectort-vectorized-1024x808.png" alt="" width="640" height="505" srcset="https://devblogs.microsoft.com/dotnet/wp-content/uploads/sites/10/2019/09/base-unrolled-vectort-vectorized-1024x808.png 1024w, https://devblogs.microsoft.com/dotnet/wp-content/uploads/sites/10/2019/09/base-unrolled-vectort-vectorized-300x237.png 300w, https://devblogs.microsoft.com/dotnet/wp-content/uploads/sites/10/2019/09/base-unrolled-vectort-vectorized-768x606.png 768w, https://devblogs.microsoft.com/dotnet/wp-content/uploads/sites/10/2019/09/base-unrolled-vectort-vectorized.png 1026w" sizes="(max-width: 640px) 100vw, 640px"></a></p><h3 id="summary" class="code-line" data-line="330">Summary</h3><p class="code-line" data-line="332">The new hardware intrinsics allow you to take advantage of platform-specific functionality for the machine you’re running on. There are approximately 1,500 APIs for x86 and x64 spread across 15 instruction sets and far too many to cover in a single blog post. By profiling your code to identify hot spots you can also potentially identify areas of your code that would benefit from vectorization and see some pretty good performance gains. There are multiple scenarios where vectorization can be applied and loop unrolling is just the beginning.</p><p class="code-line" data-line="334">Anyone wanting to see more examples can search for uses of the intrinsics in the framework (see the&nbsp;<a title="https://github.com/search?l=C%23&amp;q=org:dotnet+System.Runtime.Intrinsics&amp;type=Code" href="https://github.com/search?l=C%23&amp;q=org:dotnet+System.Runtime.Intrinsics&amp;type=Code">dotnet</a>&nbsp;and&nbsp;<a title="https://github.com/search?l=C%23&amp;q=org:aspnet+System.Runtime.Intrinsics&amp;type=Code" href="https://github.com/search?l=C%23&amp;q=org:aspnet+System.Runtime.Intrinsics&amp;type=Code">aspnet</a>&nbsp;organizations) or in various other blog posts written by the community. And while the currently exposed intrinsics are extensive, there is still a lot of functionality that&nbsp;<strong>could</strong>&nbsp;be exposed. If you have functionality you would like exposed, feel free to log an API request against&nbsp;<a title="https://github.com/dotnet/corefx/issues" href="https://github.com/dotnet/corefx/issues">dotnet/corefx on GitHub</a>. The API review process is detailed&nbsp;<a title="https://github.com/dotnet/corefx/blob/master/Documentation/project-docs/api-review-process.md" href="https://github.com/dotnet/corefx/blob/master/Documentation/project-docs/api-review-process.md">here</a>&nbsp;and there is a&nbsp;<a title="https://github.com/dotnet/corefx/issues/271" href="https://github.com/dotnet/corefx/issues/271">good example</a>&nbsp;for the API Request template listed under Step 1.</p><h3 id="special-thanks" class="code-line" data-line="336">Special Thanks</h3><p class="code-line code-active-line" data-line="338">A special thanks to our community members&nbsp;<a title="https://github.com/fiigii" href="https://github.com/fiigii">Fei Peng (@fiigii)</a>&nbsp;and&nbsp;<a title="https://github.com/4creators" href="https://github.com/4creators">Jacek Blaszczynski (@4creators)</a>&nbsp;who helped implement the hardware intrinsics. Also to all the community members who have provided valuable feedback to the design, implementation, and usability of the feature.</p><div class="authorinfoarea"><div><p>Software Engineer,&nbsp;.NET Team</p><p><strong>Follow Tanner</strong>&nbsp;&nbsp;&nbsp;<a class="no-underline stayinformed" aria-label="Tanner Gooding Twitter profile" target="_blank" href="https://twitter.com/tannergooding"></a><a class="no-underline stayinformed" aria-label="Tanner Gooding LinkedIn profile" target="_blank" href="https://www.linkedin.com/in/tanner-gooding-84b79054/"><i class="fa fa-linkedin"></i></a><a class="no-underline stayinformed" aria-label="Tanner Gooding GitHub profile" target="_blank" href="https://github.com/tannergooding"><i class="fa fa-github"></i></a><a class="no-underline stayinformed hvr-pop" aria-label="Tanner Gooding RSS Feed" target="_blank" href="https://devblogs.microsoft.com/dotnet/author/tagoomicrosoft-com/feed/"></a></p></div></div></div></div></div></div>
</div>
    </div>
    <footer>
        <div>created by buildstarted &copy; 2020 <a href="/about">about</a></div>
        <div>Share this page on social media: copy and paste this url https://linksfor.dev/</div>
        <div>If you prefer RSS: <a href="https://linksfor.dev/feed.xml">https://linksfor.dev/feed.xml</a></div>
    </footer>
    
    <script>
        (function () {
            var COLLECT_URL = "https://dna.buildstarted.com/t";
            var SITE_ID = "linksfor.devs";
            var GLOBAL_VAR_NAME = "__DNA__";

            window[GLOBAL_VAR_NAME] = {};

            window[GLOBAL_VAR_NAME].sendPageView = function () {
                var path = location.pathname;
                var referrer = document.referrer;

                var url = COLLECT_URL + "?siteid=" + SITE_ID + "&p=" + encodeURIComponent(path) + "&r=" + encodeURIComponent(referrer);

                try { fetch(url, { method: "GET" }); } catch (e) { }
            };

            window[GLOBAL_VAR_NAME].sendPageView();
        })();
    </script>
</body>
</html>