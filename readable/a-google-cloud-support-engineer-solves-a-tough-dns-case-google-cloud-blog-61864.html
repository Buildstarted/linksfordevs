<!DOCTYPE html>
<html lang="en">
<head>
    <title>
A Google Cloud support engineer solves a tough DNS case | Google Cloud Blog - linksfor.dev(s)    </title>
    <meta charset="utf-8">
    <link rel="alternate" type="application/rss+xml" title="Linksfor.dev(s) feed" href="https://linksfor.dev/feed.rss" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta name="google" value="notranslate">
    <link rel="stylesheet" type="text/css" href="/style.css" />
    <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon">
    <link rel="icon" href="/favicon.ico" type="image/x-icon">
    <meta property="og:title" content="A Google Cloud support engineer solves a tough DNS case | Google Cloud Blog - linksfor.dev(s)"/>
    <meta property="article:author" content="Antonio Messina  Technical Solutions Engineer, Google Cloud Platform Support"/>
    <meta property="og:description" content="How a Google Cloud support engineer found one customer&#x2019;s missing DNS packets"/>
    <meta property="og:type" content="website"/>
    <meta property="og:url" content="https://cloud.google.com/blog/topics/inside-google-cloud/google-cloud-support-engineer-solves-a-tough-dns-case"/>

<meta property="og:site_name" content="linksfor.dev(s)" />
</head>
<body>
    <div class="grid">
        <h1>
<a href="/" style="color:inherit">linksfor.dev(s)</a>
        </h1>
        <title>linksfor.dev(s) - A Google Cloud support engineer solves a tough DNS case | Google Cloud Blog</title>
<div class="readable">
        <h1>A Google Cloud support engineer solves a tough DNS case | Google Cloud Blog</h1>
            <div>by Antonio Messina  Technical Solutions Engineer, Google Cloud Platform Support</div>
            <div>Reading time: 13-16 minutes</div>
        <div>Posted here: 19 May 2020</div>
        <p><a href="https://cloud.google.com/blog/topics/inside-google-cloud/google-cloud-support-engineer-solves-a-tough-dns-case">https://cloud.google.com/blog/topics/inside-google-cloud/google-cloud-support-engineer-solves-a-tough-dns-case</a></p>
        <hr/>
<div id="readability-page-1" class="page"><div><article-page><main id="jump-content"><article><article-header-block><section></section></article-header-block><!----><!----><article-aspect-image-block><figure><p><span>GCP_11.jpg</span></p></figure></article-aspect-image-block><article-share-block></article-share-block><article-sticky-share-block></article-sticky-share-block><section><div><div><!----><article-content-stream-block><!----><div><!----><!----><div><div><div><!----><!----><paragraph-block _nghost-c16=""><div _ngcontent-c16=""><p><i><b>Editor’s note:</b> Ever wonder how Google Cloud Technical Solutions Engineers (TSE) approach your support cases? TSEs are the support engineers responsible for troubleshooting and identifying the technical root cause of issues reported by customers. Some are fairly simple, but, every once in a while, a support ticket comes in that takes several dedicated engineers to troubleshoot. In this blog post, we hear from a Google Cloud Technical Solution Engineer about a particularly thorny support case that they recently solved—<a href="https://medium.com/@rimantasragainis/how-sysctl-has-broken-the-network-f4cc568ea91d" target="_blank">the case of the missing DNS packets</a>. Along the way, they’ll show you the information they gathered in the course of their troubleshooting, and how they reasoned their way through to a resolution. Besides uncovering a deep-seated bug, we hope that this story will give you some insight into what to expect the next time you submit a ticket to Google Cloud support.</i></p><p>Troubleshooting is both a science and an art. The first step is to make a hypothesis about why something is behaving in an unexpected way, and then prove whether or not the hypothesis is correct. But before you can formulate a hypothesis, you first need to clearly identify the problem, and express it with precision. If the issue is too vague, then you need to brainstorm in order to narrow down the problem—this is where the “artistic” part of the process comes in.</p><p>This is doubly challenging in the Google Cloud environment. Google Cloud works hard to help ensure customer privacy, so we, your technical solutions engineers, do not have write access to your systems. Nor do we have the same visibility into the system as you do, and we absolutely cannot modify the system to quickly test if our hypothesis is correct. Some customers believe that they can send us a VM id, trusting that we will fix it like car mechanics at a garage. But, in fact, a GCP support case is more like a conversation: communicating with you is the main way we collect information, make hypotheses and prove (or disprove) them, all the way to eventually solving the case.</p><h3>The case in question&nbsp;</h3><p>This is the story of a support case with a happy ending. One of the reasons it was successful was that the case description was excellent: very detailed and precise. Here is a transcript of the first support ticket, anonymized to protect the customer’s privacy:</p></div></paragraph-block><!----><!----><!----></div></div></div><!----><!----><!----><!----><!----><!----></div><div><!----><!----><div><div><div><!----><!----><paragraph-block _nghost-c16=""><div _ngcontent-c16=""><p>This message is incredibly useful because it contains a lot of detailed information:</p><ul><li>The specific VM with the issue</li><li>The issue - DNS resolution not working</li><li>Where the issue is visible - the VM and the container</li><li>Which troubleshooting steps were already performed to narrow down the problem</li></ul><p>The case was filed as “P1: Critical Impact - Service Unusable in production“. This means that the case will “Follow the Sun” by default, to provide 24/7 support (click here to read more about <a href="https://cloud.google.com/support/docs/procedures#support_case_priority">support case prioritization</a>), and at the end of every regional shift will be assigned to the next regional team. In fact, by the time our Zurich-based team got to it, the case had already bounced to the other regional support teams around the world a couple of times. During this time the customer had already put in place a mitigation. But because they hadn’t found the root cause, they were worried that the issue could present itself again in the production system.</p><p>So far, here’s following information we’ve collected:</p><ul><li>Content of <code>/etc/hosts</code></li><li>Content of <code>/etc/resolv.conf</code></li><li>Output of <code>iptables-save</code></li><li>pcap file collected using the <code>ngrep</code> command</li><li>And with all that information, we are ready to start the "sleuthing" part of our troubleshooting.</li></ul><h3>Our first steps</h3><p>The first thing we do is check the logs and status of the metadata server to ensure it is working correctly. It is. The metadata server responds to the 169.254.169.254 IP address and is responsible, among other things, for resolving the domain names. We also double check that the firewall rules applied to the VM are correct and not blocking packets.</p><p>This issue is weird. Until now, our only hypothesis had been that UDP packets are being dropped, but the output of nmap now proves that they are not. We mentally come up with some more hypotheses and ways to verify them:</p><ul><li><p>Are packets being selectively dropped? =&gt; check iptable rules</p></li><li><p>Is the <a href="https://en.wikipedia.org/wiki/Maximum_transmission_unit" target="_blank">MTU</a> too small? =&gt; check output of <code>ip a show</code></p></li><li><p>Is the problem UDP-only or does it also involve TCP? =&gt; run <code>dig +tcp</code></p></li><li><p>Are packets generated by dig not coming back? =&gt; run <code>tcpdump</code></p></li><li><p>Is libdns working properly? =&gt; run <code>strace</code> to check it is actually sending and receiving packets</p></li></ul><p>The current owner of the case asks for suggestions, so we decide to jump on a call with the customer to do some live troubleshooting.</p><p>During the call we test a few more things:</p><ul><li><p>We flush iptables rules without success.</p></li><li><p>We check network interfaces and routing tables. We double-check that the MTU is correct.</p></li><li><p>We find out that <code>dig +tcp google.com</code> (TCP) works, but <code>dig google.com</code> (UDP) does not.</p></li><li><p>We run <code>tcpdump</code> while running <code>“dig”</code>, and we see that UDP packets are coming back.</p></li><li><p>We run <code>strace dig google.com</code> and we see that dig correctly calls <code>sendmsg()</code> and <code>recvmsg()</code> but that the latter times out.</p></li></ul><p>It’s the end of our shift, so unfortunately we have to let go of the case to a different timezone. The case is already famous in our team, and a colleague suggests using the python <a href="https://scapy.net/" target="_blank">scapy</a> module to create a raw DNS packet:</p></div></paragraph-block><!----><!----><!----></div></div></div><!----><!----><!----><!----><!----><!----></div><div><!----><!----><div><div><div><!----><!----><!----><article-code-block _nghost-c17=""><!----><pre _ngcontent-c17="">  <!----><code _ngcontent-c17="">from scapy.all import *</code><code _ngcontent-c17=""></code><code _ngcontent-c17="">answer = sr1(IP(dst="169.254.169.254")/UDP(dport=53)/DNS(rd=1,qd=DNSQR(qname="google.com")),verbose=0)</code><code _ngcontent-c17="">print ("169.254.169.254", answer[DNS].summary())</code>
</pre></article-code-block></div></div></div><!----><!----><!----><!----><!----><!----></div><div><!----><!----><div><div><div><!----><!----><paragraph-block _nghost-c16=""><div _ngcontent-c16=""><p>This snippet creates a DNS packet and sends a request to the metadata server.</p><p>The customer runs the code, the DNS reply comes back and the application receives it! This confirms that the problem cannot be on the network layer.</p><p>After another spin around the world the case comes back to our team. I decide that it’s better for the customer if the case stops spinning around the world, so I will keep the case from here on out.&nbsp;</p><p>In the meantime, the customer kindly agrees to provide a snapshot of the image. This is incredibly good news: being able to test the image myself makes troubleshooting very quick—no need to ask the customer to run commands, send me the output and analyze it, I can do everything live!</p><p>My colleagues are getting jealous. We talk about the case at lunch and nobody has any idea of what is happening yet. Luckily, the customer isn’t in too much of a hurry because they have a mitigation, so we have more time to investigate. And because we have the image, we can do all the testing we want. This is fun!&nbsp;</p><h3>Taking a step back</h3><p>A very famous interview question to ask systems engineers is “What happens when you run ping <a href="http://www.google.com/" target="_blank">www.google.com</a>?” This is an excellent question because it requires the interviewee to describe the path from the shell, to userland, to the kernel and on to the network. I smile: sometimes interview questions are actually useful in real life...&nbsp;</p><p>I decide to apply this question to the problem at hand. Roughly speaking, when you try to resolve a DNS name this is what happens:</p><ol><li><p>The application calls some system library (for instance libdns).</p></li><li><p>libdns checks the system configuration to know which DNS server to interrogate (in this diagram: 169.254.169.254, the metadata server).</p></li><li><p>libdns uses system calls to create a UDP socket (SOCKET_DGRAM) and to send and receive UDP packets containing the DNS request.</p></li><li><p>The UDP stack can be configured at kernel level using the sysctl interface.</p></li><li><p>The kernel interacts with the hardware to send the packet(s) on the network using the network interface.</p></li><li><p>When contacting the metadata server, the packet is actually captured by the hypervisor, who sends it to the metadata server.</p></li><li><p>The metadata server does some magic to actually resolve the name and then sends back the reply in the same way.</p></li></ol></div></paragraph-block><!----><!----><!----></div></div></div><!----><!----><!----><!----><!----><!----></div><div><!----><!----><div><div><div><!----><!----><paragraph-block _nghost-c16=""><div _ngcontent-c16=""><p>To recap, we have considered the following hypotheses:</p><p><b>Hypothesis: The libraries are broken</b></p><ul><li><p>Test 1: run strace in the impacted system, check if dig is calling the correct system calls</p></li><ul><li><p>Result: correct syscalls are invoked</p></li></ul><li><p>Test 2: use scapy to check if we can resolve names bypassing OS system libraries</p></li><ul><li><p>Result: resolution via scapy works</p></li></ul><li><p>Test 3: run rpm -V on the libdns package and md5sum on the library files</p></li><ul><li><p>Result: library code is exactly the same as the one in a working operating system</p></li></ul><li><p>Test 4: mount the customer image's root filesystem on a VM where the behavior is not present, run chroot, see if DNS resolution works&nbsp;</p></li><ul><li><p>Result: DNS resolution works</p></li></ul></ul><p><b>Conclusion</b>: from Tests 1-4, the libraries are not the problem</p><p><b>Hypothesis: The DNS resolution is misconfigured</b></p><ul><li><p>Test 1: inspect tcpdump to see whether DNS packets sent and received after running “dig” are correct</p></li><ul><li><p>Result: they are correct</p></li></ul><li><p>Test 2: double check <code>/etc/nsswitch.conf</code> and <code>/etc/resolv.conf</code> on the server</p></li><ul><li><p>Result: they are correct</p></li></ul></ul><p><b>Conclusion</b>: from Tests 1-2, the DNS configuration is not the problem</p><p><b>Hypothesis: The kernel is broken</b></p><ul><li><p>Test: install a new kernel, verify the signature, restart</p></li><ul><li><p>Result: same behavior</p></li></ul></ul><p><b>Hypothesis: The customer’s network (or the hypervisor network interface) is misbehaving</b></p><ul><li><p>Test 1: check firewall configuration</p></li><ul><li><p>Result: firewall configuration (both in-host and on GCP) allows DNS packets</p></li></ul></ul><ul><li><p>Test 2: capture the traffic to check DNS requests are correctly sent and replies received</p></li><ul><li><p>Result: tcpdump shows reply packets are received by the host</p></li></ul></ul><p><b>Conclusion</b>: from Tests 1-2, the network is not the problem.</p><p><b>Hypothesis: The metadata server is not working</b></p><ul><li><p>Test 1: check metadata server logs for anomalies</p></li><ul><li><p>Result: no anomalies in the metadata server logs</p></li></ul><li><p>Test 2: bypass the metadata server by using <code>dig @8.8.8.8</code></p></li><ul><li><p>Result: resolution fails even when metadata server is not used</p></li></ul></ul><p><b>Conclusion</b>: from Test 1-2, the metadata server is not the problem.</p><p><b>Final conclusion</b>: our hypotheses tested multiple subsystems, <b>except the runtime configuration!</b></p><h3>Digging into the kernel runtime config</h3><p>To configure the kernel runtime, you can use command line options (grub) or the sysctl interface. I look in /etc/sysctl.conf, and lo and behold, there are several custom configurations. I feel like I’m on to something. I ignore all options that are not network related or tcp-only. What remains is a bunch of <code>net.core</code> configurations. I then take a VM where host resolution works and apply all the settings from the broken VM, one by one. Eventually, I find the culprit:</p></div></paragraph-block><!----><!----><!----></div></div></div><!----><!----><!----><!----><!----><!----></div><div><!----><!----><div><div><div><!----><!----><!----><article-code-block _nghost-c17=""><!----><pre _ngcontent-c17="">  <!----><code _ngcontent-c17="">net.core.rmem_default = 2147483647</code>
</pre></article-code-block></div></div></div><!----><!----><!----><!----><!----><!----></div><div><!----><!----><div><div><div><!----><!----><paragraph-block _nghost-c16=""><div _ngcontent-c16=""><p>This is the setting that finally breaks DNS resolution! I’ve found the smoking gun. But why? I need a motive.</p><p><code>net.core.rmem_default</code> is how you set the default receive buffer size for UDP packets. A common value is something around 200KiB, but if your server receives a lot of UDP packets you might want to increase the buffer size. If the buffer is full when a new packet arrives, because the application was not fast enough to consume them, then you will lose packets. The customer was running an application to collect metrics that were sent as UDP packets, so they had correctly increased the buffer to ensure they didn’t lose any data points. And they had set this value to the highest value possible: 2^31 - 1 (if you try and set it to 2^31 the kernel returns “INVALID ARGUMENT”).</p><p>Suddenly I realize why nmap and scapy were working correctly: they use raw sockets! Raw sockets are different than normal sockets: they bypass iptables, and they are not buffered!</p><p>But why is having *too big of a buffer* causing issues? It is clearly not working as intended.</p><p>At this point I can reproduce the issue on multiple kernels and multiple distributions. The issue was present already in kernel 3.x and is present now on kernel 5.x. Indeed, if you run</p></div></paragraph-block><!----><!----><!----></div></div></div><!----><!----><!----><!----><!----><!----></div><div><!----><!----><div><div><div><!----><!----><!----><article-code-block _nghost-c17=""><!----><pre _ngcontent-c17="">  <!----><code _ngcontent-c17="">sysctl -w net.core.rmem_default=$((2**31-1))</code>
</pre></article-code-block></div></div></div><!----><!----><!----><!----><!----><!----></div><div><!----><!----><div><div><div><!----><!----><paragraph-block _nghost-c16=""><div _ngcontent-c16=""><p>then DNS stops working.</p><p>I start to look for a value that works. Using a simple binary search algorithm, I find that 2147481343 seems to do the trick. This number doesn't make any sense to me. I suggest the customer try this number. The customer replies back: it works with google.com, but it does not work with other domains. I continue my investigation.</p><p>I install <a href="https://linux.die.net/man/1/dropwatch" target="_blank">dropwatch</a>, a tool that shows you where in the kernel a packet is dropped. I should have used it earlier. The guilty function is <a href="https://github.com/torvalds/linux/blob/v5.4/net/ipv4/udp.c#L1993" target="_blank"><code>udp_queue_rcv_skb</code></a>. So I download kernel sources and add a few <a href="https://elinux.org/Debugging_by_printing" target="_blank"><code>printk</code> functions</a>, to follow where, exactly, the packet is dropped. I quickly find the specific <code>if</code> condition that fails. I stare at it for a while, and it all comes together: the 2^31-1, the number that doesn’t make any sense, the domain that doesn’t work. It’s a bit of code in <a href="https://github.com/torvalds/linux/blob/v5.4/net/ipv4/udp.c#L1478" target="_blank"><code>__udp_enqueue_schedule_skb</code></a>:</p></div></paragraph-block><!----><!----><!----></div></div></div><!----><!----><!----><!----><!----><!----></div><div><!----><!----><div><div><div><!----><!----><!----><article-code-block _nghost-c17=""><!----><pre _ngcontent-c17="">  <!----><code _ngcontent-c17="">if (rmem &gt; (size + sk-&gt;sk_rcvbuf))</code><code _ngcontent-c17="">		goto uncharge_drop;</code>
</pre></article-code-block></div></div></div><!----><!----><!----><!----><!----><!----></div><div><!----><!----><div><div><div><!----><!----><paragraph-block _nghost-c16=""><div _ngcontent-c16=""><p>Please note that:</p><ul><li><p><code>rmem</code> is of type int.</p></li><li><p><code>size</code> is of type u16 (unsigned int 16 bit) and stores the size of the packet.</p></li><li><p><code>sk-&gt;sk_rcvbuf</code> is of type int and stores the size of the buffer which is, by default, equal to the value in <code>net.core.rmem_default</code>.</p></li></ul><p>When <code>sk_rcvbuf</code> gets close to 2^31, adding the size of the packet can cause an <a href="https://en.wikipedia.org/wiki/Integer_overflow" target="_blank">integer overflow</a>. And since it’s an int it becomes a negative number, therefore the condition is true when it should be false (for more, also check out this discussion of <a href="https://en.wikipedia.org/wiki/Signed_number_representations" target="_blank">signed magnitude representation</a>).</p><p>The fix is trivial: cast to <code>unsigned int</code>. I apply the fix, and reboot. DNS resolution works again.</p><h3>The thrill of victory</h3><p>I send my findings to the customer and send a kernel patch to <a href="https://lkml.org/" target="_blank">LKML</a>. I feel happy: every single piece of the puzzle has finally come together. I can explain exactly why we observed what we observed. But more importantly, by working together, we were able to fix the problem!&nbsp;</p><p>Admittedly, this was a very rare case. And thankfully, very few support cases that we work on are this complicated, so we can close them much more quickly. By reading through this support case, though, hopefully you can gain an understanding of how we approach our work, and how you can help us help you resolve your support cases as fast as possible. If you enjoyed this blog post, email us at <a href="mailto:cloud-support-blog@google.com">cloud-support-blog@google.com</a> and we’ll dig around in our greatest hits archive for other tough support cases to discuss.</p></div></paragraph-block><!----><!----><!----></div></div></div><!----><!----><!----><!----><!----><!----></div></article-content-stream-block><!----><article-tag-list-block><!----></article-tag-list-block></div><related-articles-block _nghost-c14=""></related-articles-block><related-articles-drawer-block _nghost-c15=""></related-articles-drawer-block></div></section></article></main></article-page></div></div></div>
    </div>
    <footer>
        <div>created by buildstarted &copy; 2020 <a href="/about">about</a></div>
        <div>Share this page on social media: copy and paste this url https://linksfor.dev/</div>
        <div>If you prefer RSS: <a href="https://linksfor.dev/feed.xml">https://linksfor.dev/feed.xml</a></div>
    </footer>
    
    <script async defer>
        _dna = window._dna || {};
        _dna.siteId = "linksfor.devs";
        _dna.outlink = true;

        (function () {
            let dna = document.createElement('script');
            dna.type = 'text/javascript';
            dna.async = true;
            dna.src = '//dna.buildstarted.com/t.js';
            let s = document.getElementsByTagName('script')[0];
            s.parentNode.insertBefore(dna, s);
        })();
    </script>
    <noscript><img src="//dna.buildstarted.com/g?siteId=linksfor.devs"/></noscript>
</body>
</html>