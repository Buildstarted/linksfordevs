<!DOCTYPE html>
<html lang="en">
<head>
    <title>
Why use Azure Functions for ML inference ? - linksfor.dev(s)    </title>
    <meta charset="utf-8">
    <link rel="alternate" type="application/rss+xml" title="Linksfor.dev(s) feed" href="https://linksfor.dev/feed.rss" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta name="google" value="notranslate">
    <link rel="stylesheet" type="text/css" href="/style.css" />
    <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon">
    <link rel="icon" href="/favicon.ico" type="image/x-icon">
    <meta property="og:title" content="Why use Azure Functions for ML inference ? - linksfor.dev(s)"/>
    <meta property="article:author" content="https://techcommunity.microsoft.com/t5/user/viewprofilepage/user-id/53732"/>
    <meta property="og:description" content="This article highlights the current limitations in offerings by FaaS providers for ML inference applications and how capabilities in Azure Functions overcome these making it an ideal platform for such applications. It also links to an end to end sample illustrating these features. ML Inference in Fa..."/>
    <meta property="og:type" content="website"/>
    <meta property="og:url" content="https://techcommunity.microsoft.com/t5/azure-functions/why-use-azure-functions-for-ml-inference/ba-p/1416728"/>

<meta property="og:site_name" content="linksfor.dev(s)" />
</head>
<body>
    <div class="devring" style="background: #222">
        <div style="text-align:center">Explore other dev related sites in this ring. If you would like to join this ring <a href="https://devring.club">click here</a>.</div>
        <div class="grid">
            <div style="display: grid; grid-template-columns: .5fr 1fr 1fr 1fr; text-align: center;">
                <span class="devring-title"><a href="https://devring.club/">devring.club</a></span>
                <a href="https://devring.club/sites/1/prev" class="devring-previous">Previous</a>
                <a href="https://devring.club/random" class="devring-random">Random</a>
                <a href="https://devring.club/sites/1/next" class="devring-next">Next</a>
            </div>
        </div>
    </div>
    <div class="grid">
        <h1>
<a href="/" style="color:inherit">linksfor.dev(s)</a>
        </h1>
        <title>linksfor.dev(s) - Why use Azure Functions for ML inference ?</title>
<div class="readable">
        <h1>Why use Azure Functions for ML inference ?</h1>
            <div>by https://techcommunity.microsoft.com/t5/user/viewprofilepage/user-id/53732</div>
            <div>Reading time: 10-12 minutes</div>
        <div>Posted here: 29 May 2020</div>
        <p><a href="https://techcommunity.microsoft.com/t5/azure-functions/why-use-azure-functions-for-ml-inference/ba-p/1416728">https://techcommunity.microsoft.com/t5/azure-functions/why-use-azure-functions-for-ml-inference/ba-p/1416728</a></p>
        <hr/>
<div id="readability-page-1" class="page"><div>
		<div itemprop="text" id="bodyDisplay">
	
		<div>
			
				
					
					
						<blockquote>
<p>This article highlights the current limitations in offerings by FaaS providers for ML inference applications and how capabilities in Azure Functions overcome these making it an ideal platform for such applications. It also links to an end to end <a href="#sample" target="_blank" rel="noopener nofollow noopener noreferrer">sample</a> illustrating these features.</p>
</blockquote>
<h2 id="ml-inference-in-faas-services">ML Inference in FaaS Services</h2>
<p>The core promise of Serverless/FaaS (Functions as a Service) computing is to make it easier for developers to build, deploy and scale up their applications. Among the different kinds of applications - ML applications have seen an explosion in their usage and are now rapidly being deployed and bringing unprecedented novel capabilities in various domains.</p>
<p>A typical ML workflow consists of using sample data to train a model and then use that model in an application to make predictions based on input. These stages are typically called <em>training</em> and <em>inference</em> respectively. On the surface, ML <em>inference</em> should be a straightforward FaaS use case as essentially there is a single "predict" function that is called to "infer" the result for a particular input. However, due to various limits that typical FaaS platforms impose it hasn't been as optimum a platform that it can be. These are some of the limitations:</p>
<h3 id="1-ml-libraries-can-be-big-">1. <strong>ML <em>libraries</em> can be big</strong></h3>
<p>ML(especially Deep learning) libraries can be big in size. For example the <a href="https://pytorch.org/" target="_blank" rel="noopener nofollow noopener noreferrer">PyTorch</a> (even the non-CUDA version) library is around ~300MB. However, most FaaS providers only allow a maximum of ~250MB-~500MB of deployed package size <em>including</em> any modules needed in the application. Customers have to resort to getting around to these limitations by compressing shared libraries, removing test folders etc.</p>
<h3 id="2-python-ml-libraries-use-native-dependencies-">2. <strong>Python ML <em>libraries</em> use native dependencies</strong></h3>
<p>Many Python ML libraries call into other C/C++ libraries, those need to be recompiled based on the OS causing a mismatch between the development and production deployments. For instance, developers using Windows have to resort to using Docker before deploying their FaaS application.</p>
<h3 id="3-ml-models-can-be-big-">3. <strong>ML <em>models</em> can be big</strong></h3>
<p>Some of the latest ML models can be very big in size - for example the <a href="https://arxiv.org/abs/1404.5997" target="_blank" rel="noopener nofollow noopener noreferrer">AlexNet</a> pre-trained <a href="https://pytorch.org/docs/stable/torchvision/models.html#id1" target="_blank" rel="noopener nofollow noopener noreferrer">PyTorch model</a> is 230MB. Some models like <a href="https://openai.com/blog/gpt-2-1-5b-release/" target="_blank" rel="noopener nofollow noopener noreferrer">GPT-2</a> can be several GB in size. FaaS offerings have a limited deployment size package as mentioned above - further, they don't offer a way to separate out the model from the code by mounting a file system for instance.</p>
<p>Further, some large models need to be completely loaded into memory before they can be used and amount of memory offered in current FaaS offerings might not be enough.</p>
<h3 id="4-concurrency-models-for-i-o-bound-invocations-are-sub-optimal-">4. <strong>Concurrency models for I/O bound invocations are sub-optimal</strong></h3>
<p>Many ML inference API's are typically I/O bound, however, in current FaaS execution frameworks a single instance cannot handle concurrent requests. The can lead to a lot of wasted idle resources on individual instances.</p>
<h3 id="5-cold-start-">5. <strong>Cold start</strong></h3>
<p>FaaS workloads scale down to zero instances when not being used, therefore starting a new instance always incurs lag - which is called cold start. For many applications this lag is not acceptable.</p>
<h3 id="6-gpu-support-is-not-available-">6. <strong>GPU support is not available</strong></h3>
<p>Many inference applications have a very stringent latency requirements (think for instance applications related to providing auto-suggestions as you type). To meet these latency requirements often (especially for bigger models) GPU processing can lead to several times of improvement. However FaaS providers currently do not support GPU's.</p>
<h3 id="7-no-native-support-for-long-running-ml-workloads-and-or-orchestration-patterns-">7. <strong>No native support for long running ML workloads and/or orchestration patterns</strong></h3>
<p>Some ML workloads might be inherently long running and/or have complex orchestration needs. Current FaaS providers do not offer native frameworks for these scenarios and customers need to resort to external services and built the complex coordination logic themselves.</p>

<h2 id="azure-functions-python">Azure Functions Python</h2>
<p>In Azure Functions Python capabilities and features have been added to overcome some of the above limitations and make it a first class option for ML inference with all the traditional FaaS benefits of scaling on demand, fast iteration and pay-for-use.</p>
<p><img src="https://raw.githubusercontent.com/Azure-Samples/azure-functions-pytorch-image-identify/master/media/teaser.png" alt="azure-functions-ml"></p>
<h3 id="1-higher-deployment-package-sizes-">1. <strong>Higher deployment package sizes</strong></h3>
<p>Azure Functions support higher deployment packages as much as several GB in size. This means that even larger deep learning frameworks like TensorFlow and PyTorch can be supported out of the box without resorting to having to reduce their size.</p>
<h3 id="2-support-for-remote-build-">2. <strong>Support for Remote Build</strong></h3>
<p>As mentioned above, many Python libraries have native dependencies. This means if you are developing your application on Windows and then deploy it in production which typically is on Linux - it might not work as they have to be recompiled to work on another operating system.</p>
<p>To help with this problem in Azure Functions we built the <a href="https://docs.microsoft.com/en-us/azure/azure-functions/functions-reference-python#remote-build" target="_blank" rel="noopener noopener noreferrer">Remote Build</a> functionality, where we automatically compile and pull in the right libraries on the server side. Hence, they don't have to be included in your deployment package locally and only need to be referenced in your requirements.txt file. This is the default experience when using VSCode or the func core tools to deploy your Python Azure Functions application. This is a snippet from the deployment logs showing the libraries being installed on the server side.</p>
<pre><code>12:02:54 AM pytorch-image-consumption: Starting deployment...
.
12:03:01 AM pytorch-image-consumption: Running pip install...
...
12:03:14 AM pytorch-image-consumption: [07:03:14<span>+0000</span>] Collecting torch==1.4.0+cpu
12:03:14 AM pytorch-image-consumption: [07:03:14<span>+0000</span>]   Downloading https://download.pytorch.org/whl/cpu/torch<span>-1</span>.4.0%2Bcpu-cp37-cp37m-linux_x86_64.whl (127.2MB)
12:03:45 AM pytorch-image-consumption: [07:03:45<span>+0000</span>] Collecting torchvision==0.5.0
12:03:45 AM pytorch-image-consumption: [07:03:45<span>+0000</span>]   Downloading https://files.pythonhosted.org/packages/1c/32/cb0e4c43cd717da50258887b088471568990b5a749784c465a8a1962e021/torchvision<span>-0</span>.5.0-cp37-cp37m-manylinux1_x86_64.whl (4.0MB)
....
12:04:08 AM pytorch-image-consumption: [07:04:08<span>+0000</span>] Successfully installed azure-functions<span>-1</span>.2.1 certifi<span>-2020</span>.4.5.1 chardet<span>-3</span>.0.4 idna<span>-2</span>.9 numpy<span>-1</span>.15.4 pillow<span>-7</span>.1.2 requests<span>-2</span>.23.0 six<span>-1</span>.14.0 torch<span>-1</span>.4.0+cpu torchvision<span>-0</span>.5.0 urllib3<span>-1</span>.25.9
</code></pre>
<h3 id="3-support-for-mounting-an-external-file-system-through-azure-files-">3. <strong>Support for mounting an external file system through Azure Files</strong></h3>
<p>To allow for arbitrarily large ML models and to separate the workflow of building the application and training your ML model the capability to automatically mount a <a href="https://docs.microsoft.com/en-us/azure/azure-functions/scripts/functions-cli-mount-files-storage-linux" target="_blank" rel="noopener noopener noreferrer">configured Azure file share</a> has been recently introduced.</p>
<p>This is as simple as using an Azure CLI command where the <em>$shareName</em> Azure File share will automatically be mounted whenever the <em>$functionAppName</em> Function app starts up. The path available to the Function app will be with the name of <em>$mountPath</em>.</p>
<pre><code>az webapp config storage-account <span>add</span> <span></span>
  --resource-group myResourceGroup <span></span>
  --name <span>$functionAppName</span> <span></span>
  --custom-id <span>$shareId</span> <span></span>
  --storage-type AzureFiles <span></span>
  --share-name <span>$shareName</span> <span></span>
  --account-name <span>$AZURE_STORAGE_ACCOUNT</span> <span></span>
  --mount-path <span>$mountPath</span> <span></span>
  --access-key <span>$AZURE_STORAGE_KEY</span>
</code></pre>
<p>This is an Azure Files storage view with two different models.</p>
<p><img src="https://raw.githubusercontent.com/Azure-Samples/azure-functions-pytorch-image-identify/master/media/storage-account-file-share.png" alt="azure-files"></p>
<p>This functionality is available both in the Consumption and Premium plans. It is also worth noting that <a href="https://docs.microsoft.com/en-us/azure/storage/files/storage-how-to-create-premium-fileshare" target="_blank" rel="noopener noopener noreferrer">Azure Files have a Premium tier</a> which can be used as well - it offers a higher level of performance i.e. high throughput and low latency for IO-intensive workloads(many ML applications)</p>
<blockquote>
<p>The above two features Remote Build and support for Azure Files allow the deployment package to be much smaller. For example, in the PyTorch <a href="#sample" target="_blank" rel="noopener nofollow noopener noreferrer">sample</a> referenced below the total package size would have been close to 520MB in size when using the resnet100 model (~350MB for PyTorch and ~170MB for the model) while without it it is barely 50KB in size.</p>
</blockquote>
<h3 id="4-azure-functions-concurrency-model-is-optimal-for-i-o-bound-invocations-">4. <strong>Azure Functions concurrency model is optimal for I/O bound invocations</strong></h3>
<p>In Azure Functions - the same instance is re-used for concurrent calls. This can lead to optimum use of resources especially for <a href="https://docs.microsoft.com/en-us/azure/azure-functions/functions-reference-python#scaling-and-concurrency" target="_blank" rel="noopener noopener noreferrer">concurrent I/O bound invocations</a> where the underlying api has async variants.</p>
<h3 id="5-azure-functions-premium-sku-s-offer-higher-memory-limits-and-no-cold-start-">5. <strong>Azure Functions Premium SKU's offer higher memory limits and no cold start</strong></h3>
<p><a href="https://docs.microsoft.com/en-us/azure/azure-functions/functions-premium-plan" target="_blank" rel="noopener noopener noreferrer">The Azure Functions Premium SKU</a> offers <a href="https://docs.microsoft.com/en-us/azure/azure-functions/functions-premium-plan#available-instance-skus" target="_blank" rel="noopener noopener noreferrer">higher memory limits - up to 14GB</a> for cases where you would want to load the entire model in memory.</p>
<table>
<thead>
<tr>
<th>SKU</th>
<th>Cores</th>
<th>Memory</th>
<th>Storage</th>
</tr>
</thead>
<tbody>
<tr>
<td>EP1</td>
<td>1</td>
<td>3.5GB</td>
<td>250GB</td>
</tr>
<tr>
<td>EP2</td>
<td>2</td>
<td>7GB</td>
<td>250GB</td>
</tr>
<tr>
<td>EP3</td>
<td>4</td>
<td>14GB</td>
<td>250GB</td>
</tr>
</tbody>
</table>
<p data-line="95">It also avoids&nbsp;<a title="https://docs.microsoft.com/en-us/azure/azure-functions/functions-premium-plan#pre-warmed-instances" href="https://docs.microsoft.com/en-us/azure/azure-functions/functions-premium-plan#pre-warmed-instances" target="_blank" rel="noopener noopener noreferrer" data-href="https://docs.microsoft.com/en-us/azure/azure-functions/functions-premium-plan#pre-warmed-instances">cold start</a><span>&nbsp;</span>for those applications where there cannot be any latency at all.&nbsp;</p>
<p data-line="97">Further, the<span>&nbsp;</span><a title="https://docs.microsoft.com/en-us/azure/azure-functions/functions-create-function-linux-custom-image?tabs=bash%2Cportal&amp;pivots=programming-language-python" href="https://docs.microsoft.com/en-us/azure/azure-functions/functions-create-function-linux-custom-image?tabs=bash%2Cportal&amp;pivots=programming-language-python" target="_blank" rel="noopener noopener noreferrer" data-href="https://docs.microsoft.com/en-us/azure/azure-functions/functions-create-function-linux-custom-image?tabs=bash%2Cportal&amp;pivots=programming-language-python">custom container support</a><span>&nbsp;</span>enables integration with the<span>&nbsp;</span><a title="https://docs.microsoft.com/en-us/azure/machine-learning/how-to-deploy-functions" href="https://docs.microsoft.com/en-us/azure/machine-learning/how-to-deploy-functions" target="_blank" rel="noopener noopener noreferrer" data-href="https://docs.microsoft.com/en-us/azure/machine-learning/how-to-deploy-functions">Azure Machine Learning service</a><span>&nbsp;</span>such that you can package a model from the Azure ML service into an Azure Function Premium app and allows the use of a<span>&nbsp;</span><a title="https://github.com/Hazhzeng/functions-conda" href="https://github.com/Hazhzeng/functions-conda" target="_blank" rel="noopener noopener noreferrer" data-href="https://github.com/Hazhzeng/functions-conda">Conda</a><span>&nbsp;</span>environment.</p>
<h3 id="6-deploy-azure-functions-to-a-kubernetes-cluster-with-gpu-support-">6. <strong>Deploy Azure Functions to a Kubernetes cluster with GPU support</strong></h3>
<p>Finally, as in Azure Functions the <a href="https://github.com/Azure/azure-functions-host" target="_blank" rel="noopener noopener noreferrer">programming model</a> is separate from the deployment target, an Azure Function app can be deployed to a Kubernetes cluster (it can be scaled up and down using <a href="http://keda.sh/" target="_blank" rel="noopener nofollow noopener noreferrer">KEDA</a>). Many managed Kubernetes offering (including <a href="https://docs.microsoft.com/en-us/azure/aks/gpu-cluster" target="_blank" rel="noopener noopener noreferrer">AKS</a>) have GPU support.</p>
<h3 id="7-orchestrating-inference-invocations-durable-functions-python-">7. <strong>Orchestrating inference invocations - Durable Functions Python</strong></h3>
<p>With the impending release of <a href="https://github.com/Azure/azure-functions-durable-python" target="_blank" rel="noopener noopener noreferrer">Durable Functions Python</a> long running ML workloads can now be supported. Durable Functions allows many different orchestration use cases including processing multiple inputs in a batch, parallelizing classification tasks etc. These are enabled by the <a href="https://github.com/Azure/azure-functions-durable-python/tree/dev/samples/function_chaining" target="_blank" rel="noopener noopener noreferrer">Chaining</a>, <a href="https://github.com/Azure/azure-functions-durable-python/tree/dev/samples/fan_out_fan_in" target="_blank" rel="noopener noopener noreferrer">Fan-Out Fan-In</a> and other Durable Function patterns. <a href="https://github.com/Azure/azure-functions-durable-python/tree/dev/samples/fan_out_fan_in_tensorflow" target="_blank" rel="noopener noopener noreferrer">This sample</a> illustrates the use of the Fan-Out Fan-In pattern doing parallel image classification using <a href="https://www.tensorflow.org/" target="_blank" rel="noopener nofollow noopener noreferrer">TensorFlow</a>.</p>

<h2 id="sample">Sample</h2>
<p>To put this all together and show a real end to end example - please find a <a href="https://github.com/Azure-Samples/azure-functions-pytorch-image-identify" target="_blank" rel="noopener noopener noreferrer">step by step walk through example of an Azure Function app</a>&nbsp;which shows how these&nbsp;<a href="https://pytorch.org/docs/stable/torchvision/models.html#classification" target="_blank" rel="noopener nofollow noopener noreferrer">pre-trained PyTorch models</a>&nbsp;can be loaded from an Azure File share in an ML inference application which classifies images.</p>

<p><img src="https://raw.githubusercontent.com/Azure-Samples/azure-functions-pytorch-image-identify/master/media/pytorch-models.png" alt="pytorch-models"></p>
<p>It illustrates the use of <a href="https://docs.microsoft.com/en-us/azure/azure-functions/functions-reference-python#remote-build" target="_blank" rel="noopener noopener noreferrer">Remote Build</a>, gives guidance on how you can deploy the same app to the Premium plan for no-cold start and higher memory limits and gives guidance on how the same app can be deployed to a AKS cluster with GPU. More end to end examples will be added soon for text generation and other ML tasks.</p>
<p>Hopefully this blog post encourages you to consider Azure Functions for your ML inference applications. Please ask any questions in the comments, or open any issues in the <a href="https://github.com/Azure/azure-functions-python-worker" target="_blank" rel="noopener noopener noreferrer">Azure Functions Python Worker repo</a> or in the <a href="https://github.com/Azure-Samples/azure-functions-pytorch-image-identify" target="_blank" rel="noopener noopener noreferrer">sample repo</a>.</p>
					
				
			
			
			
				
			
			
			
			
			
			
		</div>
		
		
	

	
	
</div>
	</div></div></div>
    </div>
    <footer>
        <div>created by <a href="https://buildstarted.com">buildstarted</a> &copy; 2020 <a href="/about">about</a></div>
        <div>Share this page on social media: copy and paste this url https://linksfor.dev/</div>
        <div>If you prefer RSS: <a href="https://linksfor.dev/feed.xml">https://linksfor.dev/feed.xml</a></div>
        <div>Customer satisfaction guaranteed to be optional.</div>
    </footer>
    
    <script async defer>
        _dna = window._dna || {};
        _dna.siteId = "linksfor.devs";
        _dna.outlink = true;

        (function() {
            let dna = document.createElement('script');
            dna.type = 'text/javascript';
            dna.async = true;
            dna.src = '//dna.buildstarted.com/t.js';
            let s = document.getElementsByTagName('script')[0];
            s.parentNode.insertBefore(dna, s);
        })();
    </script>
    <noscript><img src="//dna.buildstarted.com/g?siteId=linksfor.devs"/></noscript>
</body>
</html>