<!DOCTYPE html>
<html lang="en">
<head>
    <title>linksfor.dev(s)</title>
    <meta charset="utf-8">
    <link rel="alternate" type="application/rss+xml" title="Linksfor.dev(s) feed" href="https://linksfor.dev/feed.rss" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta name="google" value="notranslate">
    <link rel="stylesheet" type="text/css" href="/style.css" />
    <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon">
    <link rel="icon" href="/favicon.ico" type="image/x-icon">
</head>
<body>
    <div class="grid">
        <h1>
                <span style="cursor: default" title="linksfor.dev(s) has been running for 1 year! :partypopper:">🎉</span>
<a href="/" style="color:inherit">linksfor.dev(s)</a>
        </h1>
        <div class="readable">
        <h1>10 Gradient Descent Optimisation Algorithms</h1>
        <p>
by Raimi Karim <br/>Reading time: 10-12 minutes        </p>
        <p><a href="https://towardsdatascience.com/10-gradient-descent-optimisation-algorithms-86989510b5e9?gi=fb386cd474d">https://towardsdatascience.com/10-gradient-descent-optimisation-algorithms-86989510b5e9?gi=fb386cd474d</a></p>
        <hr/>
<div id="readability-page-1" class="page"><section><div><div><p id="649f"><h2>Gradient descent optimisation algorithms you should know for deep learning</h2></p><div><div><div><p><a href="https://towardsdatascience.com/@remykarem?source=post_page-----86989510b5e9----------------------" rel="noopener"><img alt="Raimi Karim" src="https://miro.medium.com/fit/c/48/48/1*BEc6O6Z6fCy2NDHL9K462A@2x.jpeg" width="48" height="48"></a></p></div></div></div><p id="7674" data-selectable-paragraph=""><em>(I maintain a cheat sheet of these optimisers including RAdam in my blog </em><a href="https://remykarem.github.io/blog/gradient-descent-optimisers.html" target="_blank" rel="noopener nofollow"><em>here</em></a><em>.)</em></p><p id="a1e4" data-selectable-paragraph=""><em>Change logs: <br>6 Oct 2019: Corrected the idea of EMA of gradients.<br>22 Sep 2019: Rearranged the order in which optimisers appear and removed ‘evolutionary map’.</em></p><p id="0c71" data-selectable-paragraph=""><span>G</span>radient descent is an optimisation method for finding the minimum of a function. It is commonly used in deep learning models to update the weights of the neural network through backpropagation.</p><p id="b083" data-selectable-paragraph="">In this post, I will summarise the common gradient descent optimisation algorithms used in popular deep learning frameworks (e.g. TensorFlow, Keras, PyTorch, Caffe). The purpose of this post is to make it easy to read and digest (using consistent nomenclature) since there aren’t many of such summaries out there, and as a cheat sheet if you want to implement them from scratch.</p><p id="780c" data-selectable-paragraph="">I have implemented SGD, momentum, Nesterov, RMSprop and Adam in a linear regression problem using gradient descent demo <a href="https://raiboso.me/backpropagation-demo/" target="_blank" rel="noopener nofollow">here</a> using JavaScript.</p><h2 id="ff91" data-selectable-paragraph="">What do gradient descent optimisers do?</h2><p id="68f0" data-selectable-paragraph="">There are 3 main ways how these optimisers can act upon gradient descent:<br>(1) modifying the learning rate component, <strong><em>α</em></strong>, or<br>(2) modifying the gradient component, <strong><em>∂L/∂w</em></strong>, or<br>(3) both.<br>See the last term in Eqn. 1 below:</p><figure><div><div><div><p><img src="https://miro.medium.com/max/30/1*Bjh6q72YAx9RD-z4i3h3pA@2x.png?q=20" width="420" height="120" role="presentation"></p><p><img width="420" height="120" role="presentation" src="https://miro.medium.com/max/420/1*Bjh6q72YAx9RD-z4i3h3pA@2x.png"></p></div></div></div><figcaption data-selectable-paragraph="">Eqn. 1: The terms in stochastic gradient descent</figcaption></figure><blockquote><p id="56b3" data-selectable-paragraph=""><strong><em>Learning rate schedulers vs. Gradient descent optimisers</em><br></strong>The main difference between these two is that gradient descent optimisers adapt the learning rate component by multiplying the learning rate with a factor that is a function of the gradients, whereas learning rate schedulers multiply the learning rate by a factor which is a constant or a function of the time step.</p></blockquote><p id="6350" data-selectable-paragraph="">For (1), these optimisers multiply a positive factor to the learning rate, such that they become smaller (e.g. RMSprop). For (2), optimisers usually make use of the moving averages of the gradient (momentum), instead of just taking one value like in vanilla gradient descent. Optimisers that act on both (3) are like Adam and AMSGrad.</p><figure><div><div><div><div><p><img src="https://miro.medium.com/max/30/1*0xqraYy8AMQ6OvjbC3WP8w@2x.png?q=20" width="952" height="498" role="presentation"></p><p><img width="952" height="498" role="presentation"></p></div></div></div></div><figcaption data-selectable-paragraph="">Fig. 2: Gradient descent optimisers, the year in which the papers were published, and the components they act upon</figcaption></figure><p id="0845" data-selectable-paragraph="">&lt;change log 22 Sep 2019: removed evolutionary map&gt;</p><p id="22cc" data-selectable-paragraph="">Notations</p><ul><li id="2d50" data-selectable-paragraph=""><em>t — </em>time step</li><li id="b529" data-selectable-paragraph=""><em>w — </em>weight/parameter which we want to update</li><li id="bb81" data-selectable-paragraph=""><em>α — </em>learning rate</li><li id="8114" data-selectable-paragraph=""><em>∂L/∂w — </em>gradient of <em>L</em>, the loss function to minimise, w.r.t. to <em>w</em></li><li id="7427" data-selectable-paragraph="">I have also standardised the notations and Greek letters used in this post (hence might be different from the papers) so that we can explore how optimisers ‘evolve’ as we scroll.</li></ul></div></div></section><section><div><div><h2 id="89a8" data-selectable-paragraph="">1. Stochastic Gradient Descent</h2><p id="538f" data-selectable-paragraph="">The vanilla gradient descent updates the current weight <em>w </em>using the current gradient <em>∂L/∂w </em>multiplied by some factor called the learning rate, <em>α.</em></p><figure><div><div><div><p><img src="https://miro.medium.com/max/30/1*kvwgxqwUskRcuXUCr_GFZA@2x.png?q=20" width="454" height="130" role="presentation"></p><p><img width="454" height="130" role="presentation"></p></div></div></div></figure><h2 id="e802" data-selectable-paragraph="">2. Momentum</h2><p id="74af" data-selectable-paragraph="">Instead of depending only on the current gradient to update the weight, gradient descent with momentum (<a href="https://www.researchgate.net/publication/243648538_Some_methods_of_speeding_up_the_convergence_of_iteration_methods" target="_blank" rel="noopener nofollow">Polyak, 1964</a>) replaces the current gradient with <em>V </em>(which stands for velocity), the exponential moving average of current and past gradients (i.e. up to time <em>t</em>). Later in this post you will see that this momentum update becomes the standard update for the gradient component.</p><figure><div><div><div><p><img src="https://miro.medium.com/max/30/1*InCwyOtMRcrDXRDxa1xbOg@2x.png?q=20" width="402" height="54" role="presentation"></p><p><img width="402" height="54" role="presentation"></p></div></div></div></figure><p id="7d64" data-selectable-paragraph="">where</p><figure><div><div><div><div><p><img src="https://miro.medium.com/max/30/1*E54ClPDlKFdnfxtQPJL-Ig@2x.png?q=20" width="610" height="126" role="presentation"></p><p><img width="610" height="126" role="presentation"></p></div></div></div></div></figure><p id="32ca" data-selectable-paragraph="">and <em>V</em> initialised to 0.</p><p id="74ec" data-selectable-paragraph="">Common default value:</p><ul><li id="e29e" data-selectable-paragraph=""><em>β = </em>0.9</li></ul><blockquote><p id="23ff" data-selectable-paragraph="">Note that many articles reference the momentum method to the publication by <a href="https://www.semanticscholar.org/paper/On-the-momentum-term-in-gradient-descent-learning-Qian/735d4220d5579cc6afe956d9f6ea501a96ae99e2" target="_blank" rel="noopener nofollow">Ning Qian, 1999</a>. However, the paper titled <a href="http://proceedings.mlr.press/v28/sutskever13.pdf" target="_blank" rel="noopener nofollow">Sutskever et al.</a> attributed the classical momentum to a much earlier publication by Polyak in 1964, as cited above. (Thank you to <a href="https://news.ycombinator.com/item?id=18525494#18528682" target="_blank" rel="noopener nofollow">James</a> for pointing this out.)</p></blockquote><h2 id="1c7a" data-selectable-paragraph="">3. AdaGrad</h2><p id="9eb2" data-selectable-paragraph="">Adaptive gradient, or AdaGrad (<a href="http://jmlr.org/papers/v12/duchi11a.html" target="_blank" rel="noopener nofollow">Duchi et al., 2011</a>), works on the learning rate component by dividing the learning rate by the square root of <em>S</em>, which is the cumulative sum of current and past squared gradients (i.e. up to time <em>t</em>). Note that the gradient component remains unchanged like in SGD.</p><figure><div><div><div><p><img src="https://miro.medium.com/max/30/1*ZfgWLaJcGAIhzfxk3x1uQQ@2x.png?q=20" width="662" height="132" role="presentation"></p><p><img width="662" height="132" role="presentation"></p></div></div></div></figure><p id="d11b" data-selectable-paragraph="">where</p><figure><div><div><div><p><img src="https://miro.medium.com/max/30/1*OZ1C828CHMc2CKveEXfp5g@2x.png?q=20" width="490" height="148" role="presentation"></p><p><img width="490" height="148" role="presentation"></p></div></div></div></figure><p id="be16" data-selectable-paragraph="">and <em>S</em> initialised to 0.</p><p id="b525" data-selectable-paragraph="">Notice that <em>ε </em>is added to the denominator. Keras calls this the <em>fuzz factor</em>, a small floating point value to ensure that we will never have to come across division by zero.</p><p id="484a" data-selectable-paragraph="">Default values (from <a href="https://keras.io/optimizers/#adagrad" target="_blank" rel="noopener nofollow">Keras</a>):</p><ul><li id="ad73" data-selectable-paragraph=""><em>α</em> = 0.01</li><li id="1b99" data-selectable-paragraph=""><em>ε = </em>10⁻⁷</li></ul><h2 id="ef85" data-selectable-paragraph="">4. RMSprop</h2><p id="9bbc" data-selectable-paragraph="">Root mean square prop or RMSprop (<a href="http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf" target="_blank" rel="noopener nofollow">Hinton et al., 2012</a>) is another adaptive learning rate that is an improvement of AdaGrad. Instead of taking cumulative sum of squared gradients like in AdaGrad, we take the exponential moving average of these gradients.</p><figure><div><div><div><p><img src="https://miro.medium.com/max/30/1*qUXAAhjIijefqNi9O8Ck_w@2x.png?q=20" width="662" height="132" role="presentation"></p><p><img width="662" height="132" role="presentation"></p></div></div></div></figure><p id="3ea0" data-selectable-paragraph="">where</p><figure><div><div><div><p><img src="https://miro.medium.com/max/30/1*3ms0_D00ckaG2J4g182lUw@2x.png?q=20" width="700" height="148" role="presentation"></p><p><img width="700" height="148" role="presentation"></p></div></div></div></figure><p id="9b73" data-selectable-paragraph="">and <em>S</em> initialised to 0.</p><p id="17e0" data-selectable-paragraph="">Default values (from <a href="https://keras.io/optimizers/#rmsprop" target="_blank" rel="noopener nofollow">Keras</a>):</p><ul><li id="9507" data-selectable-paragraph=""><em>α</em> = 0.001</li><li id="1bc7" data-selectable-paragraph=""><em>β</em> = 0.9 (recommended by the authors of the paper)</li><li id="9332" data-selectable-paragraph=""><em>ε = </em>10⁻⁶</li></ul><h2 id="8320" data-selectable-paragraph="">5. Adadelta</h2><p id="adaa" data-selectable-paragraph="">Like RMSprop, Adadelta (<a href="https://arxiv.org/abs/1212.5701" target="_blank" rel="noopener nofollow">Zeiler, 2012</a>) is also another improvement from AdaGrad, focusing on the learning rate component. Adadelta is probably short for ‘adaptive delta’, where <em>delta</em> here refers to the difference between the current weight and the newly updated weight.</p><p id="0183" data-selectable-paragraph="">The difference between Adadelta and RMSprop is that Adadelta removes the use of the learning rate parameter completely by replacing it with <em>D,</em> the exponential moving average of squared <em>deltas</em>.</p><figure><div><div><div><div><p><img src="https://miro.medium.com/max/30/1*Y9hSuM7lorpWtSysJ63y7Q@2x.png?q=20" width="740" height="142" role="presentation"></p><p><img width="740" height="142" role="presentation"></p></div></div></div></div></figure><p id="afac" data-selectable-paragraph="">where</p><figure><div><div><div><div><p><img src="https://miro.medium.com/max/30/1*FCQJfccXuK6QaJlSMGXnQA@2x.png?q=20" width="764" height="256" role="presentation"></p><p><img width="764" height="256" role="presentation"></p></div></div></div></div></figure><p id="6158" data-selectable-paragraph="">with <em>D </em>and <em>S</em> initialised to 0, and</p><figure><div><div><div><p><img src="https://miro.medium.com/max/30/1*TscI4WyJTVJaBOXILBoO3A@2x.png?q=20" width="418" height="50" role="presentation"></p><p><img width="418" height="50" role="presentation"></p></div></div></div></figure><p id="7669" data-selectable-paragraph="">Default values (from <a href="https://keras.io/optimizers/#adadelta" target="_blank" rel="noopener nofollow">Keras</a>):</p><ul><li id="e480" data-selectable-paragraph=""><em>β</em> = 0.95</li><li id="3797" data-selectable-paragraph=""><em>ε = </em>10⁻⁶</li></ul><h2 id="ab07" data-selectable-paragraph="">6. Nesterov Accelerated Gradient (NAG)</h2><p id="809a" data-selectable-paragraph="">After Polyak had gained his momentum (pun intended 😬), a similar update was implemented using Nesterov Accelerated Gradient (<a href="http://proceedings.mlr.press/v28/sutskever13.html" target="_blank" rel="noopener nofollow">Sutskever et al., 2013</a>). This update utilises <em>V</em>, the exponential moving average of what I would call <em>projected gradients.</em></p><figure><div><div><div><p><img src="https://miro.medium.com/max/30/1*Lv3gFs4JGGExNQT-yMTG4A@2x.png?q=20" width="402" height="54" role="presentation"></p><p><img width="402" height="54" role="presentation"></p></div></div></div></figure><p id="c4b7" data-selectable-paragraph="">where</p><figure><div><div><div><p><img src="https://miro.medium.com/max/30/1*761QlS02jCLy2JN7cFhOkA@2x.png?q=20" width="618" height="120" role="presentation"></p><p><img width="618" height="120" role="presentation"></p></div></div></div></figure><p id="ae98" data-selectable-paragraph="">and <em>V</em> initialised to 0.</p><p id="acb2" data-selectable-paragraph="">The last term in the second equation is a <em>projected gradient</em>. This value can be obtained by going ‘one step ahead’ using the previous velocity (Eqn. 4). This means that for this time step <em>t</em>, we have to carry out another forward propagation before we can finally execute the backpropagation. Here’s how it goes:</p><ol><li id="002c" data-selectable-paragraph="">Update the current weight <em>w </em>to a <em>projected weight w* </em>using the previous velocity.</li></ol><figure><div><div><div><p><img src="https://miro.medium.com/max/30/1*-T21J7ihoIimw-vc2tii8w@2x.png?q=20" width="408" height="52" role="presentation"></p><p><img width="408" height="52" role="presentation"></p></div></div></div><figcaption data-selectable-paragraph="">Eqn. 4</figcaption></figure><p id="4290" data-selectable-paragraph="">2. Carry out forward propagation, but using this <em>projected weight</em>.</p><p id="3391" data-selectable-paragraph=""><em>3. </em>Obtain the <em>projected gradient</em> <em>∂L/∂w*</em>.</p><p id="cf8c" data-selectable-paragraph="">4. Compute <em>V</em> and <em>w </em>accordingly.</p><p id="2fe3" data-selectable-paragraph="">Common default value:</p><ul><li id="c59a" data-selectable-paragraph=""><em>β = </em>0.9</li></ul><blockquote><p id="d1a4" data-selectable-paragraph="">Note that the original Nesterov Accelerated Gradient paper (<a href="http://www.cis.pku.edu.cn/faculty/vision/zlin/1983-A%20Method%20of%20Solving%20a%20Convex%20Programming%20Problem%20with%20Convergence%20Rate%20O(k%5E(-2))_Nesterov.pdf" target="_blank" rel="noopener nofollow">Nesterov, 1983</a>) was not about <em>stochastic</em> gradient descent and did not explicitly use the gradient descent equation. Hence, a more appropriate reference is the above-mentioned publication by Sutskever et al. in 2013, which described NAG’s application in stochastic gradient descent. (Again, I’d like to thank James’s <a href="https://news.ycombinator.com/item?id=18525494#18528682" target="_blank" rel="noopener nofollow">comment</a> on HackerNews for pointing this out.)</p></blockquote><h2 id="2027" data-selectable-paragraph="">7. Adam</h2><p id="cdec" data-selectable-paragraph="">Adaptive moment estimation, or Adam (<a href="https://arxiv.org/abs/1412.6980" target="_blank" rel="noopener nofollow">Kingma &amp; Ba, 2014</a>), is a combination of momentum and RMSprop. It acts upon<br>(i) the gradient component by using <em>V</em>, the exponential moving average of gradients (like in momentum) and <br>(ii) the learning rate component by dividing the learning rate <em>α</em> by square root of <em>S</em>, the exponential moving average of squared gradients (like in RMSprop).</p><figure><div><div><div><p><img src="https://miro.medium.com/max/30/1*KlriCsGlpzHdPo1bDDWuVg@2x.png?q=20" width="620" height="128" role="presentation"></p><p><img width="620" height="128" role="presentation"></p></div></div></div></figure><p id="8a78" data-selectable-paragraph="">where</p><figure><div><div><div><p><img src="https://miro.medium.com/max/30/1*x3fTZJ8wQKlWmvMswbopjw@2x.png?q=20" width="286" height="282" role="presentation"></p><p><img width="286" height="282" role="presentation"></p></div></div></div></figure><p id="0254" data-selectable-paragraph="">are the bias corrections, and</p><figure><div><div><div><div><p><img src="https://miro.medium.com/max/30/1*pdnDu3qjtF7pRrWqniiY0Q@2x.png?q=20" width="744" height="294" role="presentation"></p><p><img width="744" height="294" role="presentation"></p></div></div></div></div></figure><p id="3115" data-selectable-paragraph="">with <em>V </em>and <em>S</em> initialised to 0.</p><p id="8c44" data-selectable-paragraph="">Proposed default values by the authors:</p><ul><li id="4fbe" data-selectable-paragraph=""><em>α</em> = 0.001</li><li id="4dfa" data-selectable-paragraph=""><em>β</em>₁ = 0.9</li><li id="78b0" data-selectable-paragraph=""><em>β</em>₂ = 0.999</li><li id="f8ac" data-selectable-paragraph=""><em>ε</em> = 10⁻⁸</li></ul><h2 id="2b3f" data-selectable-paragraph="">8. AdaMax</h2><p id="d728" data-selectable-paragraph="">AdaMax (<a href="https://arxiv.org/abs/1412.6980" target="_blank" rel="noopener nofollow">Kingma &amp; Ba, 2015</a>) is an adaptation of the Adam optimiser by the same authors using infinity norms (hence ‘max’). <em>V </em>is the exponential moving average of gradients, and <em>S </em>is the exponential moving average of past <em>p</em>-norm of gradients, approximated to the max function as seen below (see paper for convergence proof).</p><figure><div><div><div><p><img src="https://miro.medium.com/max/30/1*uB_qUXtHXcp49WQXBVWPrg@2x.png?q=20" width="474" height="112" role="presentation"></p><p><img width="474" height="112" role="presentation"></p></div></div></div></figure><p id="bcae" data-selectable-paragraph="">where</p><figure><div><div><div><p><img src="https://miro.medium.com/max/30/1*-41saVRK1mBkJFGB0_Ag0g@2x.png?q=20" width="284" height="132" role="presentation"></p><p><img width="284" height="132" role="presentation"></p></div></div></div></figure><p id="3cd0" data-selectable-paragraph="">is the bias correction for <em>V</em> and</p><figure><div><div><div><p><img src="https://miro.medium.com/max/30/1*f9UiRCmRPyEba01zN6HujA@2x.png?q=20" width="656" height="284" role="presentation"></p><p><img width="656" height="284" role="presentation"></p></div></div></div></figure><p id="211e" data-selectable-paragraph="">with <em>V </em>and <em>S</em> initialised to 0.</p><p id="5394" data-selectable-paragraph="">Proposed default values by the authors:</p><ul><li id="d30c" data-selectable-paragraph=""><em>α</em> = 0.002</li><li id="4143" data-selectable-paragraph=""><em>β</em>₁ = 0.9</li><li id="4d6e" data-selectable-paragraph=""><em>β</em>₂ = 0.999</li></ul><h2 id="6d4c" data-selectable-paragraph="">9. Nadam</h2><p id="dddb" data-selectable-paragraph="">Nadam (<a href="http://cs229.stanford.edu/proj2015/054_report.pdf" target="_blank" rel="noopener nofollow">Dozat, 2015</a>) is an acronym for Nesterov and Adam optimiser. The Nesterov component, however, is a more efficient modification than its original implementation.</p><p id="1892" data-selectable-paragraph="">First we’d like to show that the Adam optimiser can also be written as:</p><figure><div><div><div><div><p><img src="https://miro.medium.com/max/30/1*8Z1Y_8behfJWsr1iyIGDAA@2x.png?q=20" width="1150" height="146" role="presentation"></p><p><img width="1150" height="146" role="presentation"></p></div></div></div></div><figcaption data-selectable-paragraph="">Eqn. 5: Weight update for Adam optimiser</figcaption></figure><p id="4242" data-selectable-paragraph="">Nadam makes use of Nesterov to update the gradient one step ahead by replacing the previous <em>V_hat </em>in the above equation to the current <em>V_hat</em>:</p><figure><div><div><div><div><p><img src="https://miro.medium.com/max/30/1*6YaECEwGpMjd7hzIQcYVGg@2x.png?q=20" width="1092" height="146" role="presentation"></p><p><img width="1092" height="146" role="presentation"></p></div></div></div></div></figure><p id="cc9b" data-selectable-paragraph="">where</p><figure><div><div><div><p><img src="https://miro.medium.com/max/30/1*UJ_19zdDgfF36i1ght8dZQ@2x.png?q=20" width="286" height="282" role="presentation"></p><p><img width="286" height="282" role="presentation"></p></div></div></div></figure><p id="a1ff" data-selectable-paragraph="">and</p><figure><div><div><div><div><p><img src="https://miro.medium.com/max/30/1*vOR8hRNO7vPsBYj33f3kMQ@2x.png?q=20" width="744" height="294" role="presentation"></p><p><img width="744" height="294" role="presentation"></p></div></div></div></div></figure><p id="6395" data-selectable-paragraph="">with <em>V </em>and <em>S</em> initialised to 0.</p><p id="2dd5" data-selectable-paragraph="">Default values (taken from <a href="https://keras.io/optimizers/#nadam" target="_blank" rel="noopener nofollow">Keras</a>):</p><ul><li id="fd45" data-selectable-paragraph=""><em>α</em> = 0.002</li><li id="7f02" data-selectable-paragraph=""><em>β</em>₁ = 0.9</li><li id="f03d" data-selectable-paragraph=""><em>β</em>₂ = 0.999</li><li id="cbe3" data-selectable-paragraph=""><em>ε</em> = 10⁻⁷</li></ul><h2 id="de85" data-selectable-paragraph="">10. AMSGrad</h2><p id="3606" data-selectable-paragraph="">Another variant of Adam is the AMSGrad (<a href="https://openreview.net/pdf?id=ryQu7f-RZ" target="_blank" rel="noopener nofollow">Reddi et al., 2018</a>). This variant revisits the adaptive learning rate component in Adam and changes it to ensure that the current <em>S </em>is always larger than the previous time step<em>.</em></p><figure><div><div><div><p><img src="https://miro.medium.com/max/30/1*7CSFxtxUyba-nPLyrbLmhA@2x.png?q=20" width="620" height="128" role="presentation"></p><p><img width="620" height="128" role="presentation"></p></div></div></div></figure><p id="29b9" data-selectable-paragraph="">where</p><figure><div><div><div><p><img src="https://miro.medium.com/max/30/1*lkSDrWOrKmCdfiRMVMw44A@2x.png?q=20" width="458" height="70" role="presentation"></p><p><img width="458" height="70" role="presentation"></p></div></div></div></figure><p id="5fc6" data-selectable-paragraph="">and</p><figure><div><div><div><div><p><img src="https://miro.medium.com/max/30/1*EyoI7OQ3MFp53jq7XO3w4w@2x.png?q=20" width="744" height="294" role="presentation"></p><p><img width="744" height="294" role="presentation"></p></div></div></div></div></figure><p id="5523" data-selectable-paragraph="">with <em>V </em>and <em>S</em> initialised to 0.</p><p id="aea7" data-selectable-paragraph="">Default values (taken from <a href="https://keras.io/optimizers/#adam" target="_blank" rel="noopener nofollow">Keras</a>):</p><ul><li id="025d" data-selectable-paragraph=""><em>α</em> = 0.001</li><li id="00a1" data-selectable-paragraph=""><em>β</em>₁ = 0.9</li><li id="ab3a" data-selectable-paragraph=""><em>β</em>₂ = 0.999</li><li id="647b" data-selectable-paragraph=""><em>ε</em> = 10⁻⁷</li></ul></div></div></section><section><div><div><h2 id="be7f" data-selectable-paragraph="">Intuition</h2><p id="6888" data-selectable-paragraph="">Here I’d like to share with you some intuition why gradient descent optimisers use exponential moving average for the gradient component and root mean square for the learning rate component.</p><p id="0c9c" data-selectable-paragraph=""><strong>Why take <em>exponential moving average</em> of gradients?</strong></p><p id="4748" data-selectable-paragraph="">We need to update the weight, and to do so we need to make use of some value. The only value we have is the current gradient, so let’s utilise this to update the weight.</p><p id="48b0" data-selectable-paragraph="">But taking only the current gradient value is not enough. We want our updates to be ‘better guided’. So let’s include previous gradients too.</p><p id="21cd" data-selectable-paragraph="">One way to ‘combine’ the current gradient value and information of past gradients is that we could take a simple average of all the past and current gradients. But this means each of these gradients are equally weighted.</p><p id="4c1b" data-selectable-paragraph="">What we could do is to take the <a href="https://en.wikipedia.org/wiki/Moving_average#Exponential_moving_average" target="_blank" rel="noopener nofollow">exponential moving average</a>, where past gradient values are given higher weights (importance) than the current one. Intuitively, discounting the importance given to the current gradient would ensure that the weight update will not be sensitive to the current gradient.</p><p id="c588" data-selectable-paragraph=""><strong>Why divide learning rate by <em>root mean square </em>of gradients?</strong></p><p id="6183" data-selectable-paragraph="">The goal is to adapt the learning rate component. Adapt to what? The gradient. All we need to ensure is that when the gradient is large, we want the update to be small (otherwise, a huge value will be subtracted from the current weight!).</p><p id="259f" data-selectable-paragraph="">In order to create this effect, let’s <em>divide</em> the learning rate <em>α </em>by the current gradient to get an adapted learning rate.</p><p id="0c32" data-selectable-paragraph="">Bear in mind that the learning rate component must always be positive (because the learning rate component, when multiplied with the gradient component, should have the same sign as the latter). To ensure it’s always positive, we can take its absolute value or its square. Let’s take the square of the current gradient and ‘cancel’ back this square by taking its square root.</p><p id="a6ca" data-selectable-paragraph="">But like momentum, taking only the current gradient value is not enough. We want our updates to be ‘better guided’. So let’s make use of previous gradients too. And, as discussed above, we’ll take the exponential moving average of past gradients (‘mean square’), then taking its square root (‘root’), hence ‘root mean square’. All optimisers in this post which act on the learning rate component does this, except for AdaGrad (which takes cumulative sum of squared gradients).</p></div></div></section></div></div>
    </div>
    <footer>
        <div>created by buildstarted &copy; 2020 <a href="/about">about</a></div>
        <div>Share this page on social media: copy and paste this url https://linksfor.dev/</div>
        <div>If you prefer RSS: <a href="https://linksfor.dev/feed.xml">https://linksfor.dev/feed.xml</a></div>
    </footer>
    
    <script async defer>
        _dna = window._dna || {};
        _dna.siteId = "linksfor.devs";
        _dna.outlink = true;

        (function () {
            let dna = document.createElement('script');
            dna.type = 'text/javascript';
            dna.async = true;
            dna.src = '//dna.buildstarted.com/t.js';
            let s = document.getElementsByTagName('script')[0];
            s.parentNode.insertBefore(dna, s);
        })();
    </script>
    <noscript><img src="//dna.buildstarted.com/g?siteId=linksfor.devs"/></noscript>
</body>
</html>