<!DOCTYPE html>
<html lang="en">
<head>
    <title>
10 most common mistakes using kubernetes - linksfor.dev(s)    </title>
    <meta charset="utf-8">
    <link rel="alternate" type="application/rss+xml" title="Linksfor.dev(s) feed" href="https://linksfor.dev/feed.rss" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta name="google" value="notranslate">
    <link rel="stylesheet" type="text/css" href="/style.css" />
    <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon">
    <link rel="icon" href="/favicon.ico" type="image/x-icon">
    <meta property="og:title" content="10 most common mistakes using kubernetes - linksfor.dev(s)"/>
    <meta property="article:author" content="marek-bartik"/>
    <meta property="og:description" content="We had the chance to see quite a bit of clusters in our years of experience with kubernetes (both managed and unmanaged - on GCP, AWS and Azure), and we see some mistakes being repeated. No shame in that, we&#x2019;ve done most of these too!&#xA;I&#x27;ll try to show the ones we see very often and talk a bit about how to fix them."/>
    <meta property="og:type" content="website"/>
    <meta property="og:url" content="https://blog.pipetail.io/posts/2020-05-04-most-common-mistakes-k8s/"/>

<meta property="og:site_name" content="linksfor.dev(s)" />
</head>
<body>
    <div class="grid">
        <h1>
<a href="/" style="color:inherit">linksfor.dev(s)</a>
        </h1>
        <title>linksfor.dev(s) - 10 most common mistakes using kubernetes</title>
<div class="readable">
        <h1>10 most common mistakes using kubernetes</h1>
            <div>by marek-bartik</div>
            <div>Reading time: 14-18 minutes</div>
        <div>Posted here: 17 May 2020</div>
        <p><a href="https://blog.pipetail.io/posts/2020-05-04-most-common-mistakes-k8s/">https://blog.pipetail.io/posts/2020-05-04-most-common-mistakes-k8s/</a></p>
        <hr/>
<div id="readability-page-1" class="page"><div>
                            <p>We had the chance to see quite a bit of clusters in our years of experience with kubernetes (both managed and unmanaged - on GCP, AWS and Azure), and we see some mistakes being repeated. No shame in that, we’ve done most of these too!</p>
<p>I'll try to show the ones we see very often and talk a bit about how to fix them.</p>
<h2 id="resources---requests-and-limits">resources - requests and limits</h2>
<p>This definitely deserves the most attention and first place in this list.</p>
<p>CPU request are usually either <strong>not set</strong> or <strong>set very low</strong> (so that we can fit a lot of pods on each node) and nodes are thus overcommited. In time of high demand the CPUs of the node are fully utilized and our workload is getting only “what it had requested” and gets <strong>CPU throttled</strong>, causing increased application latency, timeouts, etc.</p>
<p>BestEffort (please don't):</p>
<figure><pre><code>    <span>resources</span><span>:</span> <span>{</span><span>}</span>
</code></pre></figure>
<p>very low cpu (please don't):</p>
<figure><pre><code>    <span>resources</span><span>:</span>
      <span>requests</span><span>:</span>
        <span>cpu</span><span>:</span> <span>"1m"</span>
</code></pre></figure>
<p>On the other hand, having a CPU limit can unnecessarily throttle pods even if the node’s CPU is not fully utilized which again can cause increased latency. There is a an open discussion around <em>CPU CFS quota</em> in linux kernel and cpu throttling based on set cpu limits and turning off the CFS quota. CPU limits can cause more problems than they solve. See more in the link below.</p>
<p>Memory overcommiting can get you in more trouble. Reaching a CPU limit results in throttling, reaching memory limit will get your pod killed. Ever seen <strong>OOMkill</strong>? Yep, that's the one we are talking about. Want to minimize how often it can happen? Don’t overcommit your memory and use Guaranteed QoS (Quality of Service) setting memory request equal to limit like in the example below. Read more about the topic <a href="https://www.slideshare.net/try_except_/optimizing-kubernetes-resource-requestslimits-for-costefficiency-and-latency-highload">in Henning Jacobs’ (Zalando) presentation</a>.</p>
<p>Burstable (more likely to get OOMkilled more often):</p>
<figure><pre><code>    <span>resources</span><span>:</span>
      <span>requests</span><span>:</span>
        <span>memory</span><span>:</span> <span>"64Mi"</span>
        <span>cpu</span><span>:</span> <span>"250m"</span>
      <span>limits</span><span>:</span>
        <span>memory</span><span>:</span> <span>"128Mi"</span>
        <span>cpu</span><span>:</span> <span>"500m"</span>
</code></pre></figure>
<p>Guaranteed:</p>
<figure><pre><code>    <span>resources</span><span>:</span>
      <span>requests</span><span>:</span>
        <span>memory</span><span>:</span> <span>"128Mi"</span>
        <span>cpu</span><span>:</span> <span>"500m"</span>
      <span>limits</span><span>:</span>
        <span>memory</span><span>:</span> <span>"128Mi"</span>
        <span>cpu</span><span>:</span> <span>"500m"</span>
</code></pre></figure>
<p>So what can help you when setting resources?</p>
<p>You can see the current cpu and memory usage of pods (and containers in them) using <em>metrics-server</em>. Chances are, you are already running it. Simply run these:</p>
<figure><pre><code>kubectl top pods
kubectl top pods --containers
kubectl top nodes
</code></pre></figure>
<p>However these show just the current usage. That is great to get the rough idea about the numbers but you end up wanting to see these <strong>usage metrics in time</strong> (to answer questions like: what was the cpu usage in peak, yesterday morning, etc.). For that, you can use <em>Prometheus</em>, <em>DataDog</em> and many others. They just ingest the metrics from metrics-server and store them, then you can query &amp; graph them.</p>
<p><a href="https://cloud.google.com/kubernetes-engine/docs/concepts/verticalpodautoscaler">VerticalPodAutoscaler</a> can help you <strong>automate away</strong> this manual process - looking at cpu/mem usage in time and setting new requests and limits based on that all over again.</p>
<p>Utilizing effectively your compute is not an easy task. It is like playing tetris all the time. If you find yourself paying a lot for compute while having low average utilization (say ~10%), you might want to check AWS Fargate or Virtual Kubelet based products that leverage more of a serverless/pay-per-usage billing model that might be cheaper for you.</p>
<h2 id="liveness-and-readiness-probes">liveness and readiness probes</h2>
<p>By default there are no liveness and readiness probes specified. And sometimes it stays that way…</p>
<p>But how else would your service get restarted when there is an unrecoverable error? How does a loadbalancer know a specific pod can start handling traffic? Or handle more traffic?</p>
<p>People usually don't know the difference between these two.</p>
<ul>
<li>Liveness probe restarts your pod if the probe fails</li>
<li>Readiness probe disconnects on fail the failing pod from the kubernetes service (you can check this in <code>kubectl get endpoints</code>) and no more traffic is being sent to it until the probe succeeds again</li>
</ul>
<p>and <strong>BOTH RUN DURING THE WHOLE POD LIFECYCLE</strong>. This is important.</p>
<p>People often think that readiness probes run only at the start to tell when the pod is <strong>Ready</strong> and can start servicing traffic. But that's just one of its use cases.</p>
<p>The other one is to tell if during a pod's life the pod becomes <strong>too hot</strong> handling too much traffic (or an expensive computation) so that we don't <em>send her more work to do</em> and let her <em>cool down</em>, then the readiness probe succeeds and <strong>we start sending in more traffic again</strong>. In this case (when failing readiness probe) failing also liveness probe would be very counterproductive. <strong>Why would you restart a pod that is healthy and doing a lot of work?</strong></p>
<p>Sometimes not having either probe defined is better than having them defined wrong. As mentioned above, if <strong>liveness probe is equal to readiness probe</strong>, you are in a big trouble. You might want to start with defining <a href="https://twitter.com/sszuecs/status/1175803113204269059">only the readiness probe alone</a> as <a href="https://srcco.de/posts/kubernetes-liveness-probes-are-dangerous.html">liveness probes are dangerous</a>.</p>
<p>Do not fail either of the probes if any of your shared dependencies is down, it would cause <strong>cascading failure</strong> of all the pods. You are <a href="https://blog.colinbreck.com/kubernetes-liveness-and-readiness-probes-how-to-avoid-shooting-yourself-in-the-foot/">shooting yourself in the foot</a>.</p>
<h2 id="loadbalancer-for-every-http-service">LoadBalancer for every http service</h2>
<p>Chances are you have more http services in your cluster which you’d like to expose to the outside world.</p>
<p>If you expose kubernetes service as a <code>type: LoadBalancer</code>, its controller (vendor specific) will provision and reconciliate an external LoadBalancer (not necessarily L7 loadbalancer, more likely an L4 lb) and those resources might get expensive (external static ipv4 address, compute, per-second pricing…) as you create many of them.</p>
<p>In that case, sharing one external loadbalancer might make more sense and you expose your services as <code>type: NodePort</code>. Or yet better, deploying something like <strong>nginx-ingress-controller</strong> (or <em>traefik</em>) being the single <em>NodePort</em> endpoint exposed to the external loadbalancer and routing the traffic in the cluster based on kubernetes <strong>ingress</strong> resources.</p>
<p>The other in-cluster (micro)services that talk to each other can talk through <strong>ClusterIP</strong> services and out-of-box dns service discovery. Be careful about not using their public DNS/IPs as it could affect their latency and cloud cost.</p>
<h2 id="non-kubernetes-aware-cluster-autoscaling">non-kubernetes-aware cluster autoscaling</h2>
<p>When adding and removing nodes to/from the cluster, you shouldn’t consider some simple metrics like a cpu utilization of those nodes. When scheduling pods, you decide based on a lot of <strong>scheduling constraints</strong> like pod &amp; node affinities, taints and tolerations, resource requests, QoS, etc. Having an external autoscaler that does not understand these constraints might be troublesome.</p>
<p>Imagine there is a new pod to be scheduled but all of the CPU available is requested and pod is <strong>stuck in Pending state</strong>. External autoscaler sees the average CPU currently used (not requested) and won’t scale out (will not add another node). The pod won’t be scheduled.</p>
<p>Scaling-in (removing a node from the cluster) is always harder. Imagine you have a stateful pod (with persistent volume attached) and as <strong>persistent volumes</strong> are usually resources that <strong>belong to a specific availability zone</strong> and are not replicated in the region, your custom autoscaler removes a node with this pod on it and scheduler cannot schedule it onto a different node as it is very limited by the only availability zone with your persistent disk in it. Pod is again stuck in Pending state.</p>
<p>The community is widely using <a href="https://github.com/kubernetes/autoscaler/tree/master/cluster-autoscaler"><strong>cluster-autoscaler</strong></a> which runs in your cluster and is integrated with most major public cloud vendors APIs, understands all these constraints and would scale-out in the mentioned cases. It will also figure out if it can gracefully scale-in without affecting any constraints we have set and saves you money on compute.</p>
<h2 id="not-using-the-power-of-iamrbac">Not using the power of IAM/RBAC</h2>
<p>Don't use IAM Users with permanent secrets for <em>machines and applications</em> rather than generating temporary ones using roles and service accounts.</p>
<p>We see it often - hardcoding access and secret keys in application configuration, never rotating the secrets when you have Cloud IAM at hand. Use IAM Roles and service accounts instead of users where suitable.</p>
<figure>
    <a href="https://blog.pipetail.io/images/2020-05-04-everywhere.jpg" data-toggle="lightbox" data-gallery="post-gallery">
        <img src="https://blog.pipetail.io/images/2020-05-04-everywhere.jpg"> 
    </a>
</figure>

<p>Skip kube2iam, go directly with IAM Roles for Service Accounts as described in <a href="https://blog.pipetail.io/posts/2020-04-13-more-eks-tips/">this blogpost by Štěpán Vraný</a>.</p>
<figure><pre><code><span>apiVersion</span><span>:</span> v1
<span>kind</span><span>:</span> ServiceAccount
<span>metadata</span><span>:</span>
  <span>annotations</span><span>:</span>
    <span>eks.amazonaws.com/role-arn</span><span>:</span> arn<span>:</span>aws<span>:</span>iam<span>:</span><span>:</span>123456789012<span>:</span>role/my<span>-</span>app<span>-</span>role
  <span>name</span><span>:</span> my<span>-</span>serviceaccount
  <span>namespace</span><span>:</span> default
</code></pre></figure>
<p>One annotation. That wasn't that hard, no?</p>
<p>Also don’t give the service accounts or instance profiles <code>admin</code> and <code>cluster-admin</code> permissions when they don’t need it. That is a bit harder, especially in k8s RBAC, but still worth the effort.</p>
<h2 id="self-anti-affinities-for-pods">self anti-affinities for pods</h2>
<p>Running e.g. 3 pod replicas of some deployment, node goes down and all the replicas with it. Huh? All the replicas were running on one node? Wasn't Kubernetes supposed to be magical and provide HA?!</p>
<p>You can't expect kubernetes scheduler to enforce anti-affinites for your pods. You have to define them explicitly.</p>
<figure><pre><code>// omitted for brevity
      <span>labels</span><span>:</span>
        <span>app</span><span>:</span> zk
// omitted for brevity
      <span>affinity</span><span>:</span>
        <span>podAntiAffinity</span><span>:</span>
          <span>requiredDuringSchedulingIgnoredDuringExecution</span><span>:</span>
            <span>-</span> <span>labelSelector</span><span>:</span>
                <span>matchExpressions</span><span>:</span>
                  <span>-</span> <span>key</span><span>:</span> <span>"app"</span>
                    <span>operator</span><span>:</span> In
                    <span>values</span><span>:</span>
                    <span>-</span> zk
              <span>topologyKey</span><span>:</span> <span>"kubernetes.io/hostname"</span>
</code></pre></figure>
<p>That's it. This will make sure the pods will be scheduled to different nodes (this is being checked only at scheduling time, not at execution time, hence the <code>requiredDuringSchedulingIgnoredDuringExecution</code>).</p>
<p>We are talking about podAntiAffinity on different node names - <code>topologyKey: "kubernetes.io/hostname"</code> - not different availability zones. If you really need proper HA, dig a bit deeper in this topic.</p>
<h2 id="no-poddisruptionbudget">no poddisruptionbudget</h2>
<p>You run production workload on kubernetes. Your nodes &amp; cluster have to be upgraded, or decommissioned, from time to time. PodDisruptionBudget (pdb) is sort of an API for service guarantees between cluster-administrators and cluster-users.</p>
<p>Make sure to create <code>pdb</code> to avoid unnecessary service outages due to draining nodes.</p>
<figure><pre><code><span>apiVersion</span><span>:</span> policy/v1beta1
<span>kind</span><span>:</span> PodDisruptionBudget
<span>metadata</span><span>:</span>
  <span>name</span><span>:</span> zk<span>-</span>pdb
<span>spec</span><span>:</span>
  <span>minAvailable</span><span>:</span> <span>2</span>
  <span>selector</span><span>:</span>
    <span>matchLabels</span><span>:</span>
      <span>app</span><span>:</span> zookeeper
</code></pre></figure>
<p>With this as a cluster-user you tell the cluster-administrators: “hey, I have this zookeeper service here and no matter what you have to do, I'd like having at least 2 replicas always available”.</p>
<p>I talked about this topic more in-depth <a href="https://blog.marekbartik.com/posts/2018-06-29_kubernetes-in-production-poddisruptionbudget/">here in this blogpost</a>.</p>

<p>Kubernetes <strong>namespaces don’t provide any strong isolation</strong>.</p>
<p>People seem to expect if they separated non-prod workload to one namespace and prod to prod namespace, one <strong>workload won’t ever affect the other</strong>. It is possible to achieve some level of fairness - resource requests and limits, quotas, priorityClasses - and isolation - affinities, tolerations, taints (or nodeselectors) - to “physically” separate the workload in data plane but that separation is quite <strong>complex</strong>.</p>
<p>If you need to have both types of workloads in the same cluster, you’ll have to bear the complexity. If you don’t need it and having <strong>another cluster</strong> is relatively cheap for you (like in public cloud), put it in different cluster to achieve much stronger level of isolation.</p>
<h2 id="externaltrafficpolicy-cluster">externalTrafficPolicy: Cluster</h2>
<p>Seeing this very often, all traffic is routed inside the cluster to a NodePort service which has, by default, <code>externalTrafficPolicy: Cluster</code>. That means the <em>NodePort</em> is opened on every node in the cluster so that you can use any of them to communicate with the desired service (set of pods).</p>
<figure>
    <a href="https://blog.pipetail.io/images/2020-05-04-externaltrafficpolicy.png" data-toggle="lightbox" data-gallery="post-gallery">
        <img src="https://blog.pipetail.io/images/2020-05-04-externaltrafficpolicy.png"> 
    </a>
</figure>

<p>More often than not the actual pods that are targeted with the NodePort service <strong>run only on a subset of those nodes</strong>. That means if I talk to a node which does not have the pod running it will forward the traffic to a different node, causing <strong>additional network hop</strong> and increased latency (if the nodes are in different AZs/datacenters, the latency can be quite high and there is additional egress cost to it).</p>
<p>Setting <code>externalTrafficPolicy: Local</code> on the kubernetes service won’t open that NodePort on every Node, but only on the nodes where the pods are actually running. If you use an external loadbalancer which is healthchecking its endpoints (like <em>AWS ELB</em> does) it will start to <strong>send the traffic only to those nodes</strong> where it is supposed to go, improving your latency, compute overhead, egress bill and sanity.</p>
<p>Chances are, you have something like <em>traefik</em> or <em>nginx-ingress-controller</em> being exposed as NodePort (or LoadBalancer, which uses NodePort too) to handle your ingress http traffic routing and this setting can greatly reduce the latency on such requests.</p>
<p>great blogpost that goes more in depth about <a href="https://www.asykim.com/blog/deep-dive-into-kubernetes-external-traffic-policies">externalTrafficPolicy and their trade-offs here</a>.</p>
<h2 id="pet-clusters--stressing-the-control-plane-too-much">pet clusters + stressing the control plane too much</h2>
<p>You went from calling your servers <a href="https://twitter.com/vorobiev_cloud"><em>Anton</em></a>, <em>HAL9000</em> and <em>Colossus</em> to having generated random ids for your nodes but you started to call your cluster by a name?</p>
<p>You know how you started with a Proof Of Concept with Kubernetes, named the cluster “testing” and STILL use it in production and everybody is scared to touch it? (true story)</p>
<p>Pet clusters are not fun and you might want to consider deleting your cluster from time to time, <strong>practice Disaster Recovery</strong> and take care of your control plane. Being afraid of touching the control plane is not a good sign. <em>Etcd</em>‘s dead? Well, you got a big problem.</p>
<p>On the other hand, touching it too much is no good either. When with time the <strong>control plane becomes slow</strong>, chances are, you are either creating a lot of objects without ever rotating them (very common when using helm with default settings which does not rotate its state in configmaps/secrets and you end up having thousands of objects in control plane) or you constantly scrape and edit tons of things from kube-api (for autoscaling, cicd, monitoring, logs from events, controllers, etc.).</p>
<p>Also, check your managed kubernetes offering “SLAs”/SLOs and guarantees. A vendor might guarantee <strong>availability of control plane</strong> (or its subcomponents) but not p99 latency of the requests you send to it. In other words, you might do <code>kubectl get nodes</code> and get correct answer in 10 minutes and that still does not violate the service guarantee.</p>
<h2 id="bonus-using-latest-tag">bonus: using latest tag</h2>
<p>This one is a classic. I feel like lately I don’t see this very often as a lot of us got burned too many times and we stopped using <code>:latest</code> and started to pin the versions. Yay!</p>
<p><a href="https://aws.amazon.com/about-aws/whats-new/2019/07/amazon-ecr-now-supports-immutable-image-tags/">ECR has a great feature of tag immutability</a>, definitely worth checking out.</p>
<h2 id="summary">summary</h2>
<p>Don't expect that everything works automagically - Kubernetes is not a silver bullet. A bad app will be <a href="https://twitter.com/sadserver/status/1032704897500598272?s=20">bad app even on kubernetes</a> (possible even worse than bad, actually). If you are not careful, you can end up with a lot of complexity, stressed and slow control plane and no DR strategy. Don't expect out-of-box multi-tenancy and high availability. Invest some time in making your app cloud native.</p>
<p>Check how we others got burned in this great <a href="https://k8s.af/">failure stories compilation</a> by Henning.</p>
<p>Do you see different mistakes being made? Just hit us up
(<a href="https://twitter.com/MarekBartik">@MarekBartik</a> <a href="https://twitter.com/MstrsObserver">@MstrsObserver</a>)
on Twitter!</p>
                            
                        </div></div></div>
    </div>
    <footer>
        <div>created by buildstarted &copy; 2020 <a href="/about">about</a></div>
        <div>Share this page on social media: copy and paste this url https://linksfor.dev/</div>
        <div>If you prefer RSS: <a href="https://linksfor.dev/feed.xml">https://linksfor.dev/feed.xml</a></div>
		<div>Customer satisfaction guaranteed to be optional.</div>
    </footer>
    
    <script async defer>
        _dna = window._dna || {};
        _dna.siteId = "linksfor.devs";
        _dna.outlink = true;

        (function () {
            let dna = document.createElement('script');
            dna.type = 'text/javascript';
            dna.async = true;
            dna.src = '//dna.buildstarted.com/t.js';
            let s = document.getElementsByTagName('script')[0];
            s.parentNode.insertBefore(dna, s);
        })();
    </script>
    <noscript><img src="//dna.buildstarted.com/g?siteId=linksfor.devs"/></noscript>
</body>
</html>