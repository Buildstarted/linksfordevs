<!DOCTYPE html>
<html lang="en">
<head>
    <title>
Web Scraping without getting blocked - linksfor.dev(s)    </title>
    <meta charset="utf-8">
    <link rel="alternate" type="application/rss+xml" title="Linksfor.dev(s) feed" href="https://linksfor.dev/feed.rss" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta name="google" value="notranslate">
    <link rel="stylesheet" type="text/css" href="/style.min.css" />
    <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon">
    <link rel="icon" href="/favicon.ico" type="image/x-icon">
    <meta property="og:title" content="Web Scraping without getting blocked - linksfor.dev(s)"/>
    <meta property="article:author" content="Pierre de Wulf"/>
    <meta property="og:description" content="Browser fingerprinting, TLS fingerprinting, Chrome headless, headers spoofing and more. Here is everything we know about how to scrape the web without getting blocked."/>
    <meta property="og:type" content="website"/>
    <meta property="og:url" content="https://www.scrapingbee.com/blog/web-scraping-without-getting-blocked/"/>

<meta property="og:site_name" content="linksfor.dev(s)" />
</head>
<body>
    <div class="devring" style="background: #222">
        <div style="text-align:center">Explore other dev related sites in this ring. If you would like to join this ring <a href="https://devring.club">click here</a>.</div>
        <div class="grid">
            <div style="display: grid; grid-template-columns: .5fr 1fr 1fr 1fr; text-align: center;">
                <span class="devring-title"><a href="https://devring.club/">devring.club</a></span>
                <a href="https://devring.club/sites/1/prev" class="devring-previous">Previous</a>
                <a href="https://devring.club/random" class="devring-random">Random</a>
                <a href="https://devring.club/sites/1/next" class="devring-next">Next</a>
            </div>
        </div>
    </div>
    <div class="grid">
        <h1 style="margin: unset">
<a href="/" style="color:inherit">linksfor.dev(s)</a>
        </h1>
        <title>linksfor.dev(s) - Web Scraping without getting blocked</title>
<div class="readable">
        <h1>Web Scraping without getting blocked</h1>
            <div>by Pierre de Wulf</div>
            <div>Reading time: 16-20 minutes</div>
        <div>Posted here: 18 Aug 2020</div>
        <p><a href="https://www.scrapingbee.com/blog/web-scraping-without-getting-blocked/">https://www.scrapingbee.com/blog/web-scraping-without-getting-blocked/</a></p>
        <hr/>
<div id="readability-page-1" class="page"><div>
          























<div>
  
  <p><img data-sizes="auto" data-srcset="
    
      https://d33wubrfki0l68.cloudfront.net/af79d36bcb5d86380406affc888678be0abea2f4/60a4f/blog/web-scraping-without-getting-blocked/cover_hu1d621f5b4a9638657d3f931a7881558b_26142_500x0_resize_box_2.png 500w
    
    
      , https://d33wubrfki0l68.cloudfront.net/068defe62929dd56d050b561500c40110e1bc3d3/ee0d9/blog/web-scraping-without-getting-blocked/cover_hu1d621f5b4a9638657d3f931a7881558b_26142_800x0_resize_box_2.png 800w
    
    
    " data-src="https://d33wubrfki0l68.cloudfront.net/8eae8438ab2ba08895ab7820b7bb3358781e92f6/e3e0d/blog/web-scraping-without-getting-blocked/cover.png" width="1024" height="512" alt="A guide to Web Scraping without getting blocked in 2020" sizes="825px" srcset="
    
      https://d33wubrfki0l68.cloudfront.net/af79d36bcb5d86380406affc888678be0abea2f4/60a4f/blog/web-scraping-without-getting-blocked/cover_hu1d621f5b4a9638657d3f931a7881558b_26142_500x0_resize_box_2.png 500w
    
    
      , https://d33wubrfki0l68.cloudfront.net/068defe62929dd56d050b561500c40110e1bc3d3/ee0d9/blog/web-scraping-without-getting-blocked/cover_hu1d621f5b4a9638657d3f931a7881558b_26142_800x0_resize_box_2.png 800w
    
    
    " src="https://d33wubrfki0l68.cloudfront.net/8eae8438ab2ba08895ab7820b7bb3358781e92f6/e3e0d/blog/web-scraping-without-getting-blocked/cover.png">
  
</p></div>

<br>


<h2 id="introduction">Introduction</h2>
<p>Web scraping or crawling is the fact of fetching data from a third party website by downloading and parsing the HTML code to extract the data you want.</p>
<p>But you should use an API for this!</p>
<p>Not every website offers an API, and APIs don't always expose every piece of information you need. So it's often the only solution to extract website data.</p>
<p>There are many use cases for web scraping:</p>
<ul>
<li>E-commerce price monitoring</li>
<li>News aggregation</li>
<li>Lead generation</li>
<li>SEO (Search engine result page monitoring)</li>
<li>Bank account aggregation (Mint in the US, Bankin’ in Europe)</li>
<li>But also lots of individual and researchers who need to build a dataset otherwise not available.</li>
</ul>
<p>So, what is the problem?</p>
<p>The main problem is that most websites do not want to be scraped. They only want to serve content to real users using real web browser (except Google, they all want to be scraped by Google).</p>
<p>So, when you scrape, you have to be careful not being recognized as a robot by basically doing two things: using human tools &amp; having a human behavior. This post will guide you through all the things you can use to cover yourself and through all the tools websites use to block you.</p>

<h3 id="why-using-headless-browsing">Why using headless browsing?</h3>
<p>When you open your browser and go to a webpage it almost always means that you are you asking an HTTP server for some content. And one of the easiest ways pull content from an HTTP server is to use a classic command-line tool such as <a href="https://en.wikipedia.org/wiki/CURL">cURL</a>.</p>
<p>Thing is if you just do a: <code>curl www.google.com</code>, Google has many ways to know that you are not a human, just by looking at the headers for examples. Headers are small pieces of information that goes with every HTTP request that hit the servers, and one of those pieces of information precisely describe the client making the request, I am talking about the infamous “User-Agent” header. And just by looking at the “User-Agent” header, Google now knows that you are using cURL. If you want to learn more about headers, the <a href="https://en.wikipedia.org/wiki/List_of_HTTP_header_fields">Wikipedia page</a> is great, and to make some experiment, just <a href="http://www.httpbin.org/headers?json">go over here</a>, it's a webpage that simply displays the headers information of your request.</p>
<p>Headers are really easy to alter with cURL, and copying the User-Agent header of a legit browser could do the trick. In the real world, you'd need to set more than just one header but more generally it is not very difficult to artificially craft an HTTP request with cURL or any library that will make this request looks exactly like a request made with a browser. Everybody knows that, and so, to know if you are using a real browser, website will check one thing that cURL and library can not do: JS execution.</p>
<h3 id="do-you-speak-js">Do you speak JS?</h3>
<p>The concept is very simple, the website embeds a little snippet of JS in its webpage that, once executed, will “unlock” the webpage. If you are using a real browser, then, you won't notice the difference, but if you're not, all you'll receive is an HTML page with some obscure JS in it.</p>
























<div>
  
  <p><img data-sizes="auto" data-srcset="
    
      https://d33wubrfki0l68.cloudfront.net/bb034cb9e8b57876140782e3e99410c19b2813f6/f5b0f/blog/web-scraping-without-getting-blocked/9jxuds0b3pa770dej6t8_hu19170f2f92f0e00f79d305ac40a3b4d7_210462_500x0_resize_box_2.png 500w
    
    
      , https://d33wubrfki0l68.cloudfront.net/314705063f126133d249a88c32d9408999220df0/e27d3/blog/web-scraping-without-getting-blocked/9jxuds0b3pa770dej6t8_hu19170f2f92f0e00f79d305ac40a3b4d7_210462_800x0_resize_box_2.png 800w
    
    
      , https://d33wubrfki0l68.cloudfront.net/c7f06b2d925fc8f0a201f9f4ca6a3367747e25da/ffbb5/blog/web-scraping-without-getting-blocked/9jxuds0b3pa770dej6t8_hu19170f2f92f0e00f79d305ac40a3b4d7_210462_1200x0_resize_box_2.png 1200w
    
    
      , https://d33wubrfki0l68.cloudfront.net/a9d65ced014f5bfbd5701600d6bd18e2a0aed48f/8c951/blog/web-scraping-without-getting-blocked/9jxuds0b3pa770dej6t8_hu19170f2f92f0e00f79d305ac40a3b4d7_210462_1500x0_resize_box_2.png 1500w 
    " data-src="https://d33wubrfki0l68.cloudfront.net/42818c4fd068a93f35b2b3d301b1856127c803e3/0ecbd/blog/web-scraping-without-getting-blocked/9jxuds0b3pa770dej6t8.png" width="2048" height="370" alt="Javascript code snippet" srcset="
    
      https://d33wubrfki0l68.cloudfront.net/bb034cb9e8b57876140782e3e99410c19b2813f6/f5b0f/blog/web-scraping-without-getting-blocked/9jxuds0b3pa770dej6t8_hu19170f2f92f0e00f79d305ac40a3b4d7_210462_500x0_resize_box_2.png 500w
    
    
      , https://d33wubrfki0l68.cloudfront.net/314705063f126133d249a88c32d9408999220df0/e27d3/blog/web-scraping-without-getting-blocked/9jxuds0b3pa770dej6t8_hu19170f2f92f0e00f79d305ac40a3b4d7_210462_800x0_resize_box_2.png 800w
    
    
      , https://d33wubrfki0l68.cloudfront.net/c7f06b2d925fc8f0a201f9f4ca6a3367747e25da/ffbb5/blog/web-scraping-without-getting-blocked/9jxuds0b3pa770dej6t8_hu19170f2f92f0e00f79d305ac40a3b4d7_210462_1200x0_resize_box_2.png 1200w
    
    
      , https://d33wubrfki0l68.cloudfront.net/a9d65ced014f5bfbd5701600d6bd18e2a0aed48f/8c951/blog/web-scraping-without-getting-blocked/9jxuds0b3pa770dej6t8_hu19170f2f92f0e00f79d305ac40a3b4d7_210462_1500x0_resize_box_2.png 1500w 
    " src="https://d33wubrfki0l68.cloudfront.net/42818c4fd068a93f35b2b3d301b1856127c803e3/0ecbd/blog/web-scraping-without-getting-blocked/9jxuds0b3pa770dej6t8.png">
  
</p></div>

<figcaption>
    <small> <em> an actual example of such a snippet </em> </small>
</figcaption>


<p>But once again, this solution is not completely bulletproof, mainly because since nodeJS it is now very easy to execute JS outside of a browser. But once again, the web evolved and there are other tricks to determine if you are using a real browser or not.</p>
<h3 id="headless-browsing">Headless Browsing</h3>
<p>Trying to execute snippet JS on the side with node is really difficult and not robust at all. And more importantly, as soon as the website has a more complicated check system or is a big single-page application cURL and pseudo-JS execution with node become useless. So the best way to look like a real browser is to actually use one.</p>
<p>Headless Browsers will behave “exactly” like a real browser except that you will easily be able to programmatically use them. The most used is Chrome Headless, a Chrome option that has the behavior of Chrome without all the UI wrapping it.</p>
<p>The easiest way to use Headless Chrome is by calling driver that wraps all its functionality into an easy API, <a href="https://github.com/SeleniumHQ/selenium">Selenium</a> and <a href="https://github.com/GoogleChrome/puppeteer">Puppeteer</a> are the two most famous solutions.</p>
<p>However, it will not be enough as websites have now tools that allow them to detect a headless browser. This arms race that's been going on for a long time.</p>
<p>While those solutions can be easy to make work on your computer, it can be trickier to do this at scale.</p>
<p>And this problem, of managing lots of Chrome headless instances, is one of the many we solve at <a href="https://www.scrapingbee.com/">ScrapingBee</a></p>
<div>
<div>
    <p><strong> Tire of getting blocked while scraping the web?</strong>
        <span> Our API handles headless browsers and rotates proxies for you. </span>
    </p>
    
</div>
</div>

<h3 id="browser-fingerprinting">Browser Fingerprinting</h3>
<p>Everyone, and mostly front dev, knows how every browser behaves differently. Sometimes it can be about rendering CSS, sometimes JS, sometimes just internal properties. Most of those differences are well known and it is now possible to detect if a browser is actually who it pretends to be. Meaning the website is asking itself “are all the browser properties and behaviors matched what I know about the User-Agent sent by this browser?".</p>
<p>This is why there is an everlasting arms race between scrapers who want to pass themselves as a real browser and websites who want to distinguish headless from the rest.</p>
<p>However, in this arms race, web scrapers tend to have a big advantage and here is why.</p>
























<div>
  
  <p><img data-sizes="auto" data-srcset="
    
      https://d33wubrfki0l68.cloudfront.net/86a153d983ee72029e35d56217a5ef05173858d7/d4e4e/blog/web-scraping-without-getting-blocked/6augn0y6fqtlqd08ratm_hu78d66bb1d6fd163ef9a2a8dd5c667378_44196_500x0_resize_box_2.png 500w
    
    
      , https://d33wubrfki0l68.cloudfront.net/f44f9c8a0a44d3c6d8861fb191196a961774eada/ce59d/blog/web-scraping-without-getting-blocked/6augn0y6fqtlqd08ratm_hu78d66bb1d6fd163ef9a2a8dd5c667378_44196_800x0_resize_box_2.png 800w
    
    
    " data-src="https://d33wubrfki0l68.cloudfront.net/3466294555380f62f649e2847f93cd82450bdde7/ccefb/blog/web-scraping-without-getting-blocked/6augn0y6fqtlqd08ratm.png" width="800" height="449" alt="Screenshot of Chrome malware alert" srcset="
    
      https://d33wubrfki0l68.cloudfront.net/86a153d983ee72029e35d56217a5ef05173858d7/d4e4e/blog/web-scraping-without-getting-blocked/6augn0y6fqtlqd08ratm_hu78d66bb1d6fd163ef9a2a8dd5c667378_44196_500x0_resize_box_2.png 500w
    
    
      , https://d33wubrfki0l68.cloudfront.net/f44f9c8a0a44d3c6d8861fb191196a961774eada/ce59d/blog/web-scraping-without-getting-blocked/6augn0y6fqtlqd08ratm_hu78d66bb1d6fd163ef9a2a8dd5c667378_44196_800x0_resize_box_2.png 800w
    
    
    " src="https://d33wubrfki0l68.cloudfront.net/3466294555380f62f649e2847f93cd82450bdde7/ccefb/blog/web-scraping-without-getting-blocked/6augn0y6fqtlqd08ratm.png">
  
</p></div>

<figcaption>
    <small> <em> Screenshot of Chrome malware alert </em> </small>
</figcaption>


<p>Most of the time, when a Javascript code tries to detect whether it's being run in headless mode is when it is a malware that is trying to evade behavioral fingerprinting. Meaning that the JS will behave nicely inside a scanning environment and badly inside real browsers. And this is why the <a href="https://transparencyreport.google.com/safe-browsing/overview">team behind the Chrome headless mode</a> are trying to make it indistinguishable from a real user's web browser in order to stop malware from doing that. And this is why web scrapers, in this arms race can profit from this effort.</p>
<p>One another thing to know is that whereas running 20 cURL in parallel is trivial, Chrome Headless while relatively easy to use for small use cases, can be tricky to put at scale. Mainly because it uses lots of RAM so managing more than 20 instances of it is a challenge.</p>
<p>If you want to learn more about browser fingerprinting I suggest you take a look at <a href="https://antoinevastel.com/">Antoine Vastel blog</a>, a blog entirely dedicated to this subject.</p>
<p>That's about all you need to know to understand how to pretend like you are using a real browser. Let's now take a look at how do you behave like a real human.</p>
<h3 id="tls-fingerprinting">TLS Fingerprinting</h3>
<h4 id="what-is-it">What is it?</h4>
<p>TLS stands for Transport Layer Security, is the successor of SSL and which was basically what the “S” of HTTPS stood for.</p>
<p>This protocol ensures privacy and data integrity between two or more communicating computer applications, in our case, a web browser or a script and an HTTP server.</p>
<p>Similarly to browser fingerprinting the goal of TLS fingerprinting is to uniquely identify users based on the way they use TLS.</p>
<p>How this protocol works can be split in two big parts.</p>
<p>First, when the client connects to the server, a <em>TLS handshake</em> happens. During this handshake, many requests are sent between the two to ensure that everyone is actually who they claim to be.</p>
<p>Then, if the handshake has been successful the protocol describes how the client and the server should encrypt and decrypt the data in a secure way. If you want a detailed explanation, check this great introduction by <a href="https://www.cloudflare.com/learning/ssl/transport-layer-security-tls/">Cloudflare</a>.</p>
<p>Most of the data point used to build the fingerprint are from the TLS handshake and if you want to see what does a TLS fingerprint looks like, you can go visit this awesome <a href="https://tlsfingerprint.io/top">online database</a>.</p>
<p>On this website, we see that the most used <a href="https://tlsfingerprint.io/id/9c673fd64a32c8dc">fingerprint</a> was used 22.19% of the time last week (at the time of writing this article).</p>
























<div>
  
  <p><img data-sizes="auto" data-srcset="
    
      https://d33wubrfki0l68.cloudfront.net/d94e2bfe0ab583be9ec688887b5ddc3599ed3321/b6df3/blog/web-scraping-without-getting-blocked/tls_fingerprint_hu2b9762a5f1a093b565f35b754eca36fc_258507_500x0_resize_box_2.png 500w
    
    
      , https://d33wubrfki0l68.cloudfront.net/305d2a8118b4d3daa0d7ae18dd365a065509d3ba/6fd9e/blog/web-scraping-without-getting-blocked/tls_fingerprint_hu2b9762a5f1a093b565f35b754eca36fc_258507_800x0_resize_box_2.png 800w
    
    
      , https://d33wubrfki0l68.cloudfront.net/7263be4b3bcdcc2fdacc7ba0b13db7d06e54ba26/70fe5/blog/web-scraping-without-getting-blocked/tls_fingerprint_hu2b9762a5f1a093b565f35b754eca36fc_258507_1200x0_resize_box_2.png 1200w
    
    
      , https://d33wubrfki0l68.cloudfront.net/2a0a36f2cce58adc621adfa4ca186d34a55dbc21/e2bb0/blog/web-scraping-without-getting-blocked/tls_fingerprint_hu2b9762a5f1a093b565f35b754eca36fc_258507_1500x0_resize_box_2.png 1500w 
    " data-src="https://d33wubrfki0l68.cloudfront.net/17e964b138592d780d893526fed9892274ae043d/6210b/blog/web-scraping-without-getting-blocked/tls_fingerprint.png" width="1624" height="1138" alt="Most used fingerprint screenshot" srcset="
    
      https://d33wubrfki0l68.cloudfront.net/d94e2bfe0ab583be9ec688887b5ddc3599ed3321/b6df3/blog/web-scraping-without-getting-blocked/tls_fingerprint_hu2b9762a5f1a093b565f35b754eca36fc_258507_500x0_resize_box_2.png 500w
    
    
      , https://d33wubrfki0l68.cloudfront.net/305d2a8118b4d3daa0d7ae18dd365a065509d3ba/6fd9e/blog/web-scraping-without-getting-blocked/tls_fingerprint_hu2b9762a5f1a093b565f35b754eca36fc_258507_800x0_resize_box_2.png 800w
    
    
      , https://d33wubrfki0l68.cloudfront.net/7263be4b3bcdcc2fdacc7ba0b13db7d06e54ba26/70fe5/blog/web-scraping-without-getting-blocked/tls_fingerprint_hu2b9762a5f1a093b565f35b754eca36fc_258507_1200x0_resize_box_2.png 1200w
    
    
      , https://d33wubrfki0l68.cloudfront.net/2a0a36f2cce58adc621adfa4ca186d34a55dbc21/e2bb0/blog/web-scraping-without-getting-blocked/tls_fingerprint_hu2b9762a5f1a093b565f35b754eca36fc_258507_1500x0_resize_box_2.png 1500w 
    " src="https://d33wubrfki0l68.cloudfront.net/17e964b138592d780d893526fed9892274ae043d/6210b/blog/web-scraping-without-getting-blocked/tls_fingerprint.png">
  
</p></div>

<figcaption>
    <small> <em> A TLS fingerprint </em> </small>
</figcaption>


<p>This number is very big and at least two orders of magnitude higher than the most common browser fingerprint. It's actually logical as a TLS fingerprint is computed using way fewer parameters than a browser fingerprint.</p>
<p>Those parameters are, amongst others:</p>
<ul>
<li>TLS version</li>
<li>Handshake version</li>
<li>Cipher suites supported</li>
<li>Extensions</li>
</ul>
<p>If you wish to know what is your TLS fingerprint I suggest you go visit <a href="https://clienttest.ssllabs.com:8443/ssltest/viewMyClient.html">this website</a>.</p>
<h4 id="how-do-i-change-it">How do I change it?</h4>
<p>Ideally, in order to increase your stealth, you should be changing your TLS parameters when doing web scraping. However, this is harder than it looks.</p>
<p>Firstly, because there are not <em>that many</em> TLS fingerprint out there, simply randomizing those parameters won't work because your fingerprint will be so rare that it will be instantly flagged as fake.</p>
<p>Secondly, TLS parameters are low-level stuff that rely heavily on system dependencies, and changing them is not as straight-forward as it seems.</p>
<p>For examples, the famous Python <code>requests</code> module doesn't support it out of the box. Here are a few resources to do this kind of things in your favorite language:</p>
<ul>
<li>Python with <a href="https://hussainaliakbar.github.io/restricting-tls-version-and-cipher-suites-in-python-requests-and-testing-with-wireshark/">requests</a></li>
<li>NodeJS with the <a href="https://nodejs.org/api/tls.html">TLS package</a></li>
<li>Ruby with <a href="https://docs.ruby-lang.org/en/master/OpenSSL/SSL/SSLContext.html">OpenSSL</a></li>
</ul>
<p>Keep in mind that most of these libraries rely on the SSL and TLS implementation of your system, OpenSSL is the most widely used, and you might need to change its version in order to completely alter your fingerprint.</p>
<h2 id="emulate-human-behaviour-ie-proxy-captchas-solving-and-request-pattern">Emulate human behaviour i.e: Proxy, Captchas solving and Request pattern</h2>
<h3 id="proxy-yourself">Proxy yourself</h3>
<p>A human using a real browser will rarely request 20 pages per second from the same website, so if you want to request a lot of page from the same website you have to trick this website into thinking that all those requests come from a different place in the world i.e: different I.P addresses. In other words, you need to use <a href="https://www.stupidproxy.com/proxy-servers/">proxies</a>.</p>
<p>Proxies are now not very expensive: ~1$ per IP. However, if you need to do more than ~10k requests per day on the same website, costs can go up quickly, with hundreds of addresses needed. One thing to consider is that proxies IPs needs to be constantly monitored in order to discard the one that is not working anymore and replace it.</p>
<p>There are <a href="https://www.bestproxyreviews.com/rotating-proxies/">several proxy solutions</a> in the market, here are the most used: <a href="https://luminati.io/">Luminati Network</a>, <a href="https://blazingseollc.com/">Blazing SEO</a> and <a href="https://smartproxy.com/">SmartProxy</a>.</p>
<p>There is also a lot of free proxy list and I don’t recommend using these because there are often slow, unreliable, and websites offering these lists are not always transparent about where these proxies are located. Those free proxy lists are most of the time public, and therefore, their IPs will be automatically banned by the most website. Proxy quality is very important, anti crawling services are known to maintain an internal list of proxy IP so every traffic coming from those IPs will also be blocked. Be careful to choose a good reputation Proxy. This is why I recommend using a paid proxy network or build your own</p>
<p>To build your on you could take a look at <a href="https://scrapoxy.io/">scrapoxy</a>, a great open-source API, allowing you to build a proxy API on top of different cloud providers. Scrapoxy will create a proxy pool by creating instances on various cloud providers (AWS, OVH, Digital Ocean). Then you will be able to configure your client so it uses the Scrapoxy URL as the main proxy, and Scrapoxy it will automatically assign a proxy inside the proxy pool. Scrapoxy is easily customizable to fit your needs (rate limit, blacklist …) but can be a little tedious to put in place.</p>
<p>You could also use the TOR network, aka, <a href="https://www.torproject.org/">The Onion Router</a>. It is a worldwide computer network designed to route traffic through many different servers to hide its origin. TOR usage makes network surveillance/traffic analysis very difficult. There are a lot of use cases for TOR usage, such as privacy, freedom of speech, journalists in the dictatorship regime, and of course, illegal activities. In the context of web scraping, TOR can hide your IP address, and change your bot’s IP address every 10 minutes. The TOR exit nodes IP addresses are public. Some websites block TOR traffic using a simple rule: if the server receives a request from one of the TOR public exit nodes, it will block it. That’s why in many cases, TOR won’t help you, compared to classic proxies. It's worth noting that traffic through TOR is also inherently much slower because of the multiple routing thing.</p>
<h3 id="captchas">Captchas</h3>
<p>But sometimes proxies will not be enough, some websites systematically ask you to confirm that you are a human with so-called CAPTCHAs. Most of the time CAPTCHAs are only displayed to suspicious IP, so switching proxy will work in those cases. For the other cases, you'll need to use CAPTCHAs solving service (<a href="https://2captcha.com/">2Captchas</a> and <a href="https://www.deathbycaptcha.com/user/login">DeathByCaptchas</a> come to mind).</p>
<p>You have to know that while some Captchas can be automatically resolved with optical character recognition (OCR), the most recent one has to be solved by hand.</p>
























<div>
  
  <p><img data-sizes="auto" data-srcset="
    
      https://d33wubrfki0l68.cloudfront.net/212a58d2c8ab1322aa224a092454c2d8fd88edee/9d845/blog/web-scraping-without-getting-blocked/fq70vlkx0ud047kn9us7_hud81d82b09dc986eaf57617dc290e36df_100748_500x0_resize_box_2.png 500w
    
    
    
    " data-src="https://d33wubrfki0l68.cloudfront.net/1eeb808b2e53750600074aeeb07f8bd1076c5e3a/24597/blog/web-scraping-without-getting-blocked/fq70vlkx0ud047kn9us7.png" width="528" height="298" alt="Old captcha screenshot" srcset="
    
      https://d33wubrfki0l68.cloudfront.net/212a58d2c8ab1322aa224a092454c2d8fd88edee/9d845/blog/web-scraping-without-getting-blocked/fq70vlkx0ud047kn9us7_hud81d82b09dc986eaf57617dc290e36df_100748_500x0_resize_box_2.png 500w
    
    
    
    " src="https://d33wubrfki0l68.cloudfront.net/1eeb808b2e53750600074aeeb07f8bd1076c5e3a/24597/blog/web-scraping-without-getting-blocked/fq70vlkx0ud047kn9us7.png">
  
</p></div>

<figcaption>
    <small> <em> Old captcha, breakable programatically </em> </small>
</figcaption>


























<div>
  
  <p><img data-sizes="auto" data-srcset="
    
      https://d33wubrfki0l68.cloudfront.net/6a5767215cf7bafba8e440ddd643d5cb1ed56a09/635b2/blog/web-scraping-without-getting-blocked/68acxo4r89rla6u385qm_huc051d04cd1512f1486d560eff4abb497_17214_500x0_resize_box_2.png 500w
    
    
    
    " data-src="https://d33wubrfki0l68.cloudfront.net/55e51d0ff1b14eee1b7275ccd4a7ac584deef754/1f002/blog/web-scraping-without-getting-blocked/68acxo4r89rla6u385qm.png" width="522" height="136" alt="Google Recaptcha V2 screenshot" srcset="
    
      https://d33wubrfki0l68.cloudfront.net/6a5767215cf7bafba8e440ddd643d5cb1ed56a09/635b2/blog/web-scraping-without-getting-blocked/68acxo4r89rla6u385qm_huc051d04cd1512f1486d560eff4abb497_17214_500x0_resize_box_2.png 500w
    
    
    
    " src="https://d33wubrfki0l68.cloudfront.net/55e51d0ff1b14eee1b7275ccd4a7ac584deef754/1f002/blog/web-scraping-without-getting-blocked/68acxo4r89rla6u385qm.png">
  
</p></div>

<figcaption>
    <small> <em> Google ReCaptcha V2 </em> </small>
</figcaption>


<p>What it means is that if you use those aforementioned services, on the other side of the API call you'll have hundreds of people resolving CAPTCHAs for as low as <a href="https://2captcha.com/make-money-online">20ct an hour</a>.</p>
<p>But then again, even if you solve CAPCHAs or switch proxy as soon as you see one, websites can still detect your little scraping job.</p>
<h3 id="request-pattern">Request Pattern</h3>
<p>A last advanced tool used by the website to detect scraping is pattern recognition. So if you plan to scrap every ids from 1 to 10 000 for the URL <a href="http://www.example.com/product/">www.example.com/product/</a><!-- raw HTML omitted -->, try not to do it sequentially and with a constant rate of request. You could, for example, maintain a set of integer going from 1 to 10 000 and randomly choose one integer inside this set and then scraping your product.</p>
<p>This one simple example, some websites also do statistic on browser fingerprint per endpoint. Which means that if you don't change some parameters in your headless browser and target a single endpoint, they might block you anyway.</p>
<p>Websites also tend to monitor the origin of traffic, so if you want to scrape a website if Brazil, try not doing it with proxies in Vietnam for example.</p>
<p>But from experience, what I can tell, is that rate is the most important factor in “Request Pattern Recognition”, sot the slower you scrape, the less chance you have to be discovered.</p>
<h2 id="emulate-machine-behaviour-ie-reverse-engineering-of-api">Emulate machine behaviour i.e Reverse engineering of API</h2>
<p>Sometimes, the server expect the client to be a machine. In those case, hiding yourself is way easier.</p>
<h3 id="reverse-engineering-of-api">Reverse engineering of API</h3>
<p>Basically, this “trick” comes down to two things:</p>
<ol>
<li>Analyzing a web page behaviour to find interesting API calls</li>
<li>Forging those API calls with your code</li>
</ol>
<p>For example, let's say that I want to get back all the comments of a famous social network. I notice that when I click on the “load more comments” button, this happens in my inspector:</p>
























<div>
  
  <p><img data-sizes="auto" data-srcset="
    
      https://d33wubrfki0l68.cloudfront.net/0eef47e0431d19f27ecb21d0d47d22aa767b4a5a/89156/blog/web-scraping-without-getting-blocked/comments_ajax_hu01cb36299423fe76ca5a49220b3bc86b_42892_500x0_resize_box_2.png 500w
    
    
    
    " data-src="https://d33wubrfki0l68.cloudfront.net/e19d103e0471f0e8b99fb896f1144732efb4f176/14e3a/blog/web-scraping-without-getting-blocked/comments_ajax.png" width="660" height="269" alt="Request being made when clicking more comments" srcset="
    
      https://d33wubrfki0l68.cloudfront.net/0eef47e0431d19f27ecb21d0d47d22aa767b4a5a/89156/blog/web-scraping-without-getting-blocked/comments_ajax_hu01cb36299423fe76ca5a49220b3bc86b_42892_500x0_resize_box_2.png 500w
    
    
    
    " src="https://d33wubrfki0l68.cloudfront.net/e19d103e0471f0e8b99fb896f1144732efb4f176/14e3a/blog/web-scraping-without-getting-blocked/comments_ajax.png">
  
</p></div>

<figcaption>
    <small> <em> Request being made when clicking more comments </em> </small>
</figcaption>


<p>Notice that we filter out every requests except “XHR” ones to avoid noise.</p>
<p>And when we try to see what request is being made and what response do we get, bingo!</p>
























<div>
  
  <p><img data-sizes="auto" data-srcset="
    
      https://d33wubrfki0l68.cloudfront.net/437eb64a9a757820c4e7fbe0b86346f02dc4c3a6/55b3a/blog/web-scraping-without-getting-blocked/response_ajax_hub69b4fe87b9562e37ec6a3a8169baeab_103075_500x0_resize_box_2.png 500w
    
    
      , https://d33wubrfki0l68.cloudfront.net/c04783be28f0269b5c02368753814bf410965d84/628cb/blog/web-scraping-without-getting-blocked/response_ajax_hub69b4fe87b9562e37ec6a3a8169baeab_103075_800x0_resize_box_2.png 800w
    
    
    " data-src="https://d33wubrfki0l68.cloudfront.net/713994ca3ec27c5d6f4a5fef502e35064c66584f/e26d3/blog/web-scraping-without-getting-blocked/response_ajax.png" width="872" height="451" alt="Request response" srcset="
    
      https://d33wubrfki0l68.cloudfront.net/437eb64a9a757820c4e7fbe0b86346f02dc4c3a6/55b3a/blog/web-scraping-without-getting-blocked/response_ajax_hub69b4fe87b9562e37ec6a3a8169baeab_103075_500x0_resize_box_2.png 500w
    
    
      , https://d33wubrfki0l68.cloudfront.net/c04783be28f0269b5c02368753814bf410965d84/628cb/blog/web-scraping-without-getting-blocked/response_ajax_hub69b4fe87b9562e37ec6a3a8169baeab_103075_800x0_resize_box_2.png 800w
    
    
    " src="https://d33wubrfki0l68.cloudfront.net/713994ca3ec27c5d6f4a5fef502e35064c66584f/e26d3/blog/web-scraping-without-getting-blocked/response_ajax.png">
  
</p></div>

<figcaption>
    <small> <em> Request response </em> </small>
</figcaption>


<p>Now if we look at the “Headers” tab we should have everything we need to replay this request and understand the value of each parameters. This will allow us to make this request from a simple HTTP client.</p>
























<div>
  
  <p><img data-sizes="auto" data-srcset="
    
      https://d33wubrfki0l68.cloudfront.net/bcf617868ca509dee121d29a96249943d2bfb462/c95d7/blog/web-scraping-without-getting-blocked/paw_response_hu97ae997b1e6d78c9689eee3181af9ae7_224120_500x0_resize_box_2.png 500w
    
    
      , https://d33wubrfki0l68.cloudfront.net/dcc965b385510a21f51915afc06caaf3af2aa0c7/c7990/blog/web-scraping-without-getting-blocked/paw_response_hu97ae997b1e6d78c9689eee3181af9ae7_224120_800x0_resize_box_2.png 800w
    
    
      , https://d33wubrfki0l68.cloudfront.net/1c5e5023e81af156a086175a8bf3e348a226f75e/cb3c5/blog/web-scraping-without-getting-blocked/paw_response_hu97ae997b1e6d78c9689eee3181af9ae7_224120_1200x0_resize_box_2.png 1200w
    
    
      , https://d33wubrfki0l68.cloudfront.net/d631b14d1f84f67264450cdea0f9de15e532c900/5e109/blog/web-scraping-without-getting-blocked/paw_response_hu97ae997b1e6d78c9689eee3181af9ae7_224120_1500x0_resize_box_2.png 1500w 
    " data-src="https://d33wubrfki0l68.cloudfront.net/5f61c5236d42174e8af75938ffdaedb32d7dc3ea/cb411/blog/web-scraping-without-getting-blocked/paw_response.png" width="1574" height="992" alt="HTTP Client response" srcset="
    
      https://d33wubrfki0l68.cloudfront.net/bcf617868ca509dee121d29a96249943d2bfb462/c95d7/blog/web-scraping-without-getting-blocked/paw_response_hu97ae997b1e6d78c9689eee3181af9ae7_224120_500x0_resize_box_2.png 500w
    
    
      , https://d33wubrfki0l68.cloudfront.net/dcc965b385510a21f51915afc06caaf3af2aa0c7/c7990/blog/web-scraping-without-getting-blocked/paw_response_hu97ae997b1e6d78c9689eee3181af9ae7_224120_800x0_resize_box_2.png 800w
    
    
      , https://d33wubrfki0l68.cloudfront.net/1c5e5023e81af156a086175a8bf3e348a226f75e/cb3c5/blog/web-scraping-without-getting-blocked/paw_response_hu97ae997b1e6d78c9689eee3181af9ae7_224120_1200x0_resize_box_2.png 1200w
    
    
      , https://d33wubrfki0l68.cloudfront.net/d631b14d1f84f67264450cdea0f9de15e532c900/5e109/blog/web-scraping-without-getting-blocked/paw_response_hu97ae997b1e6d78c9689eee3181af9ae7_224120_1500x0_resize_box_2.png 1500w 
    " src="https://d33wubrfki0l68.cloudfront.net/5f61c5236d42174e8af75938ffdaedb32d7dc3ea/cb411/blog/web-scraping-without-getting-blocked/paw_response.png">
  
</p></div>

<figcaption>
    <small> <em> HTTP Client response </em> </small>
</figcaption>


<p>The hardest part of this process is to understand the role of each parameters in the request. Know that you can left-click on any request in the Chrome dev tool inspector, export in HAR format and then import it in your favorite HTTP client, (I love <a href="https://paw.cloud/">Paw</a> and <a href="https://www.postman.com/product/api-client/">PostMan</a>).</p>
<p>This will allow you to have all the parameters of a working request laid out and will make your experimentation much faster and fun.</p>
























<div>
  
  <p><img data-sizes="auto" data-srcset="
    
      https://d33wubrfki0l68.cloudfront.net/46f2d6945ba8d22b443f67ca7477ec59449bca3c/5de49/blog/web-scraping-without-getting-blocked/paw_example_hu6c7335cedf2d96541b60b504f823a654_102621_500x0_resize_box_2.png 500w
    
    
      , https://d33wubrfki0l68.cloudfront.net/f8444b8582fefd912fc2ef526d5d8a5ede0a3573/d659e/blog/web-scraping-without-getting-blocked/paw_example_hu6c7335cedf2d96541b60b504f823a654_102621_800x0_resize_box_2.png 800w
    
    
    " data-src="https://d33wubrfki0l68.cloudfront.net/beeba2625939c3dfd01c679782eaba2528d92130/10d2f/blog/web-scraping-without-getting-blocked/paw_example.png" width="1078" height="732" alt="Paw Example" srcset="
    
      https://d33wubrfki0l68.cloudfront.net/46f2d6945ba8d22b443f67ca7477ec59449bca3c/5de49/blog/web-scraping-without-getting-blocked/paw_example_hu6c7335cedf2d96541b60b504f823a654_102621_500x0_resize_box_2.png 500w
    
    
      , https://d33wubrfki0l68.cloudfront.net/f8444b8582fefd912fc2ef526d5d8a5ede0a3573/d659e/blog/web-scraping-without-getting-blocked/paw_example_hu6c7335cedf2d96541b60b504f823a654_102621_800x0_resize_box_2.png 800w
    
    
    " src="https://d33wubrfki0l68.cloudfront.net/beeba2625939c3dfd01c679782eaba2528d92130/10d2f/blog/web-scraping-without-getting-blocked/paw_example.png">
  
</p></div>

<figcaption>
    <small> <em> Previous request imported in Paw </em> </small>
</figcaption>


<h3 id="reverse-engineering-of-mobile-app">Reverse engineering of mobile app</h3>
<p>The same principles apply when it comes to reverse engineering mobile app. You will want to intercept the request your mobile app is making to the server and replay it with your code.</p>
<p>Doing this is harder for two reasons:</p>
<ul>
<li>to intercept requests you will need a Man In The Middle proxy. (<a href="https://www.charlesproxy.com/download/">Charles</a> for example)</li>
<li>mobile app can fingerprint your request and obfuscate them more easily than a web app</li>
</ul>
<p>For example, when Pokemon Go was released a few years ago, tons of people cheated the game after reverse engineering the requests the mobile app made.</p>
<p>What they did not know was that the mobile app was sending a “secret” parameter that was not sent by the cheating script. It was very easy for Niantic to then identify the cheaters. A few weeks after, a massive amount of players got banned due to this.</p>
<p>Also, here is an interesting example about someone who reverse engineered the <a href="https://blog.tendigi.com/starbucks-should-really-make-their-apis-public-6b64a1c2e923">Starbucks API</a>.</p>
<h2 id="conclusion">Conclusion</h2>
<p>I hope that this overview will help you understand better web-scraping and that you learned things reading this post.</p>
<p>Everything I talked about in this post is things we leverage at <a href="https://www.scrapingbee.com/">ScrapingBee</a>, a web scraping API, to handle thousands of requests per seconds, without ever being blocked. Do not hesitate to test our solution if you don’t want to lose too much time setting everything up, the first 1k API calls are on us :).</p>
<p>We recently published a guide about the best <a href="https://www.scrapingbee.com/blog/web-scraping-tools/">web scraping tools</a> on the market, don't hesitate to take a look!</p>

        </div></div></div>
    </div>
    <footer>
        <div>created by <a href="https://buildstarted.com">buildstarted</a> &copy; 2020 <a href="/about">about</a></div>
        <div>Share this page on social media: copy and paste this url https://linksfor.dev/</div>
        <div>If you prefer RSS: <a href="https://linksfor.dev/feed.xml">https://linksfor.dev/feed.xml</a></div>
        <div>Customer satisfaction guaranteed to be optional.</div>
    </footer>
    
    <script async defer>
        _dna = window._dna || {};
        _dna.siteId = "linksfor.devs";
        _dna.outlink = true;

        (function() {
            let dna = document.createElement('script');
            dna.type = 'text/javascript';
            dna.async = true;
            dna.src = '//dna.buildstarted.com/t.js';
            let s = document.getElementsByTagName('script')[0];
            s.parentNode.insertBefore(dna, s);
        })();
    </script>
    <noscript><img src="//dna.buildstarted.com/g?siteId=linksfor.devs"/></noscript>
</body>
</html>