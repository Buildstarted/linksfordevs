<!DOCTYPE html>
<html lang="en">
<head>
    <title>
RL &#x2014; Proximal Policy Optimization (PPO) Explained - Jonathan Hui -
linksfor.dev(s)
    </title>
    <link rel="alternate" type="application/rss+xml" title="Linksfor.dev(s) feed" href="https://linksfor.dev/feed.rss" />
    <meta charset="utf-8">
    <meta name="Description" content="A curated source of links that devs might find interesting. Updated around the clock." />
    <meta name="google" value="notranslate">
    <link rel="stylesheet" type="text/css" href="/style.css" />
    <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon">
    <link rel="icon" href="/favicon.ico" type="image/x-icon">
</head>
<body>
    <div class="grid">
        
<div class="readable">
    <div id="readOverlay" class="style-ebook"><div id="readInner" class="margin-medium size-medium"><h1>RL — Proximal Policy Optimization (PPO) Explained - Jonathan Hui</h1><div><div class="ac ae af ag ah do aj ak"><p id="53ee" class="hm hn ar bz ho b hp hq hr hs ht hu hv hw hx hy hz dj">A quote from OpenAI on PPO:</p><blockquote class="ia ib ic"><p id="187e" class="hm hn ar id ho b hp hq hr hs ht hu hv hw hx hy hz dj">Proximal Policy Optimization (PPO), which perform comparably or better than state-of-the-art approaches while being much simpler to implement and tune.</p></blockquote><p id="56a2" class="hm hn ar bz ho b hp hq hr hs ht hu hv hw hx hy hz dj">Actually, this is a very humble statement comparing with its real impact. Policy Gradient methods have convergence problem which is addressed by the natural policy gradient. However, in practice, natural policy gradient involves a second-order derivative matrix which makes it not scalable for large scale problems. The computational complexity is too high for real tasks. Intensive research is done to reduce the complexity by approximate the second-order method. PPO uses a slightly different approach. Instead of imposing a hard constraint, it formalizes the constraint as a penalty in the objective function. By not avoiding the constraint at all cost, we can use a first-order optimizer like the Gradient Descent method to optimize the objective. Even we may violate the constraint once a while, the damage is far less and the computation is much simple. Let’s go through quickly on the basic concepts before explaining PPO in details.</p><h1 id="1cc6" class="ie if ar bz by ig ds ih du ii ij ik il im in io ip">Minorize-Maximization MM algorithm</h1><p id="fe92" class="hm hn ar bz ho b hp iq hr ir ht is hv it hx iu hz dj">How can we optimize a policy to maximize the rewards?</p><p id="980d" class="hm hn ar bz ho b hp hq hr hs ht hu hv hw hx hy hz dj">With the Minorize-Maximization <strong class="ho iv">MM</strong> algorithm, this is achieved <strong class="ho iv">iteratively</strong> by maximizing a lower bound function <strong class="ho iv"><em class="id">M</em></strong> (the blue line below) approximating the expected reward <em class="id">η </em>locally<em class="id">.</em></p><figure class="gg gh gi gj gk fw da db paragraph-image"><figcaption class="cd eu hd he hf dc da db hg hh by et">Modified from <a href="https://drive.google.com/file/d/0BxXI_RttTZAhMVhsNk5VSXU0U3c/view" class="bi cv hi hj hk hl" target="_blank" rel="noopener nofollow">source</a></figcaption></figure><p id="05a8" class="hm hn ar bz ho b hp hq hr hs ht hu hv hw hx hy hz dj">First, we start with an initial policy guess and find a lower bound <em class="id">M</em> for <em class="id">η </em>at this policy. We optimize M and use the optimal policy for <em class="id">M</em> as the next guess. We approximate a new lower bound again at the new guess and repeat the iterations until the policy converges. To make it works, we do need to find a lower bound <em class="id">M</em> that is easier to optimize.</p><h1 id="8b9f" class="ie if ar bz by ig ds ih du ii ij ik il im in io ip">Line search</h1><p id="2909" class="hm hn ar bz ho b hp iq hr ir ht is hv it hx iu hz dj">There are two major optimization methods: the line search like the gradient descent and the trust region. Gradient descent is easy, fast and simple in optimizing an objective function. This is why it is so popular in deep learning even more accurate methods are available.</p><p id="7e62" class="hm hn ar bz ho b hp hq hr hs ht hu hv hw hx hy hz dj">Line search first picks the steepest direction and then move forward by a step size. But how can this strategy go wrong in reinforcement learning RL? Let’s take a robot to the Angels Landing for hiking. As shown below, we ascend the hill by determining the direction first. If the step size is too small, it will take forever to get to the peak. But if it is too big, we can fall down the cliff. Even if the robot survives the fall, it lands in areas with height much lower than where we were. Policy gradient is mainly an on-policy method. It searches actions from the current state. Hence, we resume the exploration from a bad state with a locally bad policy. This hurts performance badly.</p><figure class="gg gh gi gj gk fw da db paragraph-image"><figcaption class="cd eu hd he hf dc da db hg hh by et">Modified from <a href="https://pronetowanderfriends.wordpress.com/2014/02/23/angels-landing-in-zion-national-park-check/" class="bi cv hi hj hk hl" target="_blank" rel="noopener nofollow">Source</a></figcaption></figure><h1 id="cf76" class="ie if ar bz by ig ds ih du ii ij ik il im in io ip">Trust region</h1><p id="4575" class="hm hn ar bz ho b hp iq hr ir ht is hv it hx iu hz dj">In the trust region, we determine the maximum step size that we want to explore first (the yellow circle below). Then we locate the optimal point within the trust region and resume the search from there.</p><figure class="gg gh gi gj gk fw da db paragraph-image"><figcaption class="cd eu hd he hf dc da db hg hh by et">Modified from <a href="https://pronetowanderfriends.wordpress.com/2014/02/23/angels-landing-in-zion-national-park-check/" class="bi cv hi hj hk hl" target="_blank" rel="noopener nofollow">Source</a></figcaption></figure><p id="cd27" class="hm hn ar bz ho b hp hq hr hs ht hu hv hw hx hy hz dj"><strong class="ho iv">What is the maximum step size in a trust region? </strong>In the trust region method, we start with an initial guess. Optionally, we can readjust the region size dynamically. For example, we can shrink the region if the divergence of the new and current policy is getting large (or vice versa). In order not to make bad decisions, we can shrink the trust region if the policy is changing too much.</p><p id="24ad" class="hm hn ar bz ho b hp hq hr hs ht hu hv hw hx hy hz dj">In PPO, we limit how far we can change our policy in each iteration through the KL-divergence. KL-divergence measures the difference between two data distributions <em class="id">p</em> and <em class="id">q</em>.</p><figure class="gg gh gi gj gk fw da db paragraph-image"><p id="aa31" class="hm hn ar bz ho b hp hq hr hs ht hu hv hw hx hy hz dj">We repurpose it to measure the difference between the two policies. We don’t want any new policy to be too different from the current one.</p><figure class="gg gh gi gj gk fw da db paragraph-image"><figcaption class="cd eu hd he hf dc da db hg hh by et">Source: Wikipedia</figcaption></figure><p id="ebef" class="hm hn ar bz ho b hp hq hr hs ht hu hv hw hx hy hz dj">So how can we limit policy change to make sure we don’t make bad decisions? It turns out we can find a lower bound function <strong class="ho iv"><em class="id">M </em></strong>as</p><figure class="gg gh gi gj gk fw da db paragraph-image"><figure class="gg gh gi gj gk fw da db paragraph-image"><p id="74b5" class="hm hn ar bz ho b hp hq hr hs ht hu hv hw hx hy hz dj">with <strong class="ho iv"><em class="id">L(θ) </em></strong>equals</p><figure class="gg gh gi gj gk fw da db paragraph-image"><p id="ad49" class="hm hn ar bz ho b hp hq hr hs ht hu hv hw hx hy hz dj">and the second terms is the KL-divergence.</p><p id="4e36" class="hm hn ar bz ho b hp hq hr hs ht hu hv hw hx hy hz dj"><em class="id">L</em> is the expected advantage function (the expected rewards minus a baseline like V(s)) for the new policy. It is estimated by an old (or current) policy and then recalibrate using the probability ratio between the new and the old policy. We use the advantage function instead of the expected reward because it reduces the variance of the estimation. As long as the baseline does not dependent on our policy parameters, the optimal policy will be the same.</p><p id="9989" class="hm hn ar bz ho b hp hq hr hs ht hu hv hw hx hy hz dj">Let’s look into more detail on the second term in <strong class="ho iv"><em class="id">M</em></strong>. After two page of proof in the TRPO paper, we can establish the following lower bound.</p><figure class="gg gh gi gj gk fw da db paragraph-image"><p id="4a28" class="hm hn ar bz ho b hp hq hr hs ht hu hv hw hx hy hz dj">The second term in <strong class="ho iv"><em class="id">M</em></strong> is the maximum of KL-divergence underlined in red above. But it is too hard to find and therefore we relax the requirement a little bit by using the mean of the KL-divergence instead. Let’s explain that intuitively.</p><p id="9595" class="hm hn ar bz ho b hp hq hr hs ht hu hv hw hx hy hz dj"><strong class="ho iv">Intuition</strong></p><p id="fcf5" class="hm hn ar bz ho b hp hq hr hs ht hu hv hw hx hy hz dj"><em class="id">L</em> approximates the advantage function locally at the current policy. But it gets less accurate as it moves away from the old policy. This inaccuracy has an upper bound. That is the second term in <strong class="ho iv"><em class="id">M</em></strong>. After considering the upper bound of this error, we can guarantee that the calculated optimal policy within the trust region is always better than the old policy. If the policy is outside the trust region, even the calculated value may be better but the accuracy can be too off and cannot be trusted. So the bet is off. Let’s summarize the objective below as:</p><figure class="gg gh gi gj gk fw da db paragraph-image"><p id="611a" class="hm hn ar bz ho b hp hq hr hs ht hu hv hw hx hy hz dj">Mathematically, both equations above can be resolved to the same optimal policy. However, the theoretical threshold for <strong class="ho iv"><em class="id">δ </em></strong>is very small and is considered to be too conservative. So we relax the condition once more by setting them as tunable hyperparameters.</p><p id="5224" class="hm hn ar bz ho b hp hq hr hs ht hu hv hw hx hy hz dj">As mentioned before, <strong class="ho iv"><em class="id">M</em></strong> should be easy to optimize. So we further approximate it to a quadratic equation which is a convex function and heavily study on how to optimize it in high dimensional space.</p><p id="a0e5" class="hm hn ar bz ho b hp hq hr hs ht hu hv hw hx hy hz dj">We use Taylor’s series to expand the terms up to the second-order. But the second-order of 𝓛 is much smaller than the KL-divergence term and will be ignored.</p><figure class="gg gh gi gj gk fw da db paragraph-image"><figure class="gg gh gi gj gk fw da db paragraph-image"><p id="7940" class="hm hn ar bz ho b hp hq hr hs ht hu hv hw hx hy hz dj">Therefore, the objective and the constraint can be approximated as:</p><figure class="gg gh gi gj gk fw da db paragraph-image"><p id="849f" class="hm hn ar bz ho b hp hq hr hs ht hu hv hw hx hy hz dj">Our objective becomes:</p><figure class="gg gh gi gj gk fw da db paragraph-image"><p id="0d27" class="hm hn ar bz ho b hp hq hr hs ht hu hv hw hx hy hz dj">We can solve this quadratic equation analytical. The solution is:</p><figure class="gg gh gi gj gk fw da db paragraph-image"><p id="e9e7" class="hm hn ar bz ho b hp hq hr hs ht hu hv hw hx hy hz dj"><strong class="ho iv">What are the challenges?</strong></p><p id="e451" class="hm hn ar bz ho b hp hq hr hs ht hu hv hw hx hy hz dj">This solution involves the calculation of the second-order derivative and its inverse, a very expensive operation.</p><figure class="gg gh gi gj gk fw da db paragraph-image"><p id="591c" class="hm hn ar bz ho b hp hq hr hs ht hu hv hw hx hy hz dj">So there are two approaches to address this problem:</p><ul class=""><li id="7451" class="hm hn ar bz ho b hp hq hr hs ht hu hv hw hx hy hz ju jv jw">Approximate some calculations involving the second order derivative and its inverse to lower the computational complexity, or</li><li id="84d2" class="hm hn ar bz ho b hp jx hr jy ht jz hv ka hx kb hz ju jv jw">Make the first order derivative solution, like the gradient descent, closer to the second-order derivative solution by adding soft constraints.</li></ul><p id="d727" class="hm hn ar bz ho b hp hq hr hs ht hu hv hw hx hy hz dj">TRPO and ACKTR adopt the first approach. PPO is closer to the second one. We can still live with a bad policy decision once a while so we stick with the first-order solution like the stochastic gradient descent. But we are going to add a soft constraint to the objective function so the optimization will have better insurance that we are optimizing within a trust region. So the chance of bad decision is smaller.</p><h1 id="dae1" class="ie if ar bz by ig ds ih du ii ij ik il im in io ip">PPO with Adaptive KL Penalty</h1><p id="b6fc" class="hm hn ar bz ho b hp iq hr ir ht is hv it hx iu hz dj">One way to formulate our objective is changing the constraint to a penalty in the objective function:</p><figure class="gg gh gi gj gk fw da db paragraph-image"><p id="b334" class="hm hn ar bz ho b hp hq hr hs ht hu hv hw hx hy hz dj"><em class="id">β </em>controls the weight of the penalty. It penalizes the objective if the new policy is different from the old policy. Borrow a page from the trust region, we can dynamically adjust <em class="id">β. d</em> below is the KL-divergence between the old and the new policy. If it is higher than a target value, we shrink <em class="id">β</em>. Similarly, if it falls below another target value, we expand the trust region.</p><figure class="gg gh gi gj gk fw da db paragraph-image"><p id="da61" class="hm hn ar bz ho b hp hq hr hs ht hu hv hw hx hy hz dj">Here is the algorithm:</p><figure class="gg gh gi gj gk fw da db paragraph-image"><figcaption class="cd eu hd he hf dc da db hg hh by et"><a href="http://rail.eecs.berkeley.edu/deeprlcourse-fa17/f17docs/lecture_13_advanced_pg.pdf" class="bi cv hi hj hk hl" target="_blank" rel="noopener nofollow">Source</a></figcaption></figure><p id="e0c8" class="hm hn ar bz ho b hp hq hr hs ht hu hv hw hx hy hz dj">This gets us the performance of TRPO with speed closer to the gradient descent method. But can we do better?</p><h1 id="aa5b" class="ie if ar bz by ig ds ih du ii ij ik il im in io ip">PPO with Clipped Objective</h1><p id="5840" class="hm hn ar bz ho b hp iq hr ir ht is hv it hx iu hz dj">PPO with clipped objective can even do better. In its implementation, we maintain two policy networks. The first one is the current policy that we want to refine.</p><figure class="gg gh gi gj gk fw da db paragraph-image"><p id="71dd" class="hm hn ar bz ho b hp hq hr hs ht hu hv hw hx hy hz dj">The second is the policy that we last used to collect samples.</p><figure class="gg gh gi gj gk fw da db paragraph-image"><p id="bfa8" class="hm hn ar bz ho b hp hq hr hs ht hu hv hw hx hy hz dj">With the idea of importance sampling, we can evaluate a new policy with samples collected from an older policy. This improves sample efficiency.</p><figure class="gg gh gi gj gk fw da db paragraph-image"><p id="5c22" class="hm hn ar bz ho b hp hq hr hs ht hu hv hw hx hy hz dj">But as we refine the current policy, the difference between the current and the old policy is getting larger. The variance of the estimation will increase and we will make bad decision because of the inaccuracy. So, say for every 4 iterations, we synchronize the second network with the refined policy again.</p><figure class="gg gh gi gj gk fw da db paragraph-image"><p id="4363" class="hm hn ar bz ho b hp hq hr hs ht hu hv hw hx hy hz dj">With clipped objective, we compute a ratio between the new policy and the old policy:</p><figure class="gg gh gi gj gk fw da db paragraph-image"><p id="1a52" class="hm hn ar bz ho b hp hq hr hs ht hu hv hw hx hy hz dj">This ratio measures how difference between two policies. We construct a new objective function to clip the estimated advantage function if the new policy is far away from the old policy. Our new objective function becomes:</p><figure class="gg gh gi gj gk fw da db paragraph-image"><p id="45e3" class="hm hn ar bz ho b hp hq hr hs ht hu hv hw hx hy hz dj">If the probability ratio between the new policy and the old policy falls outside the range (1 — ε) and (1 + ε), the advantage function will be clipped. ε is set to 0.2 for the experiments in the PPO paper.</p><figure class="gg gh gi gj gk fw da db paragraph-image"><figcaption class="cd eu hd he hf dc da db hg hh by et"><a href="https://arxiv.org/pdf/1707.06347.pdf" class="bi cv hi hj hk hl" target="_blank" rel="noopener nofollow">Source</a></figcaption></figure><p id="1fcb" class="hm hn ar bz ho b hp hq hr hs ht hu hv hw hx hy hz dj">Effectively, this discourages large policy change if it is outside our comfortable zone.</p><p id="b277" class="hm hn ar bz ho b hp hq hr hs ht hu hv hw hx hy hz dj">Here is the algorithm:</p><figure class="gg gh gi gj gk fw da db paragraph-image"><figcaption class="cd eu hd he hf dc da db hg hh by et"><a href="http://rail.eecs.berkeley.edu/deeprlcourse-fa17/f17docs/lecture_13_advanced_pg.pdf" class="bi cv hi hj hk hl" target="_blank" rel="noopener nofollow">Source</a></figcaption></figure><p id="2c06" class="hm hn ar bz ho b hp hq hr hs ht hu hv hw hx hy hz dj">This new method is simple and can use Gradient Descent like Adam to optimize it. In fact, it is anticlimax for taking so detail analysis on the issue but come up with such a simple solution.</p><h1 id="a761" class="ie if ar bz by ig ds ih du ii ij ik il im in io ip"><strong class="bo">Thoughts</strong></h1><p id="13c6" class="hm hn ar bz ho b hp iq hr ir ht is hv it hx iu hz dj">A quote from the PPO paper:</p><blockquote class="ia ib ic"><p id="ae31" class="hm hn ar id ho b hp hq hr hs ht hu hv hw hx hy hz dj">Q-learning (with function approximation) fails on many simple problems and is poorly understood, vanilla policy gradient methods have poor data efficiency and robustness; and trust region policy optimization (TRPO) is relatively complicated, and is not compatible with architectures that include noise (such as dropout) or parameter sharing (between the policy and value function, or with auxiliary tasks).</p></blockquote><p id="bf2a" class="hm hn ar bz ho b hp hq hr hs ht hu hv hw hx hy hz dj">PPO adds a soft constraint that can be optimized by a first-order optimizer. We may make some bad decisions once a while but it strikes a good balance on the speed of the optimization. Experimental results prove that this kind of balance achieves the best performance with the most simplicity.</p><blockquote class="ia ib ic"><p id="8e00" class="hm hn ar id ho b hp hq hr hs ht hu hv hw hx hy hz dj">Simplicity rules in deep learning.</p></blockquote><p id="5e79" class="hm hn ar bz ho b hp hq hr hs ht hu hv hw hx hy hz dj">Or at least until we invent a super fast GPU.</p><h1 id="9878" class="ie if ar bz by ig ds ih du ii ij ik il im in io ip">Credit and references</h1><p id="1196" class="hm hn ar bz ho b hp iq hr ir ht is hv it hx iu hz dj"><a href="https://arxiv.org/pdf/1707.06347.pdf" class="bi cv hi hj hk hl" target="_blank" rel="noopener nofollow">PPO paper</a></p><p id="d702" class="hm hn ar bz ho b hp hq hr hs ht hu hv hw hx hy hz dj"><a href="http://rail.eecs.berkeley.edu/deeprlcourse/" class="bi cv hi hj hk hl" target="_blank" rel="noopener nofollow">UC Berkeley RL course</a></p><p id="6eab" class="hm hn ar bz ho b hp hq hr hs ht hu hv hw hx hy hz dj"><a href="https://sites.google.com/view/deep-rl-bootcamp/lectures" class="bi cv hi hj hk hl" target="_blank" rel="noopener nofollow">UC Berkeley RL Bootcamp</a></p></figure></figure></figure></figure></figure></figure></figure></figure></figure></figure></figure></figure></figure></figure></figure></figure></figure></figure></figure></figure></div></div></div></div>
</div>
    </div>
    <footer>
        <div>created by buildstarted &copy; 2020 <a href="/about">about</a></div>
        <div>Share this page on social media: copy and paste this url https://linksfor.dev/</div>
        <div>If you prefer RSS: <a href="https://linksfor.dev/feed.xml">https://linksfor.dev/feed.xml</a></div>
    </footer>
    
    <script>
        (function () {
            var COLLECT_URL = "https://dna.buildstarted.com/t";
            var SITE_ID = "linksfor.devs";
            var GLOBAL_VAR_NAME = "__DNA__";

            window[GLOBAL_VAR_NAME] = {};

            window[GLOBAL_VAR_NAME].sendPageView = function () {
                var path = location.pathname;
                var referrer = document.referrer;

                var url = COLLECT_URL + "?siteid=" + SITE_ID + "&p=" + encodeURIComponent(path) + "&r=" + encodeURIComponent(referrer);

                try { fetch(url, { method: "GET" }); } catch (e) { }
            };

            window[GLOBAL_VAR_NAME].sendPageView();
        })();
    </script>
</body>
</html>