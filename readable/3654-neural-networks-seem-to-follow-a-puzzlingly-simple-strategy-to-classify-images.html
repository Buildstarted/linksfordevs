<!DOCTYPE html>
<html lang="en">
<head>
    <title>
Neural Networks seem to follow a puzzlingly simple strategy to classify images -
linksfor.dev(s)
    </title>
    <link rel="alternate" type="application/rss+xml" title="Linksfor.dev(s) feed" href="https://linksfor.dev/feed.rss" />
    <meta charset="utf-8">
    <meta name="Description" content="A curated source of links that devs might find interesting. Updated around the clock." />
    <meta name="google" value="notranslate">
    <link rel="stylesheet" type="text/css" href="/style.css" />
    <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon">
    <link rel="icon" href="/favicon.ico" type="image/x-icon">
</head>
<body>
    <div class="grid">
        
<div class="readable">
    <div id="readOverlay" class="style-ebook"><div id="readInner" class="margin-medium size-medium"><h1>Neural Networks seem to follow a puzzlingly simple strategy to classify images</h1><div><div class="ac ae af ag ah ea aj ak"><p id="04ff" class="hz ia cw bi ib b ic id ie if ig ih ii ij ik il im dv">In this post I will show why state-of-the-art Deep Neural Networks can still recognise scrambled images perfectly well and how this helps to uncover a puzzlingy simple strategy that DNNs seem to use to classify natural images. These findings, <a href="https://openreview.net/pdf?id=SkfMWhAqYQ" class="fi cn in io ip iq" target="_blank" rel="noopener nofollow">published at ICLR 2019</a>, have a number of ramifications: first, they show that solving ImageNet is much simpler than many have thought. Second, the findings allow us to build much more interpretable and transparent image classification pipelines. Third, they explain a number of phenomena observed in modern CNNs like their bias towards texture (<a href="https://openreview.net/forum?id=Bygh9j09KX" class="fi cn in io ip iq" target="_blank" rel="noopener nofollow">see our other paper at ICLR 2019</a> and our <a href="https://blog.usejournal.com/why-deep-learning-works-differently-than-we-thought-ec28823bdbc" class="fi cn in io ip iq" target="_blank" rel="noopener nofollow">corresponding blog post</a>) and their neglect of the spatial ordering of object parts.</p><h2 id="4693" class="ir is cw bi au av it iu iv iw ix iy iz ja jb jc jd">Good ol’ bag-of-features models</h2><p id="2395" class="hz ia cw bi ib b ic je ie jf ig jg ii jh ik ji im dv">In the old days, before Deep Learning, object recognition in natural images used to be fairly simple: define a set of key visual features (“words”), recognize how often each visual feature is present in an image (“bag”) and then classify the image based on these numbers. These models are therefore called “bag-of-features” models (BoF models). For illustration, say we have only two visual features, a human eye and a feather, and we want to classify images into “human” and “bird” class. The simplest BoF model would work as follows: for each eye in the image it increases evidence for “human” by +1. Vice versa, for each feather in the image it will increase evidence for “bird” by +1. Whatever class accumulates the most evidence across the image will be the predicted one.</p><p id="a44d" class="hz ia cw bi ib b ic id ie if ig ih ii ij ik il im dv">A nice property of this simplest BoF model is its interpretability and transparent decision making: we can check exactly which image features carry evidence for a given class, the spatial integration of evidence is super simple (in contrast to the deep non-linear feature integration in deep neural networks) and so it is quite straight-forward to understand how the model reaches its decisions.</p><p id="1c05" class="hz ia cw bi ib b ic id ie if ig ih ii ij ik il im dv">Traditional BoF models have been extremely popular and state-of-the-art before the onset of Deep Learning but quickly fell out of favour due to their subpar classification performance. But are we sure that Deep Neural Networks really use a fundamentally different decision-strategy as BoF-models?</p><h2 id="9537" class="ir is cw bi au av it iu iv iw ix iy iz ja jb jc jd">A deep but interpretable bag-of-feature network (BagNet)</h2><p id="e480" class="hz ia cw bi ib b ic je ie jf ig jg ii jh ik ji im dv">To test this we combine the interpretability and transparency of BoF models with the performance of DNNs. The high-level strategy is as follows:</p><ul class=""><li id="d282" class="hz ia cw bi ib b ic id ie if ig ih ii ij ik il im jj jk jl">Split the image into small <em class="jm">q</em> x <em class="jm">q</em> image patches</li><li id="2ff4" class="hz ia cw bi ib b ic jn ie jo ig jp ii jq ik jr im jj jk jl">Pass patches through a DNN to get class evidences (logits) for each patch.</li><li id="ac3e" class="hz ia cw bi ib b ic jn ie jo ig jp ii jq ik jr im jj jk jl">Sum the evidence over all patches to reach an image-level decision.</li></ul><figure class="gx gy gz ha hb gn ht jt ck ju jv jw jx jy bv jz ka kb kc kd ke paragraph-image"><figcaption class="bm fg hu hv hw do dm dn hx hy au ff">Classification strategy of BagNets: for each patch we extract class evidences (logits) using a DNN and sum up the total class evidences over all patches.</figcaption></figure><p id="8580" class="hz ia cw bi ib b ic id ie if ig ih ii ij ik il im dv">To implement this strategy in the simplest and most efficient way we take a standard ResNet-50 architecture and replace most (but not all) 3x3 convolutions with 1x1 convolutions. In this case the hidden units in the last convolutional layer each only “see” a small part of the image (i.e. their receptive field is much smaller than the size of the image). This avoids an explicit partitioning of the image and is as close as possible to standard CNNs while still implementing the outlined strategy. We call the resulting model architecture <strong class="ib kg">BagNet-<em class="jm">q</em></strong> where <em class="jm">q</em> stands for the receptive field size of the top-most layer (we test <em class="jm">q</em> = 9, 17 and 33). The runtime of BagNet-<em class="jm">q</em> is roughly 2,5 the runtime of a ResNet-50.</p><figure class="gx gy gz ha hb gn ht jt ck ju jv jw jx jy bv jz ka kb kc kd ke paragraph-image"><figcaption class="bm fg hu hv hw do dm dn hx hy au ff">Performance of BagNets with different patch sizes on ImageNet.</figcaption></figure><p id="3fba" class="hz ia cw bi ib b ic id ie if ig ih ii ij ik il im dv">The performance of BagNets on ImageNet is impressive even for very small patch sizes: image features of size 17 x 17 pixels are enough to reach AlexNet-level performance while features of size 33 x 33 pixels are sufficient to reach around 87% top-5 accuracy. Higher performance values might be achievable with a more careful placement of the 3 x 3 convolutions and additional hyperparameter tuning.</p><p id="b5fc" class="hz ia cw bi ib b ic id ie if ig ih ii ij ik il im dv">That’s our first main result: you can solve ImageNet using only a collection of small image features. Long-range spatial relationships like object shape or the relation between object parts can be completely neglected and are unnecessary to solve the task.</p><p id="0507" class="hz ia cw bi ib b ic id ie if ig ih ii ij ik il im dv">A great feature of the BagNets is their transparent decision-making. For example, we can now look which image features are most predictive for a given class (see below). For example, a tench (a very big fish) is typically recognized by fingers on top of a greenish background. Why? Because most images in this category feature a fisherman holding up the tench like a trophy. Whenever the BagNet wrongly classifies an image as a tench it’s often because there are some fingers on top of a greenish background somewhere in the image.</p></div></div></div></div>
</div>
    </div>
    <footer>
        <div>created by buildstarted &copy; 2020 <a href="/about">about</a></div>
        <div>Share this page on social media: copy and paste this url https://linksfor.dev/</div>
        <div>If you prefer RSS: <a href="https://linksfor.dev/feed.xml">https://linksfor.dev/feed.xml</a></div>
    </footer>
    
    <script>
        (function () {
            var COLLECT_URL = "https://dna.buildstarted.com/t";
            var SITE_ID = "linksfor.devs";
            var GLOBAL_VAR_NAME = "__DNA__";

            window[GLOBAL_VAR_NAME] = {};

            window[GLOBAL_VAR_NAME].sendPageView = function () {
                var path = location.pathname;
                var referrer = document.referrer;

                var url = COLLECT_URL + "?siteid=" + SITE_ID + "&p=" + encodeURIComponent(path) + "&r=" + encodeURIComponent(referrer);

                try { fetch(url, { method: "GET" }); } catch (e) { }
            };

            window[GLOBAL_VAR_NAME].sendPageView();
        })();
    </script>
</body>
</html>