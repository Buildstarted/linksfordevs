<!DOCTYPE html>
<html lang="en">
<head>
    <title>
Running Awk in parallel to process 256M records - linksfor.dev(s)    </title>
    <meta charset="utf-8">
    <link rel="alternate" type="application/rss+xml" title="Linksfor.dev(s) feed" href="https://linksfor.dev/feed.rss" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta name="google" value="notranslate">
    <link rel="stylesheet" type="text/css" href="/style.css" />
    <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon">
    <link rel="icon" href="/favicon.ico" type="image/x-icon">
    <meta property="og:title" content="Running Awk in parallel to process 256M records - linksfor.dev(s)"/>
    <meta property="og:description" content="TL;DR Awk crunches massive data; a High Performance Computing (HPC) script calls hundreds of Awk concurrently. Fast and scalable in-memory solution on a fat machine."/>
    <meta property="og:type" content="website"/>
    <meta property="og:url" content="https://ketancmaheshwari.github.io/posts/2020/05/24/SMC18-Data-Challenge-4.html"/>

<meta property="og:site_name" content="linksfor.dev(s)" />
</head>
<body>
    <div class="devring" style="background: #222">
        <div style="text-align:center">Explore other dev related sites in this ring. If you would like to join this ring <a href="https://devring.club">click here</a>.</div>
        <div class="grid">
            <div style="display: grid; grid-template-columns: .5fr 1fr 1fr 1fr; text-align: center;">
                <span class="devring-title"><a href="https://devring.club/">devring.club</a></span>
                <a href="https://devring.club/sites/1/prev" class="devring-previous">Previous</a>
                <a href="https://devring.club/random" class="devring-random">Random</a>
                <a href="https://devring.club/sites/1/next" class="devring-next">Next</a>
            </div>
        </div>
    </div>
    <div class="grid">
        <h1>
<a href="/" style="color:inherit">linksfor.dev(s)</a>
        </h1>
        <title>linksfor.dev(s) - Running Awk in parallel to process 256M records</title>
<div class="readable">
        <h1>Running Awk in parallel to process 256M records</h1>
            <div>Reading time: 19-24 minutes</div>
        <div>Posted here: 03 Jun 2020</div>
        <p><a href="https://ketancmaheshwari.github.io/posts/2020/05/24/SMC18-Data-Challenge-4.html">https://ketancmaheshwari.github.io/posts/2020/05/24/SMC18-Data-Challenge-4.html</a></p>
        <hr/>
<div id="readability-page-1" class="page"><section>
            
            <article itemscope="" itemtype="http://schema.org/BlogPosting">

  <!-- <header class="post-header">
    <h1 class="post-title" itemprop="name headline">Running Awk in parallel to process 256M records</h1>
    <p class="post-meta">
      <time datetime="2020-05-24T00:00:00+00:00" itemprop="datePublished">
        
        May 24, 2020
      </time>
      </p>
  </header> -->

  <div itemprop="articleBody">
    <h3 id="tldr">TL;DR</h3>
<p>Awk crunches massive data; a High Performance Computing (HPC) script calls
hundreds of Awk concurrently. Fast and scalable in-memory solution on a fat
machine.</p>



<p>Presenting the solution I worked on in 2018, to a <a href="https://smc-datachallenge.ornl.gov/challenges-2018/">Data
Challenge</a> organized at
work. I solve the Scientific Publications Mining challenge (no.4) that consists
of 5 problems. I use classic Unix tools with a modern scalable HPC scripting
tool to work out the solutions. The project is hosted on
<a href="https://github.com/ketancmaheshwari/SMC18">github</a>. About 12 teams entered the
contest.</p>



<h2 id="software">Software</h2>

<p><strong>Awk</strong> (gawk v4.0.2) is dominantly used for the bulk of core processing.</p>

<p>Argonne National Laboratory developed HPC scripting tool called
<a href="http://swift-lang.org/Swift-T">Swift</a> (<strong>NOT</strong> the Apple Swift) is used to run
the Awk programs concurrently over the dataset to radically improve
performance. Swift uses MPI based communication to parallelize and synchronize
independent tasks.</p>

<p>Other Unix tools such as <em>sort</em>, <em>grep</em>, <em>tr</em>, <em>sed</em> and <em>bash</em> are used as
well. Additionally, <em>jq</em>, <em>D3</em>, <em>dot/graphviz</em>, and <em>ffmpeg</em> are used.</p>

<h2 id="hardware">Hardware</h2>

<p>Fortunately, I had access to a large-memory (24 T) SGI system with 512-core
Intel Xeon (2.5GHz) CPUs. All the IO is memory (<em>/dev/shm</em>) bound ie. the data
is read from and written to <em>/dev/shm</em>.</p>

<h3 id="rationale">Rationale</h3>

<p>Awk is lightweight, concise, expressive, and fast – especially for text processing
applications. Some people find Awk programs terse and hard to read. I have
taken care to make the code readable. I wanted to see how far can I go with Awk
(and boy did I go far!). Alternative tools such as modern Python libraries
sometimes have scaling limitations, portability concerns. Some are still
evolving. Swift is used simply because I was familiar with it and confident
that it will scale well in this case.</p>



<p>The original <a href="https://www.openacademic.ai/oag">data</a> was in two sets (<em>aminer</em>
and <em>mag</em>) of 322 <code>json</code> files – each containing a million records. A file
with a list of common records appearing in both sets was available. An Awk script
(<code>src/filterdup.awk</code>) is used to exclude these duplicate records from the aminer
dataset. As a result, it came out about <strong>256 million</strong> (256,382,605 to be
exact) unique records to be processed. The total data size is 329GB. Some
fields in the data are <em>null</em>. Those records are avoided where relevant.
Additionally, records related to non-English publications were avoided as
needed. A
<a href="https://raw.githubusercontent.com/ketancmaheshwari/SMC18/master/data/aminer_papers_sample.allcols.excl.txt">snapshot</a>
of tabular data is available. String <code>qwqw</code> is chosen as a column separator to
distinguish it from text already found in data. All other 3 or less character
combinations already existed in data prohibiting them to be used as separators.</p>

<div><div><pre><code><span>#!/usr/bin/env awk -f</span>

<span># $1 magid</span>
<span># $2 aminerid</span>

<span># Filter duplicate papers and remove them</span>
<span># from aminer database based on the linking relationship </span>

BEGIN <span>{</span>FS <span>=</span> OFS <span>=</span> <span>"qwqw"</span><span>}</span>

NR <span>==</span> FNR <span>{</span>a[<span>$2</span><span>]</span> <span>=</span> <span>$1</span><span>}</span>

<span>!(</span><span>$1</span> <span>in </span>a<span>)</span> <span>&amp;&amp;</span> FILENAME ~ /aminer/ <span>{</span> print <span>}</span>
</code></pre></div></div>
<p><code>NR == FNR</code> is a cool Awk idiom that ensures the condition is true only for the first file. This is because for each file that is processed the FNR (File Record Number) gets reset but the NR does not. This means the condition <code>NR == FNR</code> yields true only for the first file.</p>

<p>In addition to the publications data, I use the following:</p>

<ol>
  <li>
    <p>A list of large cities (population 100K+) and their lat-long coordinates (3,517).</p>
  </li>
  <li>
    <p>A list of countries (190).</p>
  </li>
  <li>
    <p>A list of world universities and research institutes (8,984).</p>
  </li>
  <li>
    <p>A list of stop-words to avoid in some of the results (161 words).</p>
  </li>
</ol>



<h2 id="pre--and-post-processing">Pre- and post-processing</h2>

<p><code>jq</code> is used to transform the json data to tabular format
(<code>src/json2tabular.sh</code>). The converted tabular files have 19 original columns
(<strong>id</strong>, <strong>title</strong>, <strong>authors</strong>, <strong>year</strong>, <strong>citations</strong>,  etc) and one
additional column called <strong>num_authors</strong> showing the number of authors for a
given publication record. The authors column has a semi-colon separator for
multiple authors. Further curation of tabular data is done by removing
extraneous space, square brackets, escape characters and quotes using <code>sed</code>.</p>

<p>Some of the results obtained were postprocessed for visulization using the <code>D3</code>
graphics framework. <code>ffmpeg</code> is used to stitch images of trending terms to
create an animation. <code>dot/graphviz</code> is used to build the massive citation
network graph of the best paper.</p>

<h2 id="scaling-up">Scaling up</h2>

<p>Each solution has Awk code run concurrently over the 322 data files on 322 CPU
cores using Swift. This resulted in radical speedup at scale. None of the
solution has taken more than an hour of runtime–most took less than a minute.</p>

<h3 id="problem-1">Problem 1</h3>

<p><em>Identify the individual or group of individuals who appear to be the expert in a particular field or sub-field.</em></p>

<p>This is solved in two ways. First approach identifies all the entries with
citations higher than 500 for a given search topic
(<code>results/meditation_highly_cited.txt</code>).</p>

<div><div><pre><code><span>#!/usr/bin/env awk -f</span>

BEGIN <span>{</span>

    <span># Field Separator</span>
    FS <span>=</span> <span>"qwqw"</span>
    <span># Output field separator</span>
    OFS <span>=</span> <span>"</span><span>\t</span><span>"</span>
    IGNORECASE <span>=</span> 1

    <span># Field names for readability</span>
    <span>id</span><span>=</span>1<span>;</span> <span>title</span><span>=</span>2<span>;</span> <span>num_authors</span><span>=</span>3<span>;</span> <span>doi</span><span>=</span>4<span>;</span> <span>fos_isbn</span><span>=</span>5<span>;</span> <span>doctype_issn</span><span>=</span>6<span>;</span>
    <span>lang</span><span>=</span>7<span>;</span> <span>n_citation</span><span>=</span>8<span>;</span> <span>issue</span><span>=</span>9<span>;</span> <span>url</span><span>=</span>10<span>;</span> <span>volume</span><span>=</span>11<span>;</span> <span>page_start</span><span>=</span>12<span>;</span>
    <span>page_end</span><span>=</span>13<span>;</span> <span>year</span><span>=</span>14<span>;</span> <span>venue</span><span>=</span>15<span>;</span> <span>publisher_pdf</span><span>=</span>16<span>;</span> <span>references</span><span>=</span>17<span>;</span>
    <span>keywords</span><span>=</span>18<span>;</span> <span>abstract</span><span>=</span>19<span>;</span> <span>authors</span><span>=</span>20<span>;</span>
<span>}</span>

<span>(</span><span>$0</span>~topic <span>&amp;&amp;</span> <span>$num_authors</span> <span>&gt;</span> 0 <span>&amp;&amp;</span> <span>$n_citation</span><span>!</span>~/null/ <span>&amp;&amp;</span> <span>$n_citation</span><span>&gt;</span>500<span>){</span>
    print <span>$n_citation</span>, <span>$title</span>, <span>$authors</span>, <span>$year</span>
<span>}</span>

<span># How to run:</span>
<span># awk -v topic=meditation -f src/prob1_p1.awk data/mag_papers_sample.allcols.txt</span>
</code></pre></div></div>

<p>The second approach finds the names of authors whose names are repeating for
queried topic with at least a certain number of citations in each entry. This
gives a reasonable idea of who are the expert figures in a given research area.
One such result in <code>results/cancer_research_topauths.txt</code> shows authors in
cancer research with more than one publication with at least 1,000 citations.</p>

<div><div><pre><code><span>#!/usr/bin/env awk -f</span>

BEGIN <span>{</span>
    <span># Field separator</span>
    FS <span>=</span> <span>"qwqw"</span>
    OFS <span>=</span> <span>"</span><span>\t</span><span>"</span>
    IGNORECASE <span>=</span> 1

    <span># Field names</span>
    <span>id</span><span>=</span>1<span>;</span> <span>title</span><span>=</span>2<span>;</span> <span>num_authors</span><span>=</span>3<span>;</span> <span>doi</span><span>=</span>4<span>;</span> <span>fos_isbn</span><span>=</span>5<span>;</span> <span>doctype_issn</span><span>=</span>6<span>;</span>
    <span>lang</span><span>=</span>7<span>;</span> <span>n_citation</span><span>=</span>8<span>;</span> <span>issue</span><span>=</span>9<span>;</span> <span>url</span><span>=</span>10<span>;</span> <span>volume</span><span>=</span>11<span>;</span> <span>page_start</span><span>=</span>12<span>;</span>
    <span>page_end</span><span>=</span>13<span>;</span> <span>year</span><span>=</span>14<span>;</span> <span>venue</span><span>=</span>15<span>;</span> <span>publisher_pdf</span><span>=</span>16<span>;</span> <span>references</span><span>=</span>17<span>;</span>
    <span>keywords</span><span>=</span>18<span>;</span> <span>abstract</span><span>=</span>19<span>;</span> <span>authors</span><span>=</span>20<span>;</span>
<span>}</span>

<span>(</span><span>$0</span>~topic <span>&amp;&amp;</span> <span>$num_authors</span><span>&gt;</span>0 <span>&amp;&amp;</span> <span>$n_citation</span><span>!</span>~/null/ <span>&amp;&amp;</span> <span>$n_citation</span><span>&gt;</span>1000<span>)</span> <span>{</span>
   
   <span># find the authors whose names are repeating for a particular topic.</span>
   <span># Those authors will be considered experts. </span>
   
   gsub<span>(</span><span>"</span><span>\"</span><span>"</span>,<span>""</span>,<span>$authors</span><span>)</span>
   <span>split</span><span>(</span><span>$authors</span>, a, <span>";"</span><span>)</span>
   
   <span>for</span> <span>(</span>i <span>in </span>a<span>)</span> <span>{</span>
       <span>split</span><span>(</span>a[i], b, <span>","</span><span>)</span>
       <span># auths array will have keys as auth names and the </span>
       <span># element value increases if the key repeats</span>
       <span>if</span><span>(</span>b[1]!~/null/<span>)</span> auths[b[1]]++ 
   <span>}</span>
<span>}</span>

END <span>{</span> <span>for</span> <span>(</span>k <span>in </span>auths<span>)</span> <span>if</span><span>(</span>auths[k]&gt;1<span>)</span> print auths[k], k <span>}</span>

<span>#How to run:</span>
<span># awk -v topic=cancer -f src/prob1_p2.awk data/mag_papers_sample.allcols.txt</span>
<span># sort the results</span>
</code></pre></div></div>
<p>The HPC implementation of this solution finishes in <strong>25 seconds</strong>.</p>

<p>Alongside is the citation <strong>network graph</strong> of the most cited paper in this
<a href="https://raw.githubusercontent.com/ketancmaheshwari/SMC18/15b0519d789b0e4b86f66b6bb6199fe24c1a4730/results/best_papers.svg">diagram</a>
(too big to fit here). The result of a query for all-time list of most cited
papers with a threshold of 20,000 is in <code>results/top_papers.txt</code>.</p>

<h3 id="problem-2">Problem 2</h3>

<p><em>Identify topics that have been researched across all publications.</em></p>

<p>This is solved by identifying most frequently appearing words in the
collection. Title, abstract and keywords are parsed and top 1,000 frequently
occurring words across the whole collection is found. Several common words (aka
<em>stop-words</em>) are filtered from the results. At over 23 million, the word
“patients” occurs the most frequently. The full list of top 1,000 words is
found in <code>/results/top_1K_words_kw_abs_title.txt</code>. The target collection of
publications may be narrowed down to criteria such as years range.</p>

<div><div><pre><code><span>#!/usr/bin/env awk -f</span>

<span># Problem  Statement</span>
<span>#    Identify topics that have been researched across all publications.  </span>

<span># Solution:</span>
<span># step1. Filter the input to English language records </span>
<span># step2. Eliminate unnecessary content such as punctuation,</span>
<span>#        non-printable chars and small words such as </span>
<span>#        1 letter and 2 letter words</span>
<span># step3. Extract words used in keywords, title and abstract</span>
<span># step4. Find most frequently used words </span>

BEGIN <span>{</span>
    FS <span>=</span> <span>"qwqw"</span>
    IGNORECASE <span>=</span> 1
  
    <span># Field names</span>
    <span>id</span><span>=</span>1<span>;</span> <span>title</span><span>=</span>2<span>;</span> <span>num_authors</span><span>=</span>3<span>;</span> <span>doi</span><span>=</span>4<span>;</span> <span>fos_isbn</span><span>=</span>5<span>;</span> 
    <span>doctype_issn</span><span>=</span>6<span>;</span><span>lang</span><span>=</span>7<span>;</span> <span>n_citation</span><span>=</span>8<span>;</span> <span>issue</span><span>=</span>9<span>;</span> <span>url</span><span>=</span>10<span>;</span>
    <span>volume</span><span>=</span>11<span>;</span> <span>page_start</span><span>=</span>12<span>;</span> <span>page_end</span><span>=</span>13<span>;</span> <span>year</span><span>=</span>14<span>;</span> <span>venue</span><span>=</span>15<span>;</span> 
    <span>publisher_pdf</span><span>=</span>16<span>;</span> <span>references</span><span>=</span>17<span>;</span> <span>keywords</span><span>=</span>18<span>;</span> <span>abstract</span><span>=</span>19<span>;</span> <span>authors</span><span>=</span>20<span>;</span>
<span>}</span>

<span>#collect stop words</span>
NR <span>==</span> FNR <span>{</span>x[<span>$1</span><span>]</span><span>;</span>next<span>}</span>

<span>$lang</span>~/en/ <span>&amp;&amp;</span> <span>(</span><span>$keywords</span><span>!</span>~/null/ <span>||</span> <span>$title</span><span>!</span>~/null/ <span>||</span> <span>$abstract</span><span>!</span>~/null/<span>)</span> <span>{</span>
    <span># treat titles</span>
    <span>$title</span> <span>=</span> tolower<span>(</span><span>$title</span><span>)</span>
    <span>split</span><span>(</span><span>$title</span>, a, <span>" "</span><span>)</span>
    <span>for</span> <span>(</span>i <span>in </span>a<span>)</span> <span>if</span><span>(</span>length<span>(</span>a[i]<span>)&gt;</span>2 <span>&amp;&amp;</span> match<span>(</span>a[i],/[a-z]/<span>)</span> <span>&amp;&amp;</span> a[i] <span>in </span>x <span>==</span> 0<span>)</span> kw[a[i]]++

    <span># treat keywords</span>
    <span>$keywords</span> <span>=</span> tolower<span>(</span><span>$keywords</span><span>)</span>
    <span>split</span><span>(</span><span>$keywords</span>, b, <span>","</span><span>)</span>
    <span>for</span> <span>(</span>i <span>in </span>b<span>)</span> <span>if</span><span>(</span>length<span>(</span>b[i]<span>)&gt;</span>2 <span>&amp;&amp;</span> match<span>(</span>b[i],/[a-z]/<span>)</span> <span>&amp;&amp;</span> b[i] <span>in </span>x <span>==</span> 0<span>)</span> kw[b[i]]++

     <span># treat abstracts (Computationally expensive, results are in:</span>
     <span># top_1000_words_from_kw_abstract_title_by_freq.txt)</span>
     <span>$abstract</span> <span>=</span> tolower<span>(</span><span>$abstract</span><span>)</span>
     gsub<span>(</span><span>"</span><span>\"</span><span>"</span>,<span>""</span>,<span>$abstract</span><span>)</span>
     gsub<span>(</span><span>","</span>,<span>""</span>,<span>$abstract</span><span>)</span>
     <span>split</span><span>(</span><span>$abstract</span>, c, <span>" "</span><span>)</span>
     <span>for</span> <span>(</span>i <span>in </span>c<span>)</span> <span>if</span><span>(</span>length<span>(</span>c[i]<span>)&gt;</span>2 <span>&amp;&amp;</span> match<span>(</span>c[i],/[a-z]/<span>)</span> <span>&amp;&amp;</span> c[i] <span>in </span>x <span>==</span> 0<span>)</span> kw[c[i]]++
<span>}</span>

END <span>{</span>
    <span>for</span><span>(</span>k <span>in </span>kw<span>){</span>
        <span>if</span> <span>(</span>kw[k]&gt;1000<span>)</span> print kw[k], k
    <span>}</span>
<span>}</span>

<span># HOW TO RUN: LC_ALL=C awk -f prob2.awk stop_words.txt \</span>
              ../aminer_papers_allcols_excl/aminer_papers_<span>*</span>.allcols.excl.txt <span>\</span>
              ../mag_papers_allcols/mag_papers_<span>*</span>.allcols.txt <span>\</span>
              | <span>sort</span> <span>-nr</span> <span>&gt;</span> freq.txt
</code></pre></div></div>

<p>The HPC implementation (Swift code shown below) finishes in <strong>9 minutes</strong>.</p>

<div><div><pre><code><span>import</span> <span>files</span><span>;</span>
<span>import</span> <span>unix</span><span>;</span>

<span>/* app defines what we want to run, the input parameters,
   where the stdout should go, etc.
*/</span>
<span>app</span> <span>(</span><span>file</span> <span>out</span><span>)</span> <span>myawk</span> <span>(</span><span>file</span> <span>awkprog</span><span>,</span> <span>file</span> <span>stop_words</span><span>,</span> <span>file</span> <span>infile</span><span>){</span>
  <span>"/usr/bin/awk"</span> <span>"-f"</span> <span>awkprog</span> <span>stop_words</span> <span>infile</span> <span>@</span><span>stdout</span><span>=</span><span>out</span>
<span>}</span>

<span>/* populate the input data */</span>
<span>file</span> <span>aminer</span><span>[]</span> <span>=</span> <span>glob</span><span>(</span><span>"/dev/shm/aminer_mag_papers/*.txt"</span><span>);</span>

<span>/* output for each call will be collected here */</span>
<span>file</span> <span>outfiles</span><span>[];</span> 

<span>foreach</span> <span>v</span><span>,</span> <span>i</span> <span>in</span> <span>aminer</span> <span>{</span>
  <span>outfiles</span><span>[</span><span>i</span><span>]</span> <span>=</span> <span>myawk</span><span>(</span><span>input</span><span>(</span><span>"/home/km0/SMC18/src/prob2.awk"</span><span>),</span>
                <span>input</span><span>(</span><span>"/home/km0/SMC18/data/stop_words.txt"</span><span>),</span> 
                <span>v</span><span>);</span>
<span>}</span>

<span>/* Combine all output in one file */</span>
<span>file</span> <span>joined</span> <span>&lt;</span><span>"joined.txt"</span><span>&gt;</span> <span>=</span> <span>cat</span><span>(</span><span>outfiles</span><span>);</span>

<span>/*
 After running this swift app:
 awk '{a[$2]+=$1} END {for (k in a) print a[k],k}' joined.txt | sort -nr &gt; freq.txt
*/</span>
</code></pre></div></div>

<h3 id="problem-3">Problem 3</h3>

<p><em>Visualize the geographic distribution of the topics in the publications.</em></p>

<p>This is solved by identifying the author affiliations for the records that has
the search topic in them. The affiliation is searched against three
databases–cities, universities and countries to find out the geographic
locations for that research. The results are aggregated to present a list of
centers for which a given keyword appears most frequently. For cities, the
results are plotted on world map. One such result is shown below for the topic
of research on “birds”.</p>

<p><img src="https://raw.githubusercontent.com/ketancmaheshwari/SMC18/master/results/bird_research_cities.png" alt="bird research" title="Bird Research Around the World!"></p>

<p>The <code>results/</code> directory contains other similar results such as epilepsy,
opioid, meditation research by universities and by countries. The HPC
implementation finishes in <strong>25 seconds</strong>. The Awk code is shown below.</p>

<div><div><pre><code><span>#!/usr/bin/env awk -f</span>

<span># problem statement</span>
<span>#    visualize the geographic distribution of the topics in the publications.</span>

BEGIN <span>{</span>
    <span># Field separator</span>
    FS <span>=</span> OFS <span>=</span> <span>"qwqw"</span>
    IGNORECASE <span>=</span> 1

    <span># Field names</span>
    <span>id</span><span>=</span>1<span>;</span> <span>title</span><span>=</span>2<span>;</span> <span>num_authors</span><span>=</span>3<span>;</span> <span>doi</span><span>=</span>4<span>;</span> <span>fos_isbn</span><span>=</span>5<span>;</span> <span>doctype_issn</span><span>=</span>6<span>;</span>
    <span>lang</span><span>=</span>7<span>;</span> <span>n_citation</span><span>=</span>8<span>;</span> <span>issue</span><span>=</span>9<span>;</span> <span>url</span><span>=</span>10<span>;</span> <span>volume</span><span>=</span>11<span>;</span> <span>page_start</span><span>=</span>12<span>;</span>
    <span>page_end</span><span>=</span>13<span>;</span> <span>year</span><span>=</span>14<span>;</span> <span>venue</span><span>=</span>15<span>;</span> <span>publisher_pdf</span><span>=</span>16<span>;</span> <span>references</span><span>=</span>17<span>;</span>
    <span>keywords</span><span>=</span>18<span>;</span> <span>abstract</span><span>=</span>19<span>;</span> <span>authors</span><span>=</span>20<span>;</span>
<span>}</span>

<span>#collect the countries/cities/univs data</span>
NR <span>==</span> FNR <span>{</span>a[<span>$1</span><span>]</span><span>;</span>next<span>}</span> 

<span>#treat records with authors whose affiliation is available</span>
<span>$0</span>~topic <span>&amp;&amp;</span> <span>$num_authors</span><span>!</span>~/null/ <span>&amp;&amp;</span> <span>$authors</span>~/<span>\,</span>/ <span>{</span> 
    <span># extract words from author affiliation and compare with the countries.</span>
    <span># If a match is found increment that array entry.</span>
    w <span>=</span> <span>split</span><span>(</span><span>$authors</span>, b, <span>","</span><span>)</span>
    <span>for</span> <span>(</span><span>i</span><span>=</span>0<span>;</span>i&lt;w<span>;</span>i++<span>){</span>
        gsub<span>(</span><span>";"</span>,<span>" "</span>,b[i]<span>)</span> 
        <span>if</span> <span>(</span>b[i] <span>in </span>a<span>)</span> a[b[i]]++
    <span>}</span>
<span>}</span>

END <span>{</span>
  <span>for</span><span>(</span>k <span>in </span>a<span>){</span> 
     <span>if</span><span>(</span>a[k]<span>)</span> print a[k], k
  <span>}</span>
<span>}</span>

<span># HOW TO RUN:</span>
<span># awk -v topic=birds -f prob3.awk cities.txt \</span>
        ../mag_papers_allcols/mag_papers_<span>*</span>.allcols.txt <span>\</span>
        ../aminer_papers_allcols_excl/aminer_papers_<span>*</span>.allcols.excl.txt
<span># awk -v topic=birds -f prob3.awk countries.txt ...</span>
<span># awk -v topic=birds -f prob3.awk universities.txt ... </span>

<span># Run the following pipeline on the results:</span>
<span># sort -nr -k 1 citywise_papers.txt &gt; tmp &amp;&amp; mv tmp citywise_papers.txt </span>
<span># OR</span>
<span># After running the swift app:</span>
<span># awk -F: '{a[$2]+=$1} END {for (k in a) print a[k],k}' joined_cities.txt \</span>
         | <span>sort</span> <span>-nr</span> <span>&gt;</span> tmp <span>&amp;&amp;</span> <span>mv </span>tmp joined_cities.txt
</code></pre></div></div>

<h3 id="problem-4">Problem 4</h3>

<p><em>Identify how topics have shifted over time.</em></p>

<p>This problem may be solved in three distinct ways. The first approach processes
the database to find out year-wise occurrence of any given two topics
<em>together</em>. It generates a list of years and the number of times <em>both</em> topics
has occurred in a single publication in that year. For example, the plot shown
below shows how the terms “obesity” and “sugar” have trended together in
publications over the years.</p>

<p><img src="https://raw.githubusercontent.com/ketancmaheshwari/SMC18/master/results/obesity_sugar.png" alt="obesity sugar" title="papers in which obesity and sugar appears together"></p>

<p>Awk code shown below.</p>

<div><div><pre><code><span>#!/usr/bin/env awk -f</span>

<span># Problem Statement</span>
<span># Identify how topics have shifted over time.</span>

<span># Solution 1 below will search for any two topics</span>
<span># mentioned and show the number of occurrence of both the topics year-wise</span>
BEGIN <span>{</span>
    <span># Field Separator</span>
    FS <span>=</span> <span>"qwqw"</span>
    IGNORECASE <span>=</span> 1
    <span># Field names</span>
    <span>id</span><span>=</span>1<span>;</span> <span>title</span><span>=</span>2<span>;</span> <span>num_authors</span><span>=</span>3<span>;</span> <span>doi</span><span>=</span>4<span>;</span> <span>fos_isbn</span><span>=</span>5<span>;</span> <span>doctype_issn</span><span>=</span>6<span>;</span>
    <span>lang</span><span>=</span>7<span>;</span> <span>n_citation</span><span>=</span>8<span>;</span> <span>issue</span><span>=</span>9<span>;</span> <span>url</span><span>=</span>10<span>;</span> <span>volume</span><span>=</span>11<span>;</span> <span>page_start</span><span>=</span>12<span>;</span>
    <span>page_end</span><span>=</span>13<span>;</span> <span>year</span><span>=</span>14<span>;</span> <span>venue</span><span>=</span>15<span>;</span> <span>publisher_pdf</span><span>=</span>16<span>;</span> <span>references</span><span>=</span>17<span>;</span>
    <span>keywords</span><span>=</span>18<span>;</span> <span>abstract</span><span>=</span>19<span>;</span> <span>authors</span><span>=</span>20<span>;</span>
<span>}</span>

<span>$lang</span>~/en/ <span>&amp;&amp;</span> <span>$year</span><span>!</span>~/null/ <span>&amp;&amp;</span> <span>$0</span>~topic1 <span>&amp;&amp;</span> <span>$0</span>~topic2 <span>{</span>
    a[<span>$year</span><span>]</span>++
<span>}</span>

END <span>{</span>
    n <span>=</span> asorti<span>(</span>a,b<span>)</span>
    <span>printf</span><span>(</span><span>"Trend for topics: %s, %s</span><span>\n</span><span>"</span>, topic1, topic2<span>)</span>
    <span>for</span> <span>(</span><span>i</span><span>=</span>1<span>;</span>i&lt;<span>=</span>n<span>;</span>i++<span>)</span> <span>printf</span><span>(</span><span>"%d :- %d</span><span>\n</span><span>"</span>, b[i], a[b[i]]<span>)</span>
<span>}</span>

<span># Run as follows:</span>
<span># awk -v topic1=obesity -v topic2=sugar -f code/prob4.awk aminer_mag_papers/*.txt</span>
</code></pre></div></div>

<p>The second approach finds the papers that has highest impact in each year and
extracts the keywords in those papers. The impact is computed by the paper that
is cited the most in that year. The result for this task are in
<code>results/yearwise_trending_keywords.txt</code> in the form of year, keywords,
citations triplet.</p>

<div><div><pre><code><span>#!/usr/bin/env awk -f</span>

<span># Problem Statement</span>
<span>#    Identify how topics have shifted over time.</span>

<span># Solution 2 is to find the highest cited paper</span>
<span># year-wise and figure out the topics it was based on</span>
BEGIN <span>{</span>
    FS <span>=</span> OFS <span>=</span> <span>"qwqw"</span>
    IGNORECASE <span>=</span> 1

    <span># Field names</span>
    <span>id</span><span>=</span>1<span>;</span> <span>title</span><span>=</span>2<span>;</span> <span>num_authors</span><span>=</span>3<span>;</span> <span>doi</span><span>=</span>4<span>;</span> <span>fos_isbn</span><span>=</span>5<span>;</span> <span>doctype_issn</span><span>=</span>6<span>;</span>
    <span>lang</span><span>=</span>7<span>;</span> <span>n_citation</span><span>=</span>8<span>;</span> <span>issue</span><span>=</span>9<span>;</span> <span>url</span><span>=</span>10<span>;</span> <span>volume</span><span>=</span>11<span>;</span> <span>page_start</span><span>=</span>12<span>;</span>
    <span>page_end</span><span>=</span>13<span>;</span> <span>year</span><span>=</span>14<span>;</span> <span>venue</span><span>=</span>15<span>;</span> <span>publisher_pdf</span><span>=</span>16<span>;</span> <span>references</span><span>=</span>17<span>;</span>
    <span>keywords</span><span>=</span>18<span>;</span> <span>abstract</span><span>=</span>19<span>;</span> <span>authors</span><span>=</span>20<span>;</span>
<span>}</span>

<span>$lang</span>~/en/ <span>&amp;&amp;</span> <span>$year</span><span>!</span>~/null/ <span>&amp;&amp;</span> <span>$year</span>&lt;2020 <span>&amp;&amp;</span> <span>$keywords</span><span>!</span>~/null/ 
<span>&amp;&amp;</span> <span>$n_citation</span><span>!</span>~/null/ <span>&amp;&amp;</span> <span>$n_citation</span><span>&gt;</span>max[<span>$year</span><span>]</span> <span>{</span>
    max[<span>$year</span><span>]</span> <span>=</span> <span>$n_citation</span><span>;</span> a[<span>$year</span><span>]=</span><span>$keywords</span>
<span>}</span>

END <span>{</span>
    n <span>=</span> asorti<span>(</span>a,b<span>)</span>
    <span>for</span> <span>(</span><span>i</span><span>=</span>1<span>;</span>i&lt;<span>=</span>n<span>;</span>i++<span>)</span> print b[i], a[b[i]], max[b[i]]
<span>}</span>

<span># Run via Swift in parallel. If serial, run like so:</span>
<span># awk -f code/prob4_p2.awk aminer_mag_papers/*.txt &gt; yearwise_trending_keywords.txt</span>
</code></pre></div></div>

<p>The third approach finds the top 10 most frequently occurring terms each year
to find how the topics get in and out of trend over the years. An mkv animation
video showing a bubble plot of words trending between the year 1800 and 2017 is
<a href="https://github.com/ketancmaheshwari/SMC18/blob/master/results/freqwordsoveryears.mkv?raw=true">here</a>.
A file list of all the words is found in <code>results/trending_words_by_year</code>. A
snapshot trending words bubble in 2002 is shown below:</p>

<p><img src="https://raw.githubusercontent.com/ketancmaheshwari/SMC18/master/results/trending_words_by_year/2002.png" alt="trending bubble 2020" title="top 10 research words in 2002"></p>

<p>The Awk code that generates the raw data for above picture is shown below:</p>

<div><div><pre><code><span>#!/usr/bin/env awk -f</span>

<span># find the top 10 trending topics year-wise and see how they appear/disappear in the trend</span>
<span># We achieve this by writing keywords, titles and abstract to files named after </span>
<span># the year they appeared and do postprocessing on those files</span>

BEGIN <span>{</span>
    FS <span>=</span> <span>"qwqw"</span>
    IGNORECASE <span>=</span> 1

    <span># Field names</span>
    <span>id</span><span>=</span>1<span>;</span> <span>title</span><span>=</span>2<span>;</span> <span>num_authors</span><span>=</span>3<span>;</span> <span>doi</span><span>=</span>4<span>;</span> <span>fos_isbn</span><span>=</span>5<span>;</span> <span>doctype_issn</span><span>=</span>6<span>;</span>
    <span>lang</span><span>=</span>7<span>;</span> <span>n_citation</span><span>=</span>8<span>;</span> <span>issue</span><span>=</span>9<span>;</span> <span>url</span><span>=</span>10<span>;</span> <span>volume</span><span>=</span>11<span>;</span> <span>page_start</span><span>=</span>12<span>;</span>
    <span>page_end</span><span>=</span>13<span>;</span> <span>year</span><span>=</span>14<span>;</span> <span>venue</span><span>=</span>15<span>;</span> <span>publisher_pdf</span><span>=</span>16<span>;</span> <span>references</span><span>=</span>17<span>;</span>
    <span>keywords</span><span>=</span>18<span>;</span> <span>abstract</span><span>=</span>19<span>;</span> <span>authors</span><span>=</span>20<span>;</span>
<span>}</span>

<span>#collect stop words</span>
NR <span>==</span> FNR <span>{</span>x[<span>$1</span><span>]</span><span>;</span>next<span>}</span>

<span>$lang</span>~/en/ <span>&amp;&amp;</span> <span>$n_citation</span><span>&gt;</span>0 <span>&amp;&amp;</span> <span>$year</span><span>==</span>yr <span>&amp;&amp;</span> <span>$keywords</span><span>!</span>~/null/<span>{</span>
    <span># write title, keywords and abstract to a file </span>
    <span>#      titled by the year in which they appear</span>
    
    <span># treat title</span>
    <span>$title</span> <span>=</span> tolower<span>(</span><span>$title</span><span>)</span>
    <span>split</span><span>(</span><span>$title</span>, a, <span>" "</span><span>)</span>
    <span>for</span> <span>(</span>i <span>in </span>a<span>)</span> <span>if</span><span>(</span>length<span>(</span>a[i]<span>)&gt;</span>2 <span>&amp;&amp;</span> match<span>(</span>a[i],/[a-z]/<span>)</span> <span>&amp;&amp;</span> a[i] <span>in </span>x <span>==</span> 0<span>)</span> print a[i]

    <span># treat keywords</span>
    <span>$keywords</span> <span>=</span> tolower<span>(</span><span>$keywords</span><span>)</span>
    gsub<span>(</span><span>"</span><span>\"</span><span>"</span>,<span>""</span>,<span>$keywords</span><span>)</span>
    <span>split</span><span>(</span><span>$keywords</span>, b, <span>","</span><span>)</span>
    <span>for</span> <span>(</span>i <span>in </span>b<span>)</span> <span>if</span><span>(</span>length<span>(</span>b[i]<span>)&gt;</span>2 <span>&amp;&amp;</span> match<span>(</span>b[i],/[a-z]/<span>)</span> <span>&amp;&amp;</span> b[i] <span>in </span>x <span>==</span> 0<span>)</span> print b[i]

     <span># treat abstract</span>
     <span>$abstract</span> <span>=</span> tolower<span>(</span><span>$abstract</span><span>)</span>
     gsub<span>(</span><span>"</span><span>\"</span><span>"</span>,<span>""</span>,<span>$abstract</span><span>)</span>
     gsub<span>(</span><span>","</span>,<span>" "</span>,<span>$abstract</span><span>)</span>
     <span>split</span><span>(</span><span>$abstract</span>, c, <span>" "</span><span>)</span>
     <span>for</span> <span>(</span>i <span>in </span>c<span>)</span> <span>if</span><span>(</span>length<span>(</span>c[i]<span>)&gt;</span>2 <span>&amp;&amp;</span> match<span>(</span>c[i],/[a-z]/<span>)</span> <span>&amp;&amp;</span> c[i] <span>in </span>x <span>==</span> 0<span>)</span> print c[i]

<span>}</span>

<span>#Do the following for postprocessing:</span>
<span>#for i in 18?? 19?? 20??</span>
<span># do (grep -o -E '\w+' $i | tr [A-Z] [a-z] \</span>
<span>#     | sed -e 's/null//g' -e 's/^.$//g' -e 's/^..$//g' -e 's/^[0-9]*$//g' \</span>
<span>#     | awk NF | fgrep -v -w -f stop_words.txt \</span>
<span>#     | sort | uniq -c | sort -nr \</span>
<span>#     | head -10 &gt; trending/trending.$i.txt) &amp; done</span>
</code></pre></div></div>

<p>Parallelizing the third approach was challenging as it involved a two-level
nested foreach loop. The outer loop iterates over the years and the inner loop
iterates over the input files. The HPC implementation finishes in <strong>48
minutes</strong>. Swift code for this shown below.</p>

<div><div><pre><code><span>import</span> <span>files</span><span>;</span>
<span>import</span> <span>io</span><span>;</span>
<span>import</span> <span>unix</span><span>;</span>
<span>import</span> <span>string</span><span>;</span>

<span>app</span> <span>(</span><span>file</span> <span>out</span><span>)</span> <span>myawk</span> <span>(</span><span>file</span> <span>awkprog</span><span>,</span> <span>file</span> <span>infile</span><span>,</span> <span>file</span> <span>stopwords</span><span>,</span> <span>string</span> <span>yr</span><span>){</span>
  <span>"/usr/bin/awk"</span> <span>"-v"</span> <span>yr</span> <span>"-f"</span> <span>awkprog</span> <span>stopwords</span> <span>infile</span> <span>@</span><span>stdout</span><span>=</span><span>out</span>
<span>}</span>

<span>file</span> <span>aminer</span><span>[]</span> <span>=</span> <span>glob</span><span>(</span><span>"/dev/shm/aminer_mag_papers/*.txt"</span><span>);</span>

<span>foreach</span> <span>y</span> <span>in</span> <span>[</span><span>1800</span><span>:</span><span>2017</span><span>:</span><span>1</span><span>]{</span>
  <span>file</span> <span>yearfiles</span><span>[];</span>
  <span>foreach</span> <span>v</span><span>,</span> <span>i</span> <span>in</span> <span>aminer</span><span>{</span>
    <span>yearfiles</span><span>[</span><span>i</span><span>]</span> <span>=</span> <span>myawk</span><span>(</span><span>input</span><span>(</span><span>"/home/km0/SMC18/src/prob4_p3.awk"</span><span>),</span>  
                         <span>v</span><span>,</span>
                         <span>input</span><span>(</span><span>"/home/km0/SMC18/data/stop_words.txt"</span><span>),</span> 
                         <span>sprintf</span><span>(</span><span>"yr=%s"</span><span>,</span><span>toString</span><span>(</span><span>y</span><span>)));</span>
  <span>}</span>
  <span>file</span> <span>joined</span> <span>&lt;</span><span>sprintf</span><span>(</span><span>"year%s.txt"</span><span>,</span><span>toString</span><span>(</span><span>y</span><span>))</span><span>&gt;</span> <span>=</span> <span>cat</span><span>(</span><span>yearfiles</span><span>);</span>
<span>}</span>
</code></pre></div></div>

<h3 id="problem-5">Problem 5</h3>

<p><em>Given a research proposal, determine whether the proposed work has been accomplished previously.</em></p>

<p>This has a simple solution: Find the keywords on a new proposal.  If those
keywords appear in an existing publication record, it is a suspect. A broad
list of suspects may be found with logical <strong>OR</strong> between keywords which could
be narrowed down with logical <strong>AND</strong>. The keywords may be arbitrarily combined
in ORs and ANDs. The results file <code>/results/suspects.txt</code> shows over 1,400
suspects for an <strong>AND</strong> combination of keywords: <em>battery</em>, <em>electronics</em>,
<em>lithium</em>, and <em>energy</em> from English language papers. The HPC implementation
finishes in <strong>26 seconds</strong>. Awk code below.</p>

<div><div><pre><code><span>#!/usr/bin/env awk -f</span>

<span># Problem Statement</span>
<span>#    Given a research proposal, determine whether the proposed work has been</span>
<span>#    accomplished previously.</span>

<span># Solution: Find the keywords in the new proposal. </span>
<span># If those keywords appear in an existing publication record, it is a suspect.</span>

BEGIN <span>{</span>
    FS <span>=</span> <span>"qwqw"</span>
    IGNORECASE <span>=</span> 1 

    <span># Field names</span>
    <span>id</span><span>=</span>1<span>;</span> <span>title</span><span>=</span>2<span>;</span> <span>num_authors</span><span>=</span>3<span>;</span> <span>doi</span><span>=</span>4<span>;</span> <span>fos_isbn</span><span>=</span>5<span>;</span> <span>doctype_issn</span><span>=</span>6<span>;</span>
    <span>lang</span><span>=</span>7<span>;</span> <span>n_citation</span><span>=</span>8<span>;</span> <span>issue</span><span>=</span>9<span>;</span> <span>url</span><span>=</span>10<span>;</span> <span>volume</span><span>=</span>11<span>;</span> <span>page_start</span><span>=</span>12<span>;</span>
    <span>page_end</span><span>=</span>13<span>;</span> <span>year</span><span>=</span>14<span>;</span> <span>venue</span><span>=</span>15<span>;</span> <span>publisher_pdf</span><span>=</span>16<span>;</span> <span>references</span><span>=</span>17<span>;</span>
    <span>keywords</span><span>=</span>18<span>;</span> <span>abstract</span><span>=</span>19<span>;</span> <span>authors</span><span>=</span>20<span>;</span>
<span>}</span>

<span># topic1 .. topic4 are provided at command line</span>
<span>$0</span>~topic1 <span>&amp;&amp;</span> <span>$0</span>~topic2 <span>&amp;&amp;</span> <span>$0</span>~topic3 <span>&amp;&amp;</span> <span>$0</span>~topic4 <span>&amp;&amp;</span> <span>$lang</span>~/en/ <span>&amp;&amp;</span> <span>$authors</span><span>!</span>~/null/<span>{</span>
    print <span>$id</span>, <span>$title</span>, <span>$authors</span>, <span>$year</span>
<span>}</span>
</code></pre></div></div>



<p>I show how the classic Unix tools may be leveraged to solve modern problems
and that millions of records may be processed in under a minute at scale.
About the data itself, it seems the biosciences research dominates
the publications followed by perhaps physics. I am sure more sophisticated
tools could be used to get refined results and gain better insights – this is
my take. I <a href="https://twitter.com/SciDatathon/status/1120335746358026240">won</a>
the data challenge. Awk is awesome!</p>


  </div>

  
</article>

            
            
        </section></div></div>
    </div>
    <footer>
        <div>created by buildstarted &copy; 2020 <a href="/about">about</a></div>
        <div>Share this page on social media: copy and paste this url https://linksfor.dev/</div>
        <div>If you prefer RSS: <a href="https://linksfor.dev/feed.xml">https://linksfor.dev/feed.xml</a></div>
        <div>Customer satisfaction guaranteed to be optional.</div>
    </footer>
    
    <script async defer>
        _dna = window._dna || {};
        _dna.siteId = "linksfor.devs";
        _dna.outlink = true;

        (function() {
            let dna = document.createElement('script');
            dna.type = 'text/javascript';
            dna.async = true;
            dna.src = '//dna.buildstarted.com/t.js';
            let s = document.getElementsByTagName('script')[0];
            s.parentNode.insertBefore(dna, s);
        })();
    </script>
    <noscript><img src="//dna.buildstarted.com/g?siteId=linksfor.devs"/></noscript>
</body>
</html>