<!DOCTYPE html>
<html lang="en">
<head>
    <title>
What&#x2019;s up with my branch on GPU? - linksfor.dev(s)    </title>
    <meta charset="utf-8">
    <link rel="alternate" type="application/rss+xml" title="Linksfor.dev(s) feed" href="https://linksfor.dev/feed.rss" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta name="google" value="notranslate">
    <link rel="stylesheet" type="text/css" href="/style.css" />
    <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon">
    <link rel="icon" href="/favicon.ico" type="image/x-icon">
    <meta property="og:title" content="What&#x2019;s up with my branch on GPU? - linksfor.dev(s)"/>
    <meta property="og:description" content="About This post is a small writeup addressed to programmers who are interested to learn more about how GPU handles branching and targeted as an introduction to the topic. I recommend skimming through [1], [2], [17] and [8] to get an idea of what GPU execution model looks like in general because here we&#x2019;re gonna take a closer look at one particular detail. A curious reader shall find all references at the end of the post. If you find any mistakes please reach out."/>
    <meta property="og:type" content="website"/>
    <meta property="og:url" content="https://aschrein.github.io/jekyll/update/2019/06/13/whatsup-with-my-branches-on-gpu.html"/>

<meta property="og:site_name" content="linksfor.dev(s)" />
</head>
<body>
    <div class="devring" style="background: #222">
        <div style="text-align:center">Explore other dev related sites in this ring. If you would like to join this ring <a href="https://devring.club">click here</a>.</div>
        <div class="grid">
            <div style="display: grid; grid-template-columns: .5fr 1fr 1fr 1fr; text-align: center;">
                <span class="devring-title"><a href="https://devring.club/">devring.club</a></span>
                <a href="https://devring.club/sites/1/prev" class="devring-previous">Previous</a>
                <a href="https://devring.club/random" class="devring-random">Random</a>
                <a href="https://devring.club/sites/1/next" class="devring-next">Next</a>
            </div>
        </div>
    </div>
    <div class="grid">
        <h1>
<a href="/" style="color:inherit">linksfor.dev(s)</a>
        </h1>
        <title>linksfor.dev(s) - What&#x2019;s up with my branch on GPU?</title>
<div class="readable">
        <h1>What&#x2019;s up with my branch on GPU?</h1>
            <div>Reading time: 25-31 minutes</div>
        <div>Posted here: 21 Jun 2019</div>
        <p><a href="https://aschrein.github.io/jekyll/update/2019/06/13/whatsup-with-my-branches-on-gpu.html">https://aschrein.github.io/jekyll/update/2019/06/13/whatsup-with-my-branches-on-gpu.html</a></p>
        <hr/>
<div id="readability-page-1" class="page"><div aria-label="Content">
      <div>
        <article itemscope="" itemtype="http://schema.org/BlogPosting">

  

  <div itemprop="articleBody">
    <h2 id="about">About</h2>
<p>This post is a small writeup addressed to programmers who are interested to learn more about how GPU handles branching and targeted as an introduction to the topic. I recommend skimming through [<a href="https://fgiesen.wordpress.com/2011/07/09/a-trip-through-the-graphics-pipeline-2011-index/">1</a>], [<a href="http://cs149.stanford.edu/winter19/">2</a>], [<a href="https://anteru.net/blog/2018/intro-to-compute-shaders/">17</a>] and [<a href="https://t.co/QG28evt7QR">8</a>] to get an idea of what GPU execution model looks like in general because here we’re gonna take a closer look at one particular detail. A curious reader shall find all references at the end of the post. If you find any mistakes please reach out.</p>

<h2 id="table-of-content">Table of content</h2>
<ul id="markdown-toc">
  <li><a href="#about" id="markdown-toc-about">About</a></li>
  <li><a href="#table-of-content" id="markdown-toc-table-of-content">Table of content</a></li>
  <li><a href="#vocabulary" id="markdown-toc-vocabulary">Vocabulary</a></li>
  <li><a href="#what-is-so-special-about-gpu-core-compared-to-cpu-core" id="markdown-toc-what-is-so-special-about-gpu-core-compared-to-cpu-core">What is so special about GPU core compared to CPU core?</a>    <ul>
      <li><a href="#illustrations-of-gpu-core" id="markdown-toc-illustrations-of-gpu-core">Illustrations of GPU core</a>        <ul>
          <li><a href="#diagram-color-state-coding" id="markdown-toc-diagram-color-state-coding">Diagram color-state coding</a></li>
          <li><a href="#figure-1-execution-history-42" id="markdown-toc-figure-1-execution-history-42">Figure 1. Execution history 4:2</a></li>
          <li><a href="#figure-2-execution-history-41" id="markdown-toc-figure-2-execution-history-41">Figure 2. Execution history 4:1</a></li>
          <li><a href="#figure-3-execution-history-44" id="markdown-toc-figure-3-execution-history-44">Figure 3. Execution history 4:4</a></li>
        </ul>
      </li>
    </ul>
  </li>
  <li><a href="#what-is-coherencedivergence" id="markdown-toc-what-is-coherencedivergence">What is coherence/divergence?</a>    <ul>
      <li><a href="#example-1" id="markdown-toc-example-1">Example 1</a></li>
      <li><a href="#example-1-execution-mask-history" id="markdown-toc-example-1-execution-mask-history">Example 1. Execution mask history</a></li>
      <li><a href="#example-2" id="markdown-toc-example-2">Example 2</a></li>
      <li><a href="#example-3" id="markdown-toc-example-3">Example 3</a></li>
      <li><a href="#hw-support-for-execution-mask" id="markdown-toc-hw-support-for-execution-mask">HW support for execution mask</a>        <ul>
          <li><a href="#figure-4-some-types-of-control-flow-graphs" id="markdown-toc-figure-4-some-types-of-control-flow-graphs">Figure 4. Some types of control flow graphs</a></li>
        </ul>
      </li>
    </ul>
  </li>
  <li><a href="#execution-mask-handling-examples" id="markdown-toc-execution-mask-handling-examples">Execution mask handling examples</a>    <ul>
      <li><a href="#fictional-isa" id="markdown-toc-fictional-isa">Fictional ISA</a>        <ul>
          <li><a href="#example-1-1" id="markdown-toc-example-1-1">Example 1</a></li>
          <li><a href="#figure-5-example-1-execution-history" id="markdown-toc-figure-5-example-1-execution-history">Figure 5. Example 1 execution history</a></li>
          <li><a href="#example-2-1" id="markdown-toc-example-2-1">Example 2</a></li>
          <li><a href="#figure-6-example-2-execution-history" id="markdown-toc-figure-6-example-2-execution-history">Figure 6. Example 2 execution history</a></li>
          <li><a href="#example-3-1" id="markdown-toc-example-3-1">Example 3</a></li>
          <li><a href="#figure-7-example-3-execution-history" id="markdown-toc-figure-7-example-3-execution-history">Figure 7. Example 3 execution history</a></li>
        </ul>
      </li>
      <li><a href="#amd-gcn-isa" id="markdown-toc-amd-gcn-isa">AMD GCN ISA</a>        <ul>
          <li><a href="#example-1-2" id="markdown-toc-example-1-2">Example 1</a></li>
          <li><a href="#example-2-2" id="markdown-toc-example-2-2">Example 2</a></li>
          <li><a href="#example-3-2" id="markdown-toc-example-3-2">Example 3</a></li>
        </ul>
      </li>
      <li><a href="#avx512" id="markdown-toc-avx512">AVX512</a>        <ul>
          <li><a href="#example-1-3" id="markdown-toc-example-1-3">Example 1</a></li>
          <li><a href="#example-2-3" id="markdown-toc-example-2-3">Example 2</a></li>
          <li><a href="#example-3-3" id="markdown-toc-example-3-3">Example 3</a></li>
        </ul>
      </li>
    </ul>
  </li>
  <li><a href="#how-to-fight-divergence" id="markdown-toc-how-to-fight-divergence">How to fight divergence?</a>    <ul>
      <li><a href="#figure-8-divergent-threads-execution-time" id="markdown-toc-figure-8-divergent-threads-execution-time">Figure 8. Divergent threads execution time</a></li>
      <li><a href="#figure-9-coherent-threads-execution-time" id="markdown-toc-figure-9-coherent-threads-execution-time">Figure 9. Coherent threads execution time</a></li>
    </ul>
  </li>
  <li><a href="#divergent-memory-access" id="markdown-toc-divergent-memory-access">Divergent memory access</a>    <ul>
      <li><a href="#sample-in-a-branch" id="markdown-toc-sample-in-a-branch">Sample in a branch</a></li>
      <li><a href="#when-to-branch" id="markdown-toc-when-to-branch">When to branch?</a>        <ul>
          <li><a href="#figure-10-example-of-a-texture-mask-containing-interleaved-spots-of-constant-value" id="markdown-toc-figure-10-example-of-a-texture-mask-containing-interleaved-spots-of-constant-value">Figure 10. Example of a texture mask containing interleaved spots of constant value</a></li>
        </ul>
      </li>
    </ul>
  </li>
  <li><a href="#links" id="markdown-toc-links">Links</a></li>
  <li><a href="#comments" id="markdown-toc-comments">Comments</a></li>
</ul>
<h2 id="vocabulary">Vocabulary</h2>
<ul>
  <li>GPU - Graphics processing unit</li>
  <li>Flynn’s taxonomy
    <ul>
      <li>SIMD - Single instruction multiple data</li>
      <li>SIMT - Single instruction multiple threads</li>
    </ul>
  </li>
  <li>SIMD wave - Thread executing in SIMD mode</li>
  <li>Lane - designated data stream in SIMD model</li>
  <li>SMT - Simultaneous multi-threading (Intel Hyper-threading)[<a href="http://cs149.stanford.edu/winter19/">2</a>]
    <ul>
      <li>Multiple threads share computational resources of a core</li>
    </ul>
  </li>
  <li>IMT - Interleaved multi-threading[<a href="http://cs149.stanford.edu/winter19/">2</a>]
    <ul>
      <li>Multiple threads share computational resources of a core but only one executes per clock</li>
    </ul>
  </li>
  <li>BB - Basic Block - linear sequence of instructions with only one jump at the end</li>
  <li>ILP - Instruction Level Parallelism[<a href="https://www.elsevier.com/books/computer-architecture/hennessy/978-0-12-811905-1">3</a>]</li>
  <li>ISA - Instruction Set Architecture</li>
  <li>SLM - Shared Local Memory, Group Local</li>
</ul>

<p>Throughout this post I’m referring to this fictional taxonomy. It approximates how a modern GPU is organized.</p>
<div><div><pre><code>Hardware:
GPU  -+
      |- core 0 -+
      |          |- wave 0 +
      |          |         |- lane 0
      |          |         |- lane 1
      |          |         |- ...
      |          |         +- lane Q-1
      |          |
      |          |- ...
      |          +- wave M-1
      |            
      |- ...
      +- core N-1

* ALU - SIMD ALU unit for short

Software:
group +
      |- thread 0
      |- ...
      +- thread N-1
</code></pre></div></div>
<p>Other names:</p>
<ul>
  <li>core could be CU, SM, EU</li>
  <li>wave could be wavefront, HW thread, warp, context</li>
  <li>lane could be SW thread</li>
</ul>

<h2 id="what-is-so-special-about-gpu-core-compared-to-cpu-core">What is so special about GPU core compared to CPU core?</h2>
<p>Any current generation single GPU core is less beefy compared to what you may encounter in CPU world: simple ILP/multi-issue[<a href="https://arxiv.org/pdf/1804.06826.pdf">6</a>] and prefetch[<a href="https://arxiv.org/pdf/1509.02308&amp;ved=0ahUKEwifl_P9rt7LAhXBVxoKHRsxDIYQFgg_MAk&amp;usg=AFQjCNGchkZRzkueGqHEz78QnmcIVCSXvg&amp;sig2=IdzxfrzQgNv8yq7e1mkeVg">5</a>], no speculation or branch/return prediction. All of this coupled with tiny caches frees up quite a lot of the die area which gets filled with more cores. Memory load/store machinery is able to handle bandwidths of an order of magnitude larger(not true for integrated/mobile GPUs) than that of a typical CPU at a cost of more latency. GPU employs SMT[<a href="http://cs149.stanford.edu/winter19/">2</a>] to hide this latency - while one wave is stalled, another utilizes free computation resources of a core. Because of the high latency for memory accesses GPU keeps all the contexts in registers, spills are supported but to be avoided at all cost. Typically the number of waves handled by one core depends on the number of registers used and determined dynamically by allocating on a fixed register file[<a href="https://t.co/QG28evt7QR">8</a>]. The instruction scheduling is hybrid dynamic and static[<a href="https://arxiv.org/pdf/1804.06826.pdf">6</a>] [<a href="https://developer.amd.com/wp-content/resources/Vega_Shader_ISA_28July2017.pdf">11</a> 4.4]. SMT cores execute in SIMD mode yielding high number of FLOPS.<br>
From software point of view the programmer usually writes his kernel in some high level language which does not expose all the HW details like SIMD width or instruction scheduling. However, typically kernels get compiled into SIMD instructions where each lane corresponds to one software thread. Therefore the actual hardware thread(wave) executes N software threads in lockstep. Different ISAs expose their SIMD nature differently, some use explicit SIMD instructions(Intel SSE/AVX and Intel Gen) some use implicit(AMD GCN, Nvidia PTX).</p>
<h3 id="illustrations-of-gpu-core">Illustrations of GPU core</h3>
<p><img src="https://aschrein.github.io/assets/legend.png" alt="Figure 1"></p>
<h6 id="diagram-color-state-coding">Diagram color-state coding</h6>
<p><img src="https://aschrein.github.io/assets/interleaving.png" alt="Figure 1"></p>
<h6 id="figure-1-execution-history-42">Figure 1. Execution history 4:2</h6>
<p>Made on my toy <a href="https://aschrein.github.io/guppy/">gpu simulator</a>(not user friendly).<br>
The image shows history of execution mask where the x axis is time from left to right 1 clock per pixel and the y axis is 1 lane state per pixel and waves in SIMD32 mode from top to bottom. If it does not make sense to you, please return to it after reading the next sections.<br>
This is an illustration of how a GPU core execution history might look like for a fictional configuration: four waves share one sampler and two ALU units. Wave scheduler dispatches two instructions from two waves each cycle. When a wave stalls on memory access or long ALU operation, scheduler switches to another pair of waves making ALU units almost 100% busy all the time.<br>
<img src="https://aschrein.github.io/assets/interleaving_2.png" alt="Figure 2"></p>
<h6 id="figure-2-execution-history-41">Figure 2. Execution history 4:1</h6>
<p>This is the same workload but this time only one wave issues instructions each cycle. Note how the second ALU is starving.<br>
<img src="https://aschrein.github.io/assets/interleaving_3.png" alt="Figure 3"></p>
<h6 id="figure-3-execution-history-44">Figure 3. Execution history 4:4</h6>
<p>This time four instructions are issued each cycle. Note that ALUs are oversubscribed in this case so two waves idle almost all the time(actually it’s a pitfall of the scheduling algorithm).<br>
<strong><em>Update</em></strong> Read more about scheduling challenges[<a href="http://www.joshbarczak.com/blog/?p=823#">12</a>].</p>

<p>Real world GPUs have different configurations per core: some may have up to 40 waves per core and 4 ALUs, some have fixed 7 waves and 2 ALUs. It all depends on a variety of factors and is determined through thorough architecture simulation process.
Also real SIMD ALUs may have narrower width than those of waves they serve, it then takes multiple cycles to process one issued instruction, the multiplier is called ‘chime’ length[<a href="https://www.elsevier.com/books/computer-architecture/hennessy/978-0-12-811905-1">3</a>].</p>

<h2 id="what-is-coherencedivergence">What is coherence/divergence?</h2>
<p>Lets look at the following kernel:</p>
<h6 id="example-1">Example 1</h6>
<div><div><pre><code><span>uint</span> <span>lane_id</span> <span>=</span> <span>get_lane_id</span><span>();</span>
<span>if</span> <span>(</span><span>lane_id</span> <span>&amp;</span> <span>1</span><span>)</span> <span>{</span>
    <span>// Do smth</span>
<span>}</span>
<span>// Do some more</span>
</code></pre></div></div>
<p>Here we see instruction stream where execution path depends on the id of the lane being executed. Apparently different lanes have different values. So what should happen? There are different approaches to tackle this problem [<a href="https://hal.archives-ouvertes.fr/hal-00622654/document">4</a>] but eventually they do approximately the same thing. One of such approaches is execution mask which I will focus on. This approach is employed by pre-Volta Nvidia and AMD GCN GPUs. The core of execution mask is that we keep a bit for each lane within wave. If a lane has 0 set to its corresponding execution bit no registers will be touched for that lane by the next issued instruction. Effectively the lane shouldn’t feel the impact of all the executed instruction as long as it’s execution bit is 0. The way it works is that a wave traverses control flow graph in depth first order keeping a history of branches taken until no bits are set. I think it’s better to follow an example.<br>
So lets say we have waves of width 8. This is how execution mask will look like for the kernel:</p>
<h6 id="example-1-execution-mask-history">Example 1. Execution mask history</h6>
<div><div><pre><code>                                  <span>// execution mask</span>
<span>uint</span> <span>lane_id</span> <span>=</span> <span>get_lane_id</span><span>();</span>     <span>// 11111111</span>
<span>if</span> <span>(</span><span>lane_id</span> <span>&amp;</span> <span>1</span><span>)</span> <span>{</span>                <span>// 11111111</span>
    <span>// Do smth                    // 01010101</span>
<span>}</span>
<span>// Do some more                   // 11111111</span>
</code></pre></div></div>
<p>Now, take a look at more complicated examples:</p>
<h6 id="example-2">Example 2</h6>
<div><div><pre><code><span>uint</span> <span>lane_id</span> <span>=</span> <span>get_lane_id</span><span>();</span>
<span>for</span> <span>(</span><span>uint</span> <span>i</span> <span>=</span> <span>lane_id</span><span>;</span> <span>i</span> <span>&lt;</span> <span>16</span><span>;</span> <span>i</span><span>++</span><span>)</span> <span>{</span>
    <span>// Do smth</span>
<span>}</span>
</code></pre></div></div>
<h6 id="example-3">Example 3</h6>
<div><div><pre><code><span>uint</span> <span>lane_id</span> <span>=</span> <span>get_lane_id</span><span>();</span>
<span>if</span> <span>(</span><span>lane_id</span> <span>&lt;</span> <span>16</span><span>)</span> <span>{</span>
    <span>// Do smth</span>
<span>}</span> <span>else</span> <span>{</span>
    <span>// Do smth else</span>
<span>}</span>
</code></pre></div></div>
<p>You’ll notice that history is needed. With execution mask approach usually some kind of stack is employed by the HW. A naive approach is to keep a stack of tuples (exec_mask, address) and add reconvergence instructions that pop a mask from the stack and change the instruction pointer for the wave. In that way a wave will have enough information to traverse the whole CFG for each lane.<br>
From the performance point of view, it takes a couple of cycles just to process a control flow instruction because of all the bookkeeping. And don’t forget that the stack has limited depth.<br>
<strong><em>Update</em></strong> By courtesy of <a href="https://twitter.com/craigkolb">@craigkolb</a> I’ve read [<a href="https://tangentvector.wordpress.com/2013/04/12/a-digression-on-divergence/">13</a>] in which it is noted that AMD GCN fork/join instructions select the path with the fewer number of threads first [<a href="https://developer.amd.com/wp-content/resources/Vega_Shader_ISA_28July2017.pdf">11</a>4.6] which guarantees that log2 depth of the mask stack is enough.<br>
<strong><em>Update</em></strong> Apparently it’s almost always possible to inline everything/structurize CFGs in a shader and therefore keep all execution mask history in registers and schedule CFG traversal/reconvergence statically[<a href="https://github.com/rAzoR8/EuroLLVM19">15</a>]. Skimming through LLVM backend for AMDGPU I didn’t find any evidence of stack handling ever being emitted by the compiler.</p>

<h3 id="hw-support-for-execution-mask">HW support for execution mask</h3>
<p>Now take a look at these control flow graphs(image from Wikipedia):<br>
<img src="https://aschrein.github.io/assets/Some_types_of_control_flow_graphs.png" alt="Figure 4"></p>
<h6 id="figure-4-some-types-of-control-flow-graphs">Figure 4. Some types of control flow graphs</h6>
<p>So what is the minimal set of mask control instructions we need to handle all cases? Here is how it looks in my toy ISA with implicit parallelization, explicit mask control and fully dynamic data hazard synchronization:</p>
<div><div><pre><code><span>push_mask</span> <span>BRANCH_END</span>         <span>; Push current mask and reconvergence pointer</span>
<span>pop_mask</span>                     <span>; Pop mask and jump to reconvergence instruction</span>
<span>mask_nz</span> <span>r0</span><span>.</span><span>x</span>                 <span>; Set execution bit, pop mask if all bits are zero</span>

<span>; Branch instruction is more complicated</span>
<span>; Push current mask for reconvergence</span>
<span>; Push mask for (r0.x == 0) for else block, if any lane takes the path</span>
<span>; Set mask with (r0.x != 0), fallback to else in case no bit is 1</span>
<span>br_push</span> <span>r0</span><span>.</span><span>x</span><span>,</span> <span>ELSE</span><span>,</span> <span>CONVERGE</span> 
</code></pre></div></div>
<p>Lets take a look at how d) case might look like.</p>

<div><div><pre><code><span>A</span><span>:</span>
    <span>br_push</span> <span>r0</span><span>.</span><span>x</span><span>,</span> <span>C</span><span>,</span> <span>D</span>
<span>B</span><span>:</span>
<span>C</span><span>:</span>
    <span>mask_nz</span> <span>r0</span><span>.</span><span>y</span>
    <span>jmp</span> <span>B</span>
<span>D</span><span>:</span>
    <span>ret</span>
</code></pre></div></div>
<p>I’m not an expert in control flow analysis or ISA design so I’m sure there is a case that could not be tamed with my toy ISA, although it does not matter as structured CFG should be enough for everyone.<br>
<strong><em>Update</em></strong> Read more on GCN support for control flow instructions [<a href="https://developer.amd.com/wp-content/resources/Vega_Shader_ISA_28July2017.pdf">11</a>] ch.4 and LLVM implementation [<a href="https://github.com/rAzoR8/EuroLLVM19">15</a>].</p>

<p>Bottom line:</p>
<ul>
  <li>Divergence - emerging difference in execution paths taken by different lanes of the same wave</li>
  <li>Coherence - lack of divergence</li>
  <li>HW needs extra instructions/registers/stack to handle execution mask
    <ul>
      <li>More branches - more register pressure</li>
    </ul>
  </li>
  <li>If one lane enters the branch all lanes enter the branch
    <ul>
      <li>It could get as worse as 1/N efficiency</li>
    </ul>
  </li>
</ul>

<h2 id="execution-mask-handling-examples">Execution mask handling examples</h2>
<h3 id="fictional-isa">Fictional ISA</h3>
<p>I compiled the previous code snippets into my toy ISA and run it on simulator at SIMD32. Take a look at how it handles execution mask.<br>
<strong><em>Update</em></strong> Note that the toy simulator always selects the true path first which is not the best method.</p>
<h6 id="example-1-1">Example 1</h6>
<div><div><pre><code><span>; uint lane_id = get_lane_id();</span>
    <span>mov</span> <span>r0</span><span>.</span><span>x</span><span>,</span> <span>lane_id</span>
<span>; if (lane_id &amp; 1) {</span>
    <span>push_mask</span> <span>BRANCH_END</span>
    <span>and</span> <span>r0</span><span>.</span><span>y</span><span>,</span> <span>r0</span><span>.</span><span>x</span><span>,</span> <span>u</span><span>(</span><span>1</span><span>)</span>
    <span>mask_nz</span> <span>r0</span><span>.</span><span>y</span>
<span>LOOP_BEGIN</span><span>:</span>
    <span>; // Do smth</span>
    <span>pop_mask</span>                <span>; pop mask and reconverge</span>
<span>BRANCH_END</span><span>:</span>
    <span>; // Do some more</span>
    <span>ret</span>
</code></pre></div></div>
<p><img src="https://aschrein.github.io/assets/branch_1.png" alt="Figure 5"></p>
<h6 id="figure-5-example-1-execution-history">Figure 5. Example 1 execution history</h6>
<p>Did you Notice the black area? It is wasted time. Some lanes are waiting for others to finish iterating.</p>
<h6 id="example-2-1">Example 2</h6>
<div><div><pre><code><span>; uint lane_id = get_lane_id();</span>
    <span>mov</span> <span>r0</span><span>.</span><span>x</span><span>,</span> <span>lane_id</span>
<span>; for (uint i = lane_id; i &lt; 16; i++) {</span>
    <span>push_mask</span> <span>LOOP_END</span>        <span>; Push the current mask and the pointer to reconvergence instruction</span>
<span>LOOP_PROLOG</span><span>:</span>
    <span>lt</span><span>.</span><span>u32</span> <span>r0</span><span>.</span><span>y</span><span>,</span> <span>r0</span><span>.</span><span>x</span><span>,</span> <span>u</span><span>(</span><span>16</span><span>)</span>  <span>; r0.y &lt;- r0.x &lt; 16</span>
    <span>add</span><span>.</span><span>u32</span> <span>r0</span><span>.</span><span>x</span><span>,</span> <span>r0</span><span>.</span><span>x</span><span>,</span> <span>u</span><span>(</span><span>1</span><span>)</span>  <span>; r0.x &lt;- r0.x + 1</span>
    <span>mask_nz</span> <span>r0</span><span>.</span><span>y</span>              <span>; exec bit &lt;- r0.y != 0 - when all bits are zero next mask is popped</span>
<span>LOOP_BEGIN</span><span>:</span>
    <span>; // Do smth</span>
    <span>jmp</span> <span>LOOP_PROLOG</span>
<span>LOOP_END</span><span>:</span>
    <span>; // }</span>
    <span>ret</span>
</code></pre></div></div>
<p><img src="https://aschrein.github.io/assets/branch_2.png" alt="Figure 6"></p>
<h6 id="figure-6-example-2-execution-history">Figure 6. Example 2 execution history</h6>
<h6 id="example-3-1">Example 3</h6>
<div><div><pre><code>    <span>mov</span> <span>r0</span><span>.</span><span>x</span><span>,</span> <span>lane_id</span>
    <span>lt</span><span>.</span><span>u32</span> <span>r0</span><span>.</span><span>y</span><span>,</span> <span>r0</span><span>.</span><span>x</span><span>,</span> <span>u</span><span>(</span><span>16</span><span>)</span>
    <span>; if (lane_id &lt; 16) {</span>
        <span>; Push (current mask, CONVERGE) and (else mask, ELSE)</span>
        <span>; Also set current execution bit to r0.y != 0</span>
    <span>br_push</span> <span>r0</span><span>.</span><span>y</span><span>,</span> <span>ELSE</span><span>,</span> <span>CONVERGE</span>
<span>THEN</span><span>:</span>
    <span>; // Do smth</span>
    <span>pop_mask</span>
    <span>; } else {</span>
<span>ELSE</span><span>:</span>
    <span>; // Do smth else</span>
    <span>pop_mask</span>
    <span>; }</span>
<span>CONVERGE</span><span>:</span>
    <span>ret</span>
</code></pre></div></div>
<p><img src="https://aschrein.github.io/assets/branch_3.png" alt="Figure 7"></p>
<h6 id="figure-7-example-3-execution-history">Figure 7. Example 3 execution history</h6>

<h3 id="amd-gcn-isa">AMD GCN ISA</h3>
<p><strong><em>Update</em></strong> GCN also uses an explicit mask handling, you can read more about it here[<a href="https://developer.amd.com/wp-content/resources/Vega_Shader_ISA_28July2017.pdf">11</a> 4.x]. I decided it’s worth putting some examples with their ISA, thanks to <a href="http://shader-playground.timjones.io/">shader-playground</a> it is easy. Maybe some day I’ll come across a simulator and pull out some cool diagrams.<br>
Note that the compiler is smart, you may get a different result. I tried to fool the compiler into not optimizing my branches by putting pointer chase loops in there then cleaned up the assembly.<br>
Also note that S_CBRANCH_I/G_FORK and S_CBRANCH_JOIN instructions are not used in these snippets due to their simplicity/lack of compiler support. Therefore unfortunately the mask stack is not covered. If you know how to make the compiler spit stack handling please convey this information.<br>
<strong><em>Update</em></strong> Watch this <a href="https://youtu.be/8K8ClHoZzHw">talk</a> by <a href="https://twitter.com/SiNGUL4RiTY">@SiNGUL4RiTY</a> about the implementation of vectorized control flow in LLVM backend employed by AMD.</p>
<h6 id="example-1-2">Example 1</h6>
<div><div><pre><code><span>; uint lane_id = get_lane_id();</span>
<span>; GCN uses 64 wave width, so lane_id = thread_id &amp; 63</span>
<span>; There are scalar s* and vector v* registers</span>
<span>; Executon mask does not affect scalar or branch instructions</span>
    <span>v_mov_b32</span>     <span>v1</span><span>,</span> <span>0x00000400</span>      <span>; 1024 - group size</span>
    <span>v_mad_u32_u24</span>  <span>v0</span><span>,</span> <span>s12</span><span>,</span> <span>v1</span><span>,</span> <span>v0</span>    <span>; thread_id calculation</span>
    <span>v_and_b32</span>     <span>v1</span><span>,</span> <span>63</span><span>,</span> <span>v0</span>
<span>; if (lane_id &amp; 1) {</span>
    <span>v_and_b32</span>     <span>v2</span><span>,</span> <span>1</span><span>,</span> <span>v0</span>
    <span>s_mov_b64</span>     <span>s</span><span>[</span><span>0</span><span>:</span><span>1</span><span>]</span><span>,</span> <span>exec</span>        <span>; Save the execution mask</span>
    <span>v_cmpx_ne_u32</span>  <span>exec</span><span>,</span> <span>v2</span><span>,</span> <span>0</span>        <span>; Set the execution bit</span>
    <span>s_cbranch_execz</span>  <span>ELSE</span>             <span>; Jmp if all exec bits are zero</span>
<span>; // Do smth</span>
<span>ELSE</span><span>:</span>
<span>; }</span>
<span>; // Do some more</span>
    <span>s_mov_b64</span>     <span>exec</span><span>,</span> <span>s</span><span>[</span><span>0</span><span>:</span><span>1</span><span>]</span>        <span>; Restore the execution mask</span>
    <span>s_endpgm</span>
</code></pre></div></div>
<h6 id="example-2-2">Example 2</h6>
<div><div><pre><code><span>; uint lane_id = get_lane_id();</span>
    <span>v_mov_b32</span>     <span>v1</span><span>,</span> <span>0x00000400</span>
    <span>v_mad_u32_u24</span>  <span>v0</span><span>,</span> <span>s8</span><span>,</span> <span>v1</span><span>,</span> <span>v0</span>     <span>; Not sure why s8 this time and not s12</span>
    <span>v_and_b32</span>     <span>v1</span><span>,</span> <span>63</span><span>,</span> <span>v0</span>
<span>; LOOP PROLOG</span>
    <span>s_mov_b64</span>     <span>s</span><span>[</span><span>0</span><span>:</span><span>1</span><span>]</span><span>,</span> <span>exec</span>        <span>; Save the execution mask</span>
    <span>v_mov_b32</span>     <span>v2</span><span>,</span> <span>v1</span>
    <span>v_cmp_le_u32</span>  <span>vcc</span><span>,</span> <span>16</span><span>,</span> <span>v1</span>
    <span>s_andn2_b64</span>   <span>exec</span><span>,</span> <span>exec</span><span>,</span> <span>vcc</span>     <span>; Set the execution bit</span>
    <span>s_cbranch_execz</span>  <span>LOOP_END</span>         <span>; Jmp if all exec bits are zero</span>
<span>; for (uint i = lane_id; i &lt; 16; i++) {</span>
<span>LOOP_BEGIN</span><span>:</span>
    <span>; // Do smth</span>
    <span>v_add_u32</span>     <span>v2</span><span>,</span> <span>1</span><span>,</span> <span>v2</span>
    <span>v_cmp_le_u32</span>  <span>vcc</span><span>,</span> <span>16</span><span>,</span> <span>v2</span>
    <span>s_andn2_b64</span>   <span>exec</span><span>,</span> <span>exec</span><span>,</span> <span>vcc</span>     <span>; Mask out lanes which are beyond loop limit</span>
    <span>s_cbranch_execnz</span>  <span>LOOP_BEGIN</span>      <span>; Jmp if non zero exec mask</span>
<span>LOOP_END</span><span>:</span>
    <span>; // }</span>
    <span>s_mov_b64</span>     <span>exec</span><span>,</span> <span>s</span><span>[</span><span>0</span><span>:</span><span>1</span><span>]</span>        <span>; Restore the execution mask</span>
    <span>s_endpgm</span>
</code></pre></div></div>
<h6 id="example-3-2">Example 3</h6>
<div><div><pre><code><span>; uint lane_id = get_lane_id();</span>
    <span>v_mov_b32</span>     <span>v1</span><span>,</span> <span>0x00000400</span>
    <span>v_mad_u32_u24</span>  <span>v0</span><span>,</span> <span>s12</span><span>,</span> <span>v1</span><span>,</span> <span>v0</span>
    <span>v_and_b32</span>     <span>v1</span><span>,</span> <span>63</span><span>,</span> <span>v0</span>
    <span>v_and_b32</span>     <span>v2</span><span>,</span> <span>1</span><span>,</span> <span>v0</span>
    <span>s_mov_b64</span>     <span>s</span><span>[</span><span>0</span><span>:</span><span>1</span><span>]</span><span>,</span> <span>exec</span>        <span>; Save the execution mask</span>
<span>; if (lane_id &lt; 16) {</span>
    <span>v_cmpx_lt_u32</span>  <span>exec</span><span>,</span> <span>v1</span><span>,</span> <span>16</span>       <span>; Set the execution bit</span>
    <span>s_cbranch_execz</span>  <span>ELSE</span>             <span>; Jmp if all exec bits are zero</span>
<span>; // Do smth</span>
<span>; } else {</span>
<span>ELSE</span><span>:</span>
    <span>s_andn2_b64</span>   <span>exec</span><span>,</span> <span>s</span><span>[</span><span>0</span><span>:</span><span>1</span><span>]</span><span>,</span> <span>exec</span>  <span>; Inverse the mask and &amp; with previous</span>
    <span>s_cbranch_execz</span>  <span>CONVERGE</span>         <span>; Jmp if all exec bits are zero</span>
<span>; // Do smth else</span>
<span>; }</span>
<span>CONVERGE</span><span>:</span>
    <span>s_mov_b64</span>     <span>exec</span><span>,</span> <span>s</span><span>[</span><span>0</span><span>:</span><span>1</span><span>]</span>        <span>; Restore the execution mask</span>
<span>; // Do some more</span>
    <span>s_endpgm</span>
</code></pre></div></div>
<h3 id="avx512">AVX512</h3>
<p><strong><em>Update</em></strong> <a href="https://twitter.com/tom_forsyth">@tom_forsyth</a> pointed out that AVX512 extension comes with an explicit mask handling too, so here are some examples. You can read more about it at [<a href="https://software.intel.com/sites/default/files/managed/39/c5/325462-sdm-vol-1-2abcd-3abcd.pdf">14</a>] par. 15.x and 15.6.1. It’s not precisely a GPU but still a legit SIMD16 at 32 bit. Snippets are made using <a href="https://godbolt.org/z/kwrr1y">godbolt’s</a> ISPC(–target=avx512knl-i32x16) and tampered with heavily.</p>
<h6 id="example-1-3">Example 1</h6>
<div><div><pre><code>    <span>; Imagine zmm0 contains 16 lane_ids</span>
    <span>; AVXZ512 comes with k0-k7 mask registers</span>
    <span>; Usage:</span>
    <span>; op reg1 {k[7:0]}, reg2, reg3</span>
    <span>; k0 can not be used as a predicate operand, only k1-k7</span>
<span>; if (lane_id &amp; 1) {</span>
    <span>vpslld</span>       <span>zmm0</span> <span>{</span><span>k1</span><span>}</span><span>,</span> <span>zmm0</span><span>,</span> <span>31</span>  <span>; zmm0[i] = zmm0[i] &lt;&lt; 31</span>
    <span>kmovw</span>        <span>eax</span><span>,</span> <span>k1</span>              <span>; Save the execution mask</span>
    <span>vptestmd</span>     <span>k1</span> <span>{</span><span>k1</span><span>}</span><span>,</span> <span>zmm0</span><span>,</span> <span>zmm0</span>  <span>; k1[i] = zmm0[i] != 0</span>
    <span>kortestw</span>     <span>k1</span><span>,</span> <span>k1</span>
    <span>je</span>           <span>ELSE</span>                 <span>; Jmp if all exec bits are zero</span>
<span>; // Do smth</span>
    <span>; Now k1 contains the execution mask</span>
    <span>; We can use it like this:</span>
    <span>; vmovdqa32 zmm1 {k1}, zmm0</span>
<span>ELSE</span><span>:</span>
<span>; }</span>
    <span>kmovw</span>        <span>k1</span><span>,</span> <span>eax</span>              <span>; Restore the execution mask</span>
<span>; // Do some more</span>
    <span>ret</span>
</code></pre></div></div>
<h6 id="example-2-3">Example 2</h6>
<div><div><pre><code> <span>; Imagine zmm0 contains 16 lane_ids</span>
    <span>kmovw</span>         <span>eax</span><span>,</span> <span>k1</span>               <span>; Save the execution mask</span>
    <span>vpcmpltud</span>     <span>k1</span> <span>{</span><span>k1</span><span>}</span><span>,</span> <span>zmm0</span><span>,</span> <span>16</span>     <span>; k1[i] = zmm0[i] &lt; 16</span>
    <span>kortestw</span>      <span>k1</span><span>,</span> <span>k1</span>
    <span>je</span>            <span>LOOP_END</span>              <span>; Jmp if all exec bits are zero</span>
    <span>vpternlogd</span>    <span>zmm1</span> <span>{</span><span>k1</span><span>}</span><span>,</span> <span>zmm1</span><span>,</span> <span>zmm1</span><span>,</span> <span>255</span>   <span>; zmm1[i] = -1</span>
<span>; for (uint i = lane_id; i &lt; 16; i++) {</span>
<span>LOOP_BEGIN</span><span>:</span>
<span>; // Do smth</span>
    <span>vpsubd</span>        <span>zmm0</span> <span>{</span><span>k1</span><span>}</span><span>,</span> <span>zmm0</span><span>,</span> <span>zmm1</span> <span>; zmm0[i] = zmm0[i] + 1</span>
    <span>vpcmpltud</span>     <span>k1</span> <span>{</span><span>k1</span><span>}</span><span>,</span> <span>zmm0</span><span>,</span> <span>16</span>     <span>; masked k1[i] = zmm0[i] &lt; 16</span>
    <span>kortestw</span>      <span>k1</span><span>,</span> <span>k1</span>
    <span>jne</span>           <span>LOOP_BEGIN</span>            <span>; Break if all exec bits are zero</span>
<span>LOOP_END</span><span>:</span>
<span>; // }</span>
    <span>kmovw</span>        <span>k1</span><span>,</span> <span>eax</span>                <span>; Restore the execution mask</span>
<span>; // Do some more</span>
    <span>ret</span>
</code></pre></div></div>
<h6 id="example-3-3">Example 3</h6>
<div><div><pre><code> <span>; Imagine zmm0 contains 16 lane_ids</span>
<span>; if (lane_id &amp; 1) {</span>
    <span>vpslld</span>       <span>zmm0</span> <span>{</span><span>k1</span><span>}</span><span>,</span> <span>zmm0</span><span>,</span> <span>31</span>  <span>; zmm0[i] = zmm0[i] &lt;&lt; 31</span>
    <span>kmovw</span>        <span>eax</span><span>,</span> <span>k1</span>              <span>; Save the execution mask</span>
    <span>vptestmd</span>     <span>k1</span> <span>{</span><span>k1</span><span>}</span><span>,</span> <span>zmm0</span><span>,</span> <span>zmm0</span>  <span>; k1[i] = zmm0[i] != 0</span>
    <span>kortestw</span>     <span>k1</span><span>,</span> <span>k1</span>
    <span>je</span>           <span>ELSE</span>                 <span>; Jmp if all exec bits are zero</span>
<span>THEN</span><span>:</span>
<span>; // Do smth</span>
<span>; } else {</span>
<span>ELSE</span><span>:</span>
    <span>kmovw</span>        <span>ebx</span><span>,</span> <span>k1</span>
    <span>andn</span>         <span>ebx</span><span>,</span> <span>eax</span><span>,</span> <span>ebx</span>
    <span>kmovw</span>        <span>k1</span><span>,</span> <span>ebx</span>              <span>; mask = ~mask &amp; old_mask</span>
    <span>kortestw</span>     <span>k1</span><span>,</span> <span>k1</span>
    <span>je</span>           <span>CONVERGE</span>             <span>; Jmp if all exec bits are zero</span>
<span>; // Do smth else</span>
<span>; }</span>
<span>CONVERGE</span><span>:</span>
<span>kmovw</span>            <span>k1</span><span>,</span> <span>eax</span>              <span>; Restore the execution mask</span>
<span>; // Do some more</span>
    <span>ret</span>
</code></pre></div></div>
<h2 id="how-to-fight-divergence">How to fight divergence?</h2>
<p>I tried to come up with a simple yet complete illustration for the inefficiency introduced by combining divergent lanes.<br>
Imagine a simple kernel like this:</p>
<div><div><pre><code><span>uint</span> <span>thread_id</span> <span>=</span> <span>get_thread_id</span><span>();</span>
<span>uint</span> <span>iter_count</span> <span>=</span> <span>memory</span><span>[</span><span>thread_id</span><span>];</span>
<span>for</span> <span>(</span><span>uint</span> <span>i</span> <span>=</span> <span>0</span><span>;</span> <span>i</span> <span>&lt;</span> <span>iter_count</span><span>;</span> <span>i</span><span>++</span><span>)</span> <span>{</span>
    <span>// Do smth</span>
<span>}</span>
</code></pre></div></div>
<p>It may resemble a raymarcher, where it takes more iterations for missed rays near surface and less iterations for straight hit/miss rays.<br>
Let’s spawn 256 threads and measure the duration:<br>
<img src="https://aschrein.github.io/assets/rand.png" alt="Figure 8"></p>
<h6 id="figure-8-divergent-threads-execution-time">Figure 8. Divergent threads execution time</h6>
<p>The x axis is SW thread id, the y axis is clock cycles; the different bars show how much time is wasted by grouping threads with different wave widths compared to single threaded execution.<br>
The execution time of a wave is equal to the maximum execution time among confined lanes. You can see that the performance is already ruined at SIMD8, further widening just makes it slightly worse.<br>
<img src="https://aschrein.github.io/assets/sorted.png" alt="Figure 9"></p>
<h6 id="figure-9-coherent-threads-execution-time">Figure 9. Coherent threads execution time</h6>
<p>This figure shows the same bars but this time iteration counts are sorted over thread ids, so that threads with similar iteration counts get dispatched to the same wave.<br>
For this example the potential speedup is around 2x.</p>

<p>Bottom line:</p>
<ul>
  <li>Sort input data</li>
</ul>

<p>For example, if you are writing a ray tracer, grouping rays with similar direction and position could be beneficial because they are likely to be traversing the same nodes in BVH. For more details please follow [<a href="https://www.eecis.udel.edu/~cavazos/cisc879-spring2012/papers/a3-han.pdf">10</a>] and related articles.</p>

<ul>
  <li>Keep CFG simple</li>
</ul>

<p>If your cfg is complex it typically makes sense to split the kernel and classify your data. For example, on a deferred shading pass you could detect tiles on the offscreen buffer that contain some expensive material and spawn pixel shaders separately for those tiles instead of doing full screen uber shader.</p>

<p>It’s worth mentioning that there are some techniques to grapple with divergence on HW level, some of them are Dynamic Warp Formation[<a href="http://www.cs.cmu.edu/afs/cs/academic/class/15869-f11/www/readings/fung07_dynamicwarp.pdf">7</a>] and predicated execution for small branches.</p>

<p><strong><em>Update</em></strong> One of the advanced techniques for dynamic work balancing is called ‘persistent threads’ read more at [<a href="https://www.researchgate.net/publication/334691952_A_Specialized_Concurrent_Queue_for_Scheduling_Irregular_Workloads_on_GPUs">19</a>]. The gist of this method is that you create just enough threads to saturate your GPU with a generic kernel which dynamically grabs job items off a global queue using an atomic counter. With coherent memory like the one you could have with CUDA(Unified memory in NV terminology) it becomes even more interesting because you can stream work from CPU to GPU online.</p>

<h2 id="divergent-memory-access">Divergent memory access</h2>
<p><strong><em>Update Jun 26, 2019</em></strong> Added a few thoughts on memory access in a divergent control flow. It feels like there’s more of the speculation going on than usual. So if you spot any errors please reach out.<br>
On SIMD architecture every load is gather and every store is scatter. A memory operation is generated for each active lane when a store/load instruction is issued, typically if inactive lanes have invalid addresses at the corresponding address slots, no exception is going to be generated.<br>
Memory coalescing machinery is going to take care of optimizing apparent patterns to the global memory which is one of the benefits of gather loads. In case you access SLM you may hit a bank collision issue.</p>
<h3 id="sample-in-a-branch">Sample in a branch</h3>
<p>Now, what about this 10000 pound elephant in the room? Things indeed might get hairy if you are trying to sample a texture in a branch. Particularly, if you are sampling in a pixel shader and use anisotropic/trilinear filtering - those kind of features depend on HW gradients which require that all lanes participating in a 2x2 pixel group have valid arguments. The way it works is that HW packs adjacent pixel groups of 2x2 in the same wave. This has a consequence that 1 pixel triangle is going to spawn 4 pixels and at least 1 wave, the same happens with lone pixels on triangle boundaries. Invisible pixels are called helpers. Some helper pixels are going to have their barycentric coordinates outside the triangle(not sure what happens here, reach out if you do).<br>
A good read on the subject matter is <a href="https://microsoft.github.io/DirectX-Specs/d3d/archive/D3D11_3_FunctionalSpec.htm#16.8%20Interaction%20of%20Varying%20Flow%20Control%20With%20Screen%20Derivatives">DirectX-Specs 16.8.2 Restrictions on Derivative Calculations</a>. In short the spec allows such samples if the texture address is a shader input or a statically indexed constant. It makes sense because even if the HW does not know how to handle divergent samples, its compiler can just hoist that sample from the branch and eliminate the issue completely. I expect other APIs to enforce similar behaviour.<br>
Worth mentioning that the amount of in-flight memory requests being served is somehow limited by the HW. So if you have a kernel with tons of samples or memory loads, it might stall just trying to issue those requests. Which means that some amount of interleaving of ALU instructions and memory requests is needed to avoid such stalls. But fear not, it’s usually taken care of in the compiler, fortunately shader memory models are quite permissive when it comes to reordering.<br>
<strong><em>Update</em></strong> <a href="https://twitter.com/Themaister">@Themaister</a> Wrote a post [<a href="http://themaister.net/blog/2019/09/12/the-weird-world-of-shader-divergence-and-lod/">18</a>] about his experiments with texture samples in a divergent control flow on different HW.</p>
<h3 id="when-to-branch">When to branch?</h3>
<p>I had one use case in mind. Purely theoretical yet may be interesting.<br>
Sometimes you might be hitting the limit of on-flight requests on your HW. In this case it makes sense to reduce the amount of issued samples.
Imagine this example:</p>
<div><div><pre><code><span>a</span> <span>=</span> <span>mask</span><span>.</span><span>Sample</span><span>(</span><span>sampler</span><span>,</span> <span>uv</span><span>);</span>
<span>b</span> <span>=</span> <span>diffuse_1</span><span>.</span><span>Sample</span><span>(</span><span>sampler</span><span>,</span> <span>uv</span><span>);</span>
<span>c</span> <span>=</span> <span>diffuse_2</span><span>.</span><span>Sample</span><span>(</span><span>sampler</span><span>,</span> <span>uv</span><span>);</span>
<span>pixel</span> <span>=</span> <span>b</span> <span>*</span> <span>a</span><span>.</span><span>x</span> <span>+</span> <span>c</span> <span>*</span> <span>(</span><span>1.0</span> <span>-</span> <span>a</span><span>.</span><span>x</span><span>);</span>
</code></pre></div></div>
<p>Basically we sample a mask and then blend 4 textures. It could be a terrain rendering and we mix grass and ground textures. The mask could look like that:<br>
<img src="https://aschrein.github.io/assets/noise.png" alt="Figure 10"></p>
<h6 id="figure-10-example-of-a-texture-mask-containing-interleaved-spots-of-constant-value">Figure 10. Example of a texture mask containing interleaved spots of constant value</h6>
<p>Notice that the mask consists of big isles of constant values with a little bit of blend on boundaries.<br>
If our HW is going to stall on issue limit we are going to pay more than 1 sample latency.<br>
Now take a look at this transformed example:</p>
<div><div><pre><code><span>a</span> <span>=</span> <span>mask</span><span>.</span><span>Sample</span><span>(</span><span>sampler</span><span>,</span> <span>uv</span><span>);</span>
<span>b</span> <span>=</span> <span>diffuse_1</span><span>.</span><span>Sample</span><span>(</span><span>sampler</span><span>,</span> <span>uv</span><span>);</span>
<span>if</span> <span>(</span><span>a</span><span>.</span><span>x</span> <span>&lt;</span> <span>1.0</span><span>)</span> <span>{</span>
    <span>c</span> <span>=</span> <span>diffuse_2</span><span>.</span><span>Sample</span><span>(</span><span>sampler</span><span>,</span> <span>uv</span><span>);</span>
<span>}</span> <span>else</span> <span>{</span>
    <span>c</span> <span>=</span> <span>float4</span><span>(</span><span>0.0</span><span>,</span> <span>0.0</span><span>,</span> <span>0.0</span><span>,</span> <span>0.0</span><span>);</span>
<span>}</span>
<span>pixel</span> <span>=</span> <span>b</span> <span>*</span> <span>a</span><span>.</span><span>x</span> <span>+</span> <span>c</span> <span>*</span> <span>(</span><span>1.0</span> <span>-</span> <span>a</span><span>.</span><span>x</span><span>);</span>
</code></pre></div></div>
<p>If the wave already stalls on sampler oversubscription, introducing a data dependency between a and e may not make things worse(the wave has to wait for a to arrive before issuing c). When a.x is 1.0 we are not paying for that redundant sample. In the case of the mask depicted in Figure 10. waves skip the second sample most of the time.<br>
This is highly HW/compiler dependent. The compiler may eliminate this branch completely, however, AMD compiler <a href="http://shader-playground.timjones.io/79af193deadab308250d5bbbb7e2ca75">keeps</a> it.<br>
Note that this example is artificially bad, you shouldn’t have that much of samples to stall on issue limit and if you do probably something like virtual texture cache could be employed [<a href="https://80.lv/articles/using-next-gen-terrain-engines-for-games-production-009snw/?fbclid=IwAR3L7HgMjz8aCCIy_XQc-Tedn7JOHrYzIR10QvZRTEbOCcLFWu6izc7bS4M">16</a>].</p>



<p>[1]<a href="https://fgiesen.wordpress.com/2011/07/09/a-trip-through-the-graphics-pipeline-2011-index/">A trip through the Graphics Pipeline</a></p>

<p>[2]<a href="http://cs149.stanford.edu/winter19/">Kayvon Fatahalian: PARALLEL COMPUTING</a></p>

<p>[3]<a href="https://www.elsevier.com/books/computer-architecture/hennessy/978-0-12-811905-1">Computer Architecture A Quantitative Approach</a></p>

<p>[4]<a href="https://hal.archives-ouvertes.fr/hal-00622654/document">Stack-less SIMT reconvergence at low cost</a></p>

<p>[5]<a href="https://arxiv.org/pdf/1509.02308&amp;ved=0ahUKEwifl_P9rt7LAhXBVxoKHRsxDIYQFgg_MAk&amp;usg=AFQjCNGchkZRzkueGqHEz78QnmcIVCSXvg&amp;sig2=IdzxfrzQgNv8yq7e1mkeVg">Dissecting GPU memory hierarchy through microbenchmarking</a></p>

<p>[6]<a href="https://arxiv.org/pdf/1804.06826.pdf">Dissecting the NVIDIA Volta GPU Architecture via Microbenchmarking</a></p>

<p>[7]<a href="http://www.cs.cmu.edu/afs/cs/academic/class/15869-f11/www/readings/fung07_dynamicwarp.pdf">Dynamic Warp Formation and Scheduling for Efficient GPU Control Flow</a></p>

<p>[8]<a href="https://t.co/QG28evt7QR">Maurizio Cerrato: GPU Architectures</a></p>

<p>[9]<a href="https://aschrein.github.io/guppy/">Toy GPU simulator</a></p>

<p>[10]<a href="https://www.eecis.udel.edu/~cavazos/cisc879-spring2012/papers/a3-han.pdf">Reducing Branch Divergence in GPU Programs</a></p>

<p>[11]<a href="https://developer.amd.com/wp-content/resources/Vega_Shader_ISA_28July2017.pdf">“Vega” Instruction Set Architecture</a></p>

<p>[12]<a href="http://www.joshbarczak.com/blog/?p=823#">Joshua Barczak:Simulating Shader Execution for GCN</a></p>

<p>[13]<a href="https://tangentvector.wordpress.com/2013/04/12/a-digression-on-divergence/">Tangent Vector: A Digression on Divergence</a></p>

<p>[14]<a href="https://software.intel.com/sites/default/files/managed/39/c5/325462-sdm-vol-1-2abcd-3abcd.pdf">Intel® 64 and IA-32 ArchitecturesSoftware Developer’s Manual</a></p>

<p>[15]<a href="https://github.com/rAzoR8/EuroLLVM19">Vectorizing Divergent Control-Flow for SIMD Applications</a></p>

<p>[16]<a href="https://80.lv/articles/using-next-gen-terrain-engines-for-games-production-009snw/?fbclid=IwAR3L7HgMjz8aCCIy_XQc-Tedn7JOHrYzIR10QvZRTEbOCcLFWu6izc7bS4M">Jason Booth: Terrain Shader Generation Systems</a></p>

<p>[17]<a href="https://anteru.net/blog/2018/intro-to-compute-shaders/">Matthäus G. Chajdas: Introduction to compute shaders</a></p>

<p>[18]<a href="https://anteru.net/blog/2018/intro-to-compute-shaders/">Maister’s Graphics Adventures: The weird world of shader divergence and LOD</a></p>

<p>[19]<a href="https://www.researchgate.net/publication/334691952_A_Specialized_Concurrent_Queue_for_Scheduling_Irregular_Workloads_on_GPUs">A Specialized Concurrent Queue for Scheduling Irregular Workloads on GPUs</a></p>


<p>I don’t have comments so here’s a link to the wrapper tweet</p>
<twitter-widget id="twitter-widget-0"></twitter-widget><blockquote data-lang="en" data-twitter-extracted-i1581912102193861086="true"><p lang="en" dir="ltr">Wrote a post with basic info about control flow handling on GPU with focus on execution mask approach. Please reach out if you find any mistakes or have suggestions.<a href="https://t.co/ctUFDTkoal">https://t.co/ctUFDTkoal</a></p>— Anton Schreiner (@kokoronomagnet) <a href="https://twitter.com/kokoronomagnet/status/1141562844871385093?ref_src=twsrc%5Etfw">June 20, 2019</a></blockquote>



  </div>
</article>

      </div>
    </div></div></div>
    </div>
    <footer>
        <div>created by <a href="https://buildstarted.com">buildstarted</a> &copy; 2020 <a href="/about">about</a></div>
        <div>Share this page on social media: copy and paste this url https://linksfor.dev/</div>
        <div>If you prefer RSS: <a href="https://linksfor.dev/feed.xml">https://linksfor.dev/feed.xml</a></div>
        <div>Customer satisfaction guaranteed to be optional.</div>
    </footer>
    
    <script async defer>
        _dna = window._dna || {};
        _dna.siteId = "linksfor.devs";
        _dna.outlink = true;

        (function() {
            let dna = document.createElement('script');
            dna.type = 'text/javascript';
            dna.async = true;
            dna.src = '//dna.buildstarted.com/t.js';
            let s = document.getElementsByTagName('script')[0];
            s.parentNode.insertBefore(dna, s);
        })();
    </script>
    <noscript><img src="//dna.buildstarted.com/g?siteId=linksfor.devs"/></noscript>
</body>
</html>