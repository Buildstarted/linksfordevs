<!DOCTYPE html>
<html lang="en">
<head>
    <title>
Binary Stochastic Neurons in Tensorflow - R2RT - linksfor.dev(s)    </title>
    <meta charset="utf-8">
    <link rel="alternate" type="application/rss+xml" title="Linksfor.dev(s) feed" href="https://linksfor.dev/feed.rss" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta name="google" value="notranslate">
    <link rel="stylesheet" type="text/css" href="/style.min.css" />
    <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon">
    <link rel="icon" href="/favicon.ico" type="image/x-icon">
    <meta property="og:title" content="Binary Stochastic Neurons in Tensorflow - R2RT - linksfor.dev(s)"/>
    <meta property="og:description" content="Sat 24 September 2016"/>
    <meta property="og:type" content="website"/>
    <meta property="og:url" content="https://r2rt.com/binary-stochastic-neurons-in-tensorflow.html"/>

<meta property="og:site_name" content="linksfor.dev(s)" />
</head>
<body>
    <div class="devring" style="background: #222">
        <div style="text-align:center">Explore other dev related sites in this ring. If you would like to join this ring <a href="https://devring.club">click here</a>.</div>
        <div class="grid">
            <div style="display: grid; grid-template-columns: .5fr 1fr 1fr 1fr; text-align: center;">
                <span class="devring-title"><a href="https://devring.club/">devring.club</a></span>
                <a href="https://devring.club/sites/1/prev" class="devring-previous">Previous</a>
                <a href="https://devring.club/random" class="devring-random">Random</a>
                <a href="https://devring.club/sites/1/next" class="devring-next">Next</a>
            </div>
        </div>
    </div>
    <div class="grid">
        <h1 style="margin: unset">
<a href="/" style="color:inherit">linksfor.dev(s)</a>
        </h1>
        <title>linksfor.dev(s) - Binary Stochastic Neurons in Tensorflow - R2RT</title>
<div class="readable">
        <h1>Binary Stochastic Neurons in Tensorflow - R2RT</h1>
            <div>Reading time: 34-43 minutes</div>
        <div>Posted here: 27 Feb 2019</div>
        <p><a href="https://r2rt.com/binary-stochastic-neurons-in-tensorflow.html">https://r2rt.com/binary-stochastic-neurons-in-tensorflow.html</a></p>
        <hr/>
<div id="readability-page-1" class="page"><div>
    <section id="content">
      <header>
        <h2>
          Binary Stochastic Neurons in Tensorflow
        </h2>
        
        <p>Sat 24 September 2016</p>
      </header>
<!-- .entry-content -->
      <div>
        


  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
  <title></title>
  
  
  
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->


<p>In this post, I introduce and discuss binary stochastic neurons, implement trainable binary stochastic neurons in Tensorflow, and conduct several simple experiments on the MNIST dataset to get a feel for their behavior. Binary stochastic neurons offer two advantages over real-valued neurons: they can act as a regularizer and they enable conditional computation by enabling a network to make yes/no decisions. Conditional computation opens the door to new and exciting neural network architectures, such as the choice of experts architecture and heirarchical multiscale neural networks, which I plan to discuss in future posts.</p>
<h3 id="the-binary-stochastic-neuron">The binary stochastic neuron</h3>
<p>A binary stochastic neuron is a neuron with a noisy output: some proportion <span><span><span id="MJXp-Span-1"><span id="MJXp-Span-2">p</span></span></span><span id="MathJax-Element-1-Frame" tabindex="0"></span></span> of the time it outputs 1, otherwise 0. An easy way to turn a real-valued input, <span><span><span id="MJXp-Span-3"><span id="MJXp-Span-4">a</span></span></span><span id="MathJax-Element-2-Frame" tabindex="0"></span></span>, into this proportion, <span><span><span id="MJXp-Span-5"><span id="MJXp-Span-6">p</span></span></span><span id="MathJax-Element-3-Frame" tabindex="0"></span></span>, is to set <span><span><span id="MJXp-Span-7"><span id="MJXp-Span-8">p</span><span id="MJXp-Span-9">=</span><span id="MJXp-Span-10">sigm</span><span id="MJXp-Span-11">(</span><span id="MJXp-Span-12">a</span><span id="MJXp-Span-13">)</span></span></span><span id="MathJax-Element-4-Frame" tabindex="0"></span></span>, where <span><span><span id="MJXp-Span-14"><span id="MJXp-Span-15">sigm</span></span></span><span id="MathJax-Element-5-Frame" tabindex="0"></span></span> is the logistic sigmoid, <span><span><span id="MJXp-Span-16"><span id="MJXp-Span-17">sigm</span><span id="MJXp-Span-18">(</span><span id="MJXp-Span-19">x</span><span id="MJXp-Span-20">)</span><span id="MJXp-Span-21">=</span><span id="MJXp-Span-22"><span><span id="MJXp-Span-23">1</span></span><span><span><span><span></span></span><span><span><span id="MJXp-Span-24">1</span><span id="MJXp-Span-25">+</span><span id="MJXp-Span-26">exp</span><span id="MJXp-Span-27"></span><span id="MJXp-Span-28">(</span><span id="MJXp-Span-29">−</span><span id="MJXp-Span-30">x</span><span id="MJXp-Span-31">)</span></span></span></span></span></span></span></span><span id="MathJax-Element-6-Frame" tabindex="0"></span></span>. Thus, we define the binary stochastic neuron, <span><span><span id="MJXp-Span-32"><span id="MJXp-Span-33">BSN</span></span></span><span id="MathJax-Element-7-Frame" tabindex="0"></span></span>, as:</p>
<p><span><span><span id="MJXp-Span-34"><span id="MJXp-Span-35">BSN</span><span id="MJXp-Span-36">(</span><span id="MJXp-Span-37">a</span><span id="MJXp-Span-38">)</span><span id="MJXp-Span-39">=</span><span id="MJXp-Span-40"><span id="MJXp-Span-41"><span id="MJXp-Span-42">1</span></span><span id="MJXp-Span-43"><span id="MJXp-Span-44">z</span><span id="MJXp-Span-45">&nbsp;</span><span id="MJXp-Span-46">&lt;</span><span id="MJXp-Span-47">&nbsp;</span><span id="MJXp-Span-48">sigm</span><span id="MJXp-Span-49">(</span><span id="MJXp-Span-50">a</span><span id="MJXp-Span-51">)</span></span></span></span></span><span><span id="MathJax-Element-8-Frame" tabindex="0"></span></span></span></p>
<p>where <span><span><span id="MJXp-Span-52"><span id="MJXp-Span-53"><span id="MJXp-Span-54"><span id="MJXp-Span-55">1</span></span><span id="MJXp-Span-56"><span id="MJXp-Span-57">x</span></span></span></span></span><span id="MathJax-Element-9-Frame" tabindex="0"></span></span> is the <a href="https://en.wikipedia.org/wiki/Indicator_function">indicator function</a> on the truth value of <span><span><span id="MJXp-Span-58"><span id="MJXp-Span-59">x</span></span></span><span id="MathJax-Element-10-Frame" tabindex="0"></span></span> and <span><span><span id="MJXp-Span-60"><span id="MJXp-Span-61">z</span><span id="MJXp-Span-62">∼</span><span id="MJXp-Span-63">U</span><span id="MJXp-Span-64">[</span><span id="MJXp-Span-65">0</span><span id="MJXp-Span-66">,</span><span id="MJXp-Span-67">1</span><span id="MJXp-Span-68">]</span></span></span><span id="MathJax-Element-11-Frame" tabindex="0"></span></span>.</p>
<h3 id="advantages-of-the-binary-stochastic-neuron">Advantages of the binary stochastic neuron</h3>
<ol type="1">
<li><p>A binary stochastic neuron is a noisy modification of the logistic sigmoid: instead of outputting <span><span><span id="MJXp-Span-69"><span id="MJXp-Span-70">p</span></span></span><span id="MathJax-Element-12-Frame" tabindex="0"></span></span>, it outputs 1 with probability <span><span><span id="MJXp-Span-71"><span id="MJXp-Span-72">p</span></span></span><span id="MathJax-Element-13-Frame" tabindex="0"></span></span> and 0 otherwise. Noise generally serves as a regularizer (see, e.g., <a href="http://www.jmlr.org/papers/v15/srivastava14a.html">Srivastava et al. (2014)</a> and <a href="https://arxiv.org/abs/1511.06807">Neelakantan et al. (2015)</a>), and so we might expect the same from binary stochastic neurons as compared to the logistic neurons. Indeed, this is the claimed “unpublished result” from the end of <a href="https://www.youtube.com/watch?v=LN0xtUuJsEI&amp;list=PLoRl3Ht4JOcdU872GhiYWf6jwrk_SNhz9&amp;index=41">Hinton et al.’s Coursera Lecture 9c</a>, which I demonstrate empirically in this post.</p></li>
<li><p>Further, by enabling networks to make binary decisions, the binary stochastic neuron allows for conditional computation. This opens the door to some interesting new architectures. For example, instead of a mixture of experts architecture, which weights the outputs of several “expert” sub-networks and requires that all subnetworks be computed, we could use a <em>choice</em> of experts architecture, which conditionally uses expert sub-networks as needed. This architecture is implicitly proposed in <a href="https://arxiv.org/abs/1308.3432">Bengio et al. (2013)</a>, wherein the experiments use a choice of expert units architecture (i.e., a gated architecture where gates must be 1 or 0). Another example, proposed in <a href="https://arxiv.org/abs/1308.3432">Bengio et al. (2013)</a> and implemented by <a href="https://arxiv.org/abs/1609.01704">Chung et al. (2016)</a>, is the Heirarchical Multiscale Recurrent Neural Network (HM-RNN) architecture, which achieves great results on language modelling tasks.</p></li>
</ol>
<h3 id="training-the-binary-stochastic-neuron">Training the binary stochastic neuron</h3>
<p>For any single trial, the binary stochastic neuron generally has a derivative of 0 and cannot be trained by simple backpropagation. To see this, consider that if <span><span><span id="MJXp-Span-73"><span id="MJXp-Span-74">z</span><span id="MJXp-Span-75">≠</span><span id="MJXp-Span-76">sigm</span><span id="MJXp-Span-77">(</span><span id="MJXp-Span-78">a</span><span id="MJXp-Span-79">)</span></span></span><span id="MathJax-Element-14-Frame" tabindex="0"></span></span> in the <span><span><span id="MJXp-Span-80"><span id="MJXp-Span-81">BSN</span></span></span><span id="MathJax-Element-15-Frame" tabindex="0"></span></span> function above, there exists a <a href="https://en.wikipedia.org/wiki/Neighbourhood_(mathematics)">neighborhood</a> around <span><span><span id="MJXp-Span-82"><span id="MJXp-Span-83">a</span></span></span><span id="MathJax-Element-16-Frame" tabindex="0"></span></span> such that the output of <span><span><span id="MJXp-Span-84"><span id="MJXp-Span-85">BSN</span><span id="MJXp-Span-86">(</span><span id="MJXp-Span-87">a</span><span id="MJXp-Span-88">)</span></span></span><span id="MathJax-Element-17-Frame" tabindex="0"></span></span> is unchanged (i.e., the derivative is 0). We get around this by <em>estimating</em> the derivative with respect to the <em>expected</em> loss, rather than calculating the derivative with respect to the outcome of a single trial. We can only estimate this derivative, because in any given trial, we only see the loss value with respect to the given noise – we don’t know what the loss would have been given another level of noise. We call a method that provides such an estimate an “estimator”. An estimator is <em>unbiased</em> if the expectation of its estimate equals the expectation of the derivative it is estimating; otherwise, it is <em>biased</em>.</p>
<p>In this post we implement the two estimators discussed in <a href="https://arxiv.org/abs/1308.3432">Bengio et al. (2013)</a>:</p>
<ol type="1">
<li><p>The REINFORCE estimator, which is an unbiased estimator and a special case of the REINFORCE algorithm discussed in <a href="http://link.springer.com/article/10.1007/BF00992696">Williams (1992)</a>.</p>
<p>The REINFORCE estimator estimates the expectation of <span><span><span id="MJXp-Span-89"><span id="MJXp-Span-90"><span><span id="MJXp-Span-91">∂</span><span id="MJXp-Span-92">L</span></span><span><span><span><span></span></span><span><span><span id="MJXp-Span-93">∂</span><span id="MJXp-Span-94">a</span></span></span></span></span></span></span></span><span id="MathJax-Element-18-Frame" tabindex="0"></span></span> as <span><span><span id="MJXp-Span-95"><span id="MJXp-Span-96">(</span><span id="MJXp-Span-97">BSN</span><span id="MJXp-Span-98">(</span><span id="MJXp-Span-99">a</span><span id="MJXp-Span-100">)</span><span id="MJXp-Span-101">−</span><span id="MJXp-Span-102">sigm</span><span id="MJXp-Span-103">(</span><span id="MJXp-Span-104">a</span><span id="MJXp-Span-105">)</span><span id="MJXp-Span-106">)</span><span id="MJXp-Span-107">(</span><span id="MJXp-Span-108">L</span><span id="MJXp-Span-109">−</span><span id="MJXp-Span-110">c</span><span id="MJXp-Span-111">)</span></span></span><span id="MathJax-Element-19-Frame" tabindex="0"></span></span>, where <span><span><span id="MJXp-Span-112"><span id="MJXp-Span-113">c</span></span></span><span id="MathJax-Element-20-Frame" tabindex="0"></span></span> is a constant. <a href="https://arxiv.org/abs/1308.3432">Bengio et al. (2013)</a> proves that:</p>
<p><span><span><span id="MJXp-Span-114"><span id="MJXp-Span-115"><span id="MJXp-Span-116">E</span></span><span id="MJXp-Span-117">[</span><span id="MJXp-Span-118">(</span><span id="MJXp-Span-119">BSN</span><span id="MJXp-Span-120">(</span><span id="MJXp-Span-121">a</span><span id="MJXp-Span-122">)</span><span id="MJXp-Span-123">−</span><span id="MJXp-Span-124">sigm</span><span id="MJXp-Span-125">(</span><span id="MJXp-Span-126">a</span><span id="MJXp-Span-127">)</span><span id="MJXp-Span-128">)</span><span id="MJXp-Span-129">(</span><span id="MJXp-Span-130">L</span><span id="MJXp-Span-131">−</span><span id="MJXp-Span-132">c</span><span id="MJXp-Span-133">)</span><span id="MJXp-Span-134">]</span><span id="MJXp-Span-135">=</span><span id="MJXp-Span-136"><span id="MJXp-Span-137">E</span></span><span id="MJXp-Span-138"><span id="MJXp-Span-139"><span>[</span></span></span><span id="MJXp-Span-140"><span><span id="MJXp-Span-141">∂</span><span id="MJXp-Span-142">L</span></span><span><span><span><span></span></span><span><span><span id="MJXp-Span-143">∂</span><span id="MJXp-Span-144">a</span></span></span></span></span></span><span id="MJXp-Span-145"><span id="MJXp-Span-146"><span>]</span></span></span><span id="MJXp-Span-147">.</span></span></span><span><span id="MathJax-Element-21-Frame" tabindex="0"></span></span></span></p>
<p><a href="https://arxiv.org/abs/1308.3432">Bengio et al. (2013)</a> further shows that to minimize the variance of the estimation, we choose:</p>
<p><span><span><span id="MJXp-Span-148"><span id="MJXp-Span-149">c</span><span id="MJXp-Span-150">=</span><span id="MJXp-Span-151"><span id="MJXp-Span-152"><span><span><span><span id="MJXp-Span-154">ˉ</span></span><span><span id="MJXp-Span-153">L</span></span></span></span></span></span><span id="MJXp-Span-155">=</span><span id="MJXp-Span-156"><span><span id="MJXp-Span-157"><span id="MJXp-Span-158">E</span></span><span id="MJXp-Span-159">[</span><span id="MJXp-Span-160">BSN</span><span id="MJXp-Span-161">(</span><span id="MJXp-Span-162">a</span><span id="MJXp-Span-163">)</span><span id="MJXp-Span-164">−</span><span id="MJXp-Span-165">sigm</span><span id="MJXp-Span-166">(</span><span id="MJXp-Span-167">a</span><span id="MJXp-Span-168">)</span><span id="MJXp-Span-169"><span id="MJXp-Span-170">)</span><span id="MJXp-Span-171">2</span></span><span id="MJXp-Span-172">L</span><span id="MJXp-Span-173">]</span></span><span><span><span><span></span></span><span><span><span id="MJXp-Span-174"><span id="MJXp-Span-175">E</span></span><span id="MJXp-Span-176">[</span><span id="MJXp-Span-177">BSN</span><span id="MJXp-Span-178">(</span><span id="MJXp-Span-179">a</span><span id="MJXp-Span-180">)</span><span id="MJXp-Span-181">−</span><span id="MJXp-Span-182">sigm</span><span id="MJXp-Span-183">(</span><span id="MJXp-Span-184">a</span><span id="MJXp-Span-185">)</span><span id="MJXp-Span-186"><span id="MJXp-Span-187">)</span><span id="MJXp-Span-188">2</span></span><span id="MJXp-Span-189">]</span></span></span></span></span></span></span></span><span><span id="MathJax-Element-22-Frame" tabindex="0"></span></span></span></p>
<p>which we can practically implement by keeping track of the numerator and denominator as a moving average. Interestingly, the REINFORCE estimator does not require any backpropagated loss gradient–it operates directly on the loss of the network.</p></li>
<li><p>The straight through (ST) estimator, which is a biased estimator that was first proposed by <a href="https://www.youtube.com/watch?v=LN0xtUuJsEI&amp;list=PLoRl3Ht4JOcdU872GhiYWf6jwrk_SNhz9&amp;index=41">Hinton et al.’s Coursera Lecture 9c</a>.</p>
<p>The ST estimator simply replaces the derivative factor used during backpropagation, <span><span><span id="MJXp-Span-190"><span id="MJXp-Span-191"><span><span id="MJXp-Span-192">d</span><span id="MJXp-Span-193">BSN</span><span id="MJXp-Span-194">(</span><span id="MJXp-Span-195">a</span><span id="MJXp-Span-196">)</span></span><span><span><span><span></span></span><span><span><span id="MJXp-Span-197">d</span><span id="MJXp-Span-198">a</span></span></span></span></span></span><span id="MJXp-Span-199">=</span><span id="MJXp-Span-200">0</span></span></span><span id="MathJax-Element-23-Frame" tabindex="0"></span></span>, with the identity function <span><span><span id="MJXp-Span-201"><span id="MJXp-Span-202"><span><span id="MJXp-Span-203">d</span><span id="MJXp-Span-204">BSN</span><span id="MJXp-Span-205">(</span><span id="MJXp-Span-206">a</span><span id="MJXp-Span-207">)</span></span><span><span><span><span></span></span><span><span><span id="MJXp-Span-208">d</span><span id="MJXp-Span-209">a</span></span></span></span></span></span><span id="MJXp-Span-210">=</span><span id="MJXp-Span-211">1</span></span></span><span id="MathJax-Element-24-Frame" tabindex="0"></span></span>.<a href="#fn1" id="fnref1"><sup>1</sup></a> A variant of the ST estimator replaces the derivative factor with <span><span><span id="MJXp-Span-212"><span id="MJXp-Span-213"><span><span id="MJXp-Span-214">d</span><span id="MJXp-Span-215">BSN</span><span id="MJXp-Span-216">(</span><span id="MJXp-Span-217">a</span><span id="MJXp-Span-218">)</span></span><span><span><span><span></span></span><span><span><span id="MJXp-Span-219">d</span><span id="MJXp-Span-220">a</span></span></span></span></span></span><span id="MJXp-Span-221">=</span><span id="MJXp-Span-222"><span><span id="MJXp-Span-223">d</span><span id="MJXp-Span-224">sigm</span><span id="MJXp-Span-225">(</span><span id="MJXp-Span-226">a</span><span id="MJXp-Span-227">)</span></span><span><span><span><span></span></span><span><span><span id="MJXp-Span-228">d</span><span id="MJXp-Span-229">a</span></span></span></span></span></span></span></span><span id="MathJax-Element-25-Frame" tabindex="0"></span></span>. Whereas <a href="https://arxiv.org/abs/1308.3432">Bengio et al. (2013)</a> found that the former is more effective, the latter variant was successfully used in <a href="https://arxiv.org/abs/1609.01704">Chung et al. (2016)</a> in combination with the <em>slope-annealing trick</em> and deterministic binary neurons (which we will see perform very similarly to, if not better than, stochastic binary neurons when used with slope-annealing). The slope-anealing trick modifies <span><span><span id="MJXp-Span-230"><span id="MJXp-Span-231">BSN</span><span id="MJXp-Span-232">(</span><span id="MJXp-Span-233">a</span><span id="MJXp-Span-234">)</span></span></span><span id="MathJax-Element-26-Frame" tabindex="0"></span></span> by first multiplying the input <span><span><span id="MJXp-Span-235"><span id="MJXp-Span-236">a</span></span></span><span id="MathJax-Element-27-Frame" tabindex="0"></span></span> by a slope <span><span><span id="MJXp-Span-237"><span id="MJXp-Span-238">m</span></span></span><span id="MathJax-Element-28-Frame" tabindex="0"></span></span> as follows:</p>
<p><span><span><span id="MJXp-Span-239"><span id="MJXp-Span-240"><span id="MJXp-Span-241">BSN</span><span id="MJXp-Span-242"><span id="MJXp-Span-243">SL</span><span id="MJXp-Span-244">(</span><span id="MJXp-Span-245">m</span><span id="MJXp-Span-246">)</span></span></span><span id="MJXp-Span-247">(</span><span id="MJXp-Span-248">a</span><span id="MJXp-Span-249">)</span><span id="MJXp-Span-250">=</span><span id="MJXp-Span-251"><span id="MJXp-Span-252"><span id="MJXp-Span-253">1</span></span><span id="MJXp-Span-254"><span id="MJXp-Span-255">z</span><span id="MJXp-Span-256">&lt;</span><span id="MJXp-Span-257">sigm</span><span id="MJXp-Span-258">(</span><span id="MJXp-Span-259">m</span><span id="MJXp-Span-260">a</span><span id="MJXp-Span-261">)</span></span></span><span id="MJXp-Span-262">.</span></span></span><span><span id="MathJax-Element-29-Frame" tabindex="0"></span></span></span></p>
<p>Then, we increase the slope as training progresses and use <span><span><span id="MJXp-Span-263"><span id="MJXp-Span-264"><span><span id="MJXp-Span-265">d</span><span id="MJXp-Span-266"><span id="MJXp-Span-267">BSN</span><span id="MJXp-Span-268"><span id="MJXp-Span-269">SL</span><span id="MJXp-Span-270">(</span><span id="MJXp-Span-271">m</span><span id="MJXp-Span-272">)</span></span></span><span id="MJXp-Span-273">(</span><span id="MJXp-Span-274">a</span><span id="MJXp-Span-275">)</span></span><span><span><span><span></span></span><span><span><span id="MJXp-Span-276">d</span><span id="MJXp-Span-277">a</span></span></span></span></span></span><span id="MJXp-Span-278">=</span><span id="MJXp-Span-279"><span><span id="MJXp-Span-280">d</span><span id="MJXp-Span-281">sigm</span><span id="MJXp-Span-282">(</span><span id="MJXp-Span-283">m</span><span id="MJXp-Span-284">a</span><span id="MJXp-Span-285">)</span></span><span><span><span><span></span></span><span><span><span id="MJXp-Span-286">d</span><span id="MJXp-Span-287">a</span></span></span></span></span></span></span></span><span id="MathJax-Element-30-Frame" tabindex="0"></span></span> when computing the gradient. The idea behind this is that as the slope increases, the logistic sigmoid approaches a step function, so that it’s derivative approaches the true derivative. All three variants are tested in this post.</p></li>
</ol>
<h3 id="implementing-the-binary-stochastic-neuron-in-tensorflow">Implementing the binary stochastic neuron in Tensorflow</h3>
<p>The tricky part of implementing a binary stochastic neuron in Tensorflow is not the forward computation, but the implementation of the REINFORCE and straight through estimators. Each requires replacing the gradient of one or more Tensorflow operations. The <a href="https://www.tensorflow.org/how_tos/adding_an_op/">official approach</a> to this is to write a new op in C++, which seems wholly unnecessary. There are, however, two workable unofficial approaches, one of which is <a href="http://stackoverflow.com/questions/36456436/how-can-i-define-only-the-gradient-for-a-tensorflow-subgraph/36480182">a trick credited to Sergey Ioffe</a>, and another that uses <code>gradient_override_map</code>, an experimental feature of Tensorflow that is documented <a href="https://www.tensorflow.org/api_docs/python/framework/core_graph_data_structures#Graph.gradient_override_map">here</a>. We will use <code>gradient_override_map</code>, which works well for our purposes.</p>
<h4 id="imports-and-utility-functions">Imports and Utility Functions</h4>
<div><pre><code><span>import</span> numpy <span>as</span> np
<span>import</span> tensorflow <span>as</span> tf
<span>from</span> tensorflow.examples.tutorials.mnist <span>import</span> input_data
<span>import</span> matplotlib.pyplot <span>as</span> plt
<span>%</span>matplotlib inline
mnist <span>=</span> input_data.read_data_sets(<span>'MNIST_data'</span>, one_hot<span>=</span><span>True</span>)
<span>from</span> tensorflow.python.framework <span>import</span> ops
<span>from</span> enum <span>import</span> Enum
<span>import</span> seaborn <span>as</span> sns
sns.<span>set</span>(color_codes<span>=</span><span>True</span>)

<span>def</span> reset_graph():
    <span>if</span> <span>'sess'</span> <span>in</span> <span>globals</span>() <span>and</span> sess:
        sess.close()
    tf.reset_default_graph()

<span>def</span> layer_linear(inputs, shape, scope<span>=</span><span>'linear_layer'</span>):
    <span>with</span> tf.variable_scope(scope):
        w <span>=</span> tf.get_variable(<span>'w'</span>,shape)
        b <span>=</span> tf.get_variable(<span>'b'</span>,shape[<span>-</span><span>1</span>:])
    <span>return</span> tf.matmul(inputs,w) <span>+</span> b

<span>def</span> layer_softmax(inputs, shape, scope<span>=</span><span>'softmax_layer'</span>):
    <span>with</span> tf.variable_scope(scope):
        w <span>=</span> tf.get_variable(<span>'w'</span>,shape)
        b <span>=</span> tf.get_variable(<span>'b'</span>,shape[<span>-</span><span>1</span>:])
    <span>return</span> tf.nn.softmax(tf.matmul(inputs,w) <span>+</span> b)

<span>def</span> accuracy(y, pred):
    correct <span>=</span> tf.equal(tf.argmax(y,<span>1</span>), tf.argmax(pred,<span>1</span>))
    <span>return</span> tf.reduce_mean(tf.cast(correct, tf.float32))

<span>def</span> plot_n(data_and_labels, lower_y <span>=</span> <span>0.</span>, title<span>=</span><span>"Learning Curves"</span>):
    fig, ax <span>=</span> plt.subplots()
    <span>for</span> data, label <span>in</span> data_and_labels:
        ax.plot(<span>range</span>(<span>0</span>,<span>len</span>(data)<span>*</span><span>100</span>,<span>100</span>),data, label<span>=</span>label)
    ax.set_xlabel(<span>'Training steps'</span>)
    ax.set_ylabel(<span>'Accuracy'</span>)
    ax.set_ylim([lower_y,<span>1</span>])
    ax.set_title(title)
    ax.legend(loc<span>=</span><span>4</span>)
    plt.show()

<span>class</span> StochasticGradientEstimator(Enum):
    ST <span>=</span> <span>0</span>
    REINFORCE <span>=</span> <span>1</span></code></pre></div>
<pre><code>Extracting MNIST_data/train-images-idx3-ubyte.gz
Extracting MNIST_data/train-labels-idx1-ubyte.gz
Extracting MNIST_data/t10k-images-idx3-ubyte.gz
Extracting MNIST_data/t10k-labels-idx1-ubyte.gz</code></pre>
<h4 id="binary-stochastic-neuron-with-straight-through-estimator">Binary stochastic neuron with straight through estimator</h4>
<div><pre><code><span>def</span> binaryRound(x):
    <span>"""</span>
<span>    Rounds a tensor whose values are in [0,1] to a tensor with values in {0, 1},</span>
<span>    using the straight through estimator for the gradient.</span>
<span>    """</span>
    g <span>=</span> tf.get_default_graph()

    <span>with</span> ops.name_scope(<span>"BinaryRound"</span>) <span>as</span> name:
        <span>with</span> g.gradient_override_map({<span>"Round"</span>: <span>"Identity"</span>}):
            <span>return</span> tf.<span>round</span>(x, name<span>=</span>name)

        <span># For Tensorflow v0.11 and below use:</span>
        <span>#with g.gradient_override_map({"Floor": "Identity"}):</span>
        <span>#    return tf.round(x, name=name)</span></code></pre></div>
<div><pre><code><span>def</span> bernoulliSample(x):
    <span>"""</span>
<span>    Uses a tensor whose values are in [0,1] to sample a tensor with values in {0, 1},</span>
<span>    using the straight through estimator for the gradient.</span>

<span>    E.g.,:</span>
<span>    if x is 0.6, bernoulliSample(x) will be 1 with probability 0.6, and 0 otherwise,</span>
<span>    and the gradient will be pass-through (identity).</span>
<span>    """</span>
    g <span>=</span> tf.get_default_graph()

    <span>with</span> ops.name_scope(<span>"BernoulliSample"</span>) <span>as</span> name:
        <span>with</span> g.gradient_override_map({<span>"Ceil"</span>: <span>"Identity"</span>,<span>"Sub"</span>: <span>"BernoulliSample_ST"</span>}):
            <span>return</span> tf.ceil(x <span>-</span> tf.random_uniform(tf.shape(x)), name<span>=</span>name)

<span>@ops.RegisterGradient</span>(<span>"BernoulliSample_ST"</span>)
<span>def</span> bernoulliSample_ST(op, grad):
    <span>return</span> [grad, tf.zeros(tf.shape(op.inputs[<span>1</span>]))]</code></pre></div>
<div><pre><code><span>def</span> passThroughSigmoid(x, slope<span>=</span><span>1</span>):
    <span>"""Sigmoid that uses identity function as its gradient"""</span>
    g <span>=</span> tf.get_default_graph()
    <span>with</span> ops.name_scope(<span>"PassThroughSigmoid"</span>) <span>as</span> name:
        <span>with</span> g.gradient_override_map({<span>"Sigmoid"</span>: <span>"Identity"</span>}):
            <span>return</span> tf.sigmoid(x, name<span>=</span>name)

<span>def</span> binaryStochastic_ST(x, slope_tensor<span>=</span><span>None</span>, pass_through<span>=</span><span>True</span>, stochastic<span>=</span><span>True</span>):
    <span>"""</span>
<span>    Sigmoid followed by either a random sample from a bernoulli distribution according</span>
<span>    to the result (binary stochastic neuron) (default), or a sigmoid followed by a binary</span>
<span>    step function (if stochastic == False). Uses the straight through estimator.</span>
<span>    See https://arxiv.org/abs/1308.3432.</span>

<span>    Arguments:</span>
<span>    * x: the pre-activation / logit tensor</span>
<span>    * slope_tensor: if passThrough==False, slope adjusts the slope of the sigmoid function</span>
<span>        for purposes of the Slope Annealing Trick (see http://arxiv.org/abs/1609.01704)</span>
<span>    * pass_through: if True (default), gradient of the entire function is 1 or 0;</span>
<span>        if False, gradient of 1 is scaled by the gradient of the sigmoid (required if</span>
<span>        Slope Annealing Trick is used)</span>
<span>    * stochastic: binary stochastic neuron if True (default), or step function if False</span>
<span>    """</span>
    <span>if</span> slope_tensor <span>is</span> <span>None</span>:
        slope_tensor <span>=</span> tf.constant(<span>1.0</span>)

    <span>if</span> pass_through:
        p <span>=</span> passThroughSigmoid(x)
    <span>else</span>:
        p <span>=</span> tf.sigmoid(slope_tensor<span>*</span>x)

    <span>if</span> stochastic:
        <span>return</span> bernoulliSample(p)
    <span>else</span>:
        <span>return</span> binaryRound(p)</code></pre></div>
<h4 id="binary-stochastic-neuron-with-reinforce-estimator">Binary stochastic neuron with REINFORCE estimator</h4>
<div><pre><code><span>def</span> binaryStochastic_REINFORCE(x, stochastic <span>=</span> <span>True</span>, loss_op_name<span>=</span><span>"loss_by_example"</span>):
    <span>"""</span>
<span>    Sigmoid followed by a random sample from a bernoulli distribution according</span>
<span>    to the result (binary stochastic neuron). Uses the REINFORCE estimator.</span>
<span>    See https://arxiv.org/abs/1308.3432.</span>

<span>    </span><span>NOTE</span><span>: Requires a loss operation with name matching the argument for loss_op_name</span>
<span>    in the graph. This loss operation should be broken out by example (i.e., not a</span>
<span>    single number for the entire batch).</span>
<span>    """</span>
    g <span>=</span> tf.get_default_graph()

    <span>with</span> ops.name_scope(<span>"BinaryStochasticREINFORCE"</span>):
        <span>with</span> g.gradient_override_map({<span>"Sigmoid"</span>: <span>"BinaryStochastic_REINFORCE"</span>,
                                      <span>"Ceil"</span>: <span>"Identity"</span>}):
            p <span>=</span> tf.sigmoid(x)

            reinforce_collection <span>=</span> g.get_collection(<span>"REINFORCE"</span>)
            <span>if</span> <span>not</span> reinforce_collection:
                g.add_to_collection(<span>"REINFORCE"</span>, {})
                reinforce_collection <span>=</span> g.get_collection(<span>"REINFORCE"</span>)
            reinforce_collection[<span>0</span>][p.op.name] <span>=</span> loss_op_name

            <span>return</span> tf.ceil(p <span>-</span> tf.random_uniform(tf.shape(x)))


<span>@ops.RegisterGradient</span>(<span>"BinaryStochastic_REINFORCE"</span>)
<span>def</span> _binaryStochastic_REINFORCE(op, _):
    <span>"""Unbiased estimator for binary stochastic function based on REINFORCE."""</span>
    loss_op_name <span>=</span> op.graph.get_collection(<span>"REINFORCE"</span>)[<span>0</span>][op.name]
    loss_tensor <span>=</span> op.graph.get_operation_by_name(loss_op_name).outputs[<span>0</span>]

    sub_tensor <span>=</span> op.outputs[<span>0</span>].consumers()[<span>0</span>].outputs[<span>0</span>] <span>#subtraction tensor</span>
    ceil_tensor <span>=</span> sub_tensor.consumers()[<span>0</span>].outputs[<span>0</span>] <span>#ceiling tensor</span>

    outcome_diff <span>=</span> (ceil_tensor <span>-</span> op.outputs[<span>0</span>])

    <span># Provides an early out if we want to avoid variance adjustment for</span>
    <span># whatever reason (e.g., to show that variance adjustment helps)</span>
    <span>if</span> op.graph.get_collection(<span>"REINFORCE"</span>)[<span>0</span>].get(<span>"no_variance_adj"</span>):
        <span>return</span> outcome_diff <span>*</span> tf.expand_dims(loss_tensor, <span>1</span>)

    outcome_diff_sq <span>=</span> tf.square(outcome_diff)
    outcome_diff_sq_r <span>=</span> tf.reduce_mean(outcome_diff_sq, reduction_indices<span>=</span><span>0</span>)
    outcome_diff_sq_loss_r <span>=</span> tf.reduce_mean(outcome_diff_sq <span>*</span> tf.expand_dims(loss_tensor, <span>1</span>),
                                            reduction_indices<span>=</span><span>0</span>)

    L_bar_num <span>=</span> tf.Variable(tf.zeros(outcome_diff_sq_r.get_shape()), trainable<span>=</span><span>False</span>)
    L_bar_den <span>=</span> tf.Variable(tf.ones(outcome_diff_sq_r.get_shape()), trainable<span>=</span><span>False</span>)

    <span>#Note: we already get a decent estimate of the average from the minibatch</span>
    decay <span>=</span> <span>0.95</span>
    train_L_bar_num <span>=</span> tf.assign(L_bar_num, L_bar_num<span>*</span>decay <span>+\</span>
                                            outcome_diff_sq_loss_r<span>*</span>(<span>1</span><span>-</span>decay))
    train_L_bar_den <span>=</span> tf.assign(L_bar_den, L_bar_den<span>*</span>decay <span>+\</span>
                                            outcome_diff_sq_r<span>*</span>(<span>1</span><span>-</span>decay))


    <span>with</span> tf.control_dependencies([train_L_bar_num, train_L_bar_den]):
        L_bar <span>=</span> train_L_bar_num<span>/</span>(train_L_bar_den<span>+1e-4</span>)
        L <span>=</span> tf.tile(tf.expand_dims(loss_tensor,<span>1</span>),
                    tf.constant([<span>1</span>,L_bar.get_shape().as_list()[<span>0</span>]]))
        <span>return</span> outcome_diff <span>*</span> (L <span>-</span> L_bar)</code></pre></div>
<h4 id="wrapper-to-create-layer-of-binary-stochastic-neurons">Wrapper to create layer of binary stochastic neurons</h4>
<div><pre><code><span>def</span> binary_wrapper(<span>\</span>
                pre_activations_tensor,
                estimator<span>=</span>StochasticGradientEstimator.ST,
                stochastic_tensor<span>=</span>tf.constant(<span>True</span>),
                pass_through<span>=</span><span>True</span>,
                slope_tensor<span>=</span>tf.constant(<span>1.0</span>)):
    <span>"""</span>
<span>    Turns a layer of pre-activations (logits) into a layer of binary stochastic neurons</span>

<span>    Keyword arguments:</span>
<span>    *estimator: either ST or REINFORCE</span>
<span>    *stochastic_tensor: a boolean tensor indicating whether to sample from a bernoulli</span>
<span>        distribution (True, default) or use a step_function (e.g., for inference)</span>
<span>    *pass_through: for ST only - boolean as to whether to substitute identity derivative on the</span>
<span>        backprop (True, default), or whether to use the derivative of the sigmoid</span>
<span>    *slope_tensor: for ST only - tensor specifying the slope for purposes of slope annealing</span>
<span>        trick</span>
<span>    """</span>

    <span>if</span> estimator <span>==</span> StochasticGradientEstimator.ST:
        <span>if</span> pass_through:
            <span>return</span> tf.cond(stochastic_tensor,
                    <span>lambda</span>: binaryStochastic_ST(pre_activations_tensor),
                    <span>lambda</span>: binaryStochastic_ST(pre_activations_tensor, stochastic<span>=</span><span>False</span>))
        <span>else</span>:
            <span>return</span> tf.cond(stochastic_tensor,
                    <span>lambda</span>: binaryStochastic_ST(pre_activations_tensor, slope_tensor <span>=</span> slope_tensor,
                                             pass_through<span>=</span><span>False</span>),
                    <span>lambda</span>: binaryStochastic_ST(pre_activations_tensor, slope_tensor <span>=</span> slope_tensor,
                                             pass_through<span>=</span><span>False</span>, stochastic<span>=</span><span>False</span>))
    <span>elif</span> estimator <span>==</span> StochasticGradientEstimator.REINFORCE:
        <span># binaryStochastic_REINFORCE was designed to only be stochastic, so using the ST version</span>
        <span># for the step fn for purposes of using step fn at evaluation / not for training</span>
        <span>return</span> tf.cond(stochastic_tensor,
                <span>lambda</span>: binaryStochastic_REINFORCE(pre_activations_tensor),
                <span>lambda</span>: binaryStochastic_ST(pre_activations_tensor, stochastic<span>=</span><span>False</span>))

    <span>else</span>:
        <span>raise</span> <span>ValueError</span>(<span>"Unrecognized estimator."</span>)</code></pre></div>
<h4 id="function-to-build-graph-for-mnist-classifier">Function to build graph for MNIST classifier</h4>
<div><pre><code><span>def</span> build_classifier(hidden_dims<span>=</span>[<span>100</span>],
                        lr <span>=</span> <span>0.5</span>,
                        pass_through <span>=</span> <span>True</span>,
                        non_binary <span>=</span> <span>False</span>,
                        estimator <span>=</span> StochasticGradientEstimator.ST,
                        no_var_adj<span>=</span><span>False</span>):
    reset_graph()
    g <span>=</span> {}

    <span>if</span> no_var_adj:
        tf.get_default_graph().add_to_collection(<span>"REINFORCE"</span>, {<span>"no_variance_adj"</span>: no_var_adj})

    g[<span>'x'</span>] <span>=</span> tf.placeholder(tf.float32, [<span>None</span>, <span>784</span>], name<span>=</span><span>'x_placeholder'</span>)
    g[<span>'y'</span>] <span>=</span> tf.placeholder(tf.float32, [<span>None</span>, <span>10</span>], name<span>=</span><span>'y_placeholder'</span>)
    g[<span>'stochastic'</span>] <span>=</span> tf.constant(<span>True</span>)
    g[<span>'slope'</span>] <span>=</span> tf.constant(<span>1.0</span>)

    g[<span>'layers'</span>] <span>=</span> {<span>0</span>: g[<span>'x'</span>]}
    hidden_layers <span>=</span> <span>len</span>(hidden_dims)
    dims <span>=</span> [<span>784</span>] <span>+</span> hidden_dims

    <span>for</span> i <span>in</span> <span>range</span>(<span>1</span>, hidden_layers<span>+</span><span>1</span>):
        <span>with</span> tf.variable_scope(<span>"layer_"</span> <span>+</span> <span>str</span>(i)):
            pre_activations <span>=</span> layer_linear(g[<span>'layers'</span>][i<span>-1</span>], dims[i<span>-1</span>:i<span>+</span><span>1</span>], scope<span>=</span><span>'layer_'</span> <span>+</span> <span>str</span>(i))
            <span>if</span> non_binary:
                g[<span>'layers'</span>][i] <span>=</span> tf.sigmoid(pre_activations)
            <span>else</span>:
                g[<span>'layers'</span>][i] <span>=</span> binary_wrapper(pre_activations,
                                              estimator <span>=</span> estimator,
                                              pass_through <span>=</span> pass_through,
                                              stochastic_tensor <span>=</span> g[<span>'stochastic'</span>],
                                              slope_tensor <span>=</span> g[<span>'slope'</span>])

    g[<span>'pred'</span>] <span>=</span> layer_softmax(g[<span>'layers'</span>][hidden_layers], [dims[<span>-</span><span>1</span>], <span>10</span>])

    g[<span>'loss'</span>] <span>=</span> <span>-</span>tf.reduce_mean(g[<span>'y'</span>] <span>*</span> tf.log(g[<span>'pred'</span>]),reduction_indices<span>=</span><span>1</span>)

    <span># named loss_by_example necessary for REINFORCE estimator</span>
    tf.identity(g[<span>'loss'</span>], name<span>=</span><span>"loss_by_example"</span>)

    g[<span>'ts'</span>] <span>=</span> tf.train.GradientDescentOptimizer(lr).minimize(g[<span>'loss'</span>])

    g[<span>'accuracy'</span>] <span>=</span> accuracy(g[<span>'y'</span>], g[<span>'pred'</span>])

    g[<span>'init_op'</span>] <span>=</span> tf.global_variables_initializer()
    <span>return</span> g</code></pre></div>
<h4 id="function-to-train-the-classifier">Function to train the classifier</h4>
<div><pre><code><span>def</span> train_classifier(<span>\</span>
        hidden_dims<span>=</span>[<span>100</span>,<span>100</span>],
        estimator<span>=</span>StochasticGradientEstimator.ST,
        stochastic_train<span>=</span><span>True</span>,
        stochastic_eval<span>=</span><span>True</span>,
        slope_annealing_rate<span>=</span><span>None</span>,
        epochs<span>=</span><span>10</span>,
        lr<span>=</span><span>0.5</span>,
        non_binary<span>=</span><span>False</span>,
        no_var_adj<span>=</span><span>False</span>,
        train_set <span>=</span> mnist.train,
        val_set <span>=</span> mnist.validation,
        verbose<span>=</span><span>False</span>,
        label<span>=</span><span>None</span>):
    <span>if</span> slope_annealing_rate <span>is</span> <span>None</span>:
        g <span>=</span> build_classifier(hidden_dims<span>=</span>hidden_dims, lr<span>=</span>lr, pass_through<span>=</span><span>True</span>,
                                non_binary<span>=</span>non_binary, estimator<span>=</span>estimator, no_var_adj<span>=</span>no_var_adj)
    <span>else</span>:
        g <span>=</span> build_classifier(hidden_dims<span>=</span>hidden_dims, lr<span>=</span>lr, pass_through<span>=</span><span>False</span>,
                                non_binary<span>=</span>non_binary, estimator<span>=</span>estimator, no_var_adj<span>=</span>no_var_adj)

    <span>with</span> tf.Session() <span>as</span> sess:
        sess.run(g[<span>'init_op'</span>])
        slope <span>=</span> <span>1</span>
        res_tr, res_val <span>=</span> [], []
        <span>for</span> epoch <span>in</span> <span>range</span>(epochs):
            feed_dict<span>=</span>{g[<span>'x'</span>]: val_set.images,
                       g[<span>'y'</span>]: val_set.labels,
                       g[<span>'stochastic'</span>]: stochastic_eval,
                       g[<span>'slope'</span>]: slope}
            <span>if</span> verbose:
                <span>print</span>(<span>"Epoch"</span>, epoch, sess.run(g[<span>'accuracy'</span>], feed_dict<span>=</span>feed_dict))

            accuracy <span>=</span> <span>0</span>
            <span>for</span> i <span>in</span> <span>range</span>(<span>1001</span>):
                x, y <span>=</span> train_set.next_batch(<span>50</span>)
                feed_dict<span>=</span>{g[<span>'x'</span>]: x, g[<span>'y'</span>]: y, g[<span>'stochastic'</span>]: stochastic_train}
                acc, _ <span>=</span> sess.run([g[<span>'accuracy'</span>],g[<span>'ts'</span>]], feed_dict<span>=</span>feed_dict)
                accuracy <span>+=</span> acc
                <span>if</span> i <span>%</span> <span>100</span> <span>==</span> <span>0</span> <span>and</span> i <span>&gt;</span> <span>0</span>:
                    res_tr.append(accuracy<span>/</span><span>100</span>)
                    accuracy <span>=</span> <span>0</span>
                    feed_dict<span>=</span>{g[<span>'x'</span>]: val_set.images,
                               g[<span>'y'</span>]: val_set.labels,
                               g[<span>'stochastic'</span>]: stochastic_eval,
                               g[<span>'slope'</span>]: slope}
                    res_val.append(sess.run(g[<span>'accuracy'</span>], feed_dict<span>=</span>feed_dict))

            <span>if</span> slope_annealing_rate <span>is</span> <span>not</span> <span>None</span>:
                slope <span>=</span> slope<span>*</span>slope_annealing_rate
                <span>if</span> verbose:
                    <span>print</span>(<span>"Sigmoid slope:"</span>, slope)

        feed_dict<span>=</span>{g[<span>'x'</span>]: val_set.images, g[<span>'y'</span>]: val_set.labels,
                   g[<span>'stochastic'</span>]: stochastic_eval, g[<span>'slope'</span>]: slope}
        <span>print</span>(<span>"Epoch"</span>, epoch<span>+</span><span>1</span>, sess.run(g[<span>'accuracy'</span>], feed_dict<span>=</span>feed_dict))
        <span>if</span> label <span>is</span> <span>not</span> <span>None</span>:
            <span>return</span> (res_tr, label <span>+</span> <span>" - Training"</span>), (res_val, label <span>+</span> <span>" - Validation"</span>)
        <span>else</span>:
            <span>return</span> [(res_tr, <span>"Training"</span>), (res_val, <span>"Validation"</span>)]</code></pre></div>
<h3 id="experiments">Experiments</h3>
<p>We’ve now set up a good foundation from which we can run a number of simple experiments. The experiments are as follows:</p>
<ul>
<li><strong>Experiment 0</strong>: A non-stochastic, non-binary baseline.</li>
<li><strong>Experiment 1</strong>: A comparison of variance-adjusted REINFORCE and non-variance adjusted REINFORCE, which shows that the variance adjustment allows for faster learning and higher learning rates.</li>
<li><strong>Experiment 2</strong>: A comparison of pass-through ST and sigmoid-adjusted ST, which shows that the sigmoid-adjusted ST estimator obtains better results, a result that does not agree with the findings of <a href="https://arxiv.org/abs/1308.3432">Bengio et al. (2013</a>.</li>
<li><strong>Experiment 3</strong>: A comparison of sigmoid-adjusted ST and slope-annealed sigmoid-adjusted ST, which shows that a well-tuned slope-annealed ST outperforms the base sigmoid-adjusted ST.</li>
<li><strong>Experiment 4</strong>: A direct comparison of variance-adjusted REINFORCE and slope-annealed ST, which shows that ST performs significantly better than REINFORCE.</li>
<li><strong>Experiment 5</strong>: A look at the deterministic step function, during training and evaluation, which shows that deterministic evaluation can provide a slight boost at inference, and that with slope annealing, deterministic training is just as effective, if not more effective than stochastic training.</li>
<li><strong>Experiment 6</strong>: A look at how network depth affects performance, which shows that deep stochastic networks can be difficult to train.</li>
<li><strong>Experiment 7</strong>: A look at using binary stochastic neurons as a regularizer, which validates Hinton’s claim that stochastic neurons can serve as effective regularizers.</li>
</ul>
<h4 id="experiment-0-a-non-stochastic-non-binary-baseline">Experiment 0: A non-stochastic, non-binary baseline</h4>
<div><pre><code>res <span>=</span> train_classifier(hidden_dims<span>=</span>[<span>100</span>], epochs<span>=</span><span>20</span>, lr<span>=</span><span>1.0</span>, non_binary<span>=</span><span>True</span>)
plot_n(res, lower_y<span>=</span><span>0.8</span>, title<span>=</span><span>"Logistic Sigmoid Baseline"</span>)</code></pre></div>
<pre><code>Epoch 20 0.9698</code></pre>
<figure>
<img src="https://r2rt.com/static/images/BSN_output_16_1.png" alt="png"><figcaption>png</figcaption>
</figure>
<h4 id="experiment-1-variance-adjusted-vs.not-variance-adjusted-reinforce">Experiment 1: Variance-adjusted vs.&nbsp;not variance-adjusted REINFORCE</h4>
<p>Recall that the REINFORCE estimator estimates the expectation of <span><span><span id="MJXp-Span-288"><span id="MJXp-Span-289"><span><span id="MJXp-Span-290">∂</span><span id="MJXp-Span-291">L</span></span><span><span><span><span></span></span><span><span><span id="MJXp-Span-292">∂</span><span id="MJXp-Span-293">a</span></span></span></span></span></span></span></span><span id="MathJax-Element-31-Frame" tabindex="0"></span></span> as <span><span><span id="MJXp-Span-294"><span id="MJXp-Span-295">(</span><span id="MJXp-Span-296">BSN</span><span id="MJXp-Span-297">(</span><span id="MJXp-Span-298">a</span><span id="MJXp-Span-299">)</span><span id="MJXp-Span-300">−</span><span id="MJXp-Span-301">sigm</span><span id="MJXp-Span-302">(</span><span id="MJXp-Span-303">a</span><span id="MJXp-Span-304">)</span><span id="MJXp-Span-305">)</span><span id="MJXp-Span-306">(</span><span id="MJXp-Span-307">L</span><span id="MJXp-Span-308">−</span><span id="MJXp-Span-309">c</span><span id="MJXp-Span-310">)</span></span></span><span id="MathJax-Element-32-Frame" tabindex="0"></span></span>, where <span><span><span id="MJXp-Span-311"><span id="MJXp-Span-312">c</span></span></span><span id="MathJax-Element-33-Frame" tabindex="0"></span></span> is a constant. The non-variance-adjusted form of REINFORCE uses <span><span><span id="MJXp-Span-313"><span id="MJXp-Span-314">c</span><span id="MJXp-Span-315">=</span><span id="MJXp-Span-316">0</span></span></span><span id="MathJax-Element-34-Frame" tabindex="0"></span></span>, whereas the variance-adjusted form uses the variance minimizing result stated above. Naturally we should prefer the least variance, and the experimental results below agree.</p>
<p>It seems that both forms of REINFORCE often break down for learning rates greater than or equal to 0.3 (compare to the learning rate of 1.0 that used in Experiment 0). After a few trials, variance-adjusted REINFORCE appears to be more resistant to such failures.</p>
<div><pre><code><span>print</span>(<span>"Variance-adjusted:"</span>)
res1 <span>=</span> train_classifier(hidden_dims<span>=</span>[<span>100</span>], estimator<span>=</span>StochasticGradientEstimator.REINFORCE, epochs<span>=</span><span>3</span>,
                       lr<span>=</span><span>0.3</span>, verbose<span>=</span><span>True</span>)
<span>print</span>(<span>"Not variance-adjusted:"</span>)<span>and</span>
res2<span>=</span> train_classifier(hidden_dims<span>=</span>[<span>100</span>], estimator<span>=</span>StochasticGradientEstimator.REINFORCE, epochs<span>=</span><span>3</span>,
                       lr<span>=</span><span>0.3</span>, no_var_adj<span>=</span><span>True</span>, verbose<span>=</span><span>True</span>)</code></pre></div>
<pre><code>Variance-adjusted:
Epoch 0 0.1026
Epoch 1 0.4466
Epoch 2 0.511
Epoch 3 0.575
Not variance-adjusted:
Epoch 0 0.0964
Epoch 1 0.0958
Epoch 2 0.0958
Epoch 3 0.0958</code></pre>
<p>In terms of performance at lower learning rates, a learning rate of about 0.05 provided the best results. The results show that the variance-adjusted REINFORCE learns faster, but that its non-variance adjusted eventually catches up. This result is consistent with the mathematical result that they are both unbiased estimators. Performance is predictably worse than it was for the plain logistic sigmoid in Experiment 0, although there is almost no generalization gap, consistent with the hypothesis that binary stochastic neurons can act as regularizers.</p>
<div><pre><code>res1 <span>=</span> train_classifier(hidden_dims<span>=</span>[<span>100</span>], estimator<span>=</span>StochasticGradientEstimator.REINFORCE, epochs<span>=</span><span>20</span>,
                       lr<span>=</span><span>0.05</span>, label <span>=</span> <span>"Variance-adjusted"</span>)
res2<span>=</span> train_classifier(hidden_dims<span>=</span>[<span>100</span>], estimator<span>=</span>StochasticGradientEstimator.REINFORCE, epochs<span>=</span><span>20</span>,
                       lr<span>=</span><span>0.05</span>, no_var_adj<span>=</span><span>True</span>, label <span>=</span> <span>"Not variance-adjusted"</span>)

plot_n(res1 <span>+</span> res2, lower_y<span>=</span><span>0.6</span>, title<span>=</span><span>"Experiment 1: REINFORCE variance adjustment"</span>)</code></pre></div>
<pre><code>Epoch 20 0.9274
Epoch 20 0.923</code></pre>
<figure>
<img src="https://r2rt.com/static/images/BSN_output_20_1.png" alt="png"><figcaption>png</figcaption>
</figure>
<h4 id="experiment-2-pass-through-vs.sigmoid-adjusted-st-estimation">Experiment 2: Pass-through vs.&nbsp;sigmoid-adjusted ST estimation</h4>
<p>Recall that one variant of the straight-through estimator uses the identity function as the backpropagated gradient (pass-through), and another variant multiplies that by the gradient of the logistic sigmoid that the neuron calculates (sigmoid-adjusted). In <a href="https://arxiv.org/abs/1308.3432">Bengio et al. (2013)</a>, it was remarked that, surprisingly, the former performs better. My results below disagree, and by a surprisingly wide margin.</p>
<div><pre><code>res1 <span>=</span> train_classifier(hidden_dims<span>=</span>[<span>100</span>], estimator<span>=</span>StochasticGradientEstimator.ST, epochs<span>=</span><span>20</span>,
                       lr<span>=</span><span>0.1</span>, label <span>=</span> <span>"Pass-through - 0.1"</span>)
res2 <span>=</span> train_classifier(hidden_dims<span>=</span>[<span>100</span>], estimator<span>=</span>StochasticGradientEstimator.ST, epochs<span>=</span><span>20</span>,
                       lr<span>=</span><span>0.1</span>, slope_annealing_rate <span>=</span> <span>1.0</span>, label <span>=</span> <span>"Sigmoid-adjusted - 0.1"</span>)

res3 <span>=</span> train_classifier(hidden_dims<span>=</span>[<span>100</span>], estimator<span>=</span>StochasticGradientEstimator.ST, epochs<span>=</span><span>20</span>,
                       lr<span>=</span><span>0.3</span>, label <span>=</span> <span>"Pass-through - 0.3"</span>)
res4 <span>=</span> train_classifier(hidden_dims<span>=</span>[<span>100</span>], estimator<span>=</span>StochasticGradientEstimator.ST, epochs<span>=</span><span>20</span>,
                       lr<span>=</span><span>0.3</span>, slope_annealing_rate <span>=</span> <span>1.0</span>, label <span>=</span> <span>"Sigmoid-adjusted - 0.3"</span>)

res5 <span>=</span> train_classifier(hidden_dims<span>=</span>[<span>100</span>], estimator<span>=</span>StochasticGradientEstimator.ST, epochs<span>=</span><span>20</span>,
                       lr<span>=</span><span>1.0</span>, label <span>=</span> <span>"Pass-through - 1.0"</span>)
res6 <span>=</span> train_classifier(hidden_dims<span>=</span>[<span>100</span>], estimator<span>=</span>StochasticGradientEstimator.ST, epochs<span>=</span><span>20</span>,
                       lr<span>=</span><span>1.0</span>, slope_annealing_rate <span>=</span> <span>1.0</span>, label <span>=</span> <span>"Sigmoid-adjusted - 1.0"</span>)

plot_n(res1[<span>1</span>:] <span>+</span> res2[<span>1</span>:] <span>+</span> res3[<span>1</span>:] <span>+</span> res4[<span>1</span>:] <span>+</span> res5[<span>1</span>:] <span>+</span> res6[<span>1</span>:],
       lower_y<span>=</span><span>0.4</span>, title<span>=</span><span>"Experiment 2: Pass-through vs sigmoid-adjusted ST"</span>)</code></pre></div>
<pre><code>Epoch 20 0.8334
Epoch 20 0.9566
Epoch 20 0.8828
Epoch 20 0.9668
Epoch 20 0.0958
Epoch 20 0.9572</code></pre>
<figure>
<img src="https://r2rt.com/static/images/BSN_output_22_1.png" alt="png"><figcaption>png</figcaption>
</figure>
<h4 id="experiment-3-pass-through-vs.slope-annealed-st-estimation">Experiment 3: Pass-through vs.&nbsp;slope-annealed ST estimation</h4>
<p>Recall that <a href="https://arxiv.org/abs/1609.01704">Chung et al. (2016)</a> improves upon the sigmoid-adjusted variant of the ST estimator by using the <em>slope-annealing trick</em>, which slowly increases the slope of the logistic sigmoid as training progresses. Using the slope-annealing trick with an annealing rate of 1.1 times per epoch (so the slope at epoch 20 is <span><span><span id="MJXp-Span-317"><span id="MJXp-Span-318"><span id="MJXp-Span-319">1.1</span><span id="MJXp-Span-320"><span id="MJXp-Span-321">19</span></span></span><span id="MJXp-Span-322">≈</span><span id="MJXp-Span-323">6.1</span></span></span><span id="MathJax-Element-35-Frame" tabindex="0"></span></span>), we’re able to improve upon the sigmoid-adjusted ST estimator, and even beat our non-stochastic, non-binary baseline! Note that the slope annealed neuron used here is not the same as the one used by <a href="https://arxiv.org/abs/1609.01704">Chung et al. (2016)</a>, who employ a deterministic step function and use a hard sigmoid in place of a sigmoid for the backpropagation.</p>
<div><pre><code>res1 <span>=</span> train_classifier(hidden_dims<span>=</span>[<span>100</span>], estimator<span>=</span>StochasticGradientEstimator.ST, epochs<span>=</span><span>20</span>,
                       lr<span>=</span><span>0.1</span>, slope_annealing_rate <span>=</span> <span>1.0</span>, label <span>=</span> <span>"Sigmoid-adjusted - 0.1"</span>)
res2 <span>=</span> train_classifier(hidden_dims<span>=</span>[<span>100</span>], estimator<span>=</span>StochasticGradientEstimator.ST, epochs<span>=</span><span>20</span>,
                       lr<span>=</span><span>0.1</span>, slope_annealing_rate <span>=</span> <span>1.1</span>, label <span>=</span> <span>"Slope-annealed - 0.1"</span>)

res3 <span>=</span> train_classifier(hidden_dims<span>=</span>[<span>100</span>], estimator<span>=</span>StochasticGradientEstimator.ST, epochs<span>=</span><span>20</span>,
                       lr<span>=</span><span>0.3</span>, slope_annealing_rate <span>=</span> <span>1.0</span>, label <span>=</span> <span>"Sigmoid-adjusted - 0.3"</span>)
res4 <span>=</span> train_classifier(hidden_dims<span>=</span>[<span>100</span>], estimator<span>=</span>StochasticGradientEstimator.ST, epochs<span>=</span><span>20</span>,
                       lr<span>=</span><span>0.3</span>, slope_annealing_rate <span>=</span> <span>1.1</span>, label <span>=</span> <span>"Slope-annealed - 0.3"</span>)

res5 <span>=</span> train_classifier(hidden_dims<span>=</span>[<span>100</span>], estimator<span>=</span>StochasticGradientEstimator.ST, epochs<span>=</span><span>20</span>,
                       lr<span>=</span><span>1.0</span>, slope_annealing_rate <span>=</span> <span>1.0</span>, label <span>=</span> <span>"Sigmoid-adjusted - 1.0"</span>)
res6 <span>=</span> train_classifier(hidden_dims<span>=</span>[<span>100</span>], estimator<span>=</span>StochasticGradientEstimator.ST, epochs<span>=</span><span>20</span>,
                       lr<span>=</span><span>1.0</span>, slope_annealing_rate <span>=</span> <span>1.1</span>, label <span>=</span> <span>"Slope-annealed - 1.0"</span>)

plot_n(res1[<span>1</span>:] <span>+</span> res2[<span>1</span>:] <span>+</span> res3[<span>1</span>:] <span>+</span> res4[<span>1</span>:] <span>+</span> res5[<span>1</span>:] <span>+</span> res6[<span>1</span>:],
       lower_y<span>=</span><span>0.6</span>, title<span>=</span><span>"Experiment 3: Sigmoid-adjusted vs slope-annealed ST"</span>)</code></pre></div>
<pre><code>Epoch 20 0.9548
Epoch 20 0.974
Epoch 20 0.9704
Epoch 20 0.9764
Epoch 20 0.9608
Epoch 20 0.9624</code></pre>
<figure>
<img src="https://r2rt.com/static/images/BSN_output_24_1.png" alt="png"><figcaption>png</figcaption>
</figure>
<h4 id="experiment-4-variance-adjusted-reinforce-vs-slope-annealed-st">Experiment 4: Variance-adjusted REINFORCE vs slope-annealed ST</h4>
<p>We now directly compare the variance-adjusted REINFORCE and slope-annealed ST, both at their best learning rates. In this setting, despite being a biased estimator, the straight-through estimator displays faster learning, less variance, and better overall results than the variance-adjusted REINFORCE estimator.</p>
<div><pre><code>res1 <span>=</span> train_classifier(hidden_dims<span>=</span>[<span>100</span>], estimator<span>=</span>StochasticGradientEstimator.REINFORCE, epochs<span>=</span><span>20</span>,
                       lr<span>=</span><span>0.05</span>, label <span>=</span> <span>"Variance-adjusted REINFORCE"</span>)

res2 <span>=</span> train_classifier(hidden_dims<span>=</span>[<span>100</span>], estimator<span>=</span>StochasticGradientEstimator.ST, epochs<span>=</span><span>20</span>,
                       lr<span>=</span><span>0.3</span>, slope_annealing_rate <span>=</span> <span>1.1</span>, label <span>=</span> <span>"Slope-annealed ST"</span>)

plot_n(res1[<span>1</span>:] <span>+</span> res2[<span>1</span>:],
       lower_y<span>=</span><span>0.6</span>, title<span>=</span><span>"Experiment 4: Variance-adjusted REINFORCE vs slope-annealed ST"</span>)</code></pre></div>
<pre><code>Epoch 20 0.926
Epoch 20 0.9782</code></pre>
<figure>
<img src="https://r2rt.com/static/images/BSN_output_26_1.png" alt="png"><figcaption>png</figcaption>
</figure>
<h4 id="experiment-5-a-look-at-the-deterministic-step-function-during-training-and-evaluation">Experiment 5: A look at the deterministic step function, during training and evaluation</h4>
<p>Similar to how dropout is not applied at inference when using dropout for training, it makes sense that we might replace the stochastic sigmoid with a deterministic step function at inference when using binary neurons. We might go even further than that, and use deterministic neurons during training, which is the approach taken by <a href="https://arxiv.org/abs/1609.01704">Chung et al. (2016)</a>. The following three combinations are compared below, using the slope-annealed straight through estimator, without slope annealing:</p>
<ul>
<li>stochastic during training, stochastic during test</li>
<li>stochastic during training, deterministic during test</li>
<li>deterministic during training, deterministic during test</li>
</ul>
<p>The results show that deterministic neurons train the fastest, but also display more overfitting and may not achieve the best final results. Stochastic inference and deterministic inference, when combined with stochastic training, are closely comparable. Similar results hold for the REINFORCE estimator.</p>
<div><pre><code>res1 <span>=</span> train_classifier(hidden_dims<span>=</span>[<span>100</span>], estimator<span>=</span>StochasticGradientEstimator.ST, epochs<span>=</span><span>20</span>,
                        lr<span>=</span><span>0.3</span>, slope_annealing_rate <span>=</span> <span>1.1</span>, label <span>=</span> <span>"Stochastic, Stochastic"</span>)
res2 <span>=</span> train_classifier(hidden_dims<span>=</span>[<span>100</span>], estimator<span>=</span>StochasticGradientEstimator.ST, epochs<span>=</span><span>20</span>,
                        lr<span>=</span><span>0.3</span>, slope_annealing_rate <span>=</span> <span>1.1</span>, stochastic_eval<span>=</span><span>False</span>, label <span>=</span> <span>"Stochastic, Deterministic"</span>)
res3 <span>=</span> train_classifier(hidden_dims<span>=</span>[<span>100</span>], estimator<span>=</span>StochasticGradientEstimator.ST, epochs<span>=</span><span>20</span>,
                        lr<span>=</span><span>0.3</span>, slope_annealing_rate <span>=</span> <span>1.1</span>, stochastic_train<span>=</span><span>False</span>, stochastic_eval<span>=</span><span>False</span>,
                        label <span>=</span> <span>"Deterministic, Deterministic"</span>)

plot_n(res1 <span>+</span> res2 <span>+</span> res3,
       lower_y<span>=</span><span>0.6</span>, title<span>=</span><span>"Experiment 5: Stochastic vs Deterministic (Slope-annealed ST)"</span>)</code></pre></div>
<pre><code>Epoch 20 0.9776
Epoch 20 0.977
Epoch 20 0.9704</code></pre>
<figure>
<img src="https://r2rt.com/static/images/BSN_output_28_1.png" alt="png"><figcaption>png</figcaption>
</figure>
<h4 id="experiment-6-the-effect-of-depth-on-reinforce-and-st-estimators">Experiment 6: The effect of depth on REINFORCE and ST estimators</h4>
<p>Next, I look at how each estimator interacts with depth. From a theoretical perpective, there is reason to think the straight-through estimator will suffer from depth; as noted by <a href="https://arxiv.org/abs/1308.3432">Bengio et al. (2013)</a>, it is not even guaranteed to have the same sign as the expected gradient during backpropagation. It turns out that the slope-annealed straight-through estimator is resilient to depth, even at a reasonable learning rate. The REINFORCE estimator, on the other hand, starts to fail as depth is introduced. However, if we lower the learning rate dramatically (25x), we can start to get the deeper networks to train with the REINFORCE estimator.</p>
<div><pre><code>res1 <span>=</span> train_classifier(hidden_dims<span>=</span>[<span>100</span>], estimator<span>=</span>StochasticGradientEstimator.ST, epochs<span>=</span><span>20</span>,
                        lr<span>=</span><span>0.3</span>, slope_annealing_rate<span>=</span><span>1.1</span>, label <span>=</span> <span>"1 hidden layer"</span>)
res2 <span>=</span> train_classifier(hidden_dims<span>=</span>[<span>100</span>, <span>100</span>], estimator<span>=</span>StochasticGradientEstimator.ST, epochs<span>=</span><span>20</span>,
                        lr<span>=</span><span>0.3</span>, slope_annealing_rate<span>=</span><span>1.1</span>, label <span>=</span> <span>"2 hidden layers"</span>)
res3 <span>=</span> train_classifier(hidden_dims<span>=</span>[<span>100</span>, <span>100</span>, <span>100</span>], estimator<span>=</span>StochasticGradientEstimator.ST, epochs<span>=</span><span>20</span>,
                        lr<span>=</span><span>0.3</span>, slope_annealing_rate<span>=</span><span>1.1</span>, label <span>=</span> <span>"3 hidden layers"</span>)

plot_n(res1[<span>1</span>:] <span>+</span> res2[<span>1</span>:] <span>+</span> res3[<span>1</span>:], title<span>=</span><span>"Experiment 6: The effect of depth (straight-through)"</span>)</code></pre></div>
<pre><code>Epoch 20 0.9774
Epoch 20 0.9738
Epoch 20 0.9728</code></pre>
<figure>
<img src="https://r2rt.com/static/images/BSN_output_30_1.png" alt="png"><figcaption>png</figcaption>
</figure>
<div><pre><code>res1 <span>=</span> train_classifier(hidden_dims<span>=</span>[<span>100</span>], estimator<span>=</span>StochasticGradientEstimator.REINFORCE, epochs<span>=</span><span>20</span>,
                       lr<span>=</span><span>0.05</span>, label <span>=</span> <span>"1 hidden layer"</span>)
res2 <span>=</span> train_classifier(hidden_dims<span>=</span>[<span>100</span>,<span>100</span>], estimator<span>=</span>StochasticGradientEstimator.REINFORCE, epochs<span>=</span><span>20</span>,
                       lr<span>=</span><span>0.05</span>, label <span>=</span> <span>"2 hidden layers"</span>)
res3 <span>=</span> train_classifier(hidden_dims<span>=</span>[<span>100</span>,<span>100</span>,<span>100</span>], estimator<span>=</span>StochasticGradientEstimator.REINFORCE, epochs<span>=</span><span>20</span>,
                       lr<span>=</span><span>0.05</span>, label <span>=</span> <span>"3 hidden layers"</span>)

plot_n(res1[<span>1</span>:] <span>+</span> res2[<span>1</span>:] <span>+</span> res3[<span>1</span>:], title<span>=</span><span>"Experiment 6: The effect of depth (REINFORCE)"</span>)</code></pre></div>
<pre><code>Epoch 20 0.9302
Epoch 20 0.8788
Epoch 20 0.2904</code></pre>
<figure>
<img src="https://r2rt.com/static/images/BSN_output_31_1.png" alt="png"><figcaption>png</figcaption>
</figure>
<div><pre><code>res1 <span>=</span> train_classifier(hidden_dims<span>=</span>[<span>100</span>], epochs<span>=</span><span>50</span>, non_binary<span>=</span><span>True</span>,
                       lr<span>=</span><span>0.002</span>, label <span>=</span> <span>"1 hidden layer"</span>)
res2 <span>=</span> train_classifier(hidden_dims<span>=</span>[<span>100</span>,<span>100</span>], epochs<span>=</span><span>50</span>, non_binary<span>=</span><span>True</span>,
                       lr<span>=</span><span>0.002</span>, label <span>=</span> <span>"2 hidden layers"</span>)
res3 <span>=</span> train_classifier(hidden_dims<span>=</span>[<span>100</span>,<span>100</span>,<span>100</span>], epochs<span>=</span><span>50</span>, non_binary<span>=</span><span>True</span>,
                       lr<span>=</span><span>0.002</span>, label <span>=</span> <span>"3 hidden layers"</span>)

plot_n(res1[<span>1</span>:] <span>+</span> res2[<span>1</span>:] <span>+</span> res3[<span>1</span>:], title<span>=</span><span>"Experiment 6: The effect of depth (REINFORCE) (LR = 0.002)"</span>)</code></pre></div>
<pre><code>Epoch 50 0.931
Epoch 50 0.9294
Epoch 50 0.9096</code></pre>
<figure>
<img src="https://r2rt.com/static/images/BSN_output_32_1.png" alt="png"><figcaption>png</figcaption>
</figure>
<h4 id="experiment-7-using-binary-stochastic-neurons-as-a-regularizer.">Experiment 7: Using binary stochastic neurons as a regularizer.</h4>
<p>I now test the “unpublished result” put forth at the end of <a href="https://www.youtube.com/watch?v=LN0xtUuJsEI&amp;list=PLoRl3Ht4JOcdU872GhiYWf6jwrk_SNhz9&amp;index=41">Hinton et al.’s Coursera Lecture 9c</a>, which states that we can improve upon the performance of an overfitting multi-layer sigmoid net by turning its neurons binary stochastic neurons with a straight-through estimator.</p>
<p>To test the claim, we will need a dataset that is easier to overfit than MNIST, and so the following experiment uses the MNIST validation set for training (10x smaller than the MNIST training set and therefore much easier to overfit). The hidden layer size is also increased by a factor of 2 to increase overfitting.</p>
<p>We can see below that the stochastic net has a clear advantage in terms of both the generalization gap and training speed, ultimately resulting in a better final fit.</p>
<div><pre><code>res1 <span>=</span> train_classifier(hidden_dims <span>=</span> [<span>200</span>], epochs<span>=</span><span>20</span>, train_set<span>=</span>mnist.validation, val_set<span>=</span>mnist.test,
                        lr <span>=</span> <span>0.03</span>, non_binary <span>=</span> <span>True</span>, label <span>=</span> <span>"Deterministic sigmoid net"</span>)

res2 <span>=</span> train_classifier(hidden_dims <span>=</span> [<span>200</span>], epochs<span>=</span><span>20</span>, stochastic_eval<span>=</span><span>False</span>, train_set<span>=</span>mnist.validation,
                        val_set<span>=</span>mnist.test, slope_annealing_rate<span>=</span><span>1.1</span>, estimator<span>=</span>StochasticGradientEstimator.ST,
                        lr <span>=</span> <span>0.3</span>, label <span>=</span> <span>"Binary stochastic net"</span>)

plot_n(res1 <span>+</span> res2, lower_y<span>=</span><span>0.8</span>, title<span>=</span><span>"Experiment 8: Using binary stochastic neurons as a regularizer"</span>)</code></pre></div>
<pre><code>Epoch 20 0.9276
Epoch 20 0.941</code></pre>
<figure>
<img src="https://r2rt.com/static/images/BSN_output_34_1.png" alt="png"><figcaption>png</figcaption>
</figure>
<h3 id="conclusion">Conclusion</h3>
<p>In this post we introduced, implemented and experimented with binary stochastic neurons in Tensorflow. We saw that the biased straight-through estimator generally outperforms the unbiased REINFORCE estimator, and can even outperform a non-stochastic, non-binary sigmoid net. We explored the variants of each estimator, and showed that the slope-annealed straight through estimator is better than other straight through variants, and that it is worth using the variance-adjusted REINFORCE estimator over the not variance-adjusted REINFORCE estimator. Finally, we explored the potential use for binary stochastic neurons as regularizers, and demonstrated that a stochastic binary network trained with the slope-annealed straight through estimator trains faster and generalizes better than an ordinary sigmoid net.</p>
<section>
<hr>
<ol>
<li id="fn1"><p><strong>Note</strong>: In a previous version of this post, I had instead used <span><span><span id="MJXp-Span-324"><span id="MJXp-Span-325"><span><span id="MJXp-Span-326">d</span><span id="MJXp-Span-327">BSN</span><span id="MJXp-Span-328">(</span><span id="MJXp-Span-329">a</span><span id="MJXp-Span-330">)</span></span><span><span><span><span></span></span><span><span><span id="MJXp-Span-331">d</span><span id="MJXp-Span-332">a</span></span></span></span></span></span><span id="MJXp-Span-333">=</span><span id="MJXp-Span-334">BSN</span><span id="MJXp-Span-335">(</span><span id="MJXp-Span-336">a</span><span id="MJXp-Span-337">)</span></span></span><span id="MathJax-Element-36-Frame" tabindex="0"></span></span>. This formulation would return the identity if the binary neuron evaluated to 1, and 0 otherwise. I had similarly multiplied the derivatives of the two variants (sigmoid-adjusted and slope-annealed) by a factor of <span><span><span id="MJXp-Span-338"><span id="MJXp-Span-339">BSN</span><span id="MJXp-Span-340">(</span><span id="MJXp-Span-341">a</span><span id="MJXp-Span-342">)</span></span></span><span id="MathJax-Element-37-Frame" tabindex="0"></span></span>. This prior formulation was consistent with my reading of Bengio et al. (2013) and achieved respectable results, but in light of a comment made on this post, and upon review of Hinton’s Coursera lecture where the straight-through estimator is first proposed, I believe the version now reflected in the post, <span><span><span id="MJXp-Span-343"><span id="MJXp-Span-344"><span><span id="MJXp-Span-345">d</span><span id="MJXp-Span-346">BSN</span><span id="MJXp-Span-347">(</span><span id="MJXp-Span-348">a</span><span id="MJXp-Span-349">)</span></span><span><span><span><span></span></span><span><span><span id="MJXp-Span-350">d</span><span id="MJXp-Span-351">a</span></span></span></span></span></span><span id="MJXp-Span-352">=</span><span id="MJXp-Span-353">1</span></span></span><span id="MathJax-Element-38-Frame" tabindex="0"></span></span>, is more correct. Although the pass-through variant performs worse with the revised derivative, the sigmoid-adjusted and slope-annealed variants benefit greatly from this change, outperforming both new and old pass-through formulations by a respectable margin.<a href="#fnref1">↩</a></p></li>
</ol>
</section>



      </div>
<!-- /.entry-content -->
      
      <!-- /.post-info -->
    <!-- Comment BEGIN -->
    
    
    
    <!-- Comment END -->
    </section>
    </div></div></div>
    </div>
    <footer>
        <div>created by <a href="https://buildstarted.com">buildstarted</a> &copy; 2020 <a href="/about">about</a></div>
        <div>Share this page on social media: copy and paste this url https://linksfor.dev/</div>
        <div>If you prefer RSS: <a href="https://linksfor.dev/feed.xml">https://linksfor.dev/feed.xml</a></div>
        <div>Customer satisfaction guaranteed to be optional.</div>
    </footer>
    
    <script async defer>
        _dna = window._dna || {};
        _dna.siteId = "linksfor.devs";
        _dna.outlink = true;

        (function() {
            let dna = document.createElement('script');
            dna.type = 'text/javascript';
            dna.async = true;
            dna.src = '//dna.buildstarted.com/t.js';
            let s = document.getElementsByTagName('script')[0];
            s.parentNode.insertBefore(dna, s);
        })();
    </script>
    <noscript><img src="//dna.buildstarted.com/g?siteId=linksfor.devs"/></noscript>
</body>
</html>