<!DOCTYPE html>
<html lang="en">
<head>
    <title>
There and Back Again&#x200A;&#x2014;&#x200A;Scaling Multi-Tenant Kubernetes Cluster(s) - linksfor.dev(s)    </title>
    <meta charset="utf-8">
    <link rel="alternate" type="application/rss+xml" title="Linksfor.dev(s) feed" href="https://linksfor.dev/feed.rss" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta name="google" value="notranslate">
    <link rel="stylesheet" type="text/css" href="/style.css" />
    <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon">
    <link rel="icon" href="/favicon.ico" type="image/x-icon">
    <meta property="og:title" content="There and Back Again&#x200A;&#x2014;&#x200A;Scaling Multi-Tenant Kubernetes Cluster(s) - linksfor.dev(s)"/>
    <meta property="article:author" content="https://medium.com/@sspencer_42564"/>
    <meta property="og:description" content="Everyone loves a good war story."/>
    <meta property="og:type" content="website"/>
    <meta property="og:url" content="https://medium.com/usa-today-network/there-and-back-again-scaling-multi-tenant-kubernetes-cluster-s-67afb437716c"/>

<meta property="og:site_name" content="linksfor.dev(s)" />
</head>
<body>
    <div class="devring" style="background: #222">
        <div style="text-align:center">Explore other dev related sites in this ring. If you would like to join this ring <a href="https://devring.club">click here</a>.</div>
        <div class="grid">
            <div style="display: grid; grid-template-columns: .5fr 1fr 1fr 1fr; text-align: center;">
                <span class="devring-title"><a href="https://devring.club/">devring.club</a></span>
                <a href="https://devring.club/sites/1/prev" class="devring-previous">Previous</a>
                <a href="https://devring.club/random" class="devring-random">Random</a>
                <a href="https://devring.club/sites/1/next" class="devring-next">Next</a>
            </div>
        </div>
    </div>
    <div class="grid">
        <h1>
<a href="/" style="color:inherit">linksfor.dev(s)</a>
        </h1>
        <title>linksfor.dev(s) - There and Back Again&#x200A;&#x2014;&#x200A;Scaling Multi-Tenant Kubernetes Cluster(s)</title>
<div class="readable">
        <h1>There and Back Again&#x200A;&#x2014;&#x200A;Scaling Multi-Tenant Kubernetes Cluster(s)</h1>
            <div>by https://medium.com/@sspencer_42564</div>
            <div>Reading time: 12-15 minutes</div>
        <div>Posted here: 20 May 2020</div>
        <p><a href="https://medium.com/usa-today-network/there-and-back-again-scaling-multi-tenant-kubernetes-cluster-s-67afb437716c">https://medium.com/usa-today-network/there-and-back-again-scaling-multi-tenant-kubernetes-cluster-s-67afb437716c</a></p>
        <hr/>
<div id="readability-page-1" class="page"><div><section><div><div><div><div><div><div><p><a rel="noopener" href="https://medium.com/@sspencer_42564?source=post_page-----67afb437716c----------------------"><img alt="Stephany Spencer" src="https://miro.medium.com/fit/c/96/96/0*NBeSd6dn0c16p9Ya." width="48" height="48"></a></p></div></div></div></div><p id="ab8a" data-selectable-paragraph=""><em>Everyone loves a good war story</em>.</p><p id="7e0a" data-selectable-paragraph="">They say there are lessons to be learned in IT “war stories”. But maybe the real lessons are what happened afterwards, not the event or even what led up to the event? Just as the event is not the whole story, nor is any particular tool the whole story.</p><p id="c92a" data-selectable-paragraph="">At Gannett, we’ve got some war stories. Plenty, in fact. When you are working for the largest local news organization, you have to rapidly adapt to changing landscape or be left behind.</p></div></div><div><div><div><figure><div><div><div><div><p><img src="https://miro.medium.com/max/60/1*IlAKo4iJnvxqzotDZZGGOw.png?q=20" width="1980" height="540" role="presentation"></p><p><img width="1980" height="540" srcset="https://miro.medium.com/max/552/1*IlAKo4iJnvxqzotDZZGGOw.png 276w, https://miro.medium.com/max/1104/1*IlAKo4iJnvxqzotDZZGGOw.png 552w, https://miro.medium.com/max/1280/1*IlAKo4iJnvxqzotDZZGGOw.png 640w, https://miro.medium.com/max/1456/1*IlAKo4iJnvxqzotDZZGGOw.png 728w, https://miro.medium.com/max/1632/1*IlAKo4iJnvxqzotDZZGGOw.png 816w, https://miro.medium.com/max/1808/1*IlAKo4iJnvxqzotDZZGGOw.png 904w, https://miro.medium.com/max/1984/1*IlAKo4iJnvxqzotDZZGGOw.png 992w, https://miro.medium.com/max/2000/1*IlAKo4iJnvxqzotDZZGGOw.png 1000w" sizes="1000px" role="presentation" src="https://miro.medium.com/max/1980/1*IlAKo4iJnvxqzotDZZGGOw.png"></p></div></div></div></div></figure></div></div></div></section><hr><section><div><div><h2 id="1aa2" data-selectable-paragraph="">From Here — A Kubernetes Cluster for Everyone</h2><p id="6b83" data-selectable-paragraph="">Kubernetes is one such tool that we use to bridge that gap between where we’ve come from and where we need to go. Our first success with Kubernetes was in November 2016, when our home-grown Kubernetes clusters carried USA Today’s election coverage. It was such a success that we quickly started building out as many Kubernetes clusters as development teams wanted and were willing to manage using <a href="https://www.chef.io/" target="_blank" rel="noopener nofollow">Chef</a> and <a href="https://www.scalr.com/" target="_blank" rel="noopener nofollow">Scalr</a>.</p><figure><div></div></figure><p id="34f6" data-selectable-paragraph="">Listening to this talk, you’ll quickly realize how complicated and difficult it is to run Kubernetes the “hard way”. We built up an amazing infrastructure to automate the deployment and management of the 20+ clusters using Chef and Scalr. However, it was still hard, especially on the development teams who wanted to deploy their applications quickly without a lot of hassle. It was still a big step forward, but not far enough.</p></div></div></section><hr><section><div><div><h2 id="f065" data-selectable-paragraph="">To There — A Shared, Managed Kubernetes Cluster for Everyone</h2><p id="bb89" data-selectable-paragraph=""><em>“Provide a resilient, optimized, feature rich, and easy to use platform that increases speed of innovation while reducing developer toil.”</em></p><p id="b136" data-selectable-paragraph="">I pick up the story again in early 2018. It’s becoming clear that asking development teams to run their own Kubernetes clusters does not, in fact, “<em>reduce developer toil</em>”. A new approach is needed. <a href="https://cloud.google.com/kubernetes-engine" target="_blank" rel="noopener nofollow">Google’s Kubernetes Engine </a>offering is gathering speed and mindshare. At the same time, Gannett is migrating a large portion of our cloud infrastructure from AWS to GCP.</p><p id="dd9f" data-selectable-paragraph="">A simple, elegant solution appears before us — form a new team to manage shared GKE clusters for everyone! A hybrid model where an operations team, a managed, secure service and RBAC come together to provide Developers with all the access they need to run the applications how they want.</p><figure><div><div><div><div><p><img alt="Division of responsibilities between teams." src="https://miro.medium.com/max/60/0*F4RTWt6_G9TaN5Cv?q=20" width="1144" height="660"></p><p><img alt="Division of responsibilities between teams." width="1144" height="660" srcset="https://miro.medium.com/max/552/0*F4RTWt6_G9TaN5Cv 276w, https://miro.medium.com/max/1104/0*F4RTWt6_G9TaN5Cv 552w, https://miro.medium.com/max/1280/0*F4RTWt6_G9TaN5Cv 640w, https://miro.medium.com/max/1400/0*F4RTWt6_G9TaN5Cv 700w" sizes="700px"></p></div></div></div></div></figure><p id="1bf8" data-selectable-paragraph="">The solution seems straightforward enough. We can follow the best practice documents around multi-tenancy. We create a namespace per team. We create an admin service account in each namespace and give those credentials to the various development teams. We implement Kubernetes integration with Hashicorp Vault. We reduce developer toil by taking back most of the work around maintaining a Kubernetes cluster, including collecting logs and basic metrics in a centralized location. We run two production clusters in one GCP project and the pre-production another project.</p><figure><div><div><div><p><img src="https://miro.medium.com/max/60/1*5_J9HtUvj2I7lC0rKxbNgw.png?q=20" width="480" height="320" role="presentation"></p><p><img width="480" height="320" srcset="https://miro.medium.com/max/552/1*5_J9HtUvj2I7lC0rKxbNgw.png 276w, https://miro.medium.com/max/960/1*5_J9HtUvj2I7lC0rKxbNgw.png 480w" sizes="480px" role="presentation"></p></div></div></div></figure><p id="20c9" data-selectable-paragraph="">We start to hit a few issues here and there. We purposely did not apply limit on namespaces. We can’t predict what teams will be needing to run their production applications and why throttle access to readily available resources? A deployment which goes bad and steals resources wasn’t entirely unexpected and could easily be dealt with. We naively thought that by splitting teams across workloads and nodepools would be adequate.</p><p id="05d1" data-selectable-paragraph="">We didn’t anticipate what the combined and highly diverse load would do to GKE. We begin to hit rare bugs in the OSS Kubernetes kernel. The Jenkins K8S plugin we use triggers goroutines to leak and destabilize the entire cluster. We request Google support restart our Master API server every few days to prevent the cluster from crashing over the course of several weeks. A bug in the OSS Linux kernel gets repeatedly triggered by all of the containers starting and stopping on a single node. We start proactively monitoring nodes and rebooting them every few hours until the bug is remedied.</p><p id="5978" data-selectable-paragraph="">Our methodology for maintaining and updating clusters stops scaling well. We originally started with a dedicated helm chart &amp; <a href="https://concourse-ci.org/" target="_blank" rel="noopener nofollow">Concourse </a>job per team. With over 40 teams on three clusters, the helm charts and jobs are becoming harder to maintain and prone to simple errors that delete teams’ namespaces and deployments. The clusters were created with <a href="https://www.terraform.io/" target="_blank" rel="noopener nofollow">Terraform</a> files. Those templates are now dangerous to re-apply, as the state drift will cause entire clusters to be deleted and recreated. Ad-hoc documentation starts to spring up attempting to document all the special knobs that were turned on which cluster to fix which issue. Building a new cluster is easily a week long process and not likely to replicate everything. We don’t mention the words “disaster recovery” anymore.</p><p id="8661" data-selectable-paragraph="">Cluster upgrades take days and can cause multiple teams’ applications to break. Keeping the clusters up to date is now a break/fix situation only. Migrating to new node pools is near impossible. Each new node requires the Kubernetes service controller to update all of the backends to add the new nodes to every GCP Forwarding Rule in the whole cluster. We have thousands of services resulting in about 30 minutes of delay migrating to a new node and all of the other nodes being updated.</p><p id="e5f7" data-selectable-paragraph="">Less easy to identify and rectify are the “hard quotas”, which are documented but not visible in the GCP quotas page. We found them while troubleshooting problems through cryptic log messages or side comments from support. Some limits are hard set and in most cases cannot be adjusted. Sometimes we were able to request Google engineering increase these limits, a bit.</p><ul><li id="ed67" data-selectable-paragraph="">Internal Forwarding Rules has a default maximum of 50</li><li id="f883" data-selectable-paragraph="">Maximum services/node limits</li></ul><p id="7031" data-selectable-paragraph="">As the months wore on and the outages stacked up, it became all too clear that we had become too multi-tenant for our own good. Kubernetes is built around the idea of scaling pods and nodes, not more and more services with only a handful of pods. It expects workloads to be similar-ish and not having a large degree of churn. We were mixing too many workloads and deployments in one place.</p></div></div></section><hr><section><div><div><h2 id="df0a" data-selectable-paragraph="">And Back Again — Managed Kubernetes Clusters for Everyone</h2><p id="ca30" data-selectable-paragraph="">In 2019, it was time to go back to where we began. All 40 development teams working on the same clusters wasn’t working super well anymore. The question became how we take the best of our previous two iterations of Kubernetes and bring that forward? How can we create shared clusters that share costs by allowing teams to use the same nodes? How can we isolate some workloads, yet be multi-tenant? How can we increase our resiliency by implementing updated GKE features like regional-master and VPN hub &amp; spoke networking? How can we manage the managed service back like we did in the Scalr days, but with less effort. Mergers are looming, budget cuts and shrinking staff are likely to be in our near term future.</p><p id="9a26" data-selectable-paragraph="">We began by looking at alternative methods for managing clusters other than our home grown Concourse jobs. These jobs did what they needed to do but were going to be painful to extend to a dynamically group set of new clusters. The process to build a new cluster had drifted organically over the year and was no longer reproducible. Teams were hard coded with IPs, service accounts credentials and on-the-fly deployment customizations.</p><p id="9a35" data-selectable-paragraph=""><em>Goal 1) Divide and Conquer</em></p><p id="b953" data-selectable-paragraph="">We needed to find a middle ground between every team having a dedicated GKE cluster and all teams being on the same cluster. The obvious solutions of splitting by “mission critical” applications on some clusters, but not others wouldn’t work. Every team believes their applications are mission critical and did we really want all of those on a single cluster anyways? We went with the broader categorization of Production, Pre-Production and “Tools”.</p><figure><div><div><div><p><img src="https://miro.medium.com/max/60/1*0DZh2T-mlnjedYkUvMLWlA.png?q=20" width="620" height="358" role="presentation"></p><p><img width="620" height="358" srcset="https://miro.medium.com/max/552/1*0DZh2T-mlnjedYkUvMLWlA.png 276w, https://miro.medium.com/max/1104/1*0DZh2T-mlnjedYkUvMLWlA.png 552w, https://miro.medium.com/max/1240/1*0DZh2T-mlnjedYkUvMLWlA.png 620w" sizes="620px" role="presentation"></p></div></div></div></figure><p id="ae8c" data-selectable-paragraph="">The second categorization is more of a psychological one. What are the individual development teams’ tolerance and desire for new features? What level of risks are they willing to accept to always have access to the latest versions and newest features? Some teams are actively waiting for Alpha features to become Beta features. Other teams would much prefer to never hear the word “alpha” or “beta” in regards to their production environments.</p><figure><div><div><div><p><img src="https://miro.medium.com/max/60/1*gA9Vf6eqSNQpszBHblpndA.png?q=20" width="640" height="357" role="presentation"></p><p><img width="640" height="357" srcset="https://miro.medium.com/max/552/1*gA9Vf6eqSNQpszBHblpndA.png 276w, https://miro.medium.com/max/1104/1*gA9Vf6eqSNQpszBHblpndA.png 552w, https://miro.medium.com/max/1280/1*gA9Vf6eqSNQpszBHblpndA.png 640w" sizes="640px" role="presentation"></p></div></div></div></figure><p id="a5c6" data-selectable-paragraph=""><em>Goal 2) Automate All The Things</em></p><p id="52db" data-selectable-paragraph="">We could all agree that more GKE clusters were needed. But how can we do that simply without adding more complexity to our already overscheduled workloads? We did look at other solutions for managing clusters. None of them did everything we needed to do. Some were too expensive. Some trivialized actions to the point where they were no longer reproducible. Other tools would take a huge knowledge lift for us to implement, on top of an already painfully diverse workload.</p><p id="d0a5" data-selectable-paragraph="">Our ideal solution would cover all of these things:</p><ul><li id="7902" data-selectable-paragraph="">Cheap and allows us to re-use existing tools like Hashicorp Terraform, Concourse and <a href="https://www.vaultproject.io/" target="_blank" rel="noopener nofollow">Hashicorp Vault</a></li><li id="cb60" data-selectable-paragraph="">Custom clusters, yet all clusters share same base configurations</li><li id="31e6" data-selectable-paragraph="">Dynamically generate cluster list and credentials</li><li id="1130" data-selectable-paragraph="">Modify and update many clusters simultaneously, yet track each cluster’s unique applied yaml via git (GitOps Rules!)</li></ul><p id="278b" data-selectable-paragraph="">To briefly summarize, <a href="https://www.cloudbees.com/gitops/what-is-gitops" target="_blank" rel="noopener nofollow">GitOps upholds the principle that Git is the one and only source of truth. GitOps requires the desired state of the system to be stored in version control such that anyone can view the entire audit trail of changes. All changes to the desired state are fully traceable commits associated with committer information, commit IDs and time stamps. This means that both the application and the infrastructure are now versioned artifacts and can be audited using the gold standards of software development and delivery.</a> Our new solution must meet these clearly stated goals.</p><p id="6e4a" data-selectable-paragraph="">We were able to leverage our existing Terraform pipelines to automate building of the GKE clusters and all of the supporting GCP resources.</p><figure><div><div><div><div><p><img src="https://miro.medium.com/max/60/1*dUMZyPUYQXy0LAJxJN4JRQ.png?q=20" width="1080" height="587" role="presentation"></p><p><img width="1080" height="587" srcset="https://miro.medium.com/max/552/1*dUMZyPUYQXy0LAJxJN4JRQ.png 276w, https://miro.medium.com/max/1104/1*dUMZyPUYQXy0LAJxJN4JRQ.png 552w, https://miro.medium.com/max/1280/1*dUMZyPUYQXy0LAJxJN4JRQ.png 640w, https://miro.medium.com/max/1400/1*dUMZyPUYQXy0LAJxJN4JRQ.png 700w" sizes="700px" role="presentation"></p></div></div></div></div></figure><p id="f15f" data-selectable-paragraph="">Next came the deployment yaml’s. Our clusters still need monitoring, cost reporting and logging applications along with Hashicorp Vault integration and several other internal tools. Some clusters will need special configurations or an entirely different set of applications. We can re-use our cluster designations to apply these things.</p><p id="538e" data-selectable-paragraph="">We realized we didn’t need a complicated new product to add to our already very diverse set of toolings. A bash script with some simple looping would solve this problem for us. We could add it to Concourse as another job and no longer worry about which cluster was which type and who was using it when.</p><p id="e30a" data-selectable-paragraph="">We generate a finalized version of every custer’s unique Kubernetes manifest and save it to git. We like helm charts, but it can be tricky to see the final applied configuration without connecting to each cluster and Tiller. We also wanted a way to document changes naturally. And remember all of those teams who needed custom namespaces and RBAC rules? We built a <code>setup-namespace</code>helm chart to create the namespace, RBAC roles and other requirements for every valid combination of clusters, namespaces and gsuite groups.</p></div></div><div><div><div><figure><div><div><div><div><p><img src="https://miro.medium.com/max/60/1*7FyLXqW0nM501eXIQzXPKQ.png?q=20" width="3416" height="1832" role="presentation"></p><p><img width="3416" height="1832" srcset="https://miro.medium.com/max/552/1*7FyLXqW0nM501eXIQzXPKQ.png 276w, https://miro.medium.com/max/1104/1*7FyLXqW0nM501eXIQzXPKQ.png 552w, https://miro.medium.com/max/1280/1*7FyLXqW0nM501eXIQzXPKQ.png 640w, https://miro.medium.com/max/1456/1*7FyLXqW0nM501eXIQzXPKQ.png 728w, https://miro.medium.com/max/1632/1*7FyLXqW0nM501eXIQzXPKQ.png 816w, https://miro.medium.com/max/1808/1*7FyLXqW0nM501eXIQzXPKQ.png 904w, https://miro.medium.com/max/1984/1*7FyLXqW0nM501eXIQzXPKQ.png 992w, https://miro.medium.com/max/2000/1*7FyLXqW0nM501eXIQzXPKQ.png 1000w" sizes="1000px" role="presentation"></p></div></div></div></div><figcaption data-selectable-paragraph="">Function to generate chart manifests</figcaption></figure></div></div></div><div><p id="52cd" data-selectable-paragraph="">Applying the configurations was not much more complicated. Combining a gcloud service account and GCP labels on the projects &amp; clusters, we could dynamically identify all of the clusters and how to connect to them. The next step was to simply apply the relevant generated manifests to the matching clusters.</p></div><div><div><div><figure><div><div><div><div><p><img src="https://miro.medium.com/max/60/1*y3gfa9Pa8yGC_JE4pPZ1tQ.png?q=20" width="2752" height="2396" role="presentation"></p><p><img width="2752" height="2396" srcset="https://miro.medium.com/max/552/1*y3gfa9Pa8yGC_JE4pPZ1tQ.png 276w, https://miro.medium.com/max/1104/1*y3gfa9Pa8yGC_JE4pPZ1tQ.png 552w, https://miro.medium.com/max/1280/1*y3gfa9Pa8yGC_JE4pPZ1tQ.png 640w, https://miro.medium.com/max/1456/1*y3gfa9Pa8yGC_JE4pPZ1tQ.png 728w, https://miro.medium.com/max/1632/1*y3gfa9Pa8yGC_JE4pPZ1tQ.png 816w, https://miro.medium.com/max/1808/1*y3gfa9Pa8yGC_JE4pPZ1tQ.png 904w, https://miro.medium.com/max/1984/1*y3gfa9Pa8yGC_JE4pPZ1tQ.png 992w, https://miro.medium.com/max/2000/1*y3gfa9Pa8yGC_JE4pPZ1tQ.png 1000w" sizes="1000px" role="presentation"></p></div></div></div></div><figcaption data-selectable-paragraph="">logic for planning, applying changes to clusters</figcaption></figure></div></div></div><div><p id="86a5" data-selectable-paragraph="">All of the layers of complexity are simplified to a several, repeated Concourse jobs. An update to the cluster configurations is tested with a `pr-test` and must pass a <code>apply-approved-prs</code> job, which consists of applying and verifying the new manifest on the Sandbox clusters. If a PR can’t be applied without failing, we want to know about it before it is merged into master. After changes are merged in, a <code>create-release-tag</code> job is run and creates a git release tag. Anytime the <code>release-plan</code> or <code>release-apply</code> jobs are run, they will look for changes between this release tag and what is on the cluster already. Only changed manifests are applied to keep the changes to a minimum.</p></div><div><div><div><figure><div><div><div><div><p><img alt="View of minimal Concourse jobs required to manage all the clusters." src="https://miro.medium.com/max/60/1*OXxOGcg784krrfP2qgVDgg.png?q=20" width="4874" height="498"></p><p><img alt="View of minimal Concourse jobs required to manage all the clusters." width="4874" height="498" srcset="https://miro.medium.com/max/552/1*OXxOGcg784krrfP2qgVDgg.png 276w, https://miro.medium.com/max/1104/1*OXxOGcg784krrfP2qgVDgg.png 552w, https://miro.medium.com/max/1280/1*OXxOGcg784krrfP2qgVDgg.png 640w, https://miro.medium.com/max/1456/1*OXxOGcg784krrfP2qgVDgg.png 728w, https://miro.medium.com/max/1632/1*OXxOGcg784krrfP2qgVDgg.png 816w, https://miro.medium.com/max/1808/1*OXxOGcg784krrfP2qgVDgg.png 904w, https://miro.medium.com/max/1984/1*OXxOGcg784krrfP2qgVDgg.png 992w, https://miro.medium.com/max/2000/1*OXxOGcg784krrfP2qgVDgg.png 1000w" sizes="1000px"></p></div></div></div></div></figure></div></div></div><div><div><p id="b357" data-selectable-paragraph=""><em>Goal 3) Rebuild With Newest High Availability Features</em></p><p id="345e" data-selectable-paragraph="">Since early 2018, multiple new GKE and GCP features were released that could dramatically decrease our outages by implementing highly resilient networking and GKE masters. We redesigned our networking between GCP projects to be a hub and spoke model taking advantage of new products like HA VPN Gateways.</p><ul><li id="6163" data-selectable-paragraph="">Highly resilient external and internal networking</li><li id="cba9" data-selectable-paragraph="">Private, not public, GKE clusters</li><li id="2580" data-selectable-paragraph="">Istio enabled</li><li id="cdd7" data-selectable-paragraph=""><code>gke-security-group</code> RBAC and IAM gsuite group enabled</li><li id="8b8d" data-selectable-paragraph="">Regional multiple masters, instead of a single zonal master</li></ul></div></div></section><hr><section><div><div><p id="4fa6" data-selectable-paragraph="">What has this journey taught us? Running on the latest technology absolutely allows us to adapt quickly and continue to actively reduce our cloud spending. No debate there. However, it comes with a hidden cost of any solution never solving the problem for long. In our four year’s experience with Kubernetes, we’ve never regretted the choice. Our team’s unofficial motto “<em>become comfortable with change</em>” has never been more true.</p></div></div></section></div></div></div>
    </div>
    <footer>
        <div>created by buildstarted &copy; 2020 <a href="/about">about</a></div>
        <div>Share this page on social media: copy and paste this url https://linksfor.dev/</div>
        <div>If you prefer RSS: <a href="https://linksfor.dev/feed.xml">https://linksfor.dev/feed.xml</a></div>
        <div>Customer satisfaction guaranteed to be optional.</div>
    </footer>
    
    <script async defer>
        _dna = window._dna || {};
        _dna.siteId = "linksfor.devs";
        _dna.outlink = true;

        (function () {
            let dna = document.createElement('script');
            dna.type = 'text/javascript';
            dna.async = true;
            dna.src = '//dna.buildstarted.com/t.js';
            let s = document.getElementsByTagName('script')[0];
            s.parentNode.insertBefore(dna, s);
        })();
    </script>
    <noscript><img src="//dna.buildstarted.com/g?siteId=linksfor.devs" /></noscript>
</body>
</html>