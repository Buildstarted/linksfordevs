<!DOCTYPE html>
<html lang="en">
<head>
    <title>
.NET for Apache Spark Helps Makes Big Data Accessible | .NET Blog - linksfor.dev(s)    </title>
    <meta charset="utf-8">
    <link rel="alternate" type="application/rss+xml" title="Linksfor.dev(s) feed" href="https://linksfor.dev/feed.rss" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta name="google" value="notranslate">
    <link rel="stylesheet" type="text/css" href="/style.css" />
    <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon">
    <link rel="icon" href="/favicon.ico" type="image/x-icon">
    <meta property="og:title" content=".NET for Apache Spark Helps Makes Big Data Accessible | .NET Blog - linksfor.dev(s)"/>
    <meta property="article:author" content="Brigit MurtaughProgram Manager,&#xA0;.NETFollow Brigit"/>
    <meta property="og:description" content="In this post, we explore how to use .NET for Apache Spark, which makes big data accessible to .NET developers, to perform log analysis."/>
    <meta property="og:type" content="website"/>
    <meta property="og:url" content="https://devblogs.microsoft.com/dotnet/using-net-for-apache-spark-to-analyze-log-data/"/>

<meta property="og:site_name" content="linksfor.dev(s)" />
</head>
<body>
    <div class="grid">
        <h1>
<a href="/" style="color:inherit">linksfor.dev(s)</a>
        </h1>
        <title>linksfor.dev(s) - .NET for Apache Spark Helps Makes Big Data Accessible | .NET Blog</title>
<div class="readable">
        <h1>.NET for Apache Spark Helps Makes Big Data Accessible | .NET Blog</h1>
            <div>by Brigit MurtaughProgram Manager,&#xA0;.NETFollow Brigit</div>
            <div>Reading time: 8-10 minutes</div>
        <div>Posted here: 10 Feb 2020</div>
        <p><a href="https://devblogs.microsoft.com/dotnet/using-net-for-apache-spark-to-analyze-log-data/">https://devblogs.microsoft.com/dotnet/using-net-for-apache-spark-to-analyze-log-data/</a></p>
        <hr/>
<div id="readability-page-1" class="page"><div id="featured"><div><div><div><div><p><img src="https://devblogs.microsoft.com/dotnet/wp-content/uploads/sites/10/2020/01/Profile-Pic-150x150.jpg" width="58" height="58" alt="Brigit Murtaugh"></p><p>Brigit</p></div></div></div><p>February 10th, 2020</p><p>At Spark + AI Summit in <a href="https://devblogs.microsoft.com/dotnet/introducing-net-for-apache-spark/">May 2019</a>, we released <a href="https://dot.net/spark" target="_blank">.NET for Apache Spark</a>. .NET for Apache Spark is aimed at making <a href="https://spark.apache.org/" target="_blank">Apache® Spark™</a>, and thus the exciting world of big data analytics, accessible to .NET developers.</p><p>.NET for Spark can be used for processing batches of data, real-time streams, machine learning, and ad-hoc query. In this blog post, we’ll explore how to use .NET for Spark to perform a very popular big data task known as <strong>log analysis</strong>.</p><p>The remainder of this post describes the following topics:</p><ul><li><a href="#whatislog">What is log analysis?</a></li><li><a href="#sparkforlog">Writing a .NET for Spark log analysis app</a></li><li><a href="#running">Running a .NET for Spark app</a></li><li><a href="#wrapup">Wrap Up</a></li></ul><h3><a id="whatislog"></a>What is log analysis?</h3><p>Log analysis, also known as <em>log processing</em>, is the process of analyzing computer-generated records called logs. Logs tell us what’s happening on a tool like a computer or web server, such as what applications are being used or the top websites users visit.</p><p>The goal of log analysis is to gain meaningful insights from these logs about activity and performance of our tools or services. .NET for Spark enables us to analyze anywhere from megabytes to petabytes of log data with blazing fast and efficient processing!</p><p>In this blog post, we’ll be analyzing a set of <a href="https://httpd.apache.org/docs/1.3/logs.html" target="_blank">Apache log entries</a> that express how users are interacting with content on a web server. You can view a sample of Apache log entries <a href="https://raw.githubusercontent.com/elastic/examples/master/Common%20Data%20Formats/apache_logs/apache_logs" target="_blank">here</a>.</p><h3><a id="sparkforlog"></a>Writing a .NET for Spark log analysis app</h3><p>Log analysis is an example of <a href="https://docs.microsoft.com/en-us/dotnet/spark/tutorials/batch-processing" target="_blank">batch processing</a> with Spark. Batch processing is the transformation of data at rest, meaning that the source data has already been loaded into data storage. In our case, the input text file is already populated with logs and won’t be receiving new or updated logs as we process it.</p><p>When creating a new .NET for Spark application, there are just a few steps we need to follow to start getting those interesting insights from our data:</p><ol><li>Create a Spark Session.</li><li>Read input data, typically using a DataFrame.</li><li>Manipulate and analyze input data, typically using Spark SQL.</li></ol><h4><a id="createsession"></a> Create a Spark Session</h4><p>In any Spark application, we start off by establishing a new <a href="https://docs.microsoft.com/en-us/dotnet/api/microsoft.spark.sql.sparksession?view=spark-dotnet" target="_blank">SparkSession</a>, which is the entry point to programming with Spark:</p><pre><span>SparkSession</span><span> spark </span><span>=</span><span> </span><span>SparkSession</span><span>
    </span><span>.</span><span>Builder</span><span>()</span><span>
    </span><span>.</span><span>AppName</span><span>(</span><span>"Apache User Log Processing"</span><span>)</span><span>
    </span><span>.</span><span>GetOrCreate</span><span>();</span></pre><p>By calling on the <code>spark</code> object created above, we can now access Spark and DataFrame functionality throughout our program – great! But what is a DataFrame? Let’s learn about it in the next step.</p><h4><a id="readindata"></a> Read input data</h4><p>Now that we have access to Spark functionality, we can read in the log data we’ll be analyzing. We store input data in a <a href="https://docs.microsoft.com/en-us/dotnet/api/microsoft.spark.sql.dataframe?view=spark-dotnet" target="_blank">DataFrame</a>, which is a distributed collection of data organized into named columns:</p><pre><span>DataFrame</span><span> generalDf </span><span>=</span><span> spark</span><span>.</span><span>Read</span><span>().</span><span>Text</span><span>(</span><span>"&lt;path to input data set&gt;"</span><span>);</span></pre><p>When our input is contained in a <em>.txt</em> file, we use the <code>.Text()</code> method, as shown above. There are <a href="https://docs.microsoft.com/en-us/dotnet/api/microsoft.spark.sql.dataframereader?view=spark-dotnet#methods" target="_blank">other methods</a> to read in data from other sources, such as <code>.Csv()</code> to read in comma-separated values files.</p><h4><a id="manipulatedata"></a> Manipulate and analyze input data</h4><p>With our input logs stored in a DataFrame, we can start analyzing them – now things are getting exciting!</p><p>An important first step is <strong>data preparation</strong>. Data prep involves cleaning up our data in some way. This could include removing incomplete entries to avoid error in later calculations or removing irrelevant input to improve performance.</p><p>In our example, we should first ensure all of our entries are complete logs. We can do this by comparing each log entry to a <a href="https://docs.microsoft.com/en-us/dotnet/standard/base-types/regular-expression-language-quick-reference" target="_blank">regular expression</a> (AKA a regex), which is a sequence of characters that defines a pattern.</p><p>Let’s define a regex expressing a pattern all valid Apache log entries should follow:</p><pre><span>string</span><span> s_apacheRx </span><span>=</span><span> </span><span>"^(\S+) (\S+) (\S+) [([\w:/]+\s[+-]\d{4})] \"(\S+) (\S+) (\S+)\" (\d{3}) (\d+)"</span><span>;</span></pre><p>How do we perform a calculation on each row of a DataFrame, like comparing each log entry to the above regex? The answer is <em>Spark SQL</em>.</p><h4>Spark SQL</h4><p>Spark SQL provides many great functions for working with the structured data stored in a DataFrame. One of the most popular features of Spark SQL is <em>UDFs</em>, or user-defined functions. We define the type of input they take and the type of output they produce, and then the actual calculation or filtering they perform.</p><p>Let’s define a new UDF <code>GeneralReg</code> to compare each log entry to the <code>s_apacheRx</code> regex. Our UDF requires an Apache log entry, which is a string, and will return a true or false depending upon if the log matches the regex:</p><pre><span>spark</span><span>.</span><span>Udf</span><span>().</span><span>Register</span><span>&lt;</span><span>string</span><span>,</span><span> </span><span>bool</span><span>&gt;(</span><span>"GeneralReg"</span><span>,</span><span> log </span><span>=&gt;</span><span> </span><span>Regex</span><span>.</span><span>IsMatch</span><span>(</span><span>log</span><span>,</span><span> s_apacheRx</span><span>));</span></pre><p>So how do we call <code>GeneralReg</code>?</p><p>In addition to UDFs, Spark SQL provides the ability to write <strong>SQL calls</strong> to analyze our data – how convenient! It’s common to write a SQL call to apply a UDF to each row of data.</p><p>To call <code>GeneralReg</code> from above, let’s use the following SQL call:</p><pre><span>DataFrame</span><span> generalDf </span><span>=</span><span> spark</span><span>.</span><span>Sql</span><span>(</span><span>"SELECT logs.value, GeneralReg(logs.value) FROM Logs"</span><span>);</span></pre><p>This SQL call tests each row of <code>generalDf</code> to determine if it’s a valid and complete log.</p><p>We can use <a href="https://docs.microsoft.com/en-us/dotnet/api/microsoft.spark.sql.dataframe.filter?view=spark-dotnet" target="_blank">.Filter()</a> to only keep the complete log entries in our data, and then <a href="https://docs.microsoft.com/en-us/dotnet/api/microsoft.spark.sql.dataframe.show?view=spark-dotnet" target="_blank">.Show()</a> to display our newly filtered DataFrame:</p><pre><span>generalDf </span><span>=</span><span> generalDf</span><span>.</span><span>Filter</span><span>(</span><span>generalDf</span><span>[</span><span>"GeneralReg(value)"</span><span>]);</span><span>
generalDf</span><span>.</span><span>Show</span><span>();</span></pre><p>Now that we’ve performed some initial data prep, we can continue filtering and analyzing our data. Let’s find log entries from IP addresses starting with 10 and related to spam in some way:</p><pre><span>// Choose valid log entries that start with 10</span><span>
spark</span><span>.</span><span>Udf</span><span>().</span><span>Register</span><span>&lt;</span><span>string</span><span>,</span><span> </span><span>bool</span><span>&gt;(</span><span>
    </span><span>"IPReg"</span><span>,</span><span>
    log </span><span>=&gt;</span><span> </span><span>Regex</span><span>.</span><span>IsMatch</span><span>(</span><span>log</span><span>,</span><span> </span><span>"^(?=10)"</span><span>));</span><span>

generalDf</span><span>.</span><span>CreateOrReplaceTempView</span><span>(</span><span>"IPLogs"</span><span>);</span><span>

</span><span>// Apply UDF to get valid log entries starting with 10</span><span>
</span><span>DataFrame</span><span> ipDf </span><span>=</span><span> spark</span><span>.</span><span>Sql</span><span>(</span><span>
    </span><span>"SELECT iplogs.value FROM IPLogs WHERE IPReg(iplogs.value)"</span><span>);</span><span>
ipDf</span><span>.</span><span>Show</span><span>();</span><span>

</span><span>// Choose valid log entries that start with 10 and deal with spam</span><span>
spark</span><span>.</span><span>Udf</span><span>().</span><span>Register</span><span>&lt;</span><span>string</span><span>,</span><span> </span><span>bool</span><span>&gt;(</span><span>
    </span><span>"SpamRegEx"</span><span>,</span><span>
    log </span><span>=&gt;</span><span> </span><span>Regex</span><span>.</span><span>IsMatch</span><span>(</span><span>log</span><span>,</span><span> </span><span>"\\b(?=spam)\\b"</span><span>));</span><span>

ipDf</span><span>.</span><span>CreateOrReplaceTempView</span><span>(</span><span>"SpamLogs"</span><span>);</span><span>

</span><span>// Apply UDF to get valid, start with 10, spam entries</span><span>
</span><span>DataFrame</span><span> spamDF </span><span>=</span><span> spark</span><span>.</span><span>Sql</span><span>(</span><span>
    </span><span>"SELECT spamlogs.value FROM SpamLogs WHERE SpamRegEx(spamlogs.value)"</span><span>);</span></pre><p>Finally, let’s count the number of GET requests in our final cleaned dataset. The magic of .NET for Spark is that we can combine it with other popular .NET features to write our apps. We’ll use <a href="https://docs.microsoft.com/en-us/dotnet/csharp/programming-guide/concepts/linq/" target="_blank">LINQ</a> to analyze the data in our Spark app one last time:</p><pre><span>int</span><span> numGetRequests </span><span>=</span><span> spamDF 
    </span><span>.</span><span>Collect</span><span>()</span><span> 
    </span><span>.</span><span>Where</span><span>(</span><span>r </span><span>=&gt;</span><span> </span><span>ContainsGet</span><span>(</span><span>r</span><span>.</span><span>GetAs</span><span>&lt;string&gt;</span><span>(</span><span>"value"</span><span>)))</span><span> 
    </span><span>.</span><span>Count</span><span>();</span></pre><p>In the above code, <code>ContainsGet()</code> checks for GET requests using <a href="https://docs.microsoft.com/en-us/dotnet/api/system.text.regularexpressions.regex.match?view=netframework-4.8" target="_blank">regex matching</a>:</p><pre><span>// Use regex matching to group data </span><span>
</span><span>// Each group matches a column in our log schema </span><span>
</span><span>// i.e. first group = first column = IP</span><span>
</span><span>public</span><span> </span><span>static</span><span> </span><span>bool</span><span> </span><span>ContainsGet</span><span>(</span><span>string</span><span> logLine</span><span>)</span><span> 
</span><span>{</span><span> 
    </span><span>Match</span><span> match </span><span>=</span><span> </span><span>Regex</span><span>.</span><span>Match</span><span>(</span><span>logLine</span><span>,</span><span> s_apacheRx</span><span>);</span><span>

    </span><span>// Determine if valid log entry is a GET request</span><span>
    </span><span>if</span><span> </span><span>(</span><span>match</span><span>.</span><span>Success</span><span>)</span><span>
    </span><span>{</span><span>
        </span><span>Console</span><span>.</span><span>WriteLine</span><span>(</span><span>"Full log entry: '{0}'"</span><span>,</span><span> match</span><span>.</span><span>Groups</span><span>[</span><span>0</span><span>].</span><span>Value</span><span>);</span><span>
    
        </span><span>// 5th column/group in schema is "method"</span><span>
        </span><span>if</span><span> </span><span>(</span><span>match</span><span>.</span><span>Groups</span><span>[</span><span>5</span><span>].</span><span>Value</span><span> </span><span>==</span><span> </span><span>"GET"</span><span>)</span><span>
        </span><span>{</span><span>
            </span><span>return</span><span> </span><span>true</span><span>;</span><span>
        </span><span>}</span><span>
    </span><span>}</span><span>

    </span><span>return</span><span> </span><span>false</span><span>;</span><span>

</span><span>}</span><span> </span></pre><p>As a final step in our Spark apps, we call <code>spark.Stop()</code> to shut down the underlying Spark Session and Spark Context.</p><p>You can view the <a href="https://github.com/dotnet/spark/blob/master/examples/Microsoft.Spark.CSharp.Examples/Sql/Batch/Logging.cs" target="_blank">complete log processing example</a> in our GitHub repo.</p><h3><a id="running"></a> Running your app</h3><p><a href="https://docs.microsoft.com/en-us/dotnet/spark/tutorials/get-started" target="_blank">To run a .NET for Apache Spark app</a>, you need to use the <code>spark-submit</code> command, which will submit your application to run on Apache Spark.</p><p>The main parts of <code>spark-submit</code> include:</p><ul><li>–class, to call the DotnetRunner.</li><li>–master, to determine if this is a local or cloud Spark submission.</li><li>Path to the Microsoft.Spark jar file.</li><li>Any arguments or dependencies for your app, such as the path to your input file or the dll containing UDF definitions.</li></ul><p>You’ll also need to download and setup some dependencies before running a .NET for Spark app locally, such as Java and Apache Spark.</p><p>A sample Windows command for running your app is as follows:</p><p><code>spark-submit --class org.apache.spark.deploy.dotnet.DotnetRunner --master local /path/to/microsoft-spark-&lt;version&gt;.jar dotnet /path/to/netcoreapp&lt;version&gt;/LoggingApp.dll</code></p><h3><a id="wrapup"></a>.NET for Apache Spark Wrap Up</h3><p>We’d love to help you get started with .NET for Apache Spark and hear your feedback.</p><p>You can <a href="https://dot.net/spark" target="_blank">Request a Demo</a> from our landing page and check out the <a href="https://github.com/dotnet/spark" target="_blank">.NET for Spark GitHub repo</a> to learn more about how you can apply .NET for Spark in your apps and get involved with our effort to make .NET a great tech stack for building big data applications!</p></div></div></div></div>
    </div>
    <footer>
        <div>created by buildstarted &copy; 2020 <a href="/about">about</a></div>
        <div>Share this page on social media: copy and paste this url https://linksfor.dev/</div>
        <div>If you prefer RSS: <a href="https://linksfor.dev/feed.xml">https://linksfor.dev/feed.xml</a></div>
		<div>Customer satisfaction guaranteed to be optional.</div>
    </footer>
    
    <script async defer>
        _dna = window._dna || {};
        _dna.siteId = "linksfor.devs";
        _dna.outlink = true;

        (function () {
            let dna = document.createElement('script');
            dna.type = 'text/javascript';
            dna.async = true;
            dna.src = '//dna.buildstarted.com/t.js';
            let s = document.getElementsByTagName('script')[0];
            s.parentNode.insertBefore(dna, s);
        })();
    </script>
    <noscript><img src="//dna.buildstarted.com/g?siteId=linksfor.devs"/></noscript>
</body>
</html>