<!DOCTYPE html>
<html lang="en">
<head>
    <title>
AI Progress Measurement - linksfor.dev(s)    </title>
    <meta charset="utf-8">
    <link rel="alternate" type="application/rss+xml" title="Linksfor.dev(s) feed" href="https://linksfor.dev/feed.rss" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta name="google" value="notranslate">
    <link rel="stylesheet" type="text/css" href="/style.css" />
    <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon">
    <link rel="icon" href="/favicon.ico" type="image/x-icon">
    <meta property="og:title" content="AI Progress Measurement - linksfor.dev(s)"/>
    <meta property="og:description" content="This pilot project collects problems and metrics/datasets from the AI research literature, and tracks progress on them. You can use this notebook to see how things are progressing in specific subfields or AI/ML as a whole, as a place to report new results you&#x27;ve obtained, as a place to look for problems that might benefit from having new datasets/metrics designed for them, or as a source to build on for data science projects. At EFF, we&#x27;re ultimately most interested in how this data can influence our understanding of the likely implications of AI. To begin with, we&#x27;re focused on gathering it."/>
    <meta property="og:type" content="website"/>
    <meta property="og:url" content="https://www.eff.org/ai/metrics"/>

<meta property="og:site_name" content="linksfor.dev(s)" />
</head>
<body>
    <div class="devring" style="background: #222">
        <div style="text-align:center">Explore other dev related sites in this ring. If you would like to join this ring <a href="https://devring.club">click here</a>.</div>
        <div class="grid">
            <div style="display: grid; grid-template-columns: .5fr 1fr 1fr 1fr; text-align: center;">
                <span class="devring-title"><a href="https://devring.club/">devring.club</a></span>
                <a href="https://devring.club/sites/1/prev" class="devring-previous">Previous</a>
                <a href="https://devring.club/random" class="devring-random">Random</a>
                <a href="https://devring.club/sites/1/next" class="devring-next">Next</a>
            </div>
        </div>
    </div>
    <div class="grid">
        <h1>
<a href="/" style="color:inherit">linksfor.dev(s)</a>
        </h1>
        <title>linksfor.dev(s) - AI Progress Measurement</title>
<div class="readable">
        <h1>AI Progress Measurement</h1>
            <div>Reading time: 24-30 minutes</div>
        <div>Posted here: 27 Feb 2019</div>
        <p><a href="https://www.eff.org/ai/metrics">https://www.eff.org/ai/metrics</a></p>
        <hr/>
<div id="readability-page-1" class="page"><div>

<div>
<div>

<p>This pilot project collects problems and metrics/datasets from the AI research literature, and tracks progress on them.</p>
<p>You can use this <a href="https://jupyter.org/#about-notebook">Notebook</a> to see how things are progressing in specific subfields or AI/ML as a whole, as a place to report new results you've obtained, as a place to look for problems that might benefit from having new datasets/metrics designed for them, or as a source to build on for data science projects.</p>
<p>At EFF, we're ultimately most interested in how this data can influence our understanding of the likely implications of AI. To begin with, we're <a href="https://www.eff.org/deeplinks/2017/06/help-eff-track-progress-ai-and-machine-learning">focused on gathering it</a>.</p>
<p><a name="sources" id="sources"></a>
<em>Original authors: <a href="https://www.eff.org/about/staff/peter-eckersley">Peter Eckersley</a> and <a href="https://ynasser.github.io/">Yomna Nasser</a> at EFF</em>. Contact: <a href="mailto:ai-metrics@eff.org">ai-metrics@eff.org</a>.<br></p>
<p>With contributions from: <a href="http://yannbayle.fr/french/index.php">Yann Bayle</a>, <a href="https://www.fhi.ox.ac.uk/team/owain-evans/">Owain Evans</a>,  <a href="https://www.eff.org/about/staff/gennie-gebhart">Gennie Gebhart</a> and <a href="https://github.com/drschwenk">Dustin Schwenk</a>.<br></p>
<p>Inspired by and merging data from:</p>
<ul>
<li>Rodrigo Benenson's <a href="https://rodrigob.github.io/are_we_there_yet/build/#about">"Who is the Best at X / Are we there yet?"</a> collating machine vision datasets &amp; progress</li>
<li>Jack Clark and Miles Brundage's <a href="https://raw.githubusercontent.com/AI-metrics/master_text/master/archive/AI-metrics-data.txt">collection of AI progress measurements</a>
</li>
<li>Sarah Constantin's <a href="https://srconstantin.wordpress.com/2017/01/28/performance-trends-in-ai/">Performance Trends in AI</a>
</li>
<li>Katja Grace's <a href="https://intelligence.org/files/AlgorithmicProgress.pdf">Algorithmic Progress in Six Domains</a>
</li>
<li>The Swedish Computer Chess Association's <a href="https://en.wikipedia.org/wiki/Swedish_Chess_Computer_Association#Rating_list_year-end_leaders">History of Computer Chess performance</a>
</li>
<li>Gabriel Synnaeve's <a href="https://github.com/syhw/wer_are_we">WER are We</a> collation of speech recognition performance data</li>
<li>Qi Wu <em>et al.</em>'s <a href="https://arxiv.org/abs/1607.05910">Visual Question Answering: A survey of Methods and Datasets</a>
</li>
<li>Eric Yuan's <a href="http://eric-yuan.me/compare-popular-mrc-datasets/">Comparison of Machine Reading Comprehension Datasets</a>
</li>
</ul>
<p>Thanks to many others for valuable conversations, suggestions and corrections, including: Dario Amodei, James Bradbury, Miles Brundage, Mark Burdett, Breandan Considine, Owen Cotton-Barrett, Marc Bellemare, Will Dabny, Eric Drexler, Otavio Good, Katja Grace, Hado van Hasselt, Anselm Levskaya, Clare Lyle, Toby Ord, Michael Page, Maithra Raghu, Anders Sandberg, Laura Schatzkin, Daisy Stanton, Gabriel Synnaeve, Stacey Svetlichnaya, Helen Toner, and Jason Weston. EFF's work on this project has been supported by the <a href="http://www.openphilanthropy.org/">Open Philanthropy Project</a>.</p>

</div>
</div>
</div><div>

<div>
<div>
<h3 id="Taxonomy">Taxonomy<a href="#Taxonomy">¶</a>
</h3>
<p>It collates data with the following structure:</p>

<pre><code>problem 
    \   \
     \   metrics  -  measures 
      \
       - subproblems
            \
          metrics
             \
            measure[ment]s</code></pre>
<p>Problems describe the ability to learn an important category of task.</p>
<p>Metrics should ideally be formulated in the form "software is able to learn to do X given training data of type Y". In some cases X is the interesting part, but sometimes also Y.</p>
<p>Measurements are the score that a specific instance of a specific algorithm was able to get on a Metric.</p>
<p>problems are tagged with attributes:
eg, vision, abstract-games, language, world-modelling, safety</p>
<p>Some of these are about performance relative to humans (which is of course a very arbitrary standard, but one we're familiar with)</p>
<ul>
<li>agi -- most capable humans can do this, so AGIs can do this (note it's conceivable that an agent might pass the Turing test before all of these are won)</li>
<li>super -- the very best humans can do this, or human organisations can do this</li>
<li>verysuper -- neither humans nor human orgs can presently do this</li>
</ul>
<p>problems can have "subproblems", including simpler cases and preconditions for solving the problem in general</p>
<p>a "metric" is one way of measuring progress on a problem, commonly associated with a test dataset. There will often be several metrics
for a given problem, but in some cases we'll start out with zero metrics and will need to start proposing some...</p>
<p>a measure[ment] is a score on a given metric, by a particular codebase/team/project, at a particular time</p>
<p>The present state of the actual taxonomy is <a href="#Taxonomy-and-recorded-progress-to-date">at the bottom of this notebook</a>.</p>

</div>
</div>
</div><div>

<div>
<div>
<h2 id="Vision">Vision<a href="#Vision">¶</a>
</h2>
<div>

<p><img src="https://www.eff.org/files/ai-progress/images/imagenet.jpg"></p><p>(Imagenet example data)</p>
</div>
<p>The simplest vision subproblem is probably image classification, which determines what objects are present in a picture. From 2010-2017, Imagenet has been a closely watched contest for progress in this domain.</p>
<p>Image classification includes not only recognising single things within an image, but localising them and essentially specifying which pixels are which object. MSRC-21 is a metric that is specifically for that task:</p>

<div>
<p><img src="https://www.eff.org/files/ai-progress/images/msrc_21.png"></p><p>(MSRC 21 example data)</p>
</div>
</div>
</div>
</div><div>

<div>
<div>
<p>AWTY, not yet imported:</p>

<pre><code>Handling 'Pascal VOC 2011 comp3' detection_datasets_results.html#50617363616c20564f43203230313120636f6d7033
Skipping 40.6 mAP Fisher and VLAD with FLAIR CVPR 2014
Handling 'Leeds Sport Poses' pose_estimation_datasets_results.html#4c656564732053706f727420506f736573
69.2 %                  Strong Appearance and Expressive Spatial Models for Human Pose Estimation  ICCV 2013
64.3 %                                    Appearance sharing for collective human pose estimation  ACCV 2012
63.3 %                                                   Poselet conditioned pictorial structures  CVPR 2013
60.8 %                                Articulated pose estimation with flexible mixtures-of-parts  CVPR 2011
 55.6%           Pictorial structures revisited: People detection and articulated pose estimation  CVPR 2009
Handling 'Pascal VOC 2007 comp3' detection_datasets_results.html#50617363616c20564f43203230303720636f6d7033
Skipping 22.7 mAP Ensemble of Exemplar-SVMs for Object Detection and Beyond ICCV 2011
Skipping 27.4 mAP Measuring the objectness of image windows PAMI 2012
Skipping 28.7 mAP Automatic discovery of meaningful object parts with latent CRFs CVPR 2010
Skipping 29.0 mAP Object Detection with Discriminatively Trained Part Based Models PAMI 2010
Skipping 29.6 mAP Latent Hierarchical Structural Learning for Object Detection CVPR 2010
Skipping 32.4 mAP Deformable Part Models with Individual Part Scaling BMVC 2013
Skipping 34.3 mAP Histograms of Sparse Codes for Object Detection CVPR 2013
Skipping 34.3 mAP Boosted local structured HOG-LBP for object localization CVPR 2011
Skipping 34.7 mAP Discriminatively Trained And-Or Tree Models for Object Detection CVPR 2013
Skipping 34.7 mAP Incorporating Structural Alternatives and Sharing into Hierarchy for Multiclass Object Recognition and Detection CVPR 2013
Skipping 34.8 mAP Color Attributes for Object Detection CVPR 2012
Skipping 35.4 mAP Object Detection with Discriminatively Trained Part Based Models PAMI 2010
Skipping 36.0 mAP Machine Learning Methods for Visual Object Detection archives-ouvertes 2011
Skipping 38.7 mAP Detection Evolution with Multi-Order Contextual Co-occurrence CVPR 2013
Skipping 40.5 mAP Segmentation Driven Object Detection with Fisher Vectors ICCV 2013
Skipping 41.7 mAP Regionlets for Generic Object Detection ICCV 2013
Skipping 43.7 mAP Beyond Bounding-Boxes: Learning Object Shape by Model-Driven Grouping ECCV 2012
Handling 'Pascal VOC 2007 comp4' detection_datasets_results.html#50617363616c20564f43203230303720636f6d7034
Skipping 59.2 mAP Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition ECCV 2014
Skipping 58.5 mAP Rich feature hierarchies for accurate object detection and semantic segmentation CVPR 2014
Skipping 29.0 mAP Multi-Component Models for Object Detection ECCV 2012
Handling 'Pascal VOC 2010 comp3' detection_datasets_results.html#50617363616c20564f43203230313020636f6d7033
Skipping 24.98 mAP Learning Collections of Part Models for Object Recognition CVPR 2013
Skipping 29.4 mAP Discriminatively Trained And-Or Tree Models for Object Detection CVPR 2013
Skipping 33.4 mAP Object Detection with Discriminatively Trained Part Based Models PAMI 2010
Skipping 34.1 mAP Segmentation as selective search for object recognition ICCV 2011
Skipping 35.1 mAP Selective Search for Object Recognition IJCV 2013
Skipping 36.0 mAP Latent Hierarchical Structural Learning for Object  Detection CVPR 2010
Skipping 36.8 mAP Object Detection by Context and Boosted HOG-LBP ECCV 2010
Skipping 38.4 mAP Segmentation Driven Object Detection with Fisher Vectors ICCV 2013
Skipping 39.7 mAP Regionlets for Generic Object Detection ICCV 2013
Skipping 40.4 mAP Fisher and VLAD with FLAIR CVPR 2014
Handling 'Pascal VOC 2010 comp4' detection_datasets_results.html#50617363616c20564f43203230313020636f6d7034
Skipping 53.7 mAP Rich feature hierarchies for accurate object detection and semantic segmentation CVPR 2014
Skipping 40.4 mAP Bottom-up Segmentation for Top-down Detection CVPR 2013
Skipping 33.1 mAP Multi-Component Models for Object Detection ECCV 2012</code></pre>

</div>
</div>
</div><div>

<div>
<div>
<p><img src="https://www.eff.org/files/ai-progress/images/vqa.jpg" float="right"></p>
<h3 id="Visual-Question-Answering">Visual Question Answering<a href="#Visual-Question-Answering">¶</a>
</h3>
<p>Comprehending an image involves more than recognising what objects or entities are within it, but recognising events, relationships, and context from the image. This problem requires both sophisticated image recognition, language, world-modelling, and "image comprehension". There are several datasets in use. The illustration is from VQA, which was generated by asking Amazon Mechanical Turk workers to propose questions about photos from Microsoft's COCO image collection.</p>

</div>
</div>
</div><div>

<div>
<div>

<p>In principle, games are a sufficiently open-ended framework that all of intelligence could be captured within them. We can imagine a "ladder of games" which grow in sophistication and complexity, from simple strategy and arcade games to others which require very sophisticated language, world-modelling, vision and reasoning ability. At present, published reinforcement agents are climbing the first few rungs of this ladder.</p>
<h2 id="Abstract-Strategy-Games">Abstract Strategy Games<a href="#Abstract-Strategy-Games">¶</a>
</h2>
<p>As an easier case, abstract games like chess, go, checkers etc can be played with no knowldege of the human world or physics. Although this domain has largely been solved to super-human performance levels, there are a few ends that need to be tied up, especially in terms of having agents learn rules for arbitrary abstract games effectively given various plausible starting points (eg, textual descriptions of the rules or examples of correct play).</p>

</div>
</div>
</div><div>

<div>
<div>
<h2 id="Real-time-video-games">Real-time video games<a href="#Real-time-video-games">¶</a>
</h2>
<p>Computer and video games are a very open-ended domain. It is possible that some existing or future games could be so elaborate that they are "AI complete". In the mean time, a lot of interesting progress is likely in exploring the "ladder of games" of increasing complexity on various fronts.</p>
<h4 id="Atari-2600">Atari 2600<a href="#Atari-2600">¶</a>
</h4>
<p>Atari 2600 games have been a popular target for reinforcement learning, especially at DeepMind and OpenAI.  RL agents now play most but not all of these games better than humans.</p>
<p>In the Atari 2600 data, the label "noop" indicates the game was played with a random number, up to 30, of "no-op" moves at the beginning, while the "hs" label indicates that the starting condition was a state sampled from 100 games played by expert human players. These forms of randomisation give RL systems a diversity of game states to learn from.</p>

</div>
</div>
</div><div>

<div>
<div>
<h2 id="Music-Information-Retrieval">Music Information Retrieval<a href="#Music-Information-Retrieval">¶</a>
</h2>
<h3 id="Instrumentals-recognition">Instrumentals recognition<a href="#Instrumentals-recognition">¶</a>
</h3>
<p>Instrumentals recognition in a representative musical dataset for Instrumentals playlist generation.</p>
<ul>
<li>Experiments tested on <em>SATIN</em> database from <a href="https://www.researchgate.net/publication/317824533_SATIN_A_Persistent_Musical_Database_for_Music_Information_Retrieval">Bayle et al. (2017)</a>.</li>
<li>The ratio Instrumentals/Songs (11%/89%) of <em>SATIN</em> is representative of real uneven musical databases.</li>
<li>The human performance is at 99% of correct instrumentals detection because there are <a href="https://youtu.be/OcioBFokUnQ?t=56s">known examples of possible confusion</a>.</li>
</ul>

</div>
</div>
</div><div>

<div>
<p>Predicting the next work in a document (which is also the central algorithmic problem in text compression) is one way to see how well machine learning systems are able to model human language. Shannon's <a href="">classic 1951 paper</a> obtained an expiermental measure of human text compression performance at 0.6 - 1.3 bits per character: humans know, better than classic algorithms, what word is likely to come next in a piece of writing. More recent work (<a href="https://pdfs.semanticscholar.org/48bc/ce35ceb72068723d5f360f388a073aadadca.pdf">Moradi 1998</a>, Cover 1978) provides estimates that are text-relative and in the 1.3 bits per character (and for some texts, much higher) range.</p>
</div>
</div><div>

<div>
<div>
<p>LAMBADA is a challenging language modelling dataset in which the model has to predict a next word in a discourse, when that exact word has not occurred in the test. For instance, given a context like this:</p>
<blockquote>
<p>He shook his head, took a step back and held his hands up as he tried to smile without losing a cigarette. “Yes you can,” Julia said in a reassuring voice. “I’ve already focused on my friend. You just have to click the shutter, on top, here.”</p>
</blockquote>
<p>And a target sentence:</p>
<blockquote>
<p>He nodded sheepishly, through his cigarette away and took the _________.</p>
</blockquote>
<p>The task is to guess the target word "<strong>camera</strong>".</p>

</div>
</div>
</div><div>

<div>
<div>
<h2 id="Translation">Translation<a href="#Translation">¶</a>
</h2>
<p>Translation is a tricky problem to score, since ultimately it is human comprehension or judgement that determines whether a translation is accurate. Google for instance uses human evaluation to determine when their algorithms have improved. But that kind of measurement is expensive and difficult to replicate accurately, so automated scoring metrics are also widely used in the field. Perhaps the most common of these are <a href="https://en.wikipedia.org/wiki/BLEU">BLEU scores</a> for corpora that have extensive professional human translations, which forms the basis for the measurements included here:</p>

</div>
</div>
</div><div>

<div>
<div>

<p>Conversation is the classic AI progress measure! There is the Turing test, which involves a human judge trying to tell the difference between a humand and computer that they are chatting to online, and also easier variants of the Turing test in which the judge limits themselves to more casual, less probing conversation in various ways.</p>
<p>The Loebner Prize is an annual event that runs a somewhat easier version of the test. Since 2014, the event has also been giving standard-form tests to their entrants, and scoring the results (each question gets a plausible/semi-plausible/implausible rating). This metric is not stable, because the test questions have to change every year, they are somewhat indicative of progress. Ideally the event might apply each year's test questions to the most successful entrants from prior years. Here is an example from 2016:</p>
<p><img src="https://www.eff.org/files/ai-progress/images/loebner.png"></p>

</div>
</div>
</div><div>

<div>
<div>
<p>The Facebook BABI 20 QA dataset is an example of a basic reading comprehension task. It has been solved with large training datasets (10,000 examples per task) but not with a smaller training dataset of 1,000 examples for each of the 20 categories of tasks. It involves learning to answer simple reasoning questions like these:</p>
<p><img src="https://www.eff.org/files/ai-progress/images/babi20qa.png"></p>

</div>
</div>
</div><div>

<div>
<p>There are numerous other reading comprehension metrics that are in various ways harder than bAbi 20 QA. They are generally not solved, though progress is fairly promising.</p>
</div>
</div><div>

<div>
<div>

<p>Arguably reading and understanding scientific, technical, engineering and medical documents would be taxonomically related to general reading comprehension, but these technical tasks are probably much more difficult, and will certainly be solved with separate efforts. So we classify them separately for now. We also classify some of these problems as superintelligent, because only a tiny fraction of humans can read STEM papers, and only a miniscule fraction of humans are capable of reasonably comprehending STEM papers across a large range of fields.</p>

</div>
</div>
</div><div>

<div>
<div>
<h2 id="Answering-Science-Exam-Questions">Answering Science Exam Questions<a href="#Answering-Science-Exam-Questions">¶</a>
</h2>
<p>Science exam question answering is a multifaceted task that pushes the limits of artificial intelligence. As indicated by the example questions pictured, successful science exam QA requires natural language understanding, reasoning, situational modeling, and commonsense knowledge; a challenge problem for which information-retrieval methods alone are not sufficient to earn a "passing" grade.</p>
<p>The <a href="http://data.allenai.org/ai2-science-questions/">AI2 Science Questions</a> dataset provided by the <a href="http://allenai.org/">Allen Institute for Artificial Intelligence</a> (AI2) is a freely available collection of 5,059 real science exam questions derived from a variety of regional and state science exams. <a href="http://allenai.org/aristo/">Project Aristo</a> at AI2 is focused on the task of science question answering – the Aristo system is composed of a suite of various knowledge extraction methods, diagram processing tools, and solvers. As a reference point, the system currently achieves the following scores on these sets of non-diagram multiple choice (NDMC) and diagram multiple choice (DMC) science questions at two different grade levels. Allen Institute staff <a href="https://github.com/AI-metrics/AI-metrics/pull/60">claim these states of the art</a> for Aristo [<i>Scores are listed as "Subset (Train/Dev/Test)"</i>]:</p>
<ul>
<li>Elementary NDMC (63.2/60.2/61.3)
</li>
<li>Elementary DMC (41.8/41.3/36.3)
</li>
<li>Middle School NDMC (55.5/57.6/57.9) 
</li>
<li>Middle School DMC (38.4/35.3/34.3)
</li>
</ul>
<p><img src="https://www.eff.org/files/ai-progress/images/science_qa.jpg"></p>
<p>Another science question answering dataset that has been studied in the literature is based specifically on New York Regents 4th grade science exam tests:</p>

</div>
</div>
</div><div>

<div>
<div>

<h2 id="Generalisation-and-Transfer-Learning">Generalisation and Transfer Learning<a href="#Generalisation-and-Transfer-Learning">¶</a>
</h2>
<p>ML systems are making strong progress at solving specific problems with sufficient training data. But we know that humans are
capable of <em>transfer learning</em> -- applying things they've learned from one context, with appropriate variation, to another context.
Humans are also very general; rather than just being taught to perform specific tasks, a single agent is able to do a very
wide range of tasks, learning new things or not as required by the situation.</p>

</div>
</div>
</div><div>

<div>
<div>

<p>The notion of "safety" for AI and ML systems can encompass many things. In some cases it's about ensuring that the system meets various sorts of constraints, either in general or for specifically safety-critical purposes, such as <a href="#pedestrian%20detection">correct detection of pedestrians</a> for self driving cars.</p>

</div>
</div>
</div><div>

<div>
<div>
<h3 id="&quot;Adversarial-Examples&quot;-and-manipulation-of-ML-classifiers">"Adversarial Examples" and manipulation of ML classifiers<a href="#%22Adversarial-Examples%22-and-manipulation-of-ML-classifiers">¶</a>
</h3>
</div>
</div>
</div><div>

<div>
<div>
<h3 id="Safety-of-Reinforcement-Learning-Agents-and-similar-systems">Safety of Reinforcement Learning Agents and similar systems<a href="#Safety-of-Reinforcement-Learning-Agents-and-similar-systems">¶</a>
</h3>
</div>
</div>
</div><div>

<div>
<p>The work by <a href="https://arxiv.org/pdf/1707.05173.pdf">Saunders <em>et al.</em> (2017)</a> is an example of attempting to deal with the safe exploration problem by human-in-the-loop supervision. Without this oversight, a reinforcement learning system may engage in "reward hacking" in some Atari games. For instance in the Atari 2600 Road Runner game, an RL agent may deliberately kill itself to stay on level 1, because it can get more points on that level than it can on level 2 (particularly when it has not yet learned to master level 2). Human oversight overcomes this problem:</p>
</div>
</div><div>

<div>
<div>
<h3 id="Automated-Hacking-Systems">Automated Hacking Systems<a href="#Automated-Hacking-Systems">¶</a>
</h3>
<p>Automated tools are becoming increasingly effective both for offensive and defensive computer security purposes.</p>
<p>On the defensive side, fuzzers and static analysis tools have been used for some time by well-resourced software development teams to reduce the number of vulnerabilities in the code they ship.</p>
<p>Assisting both offense and defense, DARPA has recently started running the <a href="https://www.cybergrandchallenge.com/">Cyber Grand Challenge</a> contest to measure and improve the ability of agents to either break into systems or defend those same systems against vulnerabilities. It <a href="https://www.eff.org/deeplinks/2016/08/darpa-cgc-safety-protocol">isn't necessarily clear</a> how such initiatives would change the security of various systems.</p>
<p>This section includes some clear AI problems (like learning to find exploitable vulnerabilities in code) and some less pure AI problems, such as ensuring that defensive versions of this technology (whether in the form of fuzzers, IPSes, or other things) are deployed on all critical systems.</p>

</div>
</div>
</div><div>

<div>
<div>
<h3 id="Pedestrian-Detection">Pedestrian Detection<a href="#Pedestrian-Detection">¶</a>
</h3>
<p>Detecting pedestrians from images or video is a specific image classification problem that has received a lot of attention because of
its importance for self-driving vehicles. Many metrics in this space are based on the <a href="http://www.vision.caltech.edu/Image_Datasets/CaltechPedestrians/">Caltech pedestrians toolkit</a>, thought the <a href="http://www.cvlibs.net/datasets/kitti/eval_object.php">KITTI Vision Benchmark</a> goes beyond that to include cars and cyclists in addition to pedestrians. We may want to write scrapers for Caltech's published results and KITTI's live results table.</p>

</div>
</div>
</div><div>

<div>
<div>

<p>Biased decision making is a problem exhibited both by very simple machine learning classifiers as well as much more complicated ones. Large drivers of this problem include <a href="https://en.wikipedia.org/wiki/Omitted-variable_bias">omitted-variable bias</a>, reliance on inherently biased data sources for training data, attempts to make predictions from insufficient quantities of data, and deploying systems that create real-world incentives that change the behaviour they were measuring (see <a href="https://en.wikipedia.org/wiki/Goodhart%27s_law">Goodhart's Law</a>).</p>
<p>These problems are severe and widespread in the deployment of scoring and machine learning systems in contexts that include <a href="https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing">criminal justice</a>, <a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2388436">education policy</a>, <a href="https://www.propublica.org/article/minority-neighborhoods-higher-car-insurance-premiums-white-areas-same-risk">insurance</a> and <a href="https://arxiv.org/abs/1610.02413">lending</a>.</p>

</div>
</div>
</div><div>

<div>
<div>
<h2 id="Privacy">Privacy<a href="#Privacy">¶</a>
</h2>
<p>Many of the interesting privacy problems that will arise from AI and machine learning will come from choices about the applications of
the technology, rather than a lack of algorithmic progress within the field. But there are some exceptions, which we will track here.</p>

</div>
</div>
</div><div>

<div>
<div>
<h2 id="Problems-and-Metrics-by-category">Problems and Metrics by category<a href="#Problems-and-Metrics-by-category">¶</a>
</h2>
</div>
</div>
</div><div>

<div>
<div>

<p>This notebook is an open source, community effort. It lives on Github at <a href="https://github.com/AI-metrics/AI-metrics">https://github.com/AI-metrics/AI-metrics</a>. You can help by adding new metrics, data and problems to it! If you're feeling ambitious you can also improve its semantics or build new analyses into it. Here are some high level tips on how to do that.</p>
<h3 id="0.-The-easiest-way----just-hit-the-edit-button">0. The easiest way -- just hit the edit button<a href="#0.-The-easiest-way----just-hit-the-edit-button">¶</a>
</h3>
<p>Next to every table of results (not yet next to the graphs) you'll find an "Add/edit data on Github" link. You can just click it, and you should get a link to the Github's online editor that should make it easy to add new results, or fix existing ones, and send us a pull request. For best results, make sure you're logged in to Github</p>
<h3 id="1.-If-you're-comfortable-with-git-and-Jupyter-Notebooks,-or-are-happy-to-learn">1. If you're comfortable with git and Jupyter Notebooks, or are happy to learn<a href="#1.-If-you're-comfortable-with-git-and-Jupyter-Notebooks,-or-are-happy-to-learn">¶</a>
</h3>
<p>If you're interested in making more extensive changes to the Notebook, and you've already worked a lot with <code>git</code> and IPython/Jupyter Notebooks, you can run and edit copy locally.  This is a fairly involved process (Jupyter Notebook and git interact in a somewhat complicated way) but here's a quick list of things that should mostly work:</p>
<ol>
<li>Install <a href="https://jupyter.readthedocs.io/en/latest/install.html">Jupyter Notebook</a> and <a href="https://git-scm.com/book/en/v2/Getting-Started-Installing-Git">git</a>.<ul>
<li>On an Ubuntu or Debian system, you can do: <br><pre>sudo apt-get install git
sudo apt-get install ipython-notebook || sudo apt-get install jupyter-notebook || sudo apt-get install python-notebook</pre>
</li>
<li>Make sure you have IPython Notebook version 3 or higher. If your OS
doesn't provide it, you might need to enable backports, or use <code>pip</code> to
install it.</li>
</ul>
</li>
<li>Install this notebook's Python dependencies:<br><ul>
<li>On Ubuntu or Debian, do: <br><pre>    sudo apt-get install python-{cssselect,lxml,matplotlib{,-venn},numpy,requests,seaborn}</pre>
</li>
<li>On other systems, use your native OS packages, or use <code>pip</code>: <br><pre>    pip install cssselect lxml matplotlib{,-venn} numpy requests seaborn</pre>
</li>
</ul>
</li>
<li>Fork our repo on
github: <a href="https://github.com/AI-metrics/AI-metrics#fork-destination-box">https://github.com/AI-metrics/AI-metrics#fork-destination-box</a>
</li>
<li>
<a href="https://help.github.com/articles/cloning-a-repository/">Clone</a> the repo on your machine, and <code>cd</code> into the directory it's using</li>
<li>Configure your copy of git to use <a href="http://pascalbugnion.net/blog/ipython-notebooks-and-git.html">IPython Notebook merge filters</a> to prevent conflicts when multiple people edit the Notebook simultaneously. You can do that with these two commands in the cloned repo:<pre>git config --file .gitconfig filter.clean_ipynb.clean $PWD/ipynb_drop_output</pre>
 <pre>git config --file .gitconfig filter.clean_ipynb.smudge cat</pre>
</li>
<li>
<p>Run Jupyter Notebok in the project directory (the command may be <code>ipython notebook</code>, <code>jupyter notebook</code>, <code>jupyter-notebook</code>, or <code>python notebook</code> depending on your system), then go to <a href="http://localhost:8888/">localhost:8888</a> and edit the Notebook to your heart's content</p>
</li>
<li>
<p>Save and commit your work (<code>git commit -a -m "DESCRIPTION OF WHAT YOU CHANGED"</code>)</p>
</li>
<li>Push it to your remote repo</li>
<li>Send us a pull request!</li>
</ol>
<h2 id="Notes-on-importing-data">Notes on importing data<a href="#Notes-on-importing-data">¶</a>
</h2>
<ul>
<li>Each <code>.measure()</code> call is a data point of a specific algorithm on a specific metric/dataset. Thus one paper will often produce multiple measurements on multiple metrics. It's most important to enter results that were at or near the frontier of best performance on the date they were published. This isn't a strict requirement, though; it's nice to have a sense of the performance of the field, or of algorithms that are otherwise notable even if they aren't the frontier for a sepcific problem.</li>
<li>When multiple revisions of a paper (typically on arXiv) have the same results on some metric, use the date of the first version (the CBTest results in <a href="https://arxiv.org/abs/1606.02245v4">this paper</a> are an example)</li>
<li>When subsequent revisions of a paper improve on the original results (<a href="https://arxiv.org/abs/1606.01549v3">example</a>), use the date and scores of the first results, or if each revision is interesting / on the frontier of best performance, include each paper<ul>
<li>We didn't check this carefully for our first ~100 measurement data points :(. In order to denote when we've checked which revision of an arXiv preprint first published a result, cite the specific version (<a href="https://arxiv.org/abs/1606.01549v3">https://arxiv.org/abs/1606.01549v3</a> rather than <a href="https://arxiv.org/abs/1606.01549">https://arxiv.org/abs/1606.01549</a>). That way, we can see which previous entries should be double-checked for this form of inaccuracy.</li>
</ul>
</li>
<li>Where possible, use a clear short name or acronym for each algorithm. The full paper name can go in the <code>papername</code> field (and is auto-populated for some papers). When matplotlib 2.1 ships we may be able to get nice <a href="https://github.com/matplotlib/matplotlib/pull/5754">rollovers</a> with metadata like this. Or perhaps we can switch to D3 to get that type of interactivity.</li>
</ul>
<h2 id="What-to-work-on">What to work on<a href="#What-to-work-on">¶</a>
</h2>
<ul>
<li>If you know of ML datasets/metrics that aren't included yet, add them</li>
<li>If there are papers with interesting results for metrics that aren't included, add them</li>
<li>If you know of important problems that humans can solve, and machine learning systems may or may not yet be able to, and they're missing from our taxonomy, you can propose them</li>
<li>Look at our <a href="https://github.com/AI-metrics/master_text">Github issue list</a>, perhaps starting with those tagged as <a href="https://github.com/AI-metrics/master_text/issues?q=is%3Aissue+is%3Aopen+label%3A%22Good+volunteer+task%22">good volunteer tasks</a>.</li>
</ul>

</div>
</div>
</div><div>

<div>
<div>
<h2 id="License">License<a href="#License">¶</a>
</h2>
<div><p><a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/"><img alt="Creative Commons License" src="https://www.eff.org/files/ai-progress/images/cc-by-sa.png"></a></p><p>Much of this Notebook is uncopyrightable data. The copyrightable portions of this Notebook that are written by EFF and other Github contributors are licensed under the <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">Creative Commons Attribution-ShareAlike 4.0 International License</a>. Illustrations from datasets and text written by other parties remain copyrighted by their respective owners, if any, and may be subject to different licenses.</p></div>
<p>The source code is also dual-licensed under the <a href="https://www.gnu.org/licenses/old-licenses/gpl-2.0.en.html">GNU General Public License, version 2 or greater</a>.</p>
<h3 id="How-to-cite-this-document">How to cite this document<a href="#How-to-cite-this-document">¶</a>
</h3>
<p>In academic contexts, you can cite this document as:</p>
<p><tt>Peter Eckersley, Yomna Nasser <em>et al.</em>, EFF AI Progress Measurement Project, (2017-) https://eff.org/ai/metrics, accessed on 2017-09-09</tt>,</p>
<p>or the equivalent in the bibliographic format you are working in.</p>
<p>If you would like to deep-link an exact version of the text of the Notebook for archival or historical purposes, you can do that using the <a href="https://web.archive.org/web/*/www.eff.org/ai/metrics">Internet Archive</a> or <a href="https://github.com/AI-metrics/AI-metrics/commits/master">Github</a>. In addition to keeping a record of changes, Github will render a specific version of the Notebook using URLs like this one: <a href="https://github.com/AI-metrics/AI-metrics/blob/008993c84188094ba804882f65815c7e1cfc4d0e/AI-progress-metrics.ipynb">https://github.com/AI-metrics/AI-metrics/blob/008993c84188094ba804882f65815c7e1cfc4d0e/AI-progress-metrics.ipynb</a></p>

</div>
</div>
</div></div></div>
    </div>
    <footer>
        <div>created by buildstarted &copy; 2020 <a href="/about">about</a></div>
        <div>Share this page on social media: copy and paste this url https://linksfor.dev/</div>
        <div>If you prefer RSS: <a href="https://linksfor.dev/feed.xml">https://linksfor.dev/feed.xml</a></div>
        <div>Customer satisfaction guaranteed to be optional.</div>
    </footer>
    
    <script async defer>
        _dna = window._dna || {};
        _dna.siteId = "linksfor.devs";
        _dna.outlink = true;

        (function () {
            let dna = document.createElement('script');
            dna.type = 'text/javascript';
            dna.async = true;
            dna.src = '//dna.buildstarted.com/t.js';
            let s = document.getElementsByTagName('script')[0];
            s.parentNode.insertBefore(dna, s);
        })();
    </script>
    <noscript><img src="//dna.buildstarted.com/g?siteId=linksfor.devs" /></noscript>
</body>
</html>