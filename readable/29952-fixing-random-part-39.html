<!DOCTYPE html>
<html lang="en">
<head>
    <title>
Fixing Random, part&#xA0;39 -
linksfor.dev(s)
    </title>
    <meta charset="utf-8">
    <meta name="Description" content="A curated source of links that devs might find interesting. Updated around the clock." />
    <meta name="google" value="notranslate">
    <style type="text/css">
        html {
            font-family: sans-serif;
            line-height: 1.15;
            -webkit-text-size-adjust: 100%;
            -webkit-tap-highlight-color: transparent;
            height: 100%;
        }

        *, ::after, ::before {
            box-sizing: border-box;
        }

        body {
            margin: 0;
            font-family: -apple-system,BlinkMacSystemFont,"Segoe UI",Roboto,"Helvetica Neue",Arial,"Noto Sans",sans-serif,"Apple Color Emoji","Segoe UI Emoji","Segoe UI Symbol","Noto Color Emoji";
            font-size: 1rem;
            font-weight: 400;
            line-height: 1.5;
            color: #60656a;
            text-align: left;
            background-color: #323b44;
        }

        h1 {
            font-size: 6rem;
            font-weight: 300;
            line-height: 1.2;
            margin-top: 0;
            margin-bottom: 0.5rem;
            margin-bottom: 0.5rem
        }

        a {
            color: #007bff;
            color: #ccc;
            text-decoration: none;
            background-color: transparent;
            word-break: break-all;
        }

        .unseen a {
            font-weight: bold;
        }

        h3 {
            margin-top: 0;
            padding-top: 0;
            font-weight: normal;
        }

        .grid {
            -ms-flex-direction: column;
            flex-direction: column;
            width: 1024px;
            margin: 0 auto;
            flex: 1 0 auto;
        }

        .row {
            -ms-flex-direction: row;
            flex-direction: row;
            width: 100%;
            -ms-flex-wrap: wrap;
            flex-wrap: wrap;
            display: -ms-flexbox;
            display: flex;
        }

        .col {
            margin: 0 10px 0 10px;
            box-sizing: border-box;
            vertical-align: top;
        }

        .col-3-of-4, .col-6-of-8, .col-9-of-12 {
            width: calc(75% - 20px);
        }

        .col-1-of-4, .col-2-of-8, .col-3-of-12 {
            width: calc(25% - 20px);
        }

        @media (max-width:1023px) {
            /* big landscape tablets, laptops, and desktops */
            body {
                overflow-x: hidden;
            }

            main {
                width: 99%;
            }

            h1 {
                font-size: 50px;
            }
        }

        .text-right {
            text-align: right;
        }

        footer {
            left: 0;
            width: 100%;
            margin-top: 2em;
            padding: 50px 0;
            text-align: center;
            -moz-box-sizing: border-box;
            -webkit-box-sizing: border-box;
            box-sizing: border-box;
        }

        .readable {
            color: #949ba2;
        }

        svg:not(:root).svg-inline--fa {
            color: #60656a;
            overflow: visible;
        }

        .svg-inline--fa.fa-w-12 {
            width: 0.75em;
        }

        svg:not(:root) {
            overflow: hidden;
        }

        .svg-inline--fa {
            display: inline-block;
            font-size: inherit;
            height: 1em;
            overflow: visible;
            vertical-align: -0.125em;
        }

        img {
            max-width: 100%;
        }

        .text-center {
            text-align: center;
        }

        .readable h1 {
            font-size: 2em;
        }
    </style>
</head>
<body>
    <div class="grid">
        
<div class="readable">
    <h1>Fixing Random, part&#xA0;39</h1>
    <div class="entry-content"> <p>Let&#x2019;s sum up the last few episodes:</p>
<p>Suppose we have a distribution of doubles, <code>p</code>, and a function <code>f</code>&#xA0;from double to double. We often want to answer the question <em>&#x201C;what is the average value of <code>f</code>&#xA0;when it is given samples from <code>p</code>?&#x201D;</em> This quantity is called the <strong>expected value</strong>.</p>
<p>The obvious (or &#x201C;naive&#x201D;) way to do it is: take a bunch of samples, evaluate the function on those samples, take the average. Easy! However, this can lead to problems if there are &#x201C;black swans&#x201D;: values that are rarely sampled, but massively affect the value of the average when run through <code>f</code>. We would like to get a good estimate without having to massively increase the number of samples in our average.</p>
<p>We developed two techniques to estimate the expected value:</p>
<p>First, abandon sampling entirely and do numerical integral calculus:</p>
<ul>
<li>Use quadrature to compute two areas: the area under <code>f(x)*p.Weight(x)</code>&#xA0; and the area under <code>p.Weight(x)</code>&#xA0;(which is the normalization constant of <code>p</code>)</li>
<li>Their quotient is an extremely accurate estimate of the expected value</li>
<li>But we have to know what region to do quadrature over.</li>
</ul>
<p>Second, use importance sampling:</p>
<ul>
<li>Find a helper distribution <code>q</code> whose weight is large where <code>f(x)*p.Weight(x)</code>&#xA0;bounds a lot of area.</li>
<li>Use the naive algorithm to estimate the expected value of&#xA0;&#xA0;<code>x=&gt;f(x)*p.Weight(x)/q.Weight(x)</code>from samples of <code>q</code></li>
<li>That is <em>proportional</em> to the expected value of <code>f</code>&#xA0;with respect to <code>p</code></li>
<li>We gave a technique for estimating the proportionality constant by sampling from <code>q</code>&#xA0;also.</li>
</ul>
<p>The problem with importance sampling then is finding a good <code>q</code>. We discussed some techniques:</p>
<ul>
<li>If you know the range, just use a uniform distribution over that range.</li>
<li>Stretch and shift <code>p</code>&#xA0;so that the transformed PDF doesn&#x2019;t have a &#x201C;black swan&#x201D;, but the normalization constant is the same.</li>
<li>Use the Metropolis algorithm to generate a helper PDF from <code>Abs(f*p)</code>, though in my experiments this worked poorly</li>
<li>If we know the range of interest, we can use the VEGAS algorithm. It makes cheap, naive estimates of the area of subranges, and then uses that information to gradually refine a piecewise-uniform helper PDF that targets spikes and avoid flat areas of <code>f*p</code>.</li>
<li>However, the VEGAS algorithm is complex, and I did not attempt to implement it for this series.</li>
</ul>
<p>The question you may have been asking yourself these past few episodes is:</p>
<p><strong>If quadrature is an accurate and cheap way to estimate the expected value of <code>f</code>&#xA0;over samples from <code>p</code> then why are we even considering doing sampling at all? Surely we typically know at least approximately the range over which <code>f*p</code>&#xA0;has some area. What&#x2019;s the point of all this?</strong></p>
<p>Quadrature just splits up the range into some number &#x2014; say, a thousand &#x2014; equally-sized pieces, evaluates&#xA0;<code>f*p</code>&#xA0;on each of them, and takes the average. That sure seems cheaper and easier than all this mucking around with sampling. Have I just been wasting your time these past few episodes? And why has there been so much research and effort put into finding techniques for estimating expected value?</p>
<p>This series is called &#x201C;Fixing Random&#x201D; because the built-in base class library tools we have in C# for representing probabilities are weak. I&#x2019;ve approached everything in this series from the perspective of <em>&#x201C;I want to have an object that represents probabilities in my business domain, and I want to use that object to solve my business problems&#x201D;.</em></p>
<p><em>&#x201C;What is the expected value of this function given this distribution?&#x201D;</em> is a very natural question to ask when solving business problems that involve probabilities, and as we&#x2019;ve seen, you can answer that question by simulating integral calculus through quadrature.</p>
<p>But, as I keep on saying: <strong>things equal to the same are equal to each other.</strong> Flip the script. Suppose our business domain involves&#xA0;<em>solving integral&#xA0;calculus problems</em>. And suppose there is an integral calculus problem that we <em>cannot</em>&#xA0;efficiently solve with quadrature. What do we do?</p>
<ul>
<li>We can solve expected value problems with integral calculus techniques such as quadrature.</li>
<li>We can solve expected value problems with sampling techniques</li>
<li>Things equal to the same are equal to each other.</li>
<li>Therefore <strong>we can solve integral calculus problems with sampling techniques.&#xA0;</strong></li>
</ul>
<p>That is why there has been so much research into computing expected values: the expected value is the <em>area</em> under the function <code>f(x)*p.Weight(x)</code>&#xA0;so <strong>if we can compute the expected value by sampling, then we can compute that area</strong> and solve the integral calculus problem <em>without</em> doing quadrature!</p>
<p>I said above <em>&#x201C;if quadrature is accurate and cheap&#x201D;</em>, but there are<em> many</em> scenarios in which quadrature is not a cheap way to compute an area.</p>
<p>What&#x2019;s an example? Well, let&#x2019;s generalize. So far in this series I&#x2019;ve assumed that <code>f</code>&#xA0;is a <code>Func&lt;double, double&gt;</code>&#xA0;. What if <code>f</code>&#xA0;is a <code>Func&lt;double, double, double&gt;</code>&#xA0;&#x2014; a function from pairs of doubles to double. That is <code>f</code>&#xA0;is not a line in two dimensions, it is a surface in three.</p>
<p><span>Let&#x2019;s suppose we have </span><code>f</code><span>&#xA0;being such a function, and we would like to solve a calculus problem: what is the volume under </span><code>f</code><span>&#xA0;on the range (0,0) to (1, 1)?</span></p>
<p>We could do it by quadrature, but remember, in my example we split up the range 0-to-1 into a thousand points. If we do quadrature in two dimensions with the same granularity of 0.001, that&#x2019;s a million points we have to evaluate and sum. If we only have computational resources to do a thousand points, then we have to have a granularity of around 0.03.</p>
<p>What if the function is zero at most of those points? We could then have a really crappy estimate of the total area because our granularity is so low.</p>
<p>We now reason as follows: take a two-dimensional probability distribution. Let&#x2019;s say we have the <em>standard continuous uniform implementation</em> of&#xA0;&#xA0;<code>IWeightedDistribution&lt;(double, double)&gt;</code>&#xA0;.</p>
<p>All the techniques I have explored in this series work equally well in two dimensions as one! So we can use those techniques. Let&#x2019;s do so:</p>
<ul>
<li>What is the estimated value of <code>f</code>&#xA0;when applied to samples from this distribution?</li>
<li>It is equal to the volume under <code>f(x,y)*p.Weight((x,y)).&#xA0;</code></li>
<li><code></code>But&#xA0;<code>p.Weight((x,y))</code><span>&#xA0;is always 1.0 on the region we care about; it&#x2019;s the standard continuous uniform distribution, after all.</span></li>
<li>Therefore<strong> the estimated expected value of <code>f</code>&#xA0;when evaluated on samples from <code>p</code>&#xA0;is an estimate of the volume we care about.</strong></li>
</ul>
<p>How does that help?</p>
<p>It doesn&#x2019;t.</p>
<p>If we&#x2019;re taking a thousand points by quadrature or a thousand points by sampling from a uniform distribution over the same range, it doesn&#x2019;t matter. We&#x2019;re still computing a value at a thousand points and taking an average.</p>
<p>But now here&#x2019;s the trick.</p>
<p>Suppose we can find a <em>helper</em> distribution <code>q</code> that is large where <code>f(x,y)</code>&#xA0;has a lot of volume and very small where it has little volume.</p>
<p>We can then use importance sampling to compute a more accurate estimate of the desired expected value, and therefore the desired volume, because most of the points we sample from <code>q</code>&#xA0;are in high-volume regions. Our thousand points from <code>q</code>&#xA0;will give us a better estimate!</p>
<p>Now, up the dimensionality further. Suppose we&#x2019;ve got a function that takes <em>three</em> doubles and goes to double, and we wish to know its hypervolume over (0, 0, 0) to (1, 1, 1).</p>
<p>With quadrature, we&#x2019;re either doing a billion computations at a granularity of 0.001, or, if we can only afford to do a thousand evaluations, that&#x2019;s a granularity of 0.1.</p>
<p><strong>Every time we add a dimension, either the cost of our quadrature goes up by a factor of a thousand, or the cost stays the same but the granularity is enormously coarsened.</strong></p>
<p>Oh, but it gets worse.</p>
<p>When you are evaluating the hypervolume of a 3-d surface embedded in 4 dimensions, there are a <em>lot</em> more points where the function can be zero! There is just so much <em>room</em> in high dimensions for stuff to be. <strong>The higher the dimensionality gets, the more important it is that you find the spikes and avoid the flats.&#xA0;</strong></p> <p><strong>Exercise:</strong> Consider an n-dimensional cube of side 1. That thing always has a hypervolume of 1, no matter what n is.</p>
<p>Now consider a concentric n-dimensional cube inside it where the sides are 0.9 long.</p>
<ul>
<li>For a 1-dimensional cube &#x2014; a line &#x2014; the inner line is 90% of the length of the outer line, so we&#x2019;ll say that 10% of the length of the outer line is &#x201C;close to the surface&#x201D;.</li>
<li>For a 2-dimensional cube &#x2014; a square &#x2014; the inner square has 81% of the area of the outer square, so 19% of the area of the outer square is &#x201C;close to the surface&#x201D;.</li>
</ul>
<p>At what dimensionality is more than 50% of the hypervolume of the outer hypercube &#x201C;close to the surface&#x201D;?</p>
<p><strong>Exercise:</strong> Now consider an n-dimensional cube of side 1 again, and the concentric n-dimensional sphere. That is, a circle that exactly fits inside a square, a sphere that exactly fits inside a cube, and so on. The radius is 1/2.</p>
<ul>
<li>The area of the circle is pi/4 = 79% of the area of the square.</li>
<li>The volume of the sphere is pi/6 = 52% of the volume of the cube.</li>
<li>&#x2026; and so on</li>
</ul>
<p>At what value for n does the volume of the hypersphere become 1% of the volume of the hypercube?</p> <p>In high dimensions, <em>any</em> shape that is <em>anywhere</em> on the <em>interior</em> of a hypercube is <em>tiny</em> when compared to the massive hypervolume near the cube&#x2019;s <em>surface</em>!</p>
<p>That means: if you&#x2019;re trying to determine the hypervolume bounded by a function that has large values somewhere <em>inside</em> a hypercube, the samples <em>must</em>&#xA0;frequently hit that important region where the values are big. If you spend time &#x201C;near the edges&#x201D; where the values are small, you&#x2019;ll spend &gt;90% of your time sampling irrelevant values.</p>
<p>That&#x2019;s why importance sampling is so useful, and why we spend so much effort studying how to find distributions that compute expected values.<strong> Importance sampling allows us to numerically solve multidimensional integral calculus problems with reasonable compute resources.</strong></p> <p><strong>Aside</strong>: Now you know why I said earlier that I misled you when I said that the VEGAS algorithm was designed to find helpful distributions for importance sampling. The VEGAS algorithm absolutely does that, but that&#x2019;s not what it was <em>designed</em> to do; <em>it was designed to solve multidimensional integral calculus problems.</em> Finding good helper distributions is how it does its job.</p> <p><strong>Exercise:</strong> Perhaps you can see how we would extend the algorithms we&#x2019;ve implemented on distributions of doubles to distributions of tuples of doubles; I&#x2019;m not going to do that in this series; give it a shot and see how it goes!</p> <p><strong>Next time on FAIC:</strong> This has been one of the longest blog series I&#x2019;ve done, and looking back over the last sixteen years, I have never actually <em>completed</em> any of the really big projects I started: building a script engine, building a Zork implementation, explaining Milner&#x2019;s paper, and so on. I&#x2019;m going to complete this one!</p>
<p>There is so much more to say on this topic; people spend their careers studying this stuff. But I&#x2019;m going to wrap it up in the next couple of episodes by giving some final thoughts, a summary of the work we&#x2019;ve done, a list of some of the topics I did not cover that I&#x2019;d hoped to, and a partial bibliography of the papers and other resources that I read when doing this series.</p> </div>
</div>
    </div>
    <footer>
        <div>created by buildstarted &copy; 2019 <a href="/about">about</a></div>
        <div>Share this page on social media: copy and paste this url https://linksfor.dev/</div>
    </footer>
    
</body>
</html>