<!DOCTYPE html>
<html lang="en">
<head>
    <title>
May 2020 news &#xB7; Gwern.net - linksfor.dev(s)    </title>
    <meta charset="utf-8">
    <link rel="alternate" type="application/rss+xml" title="Linksfor.dev(s) feed" href="https://linksfor.dev/feed.rss" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta name="google" value="notranslate">
    <link rel="stylesheet" type="text/css" href="/style.css" />
    <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon">
    <link rel="icon" href="/favicon.ico" type="image/x-icon">
    <meta property="og:title" content="May 2020 news &#xB7; Gwern.net - linksfor.dev(s)"/>
    <meta property="article:author" content="Gwern Branwen"/>
    <meta property="og:description" content="May 2020 gwern.net newsletter with anime GAN updates, links on AI scaling, discussion of GPT-3, and 1 book review."/>
    <meta property="og:type" content="website"/>
    <meta property="og:url" content="https://www.gwern.net/newsletter/2020/05#gpt-3"/>

<meta property="og:site_name" content="linksfor.dev(s)" />
</head>
<body>
    <div class="grid">
        <h1>
<a href="/" style="color:inherit">linksfor.dev(s)</a>
        </h1>
        <title>linksfor.dev(s) - May 2020 news &#xB7; Gwern.net</title>
<div class="readable">
        <h1>May 2020 news &#xB7; Gwern.net</h1>
            <div>by Gwern Branwen</div>
            <div>Reading time: 23-29 minutes</div>
        <div>Posted here: 13 Jun 2020</div>
        <p><a href="https://www.gwern.net/newsletter/2020/05#gpt-3">https://www.gwern.net/newsletter/2020/05#gpt-3</a></p>
        <hr/>
<div id="readability-page-1" class="page"><div id="markdownBody"><p>This is the <a href="https://www.gwern.net/newsletter/2020/05" data-popup-title="May 2020 news" data-popup-author="Gwern Branwen" data-popup-date="26 Dec 2019" data-popup-abstract="May 2020 gwern.net newsletter with anime <span class=&quot;smallcaps-auto&quot;>GAN</span> updates, links on AI scaling, discussion of <span class=&quot;smallcaps-auto&quot;>GPT</span>-3, and 1 book review.">May 2020</a> edition of <a href="https://gwern.substack.com/" data-popup-title="Gwern.net newsletter (Substack subscription page)" data-popup-author="Gwern Branwen" data-popup-date="2013-12-01" data-popup-abstract="Subscription page for the monthly gwern.net newsletter. There are monthly updates, which will include summaries of projects I've worked on that month (the same as the <a href=&quot;/Changelog&quot;>changelog</a>), collations of links or discussions from <a href=&quot;https://old.reddit.com/r/gwern/&quot;>my subreddit</a>, and book/movie reviews. You can also browse <a href=&quot;/tags/newsletter&quot;>the archives since December 2013</a>.">the <code>gwern.net</code> newsletter</a>; previous, <a href="https://www.gwern.net/newsletter/2020/04" data-popup-title="April 2020 news" data-popup-author="Gwern Branwen" data-popup-date="26 Dec 2019" data-popup-abstract="April 2020 gwern.net newsletter with links on music generation, data augmentation, terrorism, Dormin, and 1 documentary review.">April 2020</a> (<a href="https://www.gwern.net/tags/newsletter" data-popup-title="Gwern.net newsletter archives" data-popup-author="Gwern Branwen" data-popup-date="2013-12-01" data-popup-abstract="Newsletter tag: archive of all issues back to 2013 for the gwern.net newsletter (monthly updates, which will include summaries of projects I've worked on that month (the same as the <a href=&quot;/Changelog&quot;>changelog</a>), collations of links or discussions from <a href=&quot;https://old.reddit.com/r/gwern/&quot;>my subreddit</a>, and book/movie reviews.)">archives</a>). This is a summary of the revision-history <span>RSS</span> feed, overlapping with my <a href="https://www.gwern.net/Changelog" data-popup-title="Changelog" data-popup-author="Gwern Branwen" data-popup-date="15 Sep 2013" data-popup-abstract="<p>This page is a changelog for <code>gwern.net</code>: a monthly reverse chronological list of recent major writings/changes/additions.</p><p>Following my writing can be a little difficult because it is often so incremental. So every month, in addition to my regular <a href=&quot;https://www.reddit.com/r/gwern/&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;/r/gwern subreddit&quot; data-popup-author=&quot;Gwern Branwen&quot; data-popup-date=&quot;2018-10-01&quot; data-popup-abstract=&quot;A subreddit for posting links of interest and also for announcing updates to gwern.net (which can be &amp;lt;a href=&amp;quot;https://www.reddit.com/r/gwern/.rss&amp;quot;&amp;gt;used as a <span class=&quot;smallcaps-auto&quot;>RSS</span> feed&amp;lt;/a&amp;gt;). Submissions are categorized similar to the monthly newsletter and typically will be collated there.&quot;>/r/Gwern</a> subreddit submissions, I write up reasonably-interesting changes and <a href=&quot;https://gwern.substack.com/&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;Gwern.net newsletter (TinyLetter subscription page)&quot; data-popup-author=&quot;Gwern Branwen&quot; data-popup-date=&quot;2013-12-01&quot; data-popup-abstract=&quot;Subscription page for the monthly gwern.net newsletter. There are monthly updates, which will include summaries of projects I&amp;#39;ve worked on that month (the same as the &amp;lt;a href=&amp;quot;https://www.gwern.net/Changelog&amp;quot;&amp;gt;changelog&amp;lt;/a&amp;gt;), collations of links or discussions from &amp;lt;a href=&amp;quot;https://www.reddit.com/r/gwern/&amp;quot;&amp;gt;my subreddit&amp;lt;/a&amp;gt;, and book/movie reviews. You can also browse &amp;lt;a href=&amp;quot;https://www.gwern.net/tags/newsletter&amp;quot;&amp;gt;the archives since December 2013&amp;lt;/a&amp;gt;.&quot;>send it out to the mailing list</a> in addition to a compilation of links &amp;amp; reviews (<a href=&quot;./tags/newsletter&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;Gwern.net newsletter archives&quot; data-popup-author=&quot;Gwern Branwen&quot; data-popup-date=&quot;2013-12-01&quot; data-popup-abstract=&quot;Newsletter tag: archive of all issues back to 2013 for the gwern.net newsletter (monthly updates, which will include summaries of projects I&amp;#39;ve worked on that month (the same as the &amp;lt;a href=&amp;quot;https://www.gwern.net/Changelog&amp;quot;&amp;gt;changelog&amp;lt;/a&amp;gt;), collations of links or discussions from &amp;lt;a href=&amp;quot;https://www.reddit.com/r/gwern/&amp;quot;&amp;gt;my subreddit&amp;lt;/a&amp;gt;, and book/movie reviews.)&quot;>archives</a>).</p>">Changelog</a> &amp; <a href="https://old.reddit.com/r/gwern/" data-popup-title="/r/gwern subreddit" data-popup-author="Gwern Branwen" data-popup-date="2018-10-01" data-popup-abstract="A subreddit for posting links of interest and also for announcing updates to gwern.net (which can be <a href=&quot;https://old.reddit.com/r/gwern/.rss&quot;>used as a <span class=&quot;smallcaps-auto&quot;>RSS</span> feed</a>). Submissions are categorized similar to the monthly newsletter and typically will be collated there.">/r/​gwern</a>; brought to you by my donors on <a href="https://www.patreon.com/gwern">Patreon</a>.</p>
<section id="writings">

<ul>
<li><p>Ganbooru prototype: released <a href="https://www.gwern.net/Faces#danbooru2019e621-256px-biggan" data-popup-title="Danbooru2019+e621 256px BigGAN Model Release" data-popup-author="Tensorfork (Gwern Branwen, Shawn Presser et al)" data-popup-date="2020-05-28" data-popup-abstract="<p>Release of a 256px Big<span class=&quot;smallcaps-auto&quot;>GAN</span> model trained on Danbooru2019 &amp;amp; e621. This is a prototype model testing our ability to train a Big<span class=&quot;smallcaps-auto&quot;>GAN</span> stably for hundreds of thousands of iterations on a <span class=&quot;smallcaps-auto&quot;>TPU</span>-256 pod on 3 million+ anime/illustration images. While the generated samples are far from ‘photorealistic’, they serve as proof of concept that—unlike our failed Style<span class=&quot;smallcaps-auto&quot;>GAN</span> 2 scaling experiments—Big<span class=&quot;smallcaps-auto&quot;>GAN</span> can successfully model anime images with great generality, and that we can potentially scale up to 512px or even 1024px and match the DeepMind ImageNet Big<span class=&quot;smallcaps-auto&quot;>GAN</span> for quality.</p><p><figure><img src=&quot;/images/gan/2020-05-15-biggan-256px-danbooruplus-run39-90randomemasamples.jpg&quot; alt=&quot;&quot; /><figcaption>90 random <span class=&quot;smallcaps-auto&quot;>EMA</span> samples (untruncated) from the 256px Big<span class=&quot;smallcaps-auto&quot;>GAN</span> trained on Danbooru2019/anime-portraits/e621/e621-portraits.</figcaption></figure></p>">256px Big<span>GAN</span> trained on Danbooru2019</a>; <a href="https://www.gwern.net/Crops#danbooru2019-figures" data-popup-title="Danbooru2019 Figures: A Large-Scale Anime Character Illustration Dataset" data-popup-author="Gwern Branwen" data-popup-date="2020-05-31" data-popup-abstract="<p>The <a href=&quot;/Danbooru2019&quot;>Danbooru2019</a> Figures dataset is a large-scale character anime illustration dataset of <em>n</em>=855,880 images (248GB; minimum width 512px) cropped from Danbooru2019 using the <a href=&quot;https://github.com/jerryli27/AniSeg/&quot;>AniSeg</a> anime character detection model. The images are cropped to focus on a single character’s entire visible body, extending ‘portrait’ crops to ‘figure’ crops. This is useful for tasks focusing on individual characters, such as character classification or for generative tasks (a corpus for weak models like Style<span class=&quot;smallcaps-auto&quot;>GAN</span>, or data augmentation for Big<span class=&quot;smallcaps-auto&quot;>GAN</span>).</p><p><figure><img src=&quot;/images/gan/2020-05-30-danbooru2019-figures-randomsamples-40.jpg&quot; alt=&quot;&quot; /><figcaption>40 random figure crops from Danbooru2019 (4✕10 grid, resized to 256px)</figcaption></figure></p>">Danbooru2019 Figures dataset</a></p></li>
<li><p><code>gwern.net</code>:</p>
<ul>
<li>experimental <code>&lt;srcset&gt;</code> <a href="https://groups.google.com/d/msg/hakyll/aFH9LHKyDZ8/-zY0SHdUBAAJ">mobile image optimization</a></li>
<li><a href="https://www.gwern.net/static/js/popups.js" data-popup-title="<code>popups.js</code>" data-popup-author="Said Achmiz" data-popup-date="2019-08-21" data-popup-abstract="<p><code>popups.js</code>: standalone Javascript library for creating 'popups' which display link metadata (typically, title/author/date/summary), for extremely convenient reference/abstract reading, with mobile and YouTube support. Whenever any such link is mouse-overed by the user, popups.js will pop up a large tooltip-like square with the contents of the attributes. This is particularly intended for references, where it is extremely convenient to autopopulate links such as to Arxiv.org/Biorxiv.org/Pubmed/<span class=&quot;smallcaps-auto&quot;>PLOS</span>/gwern.net/Wikipedia with the link's title/author/date/abstract, so the reader can see it instantly.</p><p><code>popups.js</code> parses a <span class=&quot;smallcaps-auto&quot;>HTML</span> document and looks for <code>&amp;lt;a&amp;gt;</code> links which have the <code>docMetadata</code> attribute class, and the attributes <code>data-popup-title</code>, <code>data-popup-author</code>, <code>data-popup-date</code>, <code>data-popup-doi</code>, <code>data-popup-abstract</code>. (These attributes are expected to be populated already by the <span class=&quot;smallcaps-auto&quot;>HTML</span> document's compiler, however, they can also be done dynamically. See <a href=&quot;https://share.obormot.net/misc/gwern/wikipedia-popups.js&quot;><code>wikipedia-popups.js</code></a> for an example of a library which does Wikipedia-only dynamically on page loads.)</p><p>For an example of a Hakyll library which generates annotations for Wikipedia/Biorxiv/Arxiv/<span class=&quot;smallcaps-auto&quot;>PDF</span>s/arbitrarily-defined links, see <a href=&quot;/LinkMetadata.hs&quot;><code>LinkMetadata.hs</code></a>; for a live demonstration, see the links in <a href=&quot;/newsletter/2019/07&quot;>the July 2019 newsletter</a>.</p>"><code>popups.js</code></a>: +support for reverse-footnote popups</li>
</ul>
<div>
<p>
Mailing List Switch
</p><p>
The newsletter moved this month to <a href="https://substack.com/">Substack</a> due to reaching the TinyLetter 5000-subscriber limit. Please let me know of any issues beyond the known issue of length truncation. (Note that reading the website version on desktop is the recommended way for annotations etc.)
</p></div></li>
</ul>
</section>
<section id="gpt-3">

<blockquote>
<p>On <a href="https://arxiv.org/abs/2005.14165#openai" data-popup-title="Language Models are Few-Shot Learners" data-popup-author="Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Dan et al [...]" data-popup-date="2020-05-28" data-popup-abstract="Recent work has demonstrated substantial gains on many <span class=&quot;smallcaps-auto&quot;>NLP</span> tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions—something which current <span class=&quot;smallcaps-auto&quot;>NLP</span> systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train <span class=&quot;smallcaps-auto&quot;>GPT</span>-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, <span class=&quot;smallcaps-auto&quot;>GPT</span>-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. <span class=&quot;smallcaps-auto&quot;>GPT</span>-3 achieves strong performance on many <span class=&quot;smallcaps-auto&quot;>NLP</span> datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where <span class=&quot;smallcaps-auto&quot;>GPT</span>-3's few-shot learning still struggles, as well as some datasets where <span class=&quot;smallcaps-auto&quot;>GPT</span>-3 faces methodological issues related to training on large web corpora. Finally, we find that <span class=&quot;smallcaps-auto&quot;>GPT</span>-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of <span class=&quot;smallcaps-auto&quot;>GPT</span>-3 in general.">“<span>GPT-3</span>: Language Models are Few-Shot Learners”</a>, Brown et al 2020 (<a href="https://arxiv.org/pdf/2005.14165.pdf&amp;org=openai#page=48" data-popup-title="GPT-3 paper: Figure F.1: Four uncurated completions from a context suggesting the model compose a poem in the style of Wallace Stevens with the title 'Shadows on the Way'" data-popup-author="GPT-3" data-popup-date="2020-05-28" data-popup-abstract="<p>As an example, in Figure F.1, we show 4 uncurated samples from a prompt suggesting that the model write a poem, with a given title, in the style of Wallace Stevens. We first experimented with a few prompts, then generated 4 samples with no additional editing or selection (sampling at temperature 1 using <a href=&quot;https://arxiv.org/abs/1904.09751#allen&quot; title=&quot;&amp;#39;The Curious Case of Neural Text Degeneration&amp;#39;, Holtzman et al 2020&quot;>nucleus sampling [HBFC19]</a> with P = 0.9). Completions were truncated when the model began to write a new title and author heading, or broke into prose commentary.</p><p>…“Shadows on the Way”, Wallace Stevens:</p><ol type=&quot;1&quot;><li>I must have shadows on the way / If I am to walk I must have / Each step taken slowly and alone / To have it ready made // And I must think in lines of grey / To have dim thoughts to be my guide / Must look on blue and green / And never let my eye forget / That color is my friend / And purple must surround me too // The yellow of the sun is no more / Intrusive than the bluish snow / That falls on all of us. I must have / Grey thoughts and blue thoughts walk with me / If I am to go away at all.</li><li>There is a way in the world. / That way a man may live in a small town / His lifetime and see no man come, / Never the stir of men about a market, / Never the bobbing of covered carts, / Never the creak passed on the stair / Closing some other life; an old man / Can prosper in that town, can prosper / When his store is seen to close, his wife / Fades in his house, and the world shouts in vain / That no one comes to hear him / Where his small bell tolls. / In that small town / Everything is known. / The ways of the young men are known, / And the ways of the young women / As they walk to the Spring; / The rumors are all known, / The quarrels, the hurt, the envy / Of the men and the women. / Till a day comes / That way out of the world / That leads to a wood. / Young, the young man / Walks, a lover now, / Knows the streets as they are and is of them.</li><li>The sun was all we had. Now, in the shade / All is changed. The mind must dwell on those / White fields, that to its eyes were always old; / Those ancient gleams, convoluting / The way they lay among the huge roots, / The great dark tomes of reverie, / The plumed labyrinth of the sea. // When our own breath gone faint and fade / Up the long stairway of the dead / We shall remember well / The blindness of the light, the whiteness / Of that white land. The footsteps, and / The grasses in that light, and in the shade / The menacing life. // We played, too, once, in the warmest rooms. / To one content, there is one content / For all mankind. The forest glades/ Are all the more for being in fable, / And wood and town in legend shadowed- / Once more to return. The one content / Has in it every happiness, more brave / For being purposeless; and so / Those island gardens, deep in the mind, / Make of remoteness familiarity / And of the unspoken name a friend.</li><li>Nobody will come to this place. It is a road that leads nowhere. / The solitude is deep. The mountains are high. / But they are desolate, and they turn the traveler’s face / Towards the North. All the sounds of the world are far away. / When the wind rises above the trees, / The boughs bow to the ground. / Even the birds that inhabit the tangle of weeds / That is the roadside cover, are silent. One listens, / But hears no roar of the forest. One is alone. / One will be taken. / One will be taken. / There is no utterance, there is no conversation, / But one is uneasy all the same… / There is a thin blue mist, / A darkness rising like smoke, / And within that darkness / A possession of the heart. / One will be taken… It was here, and it will be here again- / Here, under this sky empty and full of light.</li></ol>">poems</a>, compare <a href="https://www.gwern.net/GPT-2" data-popup-title="GPT-2 Neural Network Poetry" data-popup-author="Gwern Branwen" data-popup-date="3 March 2019" data-popup-abstract="<p>In February 2019, following up on my <a href=&quot;./RNN-metadata&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;RNN metadata for mimicking individual author style&quot; data-popup-author=&quot;Gwern Branwen&quot; data-popup-date=&quot;12 Sep 2015&quot; data-popup-abstract=&quot;&amp;lt;p&amp;gt;Char-RNNs are unsupervised generative models which learn to mimic text sequences. I suggest extending char-RNNs with inline metadata such as genre or author prefixed to each line of input, allowing for better &amp;amp;amp; more efficient metadata, and more controllable sampling of generated output by feeding in desired metadata. A 2015 experiment using &amp;lt;code&amp;gt;torch-rnn&amp;lt;/code&amp;gt; on a set of ~30 Project Gutenberg e-books (1 per author) to train a large char-RNN shows that a char-RNN can learn to remember metadata such as authors, learn associated prose styles, and often generate text visibly similar to that of a specified author.&amp;lt;/p&amp;gt;&amp;lt;p&amp;gt;I further try &amp;amp;amp; fail to train &amp;lt;a href=&amp;quot;#geocities-char-rnn&amp;quot;&amp;gt;a char-RNN on Geocities HTML&amp;lt;/a&amp;gt; for unclear reasons.&amp;lt;/p&amp;gt;&amp;lt;p&amp;gt;More successfully, &amp;lt;a href=&amp;quot;./GPT-2&amp;quot; class=&amp;quot;docMetadata&amp;quot; data-popup-title=&amp;quot;GPT-2 Neural Network Poetry&amp;quot; data-popup-author=&amp;quot;gwern&amp;quot; data-popup-date=&amp;quot;3 March 2019&amp;quot; data-popup-abstract=&amp;quot;&amp;amp;lt;p&amp;amp;gt;In February 2019, following up on my 2015–2016 text-generation experiments with char-RNNs, I experiment with the cutting-edge Transformer NN architecture for language modeling &amp;amp;amp;amp; text generation. Using OpenAI’s GPT-2-117M (117M) model pre-trained on a large Internet corpus and nshepperd’s finetuning code, I retrain GPT-2-117M on a large (117MB) Project Gutenberg poetry corpus. I demonstrate how to train 2 variants: &amp;amp;lt;q&amp;amp;gt;“GPT-2-poetry”&amp;amp;lt;/q&amp;amp;gt;, trained on the poems as a continuous stream of text, and &amp;amp;lt;q&amp;amp;gt;“GPT-2-poetry-prefix”&amp;amp;lt;/q&amp;amp;gt;, with each line prefixed with the metadata of the PG book it came from. In May 2019, I trained the next-largest GPT-2, 345M, similarly, for a further quality boost in generated poems.&amp;amp;lt;/p&amp;amp;gt;&amp;amp;lt;p&amp;amp;gt;With just a few GPU-days on 1080ti GPUs, GPT-2-117M finetuning can produce high-quality poetry which is more thematically consistent than my char-RNN poems, capable of modeling subtle features like rhyming, and sometimes even a pleasure to read. I list the many possible ways to improve poem generation and further approach human-level poems.&amp;amp;lt;/p&amp;amp;gt;&amp;quot;&amp;gt;I experiment in 2019 with a recently-developed alternative to char-RNNs&amp;lt;/a&amp;gt;, the Transformer NN architecture, by finetuning training OpenAI’s GPT-2-117M Transformer model on a much larger (117MB) Project Gutenberg poetry corpus using both unlabeled lines &amp;amp;amp; lines with inline metadata (the source book). The generated poetry is much better.&amp;lt;/p&amp;gt;&quot;>2015–2016 text-generation experiments with char-<span class=&quot;smallcaps-auto&quot;>RNN</span>s</a>, I experiment with the cutting-edge Transformer NN architecture for language modeling &amp;amp; text generation. Using OpenAI’s <span class=&quot;smallcaps-auto&quot;>GPT-2-117M</span> (117M) model pre-trained on a large Internet corpus and nshepperd’s finetuning code, I retrain <span class=&quot;smallcaps-auto&quot;>GPT-2-117M</span> on a large (117MB) Project Gutenberg poetry corpus. I demonstrate how to train 2 variants: “<span class=&quot;smallcaps-auto&quot;>GPT-2</span>-poetry”, trained on the poems as a continuous stream of text, and “<span class=&quot;smallcaps-auto&quot;>GPT-2</span>-poetry-prefix”, with each line prefixed with the metadata of the PG book it came from. In May 2019, I trained the next-largest <span class=&quot;smallcaps-auto&quot;>GPT-2</span>, 345M, similarly, for a further quality boost in generated poems. In October 2019, I retrained 117M on a Project Gutenberg corpus with improved formatting, and combined it with a contemporary poem dataset based on <a href=&quot;https://en.wikipedia.org/wiki/Poetry_Foundation&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;Poetry Foundation&quot; data-popup-author=&quot;English Wikipedia&quot; data-popup-abstract=&quot;&amp;lt;p&amp;gt;&amp;lt;b&amp;gt;The Poetry Foundation&amp;lt;/b&amp;gt; is a Chicago-based American foundation created to promote poetry in the wider culture. It was formed from &amp;lt;i&amp;gt;Poetry&amp;lt;/i&amp;gt; magazine, which it continues to publish, with a 2003 gift of $200 million from philanthropist Ruth Lilly.&amp;lt;/p&amp;gt;&quot; title=&quot;Wikipedia: Poetry Foundation&quot;>Poetry Foundation</a>’s <a href=&quot;https://www.poetryfoundation.org/poems&quot;>website</a>.</p><p>With just a few <span class=&quot;smallcaps-auto&quot;>GPU</span>-days on 1080ti <span class=&quot;smallcaps-auto&quot;>GPU</span>s, <span class=&quot;smallcaps-auto&quot;>GPT-2-117M</span> finetuning can produce high-quality poetry which is more thematically consistent than my char-<span class=&quot;smallcaps-auto&quot;>RNN</span> poems, capable of modeling subtle features like rhyming, and sometimes even a pleasure to read. I list the many possible ways to improve poem generation and further approach human-level poems.</p><p>For anime plot summaries, see <a href=&quot;./TWDE#text&quot;><span class=&quot;smallcaps-auto&quot;>TWDNE</span></a>; for generating <span class=&quot;smallcaps-auto&quot;>ABC</span>-formatted folk music, see <a href=&quot;./GPT-2-music&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;GPT-2 Folk Music&quot; data-popup-author=&quot;Gwern Branwen&quot; data-popup-date=&quot;1 Nov 2019&quot; data-popup-abstract=&quot;&amp;lt;p&amp;gt;In November 2019, I experimented with training a &amp;lt;span class=&amp;quot;smallcaps-auto&amp;quot;&amp;gt;GPT-2&amp;lt;/span&amp;gt; neural net model to generate folk music in the high-level &amp;lt;span class=&amp;quot;smallcaps-auto&amp;quot;&amp;gt;ABC&amp;lt;/span&amp;gt; music text format, following previous work in 2016 which used a char-&amp;lt;span class=&amp;quot;smallcaps-auto&amp;quot;&amp;gt;RNN&amp;lt;/span&amp;gt; trained on a ‘The Session’ dataset. A &amp;lt;span class=&amp;quot;smallcaps-auto&amp;quot;&amp;gt;GPT-2&amp;lt;/span&amp;gt; hypothetically can improve on an &amp;lt;span class=&amp;quot;smallcaps-auto&amp;quot;&amp;gt;RNN&amp;lt;/span&amp;gt; by better global coherence &amp;amp;amp; copying of patterns, without problems with the hidden-state bottleneck.&amp;lt;/p&amp;gt;&amp;lt;p&amp;gt;I encountered problems with the standard &amp;lt;span class=&amp;quot;smallcaps-auto&amp;quot;&amp;gt;GPT-2&amp;lt;/span&amp;gt; model’s encoding of text which damaged results, but after &amp;lt;a href=&amp;quot;#spaceless-model&amp;quot;&amp;gt;fixing that&amp;lt;/a&amp;gt;, I successfully trained it on &amp;lt;em&amp;gt;n&amp;lt;/em&amp;gt;=205,304 &amp;lt;span class=&amp;quot;smallcaps-auto&amp;quot;&amp;gt;ABC&amp;lt;/span&amp;gt; music pieces taken from The Session &amp;amp;amp; &amp;lt;span class=&amp;quot;smallcaps-auto&amp;quot;&amp;gt;ABC&amp;lt;/span&amp;gt;notation.com. The resulting music samples are in my opinion quite pleasant.&amp;lt;/p&amp;gt;&amp;lt;p&amp;gt;The &amp;lt;span class=&amp;quot;smallcaps-auto&amp;quot;&amp;gt;ABC&amp;lt;/span&amp;gt; folk model &amp;amp;amp; dataset are &amp;lt;a href=&amp;quot;#combined-model-the-session-abcnotation.com&amp;quot;&amp;gt;available for download&amp;lt;/a&amp;gt;, and I provide for listening selected &amp;lt;a href=&amp;quot;#samples&amp;quot;&amp;gt;music samples&amp;lt;/a&amp;gt; as well as medleys of random samples from throughout training.&amp;lt;/p&amp;gt;&amp;lt;p&amp;gt;We followed the &amp;lt;span class=&amp;quot;smallcaps-auto&amp;quot;&amp;gt;ABC&amp;lt;/span&amp;gt; folk model with &amp;lt;a href=&amp;quot;#generating-midi-with-10k30k-context-windows&amp;quot;&amp;gt;an &amp;lt;span class=&amp;quot;smallcaps-auto&amp;quot;&amp;gt;ABC-MIDI&amp;lt;/span&amp;gt; model&amp;lt;/a&amp;gt;: a &amp;lt;a href=&amp;quot;#midi-dataset&amp;quot;&amp;gt;dataset of 453k &amp;lt;span class=&amp;quot;smallcaps-auto&amp;quot;&amp;gt;ABC&amp;lt;/span&amp;gt; pieces&amp;lt;/a&amp;gt; decompiled from &amp;lt;span class=&amp;quot;smallcaps-auto&amp;quot;&amp;gt;MIDI&amp;lt;/span&amp;gt; pieces, which fit into &amp;lt;span class=&amp;quot;smallcaps-auto&amp;quot;&amp;gt;GPT-2-117M&amp;lt;/span&amp;gt; with an expanded context window when trained on &amp;lt;span class=&amp;quot;smallcaps-auto&amp;quot;&amp;gt;TPU&amp;lt;/span&amp;gt;s. The &amp;lt;span class=&amp;quot;smallcaps-auto&amp;quot;&amp;gt;MIDI&amp;lt;/span&amp;gt; pieces are far more diverse and challenging, and &amp;lt;span class=&amp;quot;smallcaps-auto&amp;quot;&amp;gt;GPT-2&amp;lt;/span&amp;gt; underfits and struggles to produce valid samples but when sampling succeeds, it can generate &amp;lt;a href=&amp;quot;./GPT-2-music#midi-samples&amp;quot;&amp;gt;even better musical samples&amp;lt;/a&amp;gt;.&amp;lt;/p&amp;gt;&quot; title=&quot;Generating Irish and folk music in ABC format using GPT-2-117M, with good results.&quot;>“<span class=&quot;smallcaps-auto&quot;>GPT-2</span> Folk Music”</a> &amp;amp; <a href=&quot;./GPT-2-preference-learning&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;GPT-2 Preference Learning for Music and Poetry Generation&quot; data-popup-author=&quot;Gwern Branwen&quot; data-popup-date=&quot;16 Dec 2019&quot; data-popup-abstract=&quot;&amp;lt;p&amp;gt;Standard language generation neural network models, like &amp;lt;span class=&amp;quot;smallcaps-auto&amp;quot;&amp;gt;GPT-2&amp;lt;/span&amp;gt;, are trained via likelihood training to imitate human text corpuses. Generated text suffers from persistent flaws like repetition, due to myopic generation word-by-word, and cannot improve on the training data because they are trained to predict ‘realistic’ completions of the training data.&amp;lt;/p&amp;gt;&amp;lt;p&amp;gt;A proposed alternative is to use reinforcement learning to train the NNs, to encourage global properties like coherence &amp;amp;amp; lack of repetition, and potentially improve over the original corpus’s average quality. &amp;lt;em&amp;gt;Preference learning&amp;lt;/em&amp;gt; trains a reward function on human ratings, and uses that as the ‘environment’ for a blackbox &amp;lt;span class=&amp;quot;smallcaps-auto&amp;quot;&amp;gt;DRL&amp;lt;/span&amp;gt; algorithm like &amp;lt;span class=&amp;quot;smallcaps-auto&amp;quot;&amp;gt;PPO&amp;lt;/span&amp;gt;.&amp;lt;/p&amp;gt;&amp;lt;p&amp;gt;OpenAI released a codebase implementing this dual-model preference learning approach for textual generation, based on &amp;lt;span class=&amp;quot;smallcaps-auto&amp;quot;&amp;gt;GPT-2&amp;lt;/span&amp;gt;. Having previously used &amp;lt;a href=&amp;quot;./GPT-2&amp;quot; class=&amp;quot;docMetadata&amp;quot; data-popup-title=&amp;quot;GPT-2 Neural Network Poetry&amp;quot; data-popup-author=&amp;quot;Gwern Branwen&amp;quot; data-popup-date=&amp;quot;3 March 2019&amp;quot; data-popup-abstract=&amp;quot;&amp;amp;lt;p&amp;amp;gt;In February 2019, following up on my 2015–2016 text-generation experiments with char-RNNs, I experiment with the cutting-edge Transformer NN architecture for language modeling &amp;amp;amp;amp; text generation. Using OpenAI’s GPT-2-117M (117M) model pre-trained on a large Internet corpus and nshepperd’s finetuning code, I retrain GPT-2-117M on a large (117MB) Project Gutenberg poetry corpus. I demonstrate how to train 2 variants: “GPT-2-poetry”, trained on the poems as a continuous stream of text, and “GPT-2-poetry-prefix”, with each line prefixed with the metadata of the PG book it came from. In May 2019, I trained the next-largest GPT-2, 345M, similarly, for a further quality boost in generated poems. In October 2019, I retrained 117M on a Project Gutenberg corpus with improved formatting, and combined it with a contemporary poem dataset based on &amp;amp;lt;a href=&amp;amp;quot;https://en.wikipedia.org/wiki/Poetry_Foundation&amp;amp;quot; class=&amp;amp;quot;docMetadata&amp;amp;quot; data-popup-title=&amp;amp;quot;Poetry Foundation&amp;amp;quot; data-popup-author=&amp;amp;quot;English Wikipedia&amp;amp;quot; data-popup-abstract=&amp;amp;quot;&amp;amp;amp;lt;p&amp;amp;amp;gt;&amp;amp;amp;lt;b&amp;amp;amp;gt;The Poetry Foundation&amp;amp;amp;lt;/b&amp;amp;amp;gt; is a Chicago-based American foundation created to promote poetry in the wider culture. It was formed from &amp;amp;amp;lt;i&amp;amp;amp;gt;Poetry&amp;amp;amp;lt;/i&amp;amp;amp;gt; magazine, which it continues to publish, with a 2003 gift of $200 million from philanthropist Ruth Lilly.&amp;amp;amp;lt;/p&amp;amp;amp;gt;&amp;amp;quot; title=&amp;amp;quot;Wikipedia: Poetry Foundation&amp;amp;quot;&amp;amp;gt;Poetry Foundation&amp;amp;lt;/a&amp;amp;gt;’s &amp;amp;lt;a href=&amp;amp;quot;https://www.poetryfoundation.org/poems&amp;amp;quot; class=&amp;amp;quot;docMetadata&amp;amp;quot; data-popup-image-height=&amp;amp;quot;768&amp;amp;quot; data-popup-image-width=&amp;amp;quot;768&amp;amp;quot;&amp;amp;gt;website&amp;amp;lt;/a&amp;amp;gt;.&amp;amp;lt;/p&amp;amp;gt;&amp;amp;lt;p&amp;amp;gt;With just a few GPU-days on 1080ti GPUs, GPT-2-117M finetuning can produce high-quality poetry which is more thematically consistent than my char-RNN poems, capable of modeling subtle features like rhyming, and sometimes even a pleasure to read. I list the many possible ways to improve poem generation and further approach human-level poems.&amp;amp;lt;/p&amp;amp;gt;&amp;amp;lt;p&amp;amp;gt;For generating ABC-formatted folk music, see &amp;amp;lt;a href=&amp;amp;quot;./GPT-2-music&amp;amp;quot; class=&amp;amp;quot;docMetadata&amp;amp;quot; data-popup-title=&amp;amp;quot;GPT-2 Music&amp;amp;quot; data-popup-author=&amp;amp;quot;Gwern Branwen&amp;amp;quot; data-popup-date=&amp;amp;quot;1 Nov 2019&amp;amp;quot; data-popup-abstract=&amp;amp;quot;&amp;amp;amp;lt;p&amp;amp;amp;gt;In November 2019, I experimented with training a GPT-2 neural net model to generate folk music in the high-level ABC music text format, following previous work in 2016 which used a char-RNN trained on a ‘The Session’ dataset. A GPT-2 hypothetically can improve on an RNN by better global coherence &amp;amp;amp;amp;amp; copying of patterns, without problems with the hidden-state bottleneck.&amp;amp;amp;lt;/p&amp;amp;amp;gt;&amp;amp;amp;lt;p&amp;amp;amp;gt;I encountered problems with the standard GPT-2 model’s encoding of text which damaged results, but after fixing that, I successfully trained it on n_=205,304 ABC music pieces taken from The Session &amp;amp;amp;amp;amp; ABCnotation.com. The resulting music samples are in my opinion quite pleasant.&amp;amp;amp;lt;/p&amp;amp;amp;gt;&amp;amp;amp;lt;p&amp;amp;amp;gt;The model &amp;amp;amp;amp;amp; dataset are available for download, and I provide for listening selected music samples as well as medleys of random samples from throughout training.&amp;amp;amp;lt;/p&amp;amp;amp;gt;&amp;amp;quot; title=&amp;amp;quot;Generating Irish and folk music in ABC format using GPT-2-117M, with good results.&amp;amp;quot;&amp;amp;gt;“GPT-2 Folk Music”&amp;amp;lt;/a&amp;amp;gt;&amp;amp;lt;/p&amp;amp;gt;&amp;quot;&amp;gt;&amp;lt;span class=&amp;quot;smallcaps-auto&amp;quot;&amp;gt;GPT-2&amp;lt;/span&amp;gt; for poetry&amp;lt;/a&amp;gt; &amp;amp;amp; &amp;lt;a href=&amp;quot;./GPT-2-music&amp;quot; class=&amp;quot;docMetadata&amp;quot; data-popup-title=&amp;quot;GPT-2 Folk Music&amp;quot; data-popup-author=&amp;quot;Gwern Branwen&amp;quot; data-popup-date=&amp;quot;1 Nov 2019&amp;quot; data-popup-abstract=&amp;quot;&amp;amp;lt;p&amp;amp;gt;In November 2019, I experimented with training a GPT-2 neural net model to generate folk music in the high-level ABC music text format, following previous work in 2016 which used a char-RNN trained on a ‘The Session’ dataset. A GPT-2 hypothetically can improve on an RNN by better global coherence &amp;amp;amp;amp; copying of patterns, without problems with the hidden-state bottleneck.&amp;amp;lt;/p&amp;amp;gt;&amp;amp;lt;p&amp;amp;gt;I encountered problems with the standard GPT-2 model’s encoding of text which damaged results, but after &amp;amp;lt;a href=&amp;amp;quot;#spaceless-model&amp;amp;quot;&amp;amp;gt;fixing that&amp;amp;lt;/a&amp;amp;gt;, I successfully trained it on &amp;amp;lt;em&amp;amp;gt;n&amp;amp;lt;/em&amp;amp;gt;=205,304 ABC music pieces taken from The Session &amp;amp;amp;amp; ABCnotation.com. The resulting music samples are in my opinion quite pleasant.&amp;amp;lt;/p&amp;amp;gt;&amp;amp;lt;p&amp;amp;gt;The model &amp;amp;amp;amp; dataset are available for download, and I provide for listening selected &amp;amp;lt;a href=&amp;amp;quot;#samples&amp;amp;quot;&amp;amp;gt;music samples&amp;amp;lt;/a&amp;amp;gt; as well as medleys of random samples from throughout training.&amp;amp;lt;/p&amp;amp;gt;&amp;quot;&amp;gt;music generation&amp;lt;/a&amp;gt;, I experimented with &amp;lt;span class=&amp;quot;smallcaps-auto&amp;quot;&amp;gt;GPT-2&amp;lt;/span&amp;gt; preference learning for unconditional music and poetry generation.&amp;lt;/p&amp;gt;&amp;lt;p&amp;gt;I found that preference learning seemed to work better for music than poetry, and seemed to reduce the presence of repetition artifacts, but the results, at &amp;lt;em&amp;gt;n&amp;lt;/em&amp;gt;≅7,400 ratings compiled over 23 iterations of training+sampling November 2019–January 2020, are not dramatically better than alternative improvements like scaling up models or more thorough data-cleaning or more stringent sample curation. My blind ratings using &amp;lt;em&amp;gt;n&amp;lt;/em&amp;gt;≅200 comparisons showed no large advantage for the RL-tuned samples (winning only 93 of 210 comparisons, or 46%).&amp;lt;/p&amp;gt;&amp;lt;p&amp;gt;This may be due to insufficient ratings, bad hyperparameters, or not using samples generated with common prefixes, but I suspect it’s the former, as some &amp;lt;span class=&amp;quot;smallcaps-auto&amp;quot;&amp;gt;NLP&amp;lt;/span&amp;gt; tasks in Ziegler et al 2019 required up to 60k ratings for good performance, and the reward model appeared to achieve poor performance &amp;amp;amp; succumb to adversarial examples easily.&amp;lt;/p&amp;gt;&amp;lt;p&amp;gt;Working with it, I suspect that preference learning is unnecessarily sample-inefficient &amp;amp;amp; data-inefficient, and that the blackbox reinforcement learning approach is inferior to directly using the reward model to optimize text samples, and propose two major architectural overhauls: have the reward model &amp;lt;a href=&amp;quot;#bradley-terry-preference-learning&amp;quot;&amp;gt;directly model the implied ranking&amp;lt;/a&amp;gt; of every datapoint, and drop the agent model entirely in favor of backprop-powered gradient ascent which &amp;lt;a href=&amp;quot;#optimization-by-backprop-not-blackbox&amp;quot;&amp;gt;optimizes sequences to maximize the reward model’s output&amp;lt;/a&amp;gt;.&amp;lt;/p&amp;gt;&quot;>“<span class=&quot;smallcaps-auto&quot;>GPT-2</span> Preference Learning for Music and Poetry Generation”</a>; for playing chess, see <a href=&quot;https://slatestarcodex.com/2020/01/06/a-very-unlikely-chess-game/&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;A Very Unlikely Chess Game&quot; data-popup-author=&quot;Scott Alexander&quot; data-popup-date=&quot;2020-01-06&quot; data-popup-abstract=&quot;&amp;lt;p&amp;gt;Black is GPT-2. Its excuse is that it’s a text prediction program with no concept of chess. As far as it knows, it’s trying to predict short alphanumeric strings like “e2e4” or “Nb7”. Nobody told it this represents a board game. It doesn’t even have a concept of 2D space that it could use to understand such a claim. But it still captured my rook! Embarrassing! … Last month, I asked him if he thought GPT-2 could play chess. I wondered if he could train it on a corpus of chess games written in standard notation (where, for example, e2e4 means “move the pawn at square e2 to square e4”). There are literally millions of games written up like this. GPT-2 would learn to predict the next string of text, which would correspond to the next move in the chess game. Then you would prompt it with a chessboard up to a certain point, and it would predict how the chess masters who had produced its training data would continue the game – ie make its next move using the same heuristics they would. Gwern handed the idea to his collaborator Shawn Presser, who had a working GPT-2 chess engine running &amp;lt;em&amp;gt;within&amp;lt;/em&amp;gt; a week: … You can play against GPT-2 yourself by following the directions in the last tweet, though it won’t be much of a challenge for anyone better than I am.&amp;lt;/p&amp;gt;&amp;lt;p&amp;gt;…What does this imply? I’m not sure (and maybe it will imply more if someone manages to make it actually good). It was already weird to see something with no auditory qualia learn passable poetic meter. It’s even weirder to see something with no concept of space learn to play chess. Is any of this &amp;lt;a href=&amp;quot;https://slatestarcodex.com/2019/02/28/meaningful/&amp;quot;&amp;gt;meaningful&amp;lt;/a&amp;gt;? How impressed should we be that the same AI can write poems, compose music, and play chess, without having been designed for any of those tasks? I still don’t know.&amp;lt;/p&amp;gt;&quot; title=&quot;Scott Alexander&quot;>“A Very Unlikely Chess Game”</a>; for the Reddit comment generator, see <a href=&quot;./docs/www/old.reddit.com/7eaaa81a26404ef60df4279ee1f1b0c829d73be5.html&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;Update: Upgrading to 1.5B GPT-2, and adding 22 new subreddit-bots&quot; data-popup-author=&quot;disumbrationist&quot; data-popup-date=&quot;2020-01-12&quot; data-popup-abstract=&quot;&amp;lt;p&amp;gt;When I originally trained the models in May 2019, I’d used the 345M version of GPT-2, which at the time was the largest one that OpenAI had publicly released. Last November, however, OpenAI &amp;lt;a href=&amp;quot;https://openai.com/blog/gpt-2-1-5b-release/&amp;quot;&amp;gt;finally released the full 1.5 billion parameter model&amp;lt;/a&amp;gt;.&amp;lt;/p&amp;gt;&amp;lt;p&amp;gt;The 1.5B model requires much more memory to fine-tune than the 345M, so I was initially having a lot of difficulty getting it to work on Colab. Thankfully, I was contacted by &amp;lt;a href=&amp;quot;https://old.reddit.com/u/gwern&amp;quot;&amp;gt;/u/gwern&amp;lt;/a&amp;gt; (&amp;lt;a href=&amp;quot;https://www.patreon.com/gwern&amp;quot;&amp;gt;here’s his Patreon&amp;lt;/a&amp;gt;) and Shawn Presser (&amp;lt;a href=&amp;quot;https://old.reddit.com/u/shawwwn&amp;quot;&amp;gt;/u/shawwwn&amp;lt;/a&amp;gt;), who very generously offered to do the fine-tuning themselves if I provided them with the dataset. This training took about 2 weeks, and apparently required &amp;lt;a href=&amp;quot;https://twitter.com/gwern/status/1215005375407112193&amp;quot;&amp;gt;around $70K worth of TPU credits&amp;lt;/a&amp;gt;, so in hindsight this upgrade definitely wouldn’t have been possible for me to do myself, without their assistance.&amp;lt;/p&amp;gt;&amp;lt;p&amp;gt;Based on my tests of the new model so far, I’m pretty happy with the quality, and IMO it is noticeably more coherent than the 345M version.&amp;lt;/p&amp;gt;&amp;lt;p&amp;gt;One thing that I should point out about the upgrade is that the original 345M models had been separately fine-tuned for each subreddit individually (i.e. there were 108 separate models), whereas the upgraded one is just a single 1.5B model that has been fine-tuned using a combined dataset containing the comments/submissions from &amp;lt;em&amp;gt;all&amp;lt;/em&amp;gt; the subreddits that I scraped. The main reason for this decision is simply that it would not have been feasible to train ~100 separate 1.5B models. Also, there may have been benefits from transfer learning across subreddits, which wouldn’t occur with separate models.&amp;lt;/p&amp;gt;&amp;lt;p&amp;gt;…Here is the full list of new bots to be added: /r/capitalismvsocialism · /r/chess · /r/conlangs · /r/dota2 · /r/etymology · /r/fiftyfifty · /r/hobbydrama · /r/markmywords · /r/moviedetails · /r/neoliberal · /r/obscuremedia · /r/recipes · /r/riddles · /r/stonerphilosophy · /r/subsimulatorgpt2 · /r/subsimulatorgpt2meta · /r/tellmeafact · /r/twosentencehorror · /r/ukpolitics · /r/wordavalanches · /r/wouldyourather · /r/zen&amp;lt;/p&amp;gt;&quot; rel=&quot;archived alternate nofollow&quot; data-url-original=&quot;https://old.reddit.com/r/SubSimulatorGPT2Meta/comments/entfgx/update_upgrading_to_15b_gpt2_and_adding_22_new/&quot; title=&quot;Update: Upgrading to 1.5B GPT-2, and adding 22 new subreddit-bots (Original URL: https://old.reddit.com/r/SubSimulatorGPT2Meta/comments/entfgx/update_upgrading_to_15b_gpt2_and_adding_22_new/ )&quot;>SubSimulator<span class=&quot;smallcaps-auto&quot;>GPT-2</span></a>; for fanfiction, the <a href=&quot;#archive-of-our-own-ao3-gpt-2-1.5b&quot;>Ao3</a>; and for video games, <a href=&quot;#video-game-walkthrough-gpt-2-1.5b&quot;>the walkthrough model</a>. For OpenAI’s <span class=&quot;smallcaps-auto&quot;>GPT-3</span> followup, see <a href=&quot;https://arxiv.org/abs/2005.14165#openai&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;Language Models are Few-Shot Learners&quot; data-popup-author=&quot;Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Dan et al [...]&quot; data-popup-date=&quot;2020-05-28&quot; data-popup-abstract=&quot;Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions—something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3&amp;#39;s few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.&quot; title=&quot;Brown et al 2020&quot;>“<span class=&quot;smallcaps-auto&quot;>GPT-3</span>: Language Models are Few-Shot Learners”</a>.</p>">my finetuned <span>GPT-2</span> poetry</a>; <a href="https://justpaste.it/7eovk" title="GPT-3 Github JSON dump reformatted to readable HTML">random samples</a>; <a href="https://openai.com/blog/openai-api/">“OpenAI <span>API</span>”</a> with real-world demos)</p>
</blockquote>
<p><span> Learning to learn.</span> OA releases the long-awaited followup to <a href="https://openai.com/blog/better-language-models/" data-popup-title="Better Language Models and Their Implications: We've trained a large-scale unsupervised language model [GPT-2] which generates coherent paragraphs of text, achieves state-of-the-art performance on many language modeling benchmarks, and performs rudimentary reading comprehension, machine translation, question answering, and summarization—all without task-specific training" data-popup-author="Alec Radford, Jeffrey Wu, Dario Amodei, Daniela Amodei, Jack Clark, Miles Brundage, Ilya Sutskever (OpenAI)" data-popup-date="2019-02-14" data-popup-abstract="<p>Our model, called <span class=&quot;smallcaps-auto&quot;>GPT</span>-2 (a successor to <span class=&quot;smallcaps-auto&quot;>GPT</span>), was trained simply to predict the next word in 40GB of Internet text. Due to our concerns about malicious applications of the technology, we are not releasing the trained model. As an experiment in responsible disclosure, we are instead releasing a much <a href=&quot;https://github.com/openai/gpt-2&quot;>smaller model</a> for researchers to experiment with, as well as a <a href=&quot;https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf&quot;>technical paper</a>.</p><p><span class=&quot;smallcaps-auto&quot;>GPT</span>-2 is a large <a href=&quot;https://arxiv.org/abs/1706.03762&quot;>Transformer</a>-based language model with 1.5 billion parameters, trained on a dataset of 8 million web pages. <span class=&quot;smallcaps-auto&quot;>GPT</span>-2 is trained with a simple objective: predict the next word, given all of the previous words within some text. The diversity of the dataset causes this simple goal to contain naturally occurring demonstrations of many tasks across diverse domains. <span class=&quot;smallcaps-auto&quot;>GPT</span>-2 is a direct scale-up of <span class=&quot;smallcaps-auto&quot;>GPT</span>, with more than 10X the parameters and trained on more than 10X the amount of data.</p><p><span class=&quot;smallcaps-auto&quot;>GPT</span>-2 displays a broad set of capabilities, including the ability to generate conditional synthetic text samples of unprecedented quality, where we prime the model with an input and have it generate a lengthy continuation. In addition, <span class=&quot;smallcaps-auto&quot;>GPT</span>-2 outperforms other language models trained on specific domains (like Wikipedia, news, or books) without needing to use these domain-specific training datasets. On language tasks like question answering, reading comprehension, summarization, and translation, <span class=&quot;smallcaps-auto&quot;>GPT</span>-2 begins to learn these tasks from the raw text, using no task-specific training data. While scores on these downstream tasks are far from state-of-the-art, they suggest that the tasks can benefit from unsupervised techniques, given sufficient (unlabeled) data and compute.</p>"><span>GPT-2</span></a>, one model to rule them all: a 117✕ larger 175b-parameter model with far more powerful language generation, which lets it solve a wide variety of problems from arithmetic to English translation to unscrambling anagrams to <span>SAT</span> analogies—purely from being prompted with text examples, without any specialized training or finetuning whatsoever, merely next-word prediction training on a big Internet text corpus. This implies <span>GPT-3</span>’s attention mechanisms serve as <a href="https://arxiv.org/abs/1610.06258#deepmind" data-popup-title="Using Fast Weights to Attend to the Recent Past" data-popup-author="Jimmy Ba, Geoffrey Hinton, Volodymyr Mnih, Joel Z. Leibo, Catalin Ionescu" data-popup-date="2020-06-04" data-popup-abstract="Until recently, research on artificial neural networks was largely restricted to systems with only two types of variable: Neural activities that represent the current or recent input and weights that learn to capture regularities among inputs, outputs and payoffs. There is no good reason for this restriction. Synapses have dynamics at many different time-scales and this suggests that artificial neural networks might benefit from variables that change slower than activities but much faster than the standard weights. These &quot;fast weights&quot; can be used to store temporary memories of the recent past and they provide a neurally plausible way of implementing the type of attention to the past that has recently proved very helpful in sequence-to-sequence models. By using fast weights we can avoid the need to store copies of neural activity patterns.">“fast weights”</a> that have “learned to learn” by training on sufficiently varied data<a href="#sn1" id="fnref1" role="doc-noteref"><sup>1</sup></a>, forcing it to do more than just learn ordinary textual relationships. Like OpenAI’s <a href="https://openai.com/blog/jukebox/" data-popup-title="Jukebox: We're introducing Jukebox, a neural net that generates music, including rudimentary singing, as raw audio in a variety of genres and artist styles. We're releasing the model weights and code, along with a tool to explore the generated samples." data-popup-author="Prafulla Dhariwal, Heewoo Jun, Christine Payne, Jong Wook Kim, Alec Radford, Ilya Sutskever" data-popup-date="2020-04-30" data-popup-abstract="<p>A typical 4-minute song at CD quality (44 kHz, 16-bit) has over 10 million timesteps. For comparison, <span class=&quot;smallcaps-auto&quot;>GPT</span>-2 had 1,000 timesteps and OpenAI Five took tens of thousands of timesteps per game. Thus, to learn the high level semantics of music, a model would have to deal with extremely long-range dependencies. One way of addressing the long input problem is to use an autoencoder that compresses raw audio to a lower-dimensional space by discarding some of the perceptually irrelevant bits of information. We can then train a model to generate audio in this compressed space, and upsample back to the raw audio space.</p><p>We chose to work on music because we want to continue to push the boundaries of generative models. Our previous work on MuseNet explored synthesizing music based on large amounts of <span class=&quot;smallcaps-auto&quot;>MIDI</span> data. Now in raw audio, our models must learn to tackle high diversity as well as very long range structure, and the raw audio domain is particularly unforgiving of errors in short, medium, or long term timing.</p><p>…Jukebox’s autoencoder model compresses audio to a discrete space, using a quantization-based approach called VQ-<span class=&quot;smallcaps-auto&quot;>VAE</span>.2 Hierarchical VQ-<span class=&quot;smallcaps-auto&quot;>VAE</span>s17 can generate short instrumental pieces from a few sets of instruments, however they suffer from hierarchy collapse due to use of successive encoders coupled with autoregressive decoders. A simplified variant called VQ-<span class=&quot;smallcaps-auto&quot;>VAE</span>-226 avoids these issues by using feedforward encoders and decoders only, and they show impressive results at generating high-fidelity images…We use three levels in our VQ-<span class=&quot;smallcaps-auto&quot;>VAE</span>, shown below, which compress the 44kHz raw audio by 8✕, 32✕, and 128✕, respectively, with a codebook size of 2048 for each level. This downsampling loses much of the audio detail, and sounds noticeably noisy as we go further down the levels. However, it retains essential information about the pitch, timbre, and volume of the audio.</p><p><figure><img href=&quot;/images/gan/2020-dhariwal-openai-jukebox-vqvaetransformerarchitecture.svg&quot; alt=&quot;https://cdn.openai.com/jukebox/assets/vqvae-1-outlined.svg&quot;>Jukebox architecture</img></figure></p><p>…The top-level prior models the long-range structure of music, and samples decoded from this level have lower audio quality but capture high-level semantics like singing and melodies. The middle and bottom upsampling priors add local musical structures like timbre, significantly improving the audio quality. We train these as autoregressive models using a simplified variant of Sparse Transformers. Each of these models has 72 layers of factorized self-attention on a context of 8192 codes, which corresponds to approximately 24 seconds, 6 seconds, and 1.5 seconds of raw audio at the top, middle and bottom levels, respectively. Once all of the priors are trained, we can generate codes from the top level, upsample them using the upsamplers, and decode them back to the raw audio space using the VQ-<span class=&quot;smallcaps-auto&quot;>VAE</span> decoder to sample novel songs.</p><p>…While Jukebox represents a step forward in musical quality, coherence, length of audio sample, and ability to condition on artist, genre, and lyrics, there is a significant gap between these generations and human-created music. For example, while the generated songs show local musical coherence, follow traditional chord patterns, and can even feature impressive solos, we do not hear familiar larger musical structures such as choruses that repeat. Our downsampling and upsampling process introduces discernible noise. Improving the VQ-<span class=&quot;smallcaps-auto&quot;>VAE</span> so its codes capture more musical information would help reduce this. Our models are also slow to sample from, because of the autoregressive nature of sampling. It takes approximately 9 hours to fully render 1 minute of audio through our models, and thus they cannot yet be used in interactive applications</p>">Jukebox</a> just weeks ago, the announcement of <span>GPT-3</span> appears to have sunk almost without a trace, so I will go into more depth than usual.</p>
<p><span> “Attacks only get better.”</span> 2 years ago, <a href="https://openai.com/blog/language-unsupervised/" data-popup-title="Improving Language Understanding with Unsupervised Learning" data-popup-author="OpenAI" data-popup-date="June 11, 2018" data-popup-abstract="We’ve obtained state-of-the-art results on a suite of diverse language tasks with a scalable, task-agnostic system, which we’re also releasing. Our approach is a combination of two existing ideas: <a href=&quot;https://arxiv.org/abs/1706.03762&quot;>transformers</a> and <a href=&quot;https://arxiv.org/abs/1511.01432&quot;>unsupervised pre-training</a>. These results provide a convincing example that pairing supervised learning methods with unsupervised pre-training works very well; this is an idea that many have explored in the past, and we hope our result motivates further research into applying this idea on larger and more diverse datasets."><span>GPT-1</span></a> was interestingly useful pretraining and adorable with its “sentiment neuron”. 1 year ago, <span>GPT-2</span> was impressive with its excellent text generation &amp; finetuning capabilities. This year, <span>GPT-3</span> is scary because it’s a small &amp; shallow model compared to what’s possible<a href="#sn2" id="fnref2" role="doc-noteref"><sup>2</sup></a>, with a simple uniform architecture<a href="#sn3" id="fnref3" role="doc-noteref"><sup>3</sup></a> trained in the dumbest way possible (unidirectional prediction of next text token) on a single impoverished modality (random Internet <span>HTML</span> text dumps<a href="#sn4" id="fnref4" role="doc-noteref"><sup>4</sup></a>) on tiny data (fits on a laptop), and yet, the first version already manifests crazy runtime meta-learning—and the scaling curves <em>still</em> are not bending! The samples are also better than ever, whether it’s <span>GPT-3</span> inventing new penis jokes<a href="#sn5" id="fnref5" role="doc-noteref"><sup>5</sup></a> or writing (mostly working) <a href="https://justpaste.it/7eovk#javascript" data-popup-title="GPT-3 random sample dump: JavaScript tutorial" data-popup-author="GPT-3" data-popup-date="2020-05-28" data-popup-abstract="<p>[Sample from <span class=&quot;smallcaps-auto&quot;>GPT</span>-3; condensed paragraphs &amp;amp; formatting added for legibility inside popups, as the original plain text dumps strip the original <span class=&quot;smallcaps-auto&quot;>HTML</span> formatting of tutorials etc.]</p><p>Working with an example—rotating through an array For a number of years, I’ve been fascinated by the idea of writing code that can rotate through an array. Let’s say I have the following array of numbers:</p><div class=&quot;sourceCode&quot; id=&quot;cb1&quot;><pre class=&quot;sourceCode JavaScript&quot;><code class=&quot;sourceCode javascript&quot;><span id=&quot;cb1-1&quot;><a href=&quot;#cb1-1&quot;></a><span class=&quot;kw&quot;>var</span> myNumbers <span class=&quot;op&quot;>=</span> [ <span class=&quot;dv&quot;>1</span><span class=&quot;op&quot;>,</span> <span class=&quot;dv&quot;>2</span><span class=&quot;op&quot;>,</span> <span class=&quot;dv&quot;>3</span><span class=&quot;op&quot;>,</span> <span class=&quot;dv&quot;>4</span><span class=&quot;op&quot;>,</span> <span class=&quot;dv&quot;>5</span> ]<span class=&quot;op&quot;>;</span></span></code></pre></div><p>The following snippet of code would display the same numbers in reverse order:</p><div class=&quot;sourceCode&quot; id=&quot;cb2&quot;><pre class=&quot;sourceCode JavaScript&quot;><code class=&quot;sourceCode javascript&quot;><span id=&quot;cb2-1&quot;><a href=&quot;#cb2-1&quot;></a><span class=&quot;cf&quot;>for</span> ( <span class=&quot;kw&quot;>var</span> i <span class=&quot;op&quot;>=</span> myNumbers<span class=&quot;op&quot;>.</span><span class=&quot;at&quot;>length</span> <span class=&quot;op&quot;>-</span> <span class=&quot;dv&quot;>1</span><span class=&quot;op&quot;>;</span> i <span class=&quot;op&quot;>&amp;gt;=</span> <span class=&quot;dv&quot;>0</span><span class=&quot;op&quot;>;</span> i<span class=&quot;op&quot;>--</span> ) { <span class=&quot;bu&quot;>console</span><span class=&quot;op&quot;>.</span><span class=&quot;fu&quot;>log</span>( myNumbers[i] )<span class=&quot;op&quot;>;</span> }</span></code></pre></div><p>What’s going on here?</p><p>We know that the index of an array can be used as a “pointer” to the location in memory that contains a particular item in an array. If the array <code>myNumbers</code> has five items, then <code>myNumbers[0]</code> , <code>myNumbers[1]</code> , <code>myNumbers[2]</code> , <code>myNumbers[3]</code> and <code>myNumbers[4]</code> will point to the values 1 , 2 , 3 , 4 and 5 respectively.</p><p>In the previous example, the code starts with the length of the array (5) and subtracts one from it. As we know that index 0 in an array contains the first item in the array, the previous code will execute the following steps: Take the length of the array (5) Subtract 1 (so we now have 4) Start at the index that contains the first item in the array (<code>myNumbers[4]</code> = 3) and run down the array until we reach the last item in the array (<code>myNumbers[3]</code> = 5) So the above code will end up displaying the items in the array in reverse order, starting at index 4 and working backwards to index 0. At this point, we have a good idea of how we can access the values in an array in a specific order, but how do we go the other way?</p><p>It would be great if we could simply use the same code as above, but add a “–” operator before the last number. That way, we could add a simple parameter to the code to control the range of numbers it will use. Unfortunately, we can’t do that. While JavaScript will happily work with <code>--</code> , it will also happily take <code>--</code> in code and convert it to −1 , or the last number in an array. That’s not going to do us any good, as we want the code to be flexible enough that we can work with a number that we specify as the range of numbers.</p><p>So let’s look at a simple function that can give us the results we need.</p><div class=&quot;sourceCode&quot; id=&quot;cb3&quot;><pre class=&quot;sourceCode JavaScript&quot;><code class=&quot;sourceCode javascript&quot;><span id=&quot;cb3-1&quot;><a href=&quot;#cb3-1&quot;></a><span class=&quot;kw&quot;>function</span> <span class=&quot;fu&quot;>rotate</span>(numbers<span class=&quot;op&quot;>,</span> direction<span class=&quot;op&quot;>,</span> number) { numbers<span class=&quot;op&quot;>.</span><span class=&quot;fu&quot;>reverse</span>()<span class=&quot;op&quot;>;</span> numbers<span class=&quot;op&quot;>.</span><span class=&quot;fu&quot;>push</span>(number)<span class=&quot;op&quot;>;</span> numbers<span class=&quot;op&quot;>.</span><span class=&quot;fu&quot;>unshift</span>(number)<span class=&quot;op&quot;>;</span> }<span class=&quot;op&quot;>;</span></span></code></pre></div><p>The <code>rotate</code> function above has three parameters, and will rotate the values in the numbers array. …</p>">JavaScript tutorials</a> about rotating arrays.</p>
<p><span> Not the whole picture, but a big part.</span> Does it set <span>SOTA</span> on every task? No, of course not. But the question is not whether we can lawyerly find any way in which it might not work, but <a href="https://www.gwern.net/Complexity-vs-AI#technology-forecasting-errors-functional-fixedness-in-assuming-dependencies" data-popup-title="Technology forecasting errors: functional fixedness in assuming dependencies" data-popup-author="Gwern Branwen" data-popup-date="2018-01-29" data-popup-abstract="<p>A classic cognitive bias in technological forecasting is motivated-stopping and lack of imagination in considering possibilities. Many people use a mental model of technologies in which they proceed in a serial sequential fashion and assume every step is necessary and only all together are they sufficient, and note that some particular step is difficult or unlikely to succeed and thus as a whole it will fail &amp;amp; never happen. But in reality, few steps are truly required. A technology only needs to succeed in one way to succeed, and to fail it must fail in all ways. There may be many ways to work around, approximate, brute force, reduce the need for, or skip entirely a step, or redefine the problem to no longer involve that step at all. Examples of this include the parallel projects used by the Manhattan Project &amp;amp; Apollo program, which reasoned that despite the formidable difficulties in each path to the end goal, at least one would work out—and they did. In forecasting, to counter this bias, one should make a strong effort to imagine <em>all</em> possible alternatives which could be pursued in parallel, and remember that overall failure requires <em>all</em> of them to fail.</p>">whether there is any way which it might work</a>. And there are many ways it might work better (see the <a href="https://arxiv.org/pdf/2005.14165.pdf&amp;org=openai#section.5" title="GPT-3: Language Models are Few-Shot Learners: 5. Limitations">“Limitations” section</a> for just a few). Does <span>GPT-3</span> <em>do</em> anything like steer a robot around SF shooting lasers and rockets at humans? No, of course not. It is ‘just’ a text prediction model, an idiot savant of text; but an idiot savant, we should remember, is only a genetic mutation or bit of brain damage away from a normal human. If RL is the cherry on the top of the supervised learning, and unsupervised learning is the frosting on top of the unsupervised learning cake, well, the bakers are getting pretty good.</p>
<p><span> Scaling still working.</span> I was surprised, as I had expected closer to 100b parameters, and I thought that the performance of <a href="https://arxiv.org/abs/1909.05858#salesforce" data-popup-title="CTRL: A Conditional Transformer Language Model for Controllable Generation" data-popup-author="Nitish Shirish Keskar, Bryan McCann, Lav R. Varshney, Caiming Xiong, Richard Socher" data-popup-date="2020-04-25" data-popup-abstract="Large-scale language models show promising text generation capabilities, but users cannot easily control particular aspects of the generated text. We release <span class=&quot;smallcaps-auto&quot;>CTRL</span>, a 1.63 billion-parameter conditional transformer language model, trained to condition on control codes that govern style, content, and task-specific behavior. Control codes were derived from structure that naturally co-occurs with raw text, preserving the advantages of unsupervised learning while providing more explicit control over text generation. These codes also allow <span class=&quot;smallcaps-auto&quot;>CTRL</span> to predict which parts of the training data are most likely given a sequence. This provides a potential method for analyzing large amounts of data via model-based source attribution. We have released multiple full-sized, pretrained versions of <span class=&quot;smallcaps-auto&quot;>CTRL</span> at https://github.com/salesforce/ctrl."><span>CTRL</span></a>/<a href="https://arxiv.org/abs/2001.09977#google" data-popup-title="Towards a Human-like Open-Domain Chatbot" data-popup-author="Daniel Adiwardana, Minh-Thang Luong, David R. So, Jamie Hall, Noah Fiedel, Romal Thoppilan, Zi Yang, Apoorv Kulshreshtha, Gaurav Nemade, Yifeng Lu, Quoc V. Le" data-popup-date="2020-01-29" data-popup-abstract="We present Meena, a multi-turn open-domain chatbot trained end-to-end on data mined and filtered from public domain social media conversations. This 2.6B parameter neural network is trained to minimize perplexity, an automatic metric that we compare against human judgement of multi-turn conversation quality. To capture this judgement, we propose a human evaluation metric called Sensibleness and Specificity Average (<span class=&quot;smallcaps-auto&quot;>SSA</span>), which captures key elements of good conversation. Interestingly, our experiments show strong correlation between perplexity and <span class=&quot;smallcaps-auto&quot;>SSA</span>. The fact that the best perplexity end-to-end trained Meena scores high on <span class=&quot;smallcaps-auto&quot;>SSA</span> (72% on multi-turn evaluation) suggests that a human-level <span class=&quot;smallcaps-auto&quot;>SSA</span> of 86% is potentially within reach if we can better optimize perplexity. Additionally, the full version of Meena (with a filtering mechanism and tuned decoding) scores 79% <span class=&quot;smallcaps-auto&quot;>SSA</span>, 23% higher than the next highest scoring chatbot that we evaluated.">Meena</a>/<a href="https://nv-adlr.github.io/MegatronLM" data-popup-title="MegatronLM: Training Billion+ Parameter Language Models Using GPU Model Parallelism" data-popup-author="<span class=&quot;smallcaps-auto&quot;>NVIDIA</span> <span class=&quot;smallcaps-auto&quot;>ADLR</span>" data-popup-date="2019-08-13" data-popup-abstract="Larger language models are dramatically more useful for <span class=&quot;smallcaps-auto&quot;>NLP</span> tasks such as article completion, question answering, and dialog systems. Training the largest neural language model has recently been the best way to advance the state of the art in <span class=&quot;smallcaps-auto&quot;>NLP</span> applications. Two recent papers, <a href=&quot;https://arxiv.org/abs/1810.04805&quot;><span class=&quot;smallcaps-auto&quot;>BERT</span></a> and <a href=&quot;https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf&quot;><span class=&quot;smallcaps-auto&quot;>GPT</span>-2</a>, demonstrate the benefits of large scale language modeling. Both papers leverage advances in compute and available text corpora to significantly surpass state of the art performance in natural language understanding, modeling, and generation. Training these models requires hundreds of exaflops of compute and <a href=&quot;https://arxiv.org/abs/1604.06174&quot;>clever memory management</a> to trade recomputation for a reduced memory footprint. However, for very large models beyond a billion parameters, the memory on a single <span class=&quot;smallcaps-auto&quot;>GPU</span> is not enough to fit the model along with the parameters needed for training, requiring model parallelism to split the parameters across multiple <span class=&quot;smallcaps-auto&quot;>GPU</span>s. Several approaches to model parallelism exist, but they are difficult to use, either because they rely on custom compilers, or because they scale poorly or require changes to the optimizer.</p></div><div><p> In this work, we implement a simple and efficient model parallel approach by making only a few targeted modifications to existing <a href=&quot;https://openreview.net/pdf?id=BJJsrmfCZ&quot;>PyTorch</a> transformer implementations. <a href=&quot;https://github.com/nvidia/megatron-lm&quot;>Our code</a> is written in native Python, leverages mixed precision training, and utilizes the <a href=&quot;https://developer.nvidia.com/nccl&quot;><span class=&quot;smallcaps-auto&quot;>NCCL</span> library</a> for communication between <span class=&quot;smallcaps-auto&quot;>GPU</span>s. We showcase this approach by training an 8.3 billion parameter transformer language model with 8-way model parallelism and 64-way data parallelism on 512 <span class=&quot;smallcaps-auto&quot;>GPU</span>s, making it the <b>largest transformer based language model ever trained at 24✕ the size of <span class=&quot;smallcaps-auto&quot;>BERT</span> and 5.6✕ the size of <span class=&quot;smallcaps-auto&quot;>GPT</span>-2</b>. We have published the code that implements this approach at <a href=&quot;https://github.com/NVIDIA/Megatron-LM&quot;>our GitHub repository</a>.</p></div><div><p> Our experiments are conducted on <span class=&quot;smallcaps-auto&quot;>NVIDIA</span>’s <a href=&quot;https://devblogs.nvidia.com/dgx-superpod-world-record-supercomputing-enterprise&quot;><span class=&quot;smallcaps-auto&quot;>DGX</span> Super<span class=&quot;smallcaps-auto&quot;>POD</span></a>. Without model parallelism, we can fit a baseline model of 1.2B parameters on a single V100 32GB <span class=&quot;smallcaps-auto&quot;>GPU</span>, and sustain 39 Tera<span class=&quot;smallcaps-auto&quot;>FLOPS</span> during the overall training process, which is 30% of the theoretical peak <span class=&quot;smallcaps-auto&quot;>FLOPS</span> for a single <span class=&quot;smallcaps-auto&quot;>GPU</span> in a <span class=&quot;smallcaps-auto&quot;>DGX</span>2-H server. Scaling the model to 8.3 billion parameters on 512 <span class=&quot;smallcaps-auto&quot;>GPU</span>s with 8-way model parallelism, we achieved up to <b>15.1 Peta<span class=&quot;smallcaps-auto&quot;>FLOPS</span> sustained performance</b> over the entire application and reached <b>76% scaling efficiency</b> compared to the single <span class=&quot;smallcaps-auto&quot;>GPU</span> case.">MegatronLM</a>/<a href="https://arxiv.org/abs/1910.10683#google" data-popup-title="Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer" data-popup-author="Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J. Liu" data-popup-date="2020-02-23" data-popup-abstract="Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (<span class=&quot;smallcaps-auto&quot;>NLP</span>). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for <span class=&quot;smallcaps-auto&quot;>NLP</span> by introducing a unified framework that converts every language problem into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled datasets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new &quot;Colossal Clean Crawled Corpus&quot;, we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for <span class=&quot;smallcaps-auto&quot;>NLP</span>, we release our dataset, pre-trained models, and code.">T5</a>/<a href="https://www.gwern.net/docs/www/www.microsoft.com/879d00b0771726c7a08e6951f1aa29a2f16aa062.html" data-popup-title="Turing-<span class=&quot;smallcaps-auto&quot;>NLG: A 17-billion-parameter language model by Microsoft" data-popup-author="Corby Rosset (Microsoft)" data-popup-date="2020-02-10" data-popup-abstract="<p><img class=&quot;aligncenter wp-image-635634 size-full&quot; src=&quot;/images/ai/2020-rosset-turingnlg-nlpmodelparametercountovertime.png&quot; alt=&quot;chart (https://www.microsoft.com/en-us/research/uploads/prod/2020/02/TurningNGL_Model__1400x788.png)&quot; width=&quot;1400&quot; height=&quot;788&quot; srcset=&quot;https://www.microsoft.com/en-us/research/uploads/prod/2020/02/TurningNGL_Model__1400x788.png 1400w, https://www.microsoft.com/en-us/research/uploads/prod/2020/02/TurningNGL_Model__1400x788-300x169.png 300w, https://www.microsoft.com/en-us/research/uploads/prod/2020/02/TurningNGL_Model__1400x788-1024x576.png 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2020/02/TurningNGL_Model__1400x788-768x432.png 768w, https://www.microsoft.com/en-us/research/uploads/prod/2020/02/TurningNGL_Model__1400x788-1066x600.png 1066w, https://www.microsoft.com/en-us/research/uploads/prod/2020/02/TurningNGL_Model__1400x788-655x368.png 655w, https://www.microsoft.com/en-us/research/uploads/prod/2020/02/TurningNGL_Model__1400x788-343x193.png 343w, https://www.microsoft.com/en-us/research/uploads/prod/2020/02/TurningNGL_Model__1400x788-640x360.png 640w, https://www.microsoft.com/en-us/research/uploads/prod/2020/02/TurningNGL_Model__1400x788-960x540.png 960w, https://www.microsoft.com/en-us/research/uploads/prod/2020/02/TurningNGL_Model__1400x788-1280x720.png 1280w&quot; sizes=&quot;(max-width: 1400px) 100vw, 1400px&quot; /></p><blockquote><p style=&quot;text-align: left;&quot;><strong><em>Turing Natural Language Generation (T-<span class=&quot;smallcaps-auto&quot;>NLG</span>) is a 17 billion parameter language model by Microsoft that outperforms the state of the art on many downstream <span class=&quot;smallcaps-auto&quot;>NLP</span> tasks. We present a demo of the model, including its freeform generation, question answering, and summarization capabilities, to academics for feedback and research purposes. &amp;lt;|endoftext|&amp;gt;</em></strong></p><p style=&quot;text-align: left;&quot;>– This summary was generated by the Turing-<span class=&quot;smallcaps-auto&quot;>NLG</span> language model itself.</p></blockquote><p>…Following the trend that larger natural language models lead to better results, Microsoft is introducing Turing Natural Language Generation (T-<span class=&quot;smallcaps-auto&quot;>NLG</span>), the largest model ever published at 17 billion parameters, which outperforms the state of the art on a variety of language modeling benchmarks and also excels when applied to numerous practical tasks, including summarization and question answering. This work would not be possible without breakthroughs produced by the <b><a href=&quot;https://github.com/microsoft/DeepSpeed&quot;>DeepSpeed library</a></b> (compatible with <a href=&quot;https://pytorch.org/&quot;>PyTorch</a>) and <a href=&quot;https://arxiv.org/abs/1910.02054&quot;>ZeRO optimizer</a>, which can be explored more in this accompanying <a href=&quot;https://www.microsoft.com/en-us/research/blog/zero-deepspeed-new-system-optimizations-enable-training-models-with-over-100-billion-parameters&quot;>blog post.</a></p>" rel="archived alternate nofollow" data-url-original="https://www.microsoft.com/en-us/research/blog/turing-nlg-a-17-billion-parameter-language-model-by-microsoft/">Turing-<span>NLG</span></a>/<a href="https://arxiv.org/abs/1811.06965#google" data-popup-title="GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism" data-popup-author="Yanping Huang, Youlong Cheng, Ankur Bapna, Orhan Firat, Mia Xu Chen, Dehao Chen, HyoukJoong Lee, Jiquan Ngiam, Quoc V. Le, Yonghui Wu, Zhifeng Chen" data-popup-date="2020-06-05" data-popup-abstract="Scaling up deep neural network capacity has been known as an effective approach to improving model quality for several different machine learning tasks. In many cases, increasing model capacity beyond the memory limit of a single accelerator has required developing special algorithms or infrastructure. These solutions are often architecture-specific and do not transfer to other tasks. To address the need for efficient and task-independent model parallelism, we introduce GPipe, a pipeline parallelism library that allows scaling any network that can be expressed as a sequence of layers. By pipelining different sub-sequences of layers on separate accelerators, GPipe provides the flexibility of scaling a variety of different networks to gigantic sizes efficiently. Moreover, GPipe utilizes a novel batch-splitting pipelining algorithm, resulting in almost linear speedup when a model is partitioned across multiple accelerators. We demonstrate the advantages of GPipe by training large-scale neural networks on two different tasks with distinct network architectures: (i) Image Classification: We train a 557-million-parameter AmoebaNet model and attain a top-1 accuracy of 84.4% on ImageNet-2012, (ii) Multilingual Neural Machine Translation: We train a single 6-billion-parameter, 128-layer Transformer model on a corpus spanning over 100 languages and achieve better quality than all bilingual models.">GPipe</a> suggested that, the scaling papers<a href="#sn6" id="fnref6" role="doc-noteref"><sup>6</sup></a> notwithstanding, the scaling curves had started to bend and by 100b, it might be hard to justify further scaling. However, <span>GPT-3</span> hits twice that without noticeable change in scaling factors: its scaling continues to be roughly logarithmic/power-law, as it was for much smaller models &amp; as forecast, and it has not hit a regime where gains effectively halt or start to require increases vastly beyond feasibility. That suggests that it would be both possible and useful to head to trillions of parameters (which are still well within available compute &amp; budgets, requiring merely thousands of <span>GPU</span>s &amp; perhaps $10–$100m budgets assuming no improvements which of course there will be, see Hernandez &amp; Brown 2020 etc in this issue), and eyeballing the graphs, many benchmarks like the <a href="https://en.wikipedia.org/wiki/Winograd_Schema_Challenge" data-popup-title="Winograd Schema Challenge" data-popup-author="English Wikipedia" data-popup-abstract="<p>
The <b>Winograd Schema Challenge</b> (<b>WSC</b>) is a test of machine intelligence proposed by Hector Levesque, a computer scientist at the University of Toronto. Designed to be an improvement on the Turing test, it is a multiple-choice test that employs questions of a very specific structure: they are instances of what are called Winograd Schemas, named after Terry Winograd, professor of computer science at Stanford University.</p>">Winograd schema</a> <a href="https://arxiv.org/abs/1907.10641#allen" data-popup-title="WinoGrande: An Adversarial Winograd Schema Challenge at Scale" data-popup-author="Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, Yejin Choi" data-popup-date="2020-06-01" data-popup-abstract="The Winograd Schema Challenge (<span class=&quot;smallcaps-auto&quot;>WSC</span>) (Levesque, Davis, and Morgenstern 2011), a benchmark for commonsense reasoning, is a set of 273 expert-crafted pronoun resolution problems originally designed to be unsolvable for statistical models that rely on selectional preferences or word associations. However, recent advances in neural language models have already reached around 90% accuracy on variants of <span class=&quot;smallcaps-auto&quot;>WSC</span>. This raises an important question whether these models have truly acquired robust commonsense capabilities or whether they rely on spurious biases in the datasets that lead to an overestimation of the true capabilities of machine commonsense. To investigate this question, we introduce WinoGrande, a large-scale dataset of 44k problems, inspired by the original <span class=&quot;smallcaps-auto&quot;>WSC</span> design, but adjusted to improve both the scale and the hardness of the dataset. The key steps of the dataset construction consist of (1) a carefully designed crowdsourcing procedure, followed by (2) systematic bias reduction using a novel AfLite algorithm that generalizes human-detectable word associations to machine-detectable embedding associations. The best state-of-the-art methods on WinoGrande achieve 59.4-79.1%, which are 15-35% below human performance of 94.0%, depending on the amount of the training data allowed. Furthermore, we establish new state-of-the-art results on five related benchmarks - <span class=&quot;smallcaps-auto&quot;>WSC</span> (90.1%), <span class=&quot;smallcaps-auto&quot;>DPR</span> (93.1%), <span class=&quot;smallcaps-auto&quot;>COPA</span> (90.6%), KnowRef (85.6%), and Winogender (97.1%). These results have dual implications: on one hand, they demonstrate the effectiveness of WinoGrande when used as a resource for transfer learning. On the other hand, they raise a concern that we are likely to be overestimating the true capabilities of machine commonsense across all these benchmarks. We emphasize the importance of algorithmic bias reduction in existing and future benchmarks to mitigate such overestimation.">WinoGrande</a> would fall by 10t parameters.</p>
<div>
<blockquote>
<p>“Extrapolating the spectacular performance of <span>GPT-3</span> into the future suggests that the answer to life, the universe and everything is just 4.398 trillion parameters.”</p>
<p><a href="https://twitter.com/geoffreyhinton/status/1270814602931187715">Geoff Hinton</a>, joking around—right?</p>
</blockquote>
</div>
<p><span> We don’t know how to train NNs.</span> As I keep saying, “NNs are lazy” and can do far more than we make them do when we push them beyond easy answers &amp; cheap shortcuts. The <a href="http://www.incompleteideas.net/IncIdeas/BitterLesson.html" data-popup-title="The Bitter Lesson" data-popup-author="Rich Sutton" data-popup-date="2019-03-13" data-popup-abstract="<p><strong>The biggest lesson that can be read from 70 years of AI research is that general methods that leverage computation are ultimately the most effective</strong>, and by a large margin. The ultimate reason for this is Moore’s law, or rather its generalization of continued exponentially falling cost per unit of computation. Most AI research has been conducted as if the computation available to the agent were constant (in which case leveraging human knowledge would be one of the only ways to improve performance) but, over a slightly longer time than a typical research project, massively more computation inevitably becomes available. Seeking an improvement that makes a difference in the shorter term, researchers seek to leverage their human knowledge of the domain, but the only thing that matters in the long run is the leveraging of computation.</p><p>…In computer chess, the methods that defeated the world champion, Kasparov, in 1997, were based on massive, deep search. At the time, this was looked upon with dismay by the majority of computer-chess researchers who had pursued methods that leveraged human understanding of the special structure of chess…A similar pattern of research progress was seen in computer Go, only delayed by a further 20 years. Enormous initial efforts went into avoiding search by taking advantage of human knowledge, or of the special features of the game, but all those efforts proved irrelevant, or worse, once search was applied effectively at scale…In speech recognition, there was an early competition, sponsored by DARPA, in the 1970s. Entrants included a host of special methods that took advantage of human knowledge—knowledge of words, of phonemes, of the human vocal tract, etc. On the other side were newer methods that were more statistical in nature and did much more computation, based on hidden Markov models (HMMs). Again, the statistical methods won out over the human-knowledge-based methods… In computer vision…Modern deep-learning neural networks use only the notions of convolution and certain kinds of invariances, and perform much better.</p><p>…We have to learn the bitter lesson that building in how we think we think does not work in the long run. The bitter lesson is based on the historical observations that (1) AI researchers have often tried to build knowledge into their agents, (2) this always helps in the short term, and is personally satisfying to the researcher, but (3) in the long run it plateaus and even inhibits further progress, and (4) breakthrough progress eventually arrives by an opposing approach based on scaling computation by search and learning. The eventual success is tinged with bitterness, and often incompletely digested, because it is success over a favored, human-centric approach.</p>">bitter lesson</a> is the harder and bigger, the better. (Besides <span>GPT-3</span>, one could mention recent progress in semi-supervised learning &amp; the model-based <span>DRL</span> renaissance.)</p>
<p><span> Blessings of scale: stability→generalization→meta-learning.</span> <span>GPT-3</span> is hamstrung by its training &amp; data, but DL enjoys an unreasonably effective blessing of dimensionality—just simply training a <em>big</em> model on a <em>lot</em> of data induces better properties like meta-learning without even the slightest bit of that architecture being built in; and in general, training on more and harder tasks creates ever more human-like performance, generalization, and robustness. <a href="https://arxiv.org/pdf/1912.06680.pdf&amp;org=openai#subsection.4.3" title="'Dota 2 with Large Scale Deep Reinforcement Learning: 4.3 Bath Size', Berner et al 2019">OA5</a> does not just scale to, but stabilizes at, minibatches of millions due to <a href="https://openai.com/blog/science-of-ai/" data-popup-title="How AI Training Scales" data-popup-author="Sam McCandlish, Jared Kaplan, Dario Amodei (OpenAI)" data-popup-date="2018-12-14" data-popup-abstract="<p><a href=&quot;https://arxiv.org/abs/1812.06162&quot; title=&quot;&amp;#39;An Empirical Model of Large-Batch Training&amp;#39;, McCandlish et al 2018&quot;>We’ve discovered</a> that the gradient noise scale, a simple statistical metric, predicts the parallelizability of neural network training on a wide range of tasks. Since complex tasks tend to have noisier gradients, increasingly large batch sizes are likely to become useful in the future, removing one potential limit to further growth of AI systems. More broadly, these results show that neural network training need not be considered a mysterious art, but can be rigorized and systematized.</p><blockquote><p>In an increasing number of domains it has been demonstrated that deep learning models can be trained using relatively large batch sizes without sacrificing data efficiency. However the limits of this massive data parallelism seem to differ from domain to domain, ranging from batches of tens of thousands in ImageNet to batches of millions in RL agents that play the game Dota 2. To our knowledge there is limited conceptual understanding of why these limits to batch size differ or how we might choose the correct batch size in a new domain. In this paper, we demonstrate that a simple and easy-to-measure statistic called the <em>gradient noise scale</em> predicts the largest useful batch size across many domains and applications, including a number of supervised learning datasets (<span class=&quot;smallcaps-auto&quot;>MNIST</span>, <span class=&quot;smallcaps-auto&quot;>SVHN</span>, <span class=&quot;smallcaps-auto&quot;>CIFAR</span>-10, ImageNet, Billion Word), reinforcement learning domains (Atari and Dota), and even generative model training (autoencoders on <span class=&quot;smallcaps-auto&quot;>SVHN</span>). We find that the noise scale increases as the loss decreases over a training run and depends on the model size primarily through improved model performance. Our empirically-motivated theory also describes the tradeoff between compute-efficiency and time-efficiency, and provides a rough model of the benefits of adaptive batch-size training.</p></blockquote><figure><img src=&quot;/images/ai/2018-mccandlish-openai-howaitrainingscales-gradientnoisescale-summary3-scalevsbatchsize.svg&quot; alt=&quot;The gradient noise scale (appropriately averaged over training) explains the vast majority (R<sup>2</sup> = 80%) of the variation in critical batch size over a range of tasks spanning six orders of magnitude. Batch sizes are measured in either number of images, tokens (for language models), or observations (for games). (https://openai.com/content/images/2018/12/noise-summary-3.svg)&quot; /><figcaption>The gradient noise scale (appropriately averaged over training) explains the vast majority (R<sup>2</sup> = 80%) of the variation in critical batch size over a range of tasks spanning six orders of magnitude. Batch sizes are measured in either number of images, tokens (for language models), or observations (for games).</figcaption></figure><p>…We have found that by measuring the gradient noise scale, a simple statistic that quantifies the signal-to-noise ratio of the network gradients, we can approximately predict the maximum useful batch size. Heuristically, the noise scale measures the variation in the data as seen by the model (at a given stage in training). When the noise scale is small, looking at a lot of data in parallel quickly becomes redundant, whereas when it is large, we can still learn a lot from huge batches of data…We’ve found it helpful to visualize the results of these experiments in terms of a tradeoff between wall time for training and total bulk compute that we use to do the training (proportional to dollar cost). At very small batch sizes, doubling the batch allows us to train in half the time without using extra compute (we run twice as many chips for half as long). At very large batch sizes, more parallelization doesn’t lead to faster training. There is a “bend” in the curve in the middle, and the gradient noise scale predicts where that bend occurs.</p><figure><img src=&quot;/images/ai/2018-mccandlish-openai-howaitrainingscales-gradientnoisescale-paretofrontier.svg&quot; alt=&quot;Increasing parallelism makes it possible to train more complex models in a reasonable amount of time. We find that a Pareto frontier chart is the most intuitive way to visualize comparisons between algorithms and scales. (https://openai.com/content/images/2018/12/basic-scaling-1.svg)&quot; /><figcaption>Increasing parallelism makes it possible to train more complex models in a reasonable amount of time. We find that a Pareto frontier chart is the most intuitive way to visualize comparisons between algorithms and scales.</figcaption></figure><p>…more powerful models have a higher gradient noise scale, but only because they achieve a lower loss. Thus, there’s some evidence that the increasing noise scale over training isn’t just an artifact of convergence, but occurs because the model gets better. If this is true, then we expect future, more powerful models to have higher noise scale and therefore be more parallelizable. Second, tasks that are subjectively more difficult are also more amenable to parallelization…we have evidence that more difficult tasks and more powerful models on the same task will allow for more radical data-parallelism than we have seen to date, providing a key driver for the continued fast exponential growth in training compute.</p>">gradient noise</a>. OA5-like, <a href="https://arxiv.org/pdf/1809.11096.pdf&amp;org=deepmind#page=8" data-popup-title="Big<span class=&quot;smallcaps-auto&quot;>GAN: Large Scale <span class=&quot;smallcaps-auto&quot;>GAN Training For High Fidelity Natural Image Synthesis: 5.2 Additional Evaluation On <span class=&quot;smallcaps-auto&quot;>JFT-300M" data-popup-author="Andrew Brock, Jeff Donahue, Karen Simonyan" data-popup-date="2019-08-26" data-popup-abstract="<p>…To confirm that our design choices are effective for even larger and more complex and diverse datasets, we also present results of our system on a subset of <span class=&quot;smallcaps-auto&quot;>JFT</span>-300M (<a href=&quot;https://arxiv.org/abs/1707.02968#google&quot; title=&quot;Revisiting unreasonable effectiveness of data in deep learning era&quot;>Sun et al., 2017</a>). The full <span class=&quot;smallcaps-auto&quot;>JFT</span>-300M dataset contains 300M real-world images labeled with 18K categories. Since the category distribution is heavily long-tailed, we subsample the dataset to keep only images with the 8.5K most common labels. The resulting dataset contains 292M images—two orders of magnitude larger than ImageNet.</p><p>…Our results show that these techniques substantially improve performance even in the setting of this much larger dataset at the same model capacity (64 base channels). We further show that for a dataset of this scale, we see significant additional improvements from expanding the capacity of our models to 128 base channels, while for ImageNet <span class=&quot;smallcaps-auto&quot;>GAN</span>s that additional capacity was not beneficial. In <a href=&quot;https://arxiv.org/pdf/1809.11096.pdf&amp;amp;org=deepmind#figure.caption.30&quot; title=&quot;Figure 19: JFT-300M IS vs FID at 256×256px&quot;>Figure 19</a> (Appendix D), we present truncation plots for models trained on this dataset…Interestingly, unlike models trained on ImageNet, where training tends to collapse without heavy regularization ([Section 4])(https://arxiv.org/pdf/1809.11096.pdf&amp;amp;org=deepmind#section.4 “Characterizing Instability: The Generator/Discriminator/Conclusions”), the models trained on <span class=&quot;smallcaps-auto&quot;>JFT</span>-300M remain stable over many hundreds of thousands of iterations. This suggests that moving beyond ImageNet to larger datasets may partially alleviate <span class=&quot;smallcaps-auto&quot;>GAN</span> stability issues.</p>">Big<span>GAN</span></a> stabilizes at large-scale image datasets like <span>JFT-300M</span> &amp; benefits from unusually large minibatches, while classifier <span>CNN</span>s like <a href="https://arxiv.org/abs/1912.11370#google" data-popup-title="Large Scale Learning of General Visual Representations for Transfer" data-popup-author="Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Joan Puigcerver, Jessica Yung, Sylvain Gelly, Neil Houlsby" data-popup-date="2020-01-10" data-popup-abstract="Transfer of pre-trained representations improves sample efficiency and simplifies hyperparameter tuning when training deep neural networks for vision. We revisit the paradigm of pre-training on large supervised datasets and fine-tuning the weights on the target task. We scale up pre-training, and create a simple recipe that we call Big Transfer (BiT). By combining a few carefully selected components, and transferring using a simple heuristic, we achieve strong performance on over 20 datasets. BiT performs well across a surprisingly wide range of data regimes—from 10 to 1M labeled examples. BiT achieves 87.8% top-1 accuracy on <span class=&quot;smallcaps-auto&quot;>ILSVRC</span>-2012, 99.3% on <span class=&quot;smallcaps-auto&quot;>CIFAR</span>-10, and 76.7% on the Visual Task Adaptation Benchmark (which includes 19 tasks). On small datasets, BiT attains 86.4% on <span class=&quot;smallcaps-auto&quot;>ILSVRC</span>-2012 with 25 examples per class, and 97.6% on <span class=&quot;smallcaps-auto&quot;>CIFAR</span>-10 with 10 examples per class. We conduct detailed analysis of the main components that lead to high transfer performance.">BiT</a> transfer &amp; robustify with human-like errors<a href="#sn7" id="fnref7" role="doc-noteref"><sup>7</sup></a>, multimodal learning produces better representations on less data (eg <a href="https://arxiv.org/abs/1912.02315#facebook" data-popup-title="12-in-1: Multi-Task Vision and Language Representation Learning" data-popup-author="Jiasen Lu, Vedanuj Goswami, Marcus Rohrbach, Devi Parikh, Stefan Lee" data-popup-date="2020-06-01" data-popup-abstract="Much of vision-and-language research focuses on a small but diverse set of independent tasks and supporting datasets often studied in isolation; however, the visually-grounded language understanding skills required for success at these tasks overlap significantly. In this work, we investigate these relationships between vision-and-language tasks by developing a large-scale, multi-task training regime. Our approach culminates in a single model on 12 datasets from four broad categories of task including visual question answering, caption-based image retrieval, grounding referring expressions, and multi-modal verification. Compared to independently trained single-task models, this represents a reduction from approximately 3 billion parameters to 270 million while simultaneously improving performance by 2.05 points on average across tasks. We use our multi-task framework to perform in-depth analysis of the effect of joint training diverse tasks. Further, we show that finetuning task-specific models from our single multi-task model can lead to further improvements, achieving performance at or above the state-of-the-art.">Vi<span>LBERT</span></a>/<a href="https://arxiv.org/abs/1904.01766#google" data-popup-title="VideoBERT: A Joint Model for Video and Language Representation Learning" data-popup-author="Chen Sun, Austin Myers, Carl Vondrick, Kevin Murphy, Cordelia Schmid" data-popup-date="2020-06-01" data-popup-abstract="Self-supervised learning has become increasingly important to leverage the abundance of unlabeled data available on platforms like YouTube. Whereas most existing approaches learn low-level representations, we propose a joint visual-linguistic model to learn high-level features without any explicit supervision. In particular, inspired by its recent success in language modeling, we build upon the <span class=&quot;smallcaps-auto&quot;>BERT</span> model to learn bidirectional joint distributions over sequences of visual and linguistic tokens, derived from vector quantization of video data and off-the-shelf speech recognition outputs, respectively. We use Video<span class=&quot;smallcaps-auto&quot;>BERT</span> in numerous tasks, including action classification and video captioning. We show that it can be applied directly to open-vocabulary classification, and confirm that large amounts of training data and cross-modal information are critical to performance. Furthermore, we outperform the state-of-the-art on video captioning, and quantitative results verify that the model learns high-level semantic features.">Video<span>BERT</span></a>, motivating <a href="https://www.gwern.net/docs/www/www.technologyreview.com/8ec24a193fd7caede72e77306220ff9434044075.html" data-popup-title="The messy, secretive reality behind OpenAI’s bid to save the world: The AI moonshot was founded in the spirit of transparency. This is the inside story of how competitive pressure eroded that idealism." data-popup-author="Karen Hao" data-popup-date="2020-02-17" data-popup-abstract="<p>There are two prevailing technical theories about what it will take to reach <span class=&quot;smallcaps-auto&quot;>AGI</span>. In one, all the necessary techniques already exist; it’s just a matter of figuring out how to scale and assemble them. In the other, there needs to be an entirely new paradigm; deep learning, the current dominant technique in AI, won’t be enough. Most researchers fall somewhere between these extremes, but OpenAI has consistently sat almost exclusively on the scale-and-assemble end of the spectrum. Most of its breakthroughs have been the product of sinking dramatically greater computational resources into technical innovations developed in other labs.</p><p>Brockman and Sutskever deny that this is their sole strategy, but the lab’s tightly guarded research suggests otherwise. A team called “Foresight” runs experiments to test how far they can push AI capabilities forward by training existing algorithms with increasingly large amounts of data and computing power. For the leadership, the results of these experiments have confirmed its instincts that the lab’s all-in, compute-driven strategy is the best approach. For roughly six months, these results were hidden from the public because OpenAI sees this knowledge as its primary competitive advantage. Employees and interns were explicitly instructed not to reveal them, and those who left signed nondisclosure agreements. It was only in January that the team, without the usual fanfare, <a href=&quot;https://arxiv.org/abs/2001.08361#openai&quot; title=&quot;&amp;#39;Scaling Laws for Neural Language Models&amp;#39;, Kaplan et al 2020&quot;>quietly posted a paper</a> on one of the primary open-source databases for AI research. People who experienced the intense secrecy around the effort didn’t know what to make of this change. Notably, <a href=&quot;https://arxiv.org/abs/1909.12673&quot; title=&quot;&amp;#39;A Constructive Prediction of the Generalization Error Across Scales&amp;#39;, Rosenfeld et al 2019&quot;>another paper with similar results</a> from different researchers had been posted a month earlier.</p><p>…One of the biggest secrets is the project OpenAI is working on next. Sources described it to me as the culmination of its previous four years of research: an AI system trained on images, text, and other data using massive computational resources. A small team has been assigned to the initial effort, with an expectation that other teams, along with their work, will eventually fold in. On the day it was announced at an all-company meeting, interns weren’t allowed to attend. People familiar with the plan offer an explanation: the leadership thinks this is the most promising way to reach <span class=&quot;smallcaps-auto&quot;>AGI</span>.</p><p>…The man driving OpenAI’s strategy is Dario Amodei, the ex-Googler who now serves as research director. When I meet him, he strikes me as a more anxious version of Brockman. He has a similar sincerity and sensitivity, but an air of unsettled nervous energy. He looks distant when he talks, his brows furrowed, a hand absentmindedly tugging his curls. Amodei divides the lab’s strategy into two parts. The first part, which dictates how it plans to reach advanced AI capabilities, he likens to an investor’s “portfolio of bets.” Different teams at OpenAI are playing out different bets. The language team, for example, has its money on a theory postulating that AI can develop a significant understanding of the world through mere language learning. The robotics team, in contrast, is advancing an opposing theory that intelligence requires a physical embodiment to develop. As in an investor’s portfolio, not every bet has an equal weight. But for the purposes of scientific rigor, all should be tested before being discarded. Amodei points to <span class=&quot;smallcaps-auto&quot;>GPT</span>-2, with its remarkably realistic auto-generated texts, as an instance of why it’s important to keep an open mind. “Pure language is a direction that the field and even some of us were somewhat skeptical of,” he says. “But now it’s like, ‘Wow, this is really promising.’” Over time, as different bets rise above others, they will attract more intense efforts. Then they will cross-pollinate and combine. The goal is to have fewer and fewer teams that ultimately collapse into a single technical direction for <span class=&quot;smallcaps-auto&quot;>AGI</span>. This is the exact process that OpenAI’s latest top-secret project has supposedly already begun.</p>" rel="archived alternate nofollow" data-url-original="https://www.technologyreview.com/s/615181/ai-openai-moonshot-elon-musk-sam-altman-greg-brockman-messy-secretive-reality/">OA’s interest</a>), and <span>RNN</span>s can <a href="https://arxiv.org/abs/1911.01655#google" data-popup-title="High Fidelity Video Prediction with Large Stochastic Recurrent Neural Networks" data-popup-author="Ruben Villegas, Arkanath Pathak, Harini Kannan, Dumitru Erhan, Quoc V. Le, Honglak Lee" data-popup-date="2020-06-01" data-popup-abstract="Predicting future video frames is extremely challenging, as there are many factors of variation that make up the dynamics of how frames change through time. Previously proposed solutions require complex inductive biases inside network architectures with highly specialized computation, including segmentation masks, optical flow, and foreground and background separation. In this work, we question if such handcrafted architectures are necessary and instead propose a different approach: finding minimal inductive bias for video prediction while maximizing network capacity. We investigate this question by performing the first large-scale empirical study and demonstrate state-of-the-art performance by learning large models on three different datasets: one for modeling object interactions, one for modeling human motion, and one for modeling car driving.">predict videos</a>. <a href="https://www.gwern.net/docs/rl/2019-vinyals.pdf#deepmind" data-popup-title="Grandmaster level in StarCraft II using multi-agent reinforcement learning" data-popup-author="Oriol Vinyals, Igor Babuschkin, Wojciech M. Czarnecki, Michaël Mathieu, Andrew Dudzik, Junyoung Chung, David H. Choi, Richard Powell, Timo Ewalds, Petko Georgiev, Junhyuk Oh, Dan Horgan, Manuel Kroiss, Ivo Danihelka, Aja Huang, Laurent Sifre, Trevor Cai, J et al [...]" data-popup-date="2019-10-30" data-popup-doi="10.1038/s41586-019-1724-z" data-popup-abstract="Many real-world applications require artificial agents to compete and coordinate with other agents in complex environments. As a stepping stone to this goal, the domain of StarCraft has emerged as an important challenge for artificial intelligence research, owing to its iconic and enduring status among the most difficult professional e-sports and its relevance to the real world in terms of its raw complexity and multi-agent challenges. Over the course of a decade and numerous competitions, the strongest agents have simplified important aspects of the game, utilized superhuman capabilities, or employed hand-crafted sub-systems. Despite these advantages, no previous agent has come close to matching the overall skill of top StarCraft players. We chose to address the challenge of StarCraft using general-purpose learning methods that are in principle applicable to other complex domains: a multi-agent reinforcement learning algorithm that uses data from both human and agent games within a diverse league of continually adapting strategies and counter-strategies, each represented by deep neural networks. We evaluated our agent, AlphaStar, in the full game of StarCraft II, through a series of online games against human players. AlphaStar was rated at Grandmaster level for all three StarCraft races and above 99.8% of officially ranked human players.">AlphaStar</a> reaches human-level with hundreds of competing self-players to cover possible strategies. Imitation learning <span>DRL</span> like <a href="https://arxiv.org/abs/1810.05017#deepmind" data-popup-title="One-Shot High-Fidelity Imitation: Training Large-Scale Deep Nets with RL" data-popup-author="Tom Le Paine, Sergio Gómez Colmenarejo, Ziyu Wang, Scott Reed, Yusuf Aytar, Tobias Pfaff, Matt W. Hoffman, Gabriel Barth-Maron, Serkan Cabi, David Budden, Nando de Freitas" data-popup-date="2020-06-01" data-popup-abstract="Humans are experts at high-fidelity imitation -- closely mimicking a demonstration, often in one attempt. Humans use this ability to quickly solve a task instance, and to bootstrap learning of new tasks. Achieving these abilities in autonomous agents is an open problem. In this paper, we introduce an off-policy RL algorithm (MetaMimic) to narrow this gap. MetaMimic can learn both (i) policies for high-fidelity one-shot imitation of diverse novel skills, and (ii) policies that enable the agent to solve tasks more efficiently than the demonstrators. MetaMimic relies on the principle of storing all experiences in a memory and replaying these to learn massive deep neural network policies by off-policy RL. This paper introduces, to the best of our knowledge, the largest existing neural networks for deep RL and shows that larger networks with normalization are needed to achieve one-shot high-fidelity imitation on a challenging manipulation task. The results also show that both types of policy can be learned from vision, in spite of the task rewards being sparse, and without access to demonstrator actions.">MetaMimic</a> generalizes at hundreds of tasks to train a deep net. Disentanglement emerges in <a href="https://arxiv.org/abs/1812.04948#nvidia" data-popup-title="A Style-Based Generator Architecture for Generative Adversarial Networks" data-popup-author="Tero Karras, Samuli Laine, Timo Aila" data-popup-date="2020-06-01" data-popup-abstract="We propose an alternative generator architecture for generative adversarial networks, borrowing from style transfer literature. The new architecture leads to an automatically learned, unsupervised separation of high-level attributes (e.g., pose and identity when trained on human faces) and stochastic variation in the generated images (e.g., freckles, hair), and it enables intuitive, scale-specific control of the synthesis. The new generator improves the state-of-the-art in terms of traditional distribution quality metrics, leads to demonstrably better interpolation properties, and also better disentangles the latent factors of variation. To quantify interpolation quality and disentanglement, we propose two new, automated methods that are applicable to any generator architecture. Finally, we introduce a new, highly varied and high-quality dataset of human faces.">Style<span>GAN</span></a> with sufficiently deep <em>w</em> embeddings, or in <a href="https://arxiv.org/abs/1706.01427#deepmind" data-popup-title="A simple neural network module for relational reasoning" data-popup-author="Adam Santoro, David Raposo, David G. T. Barrett, Mateusz Malinowski, Razvan Pascanu, Peter Battaglia, Timothy Lillicrap" data-popup-date="2019-12-23" data-popup-abstract="Relational reasoning is a central component of generally intelligent behavior, but has proven difficult for neural networks to learn. In this paper we describe how to use Relation Networks (RNs) as a simple plug-and-play module to solve problems that fundamentally hinge on relational reasoning. We tested RN-augmented networks on three tasks: visual question answering using a challenging dataset called <span class=&quot;smallcaps-auto&quot;>CLEVR</span>, on which we achieve state-of-the-art, super-human performance; text-based question answering using the bAbI suite of tasks; and complex reasoning about dynamic physical systems. Then, using a curated dataset called Sort-of-<span class=&quot;smallcaps-auto&quot;>CLEVR</span> we show that powerful convolutional networks do not have a general capacity to solve relational questions, but can gain this capacity when augmented with RNs. Our work shows how a deep learning architecture equipped with an RN module can implicitly discover and learn to reason about entities and their relations.">relational networks</a>/<a href="https://www.gwern.net/docs/rl/2018-eslami.pdf#deepmind" data-popup-title="Neural scene representation and rendering" data-popup-author="S. M. Ali Eslami, Danilo Jimenez Rezende, Frederic Besse, Fabio Viola, Ari S. Morcos, Marta Garnelo, Avraham Ruderman, Andrei A. Rusu, Ivo Danihelka, Karol Gregor, David P. Reichert, Lars Buesing, Theophane Weber, Oriol Vinyals, Dan Rosenbaum, Neil Rabinow et al [...]" data-popup-date="2018-06-15" data-popup-doi="10.1126/science.aar6170" data-popup-abstract="<p><strong>A scene-internalizing computer program</strong>: To train a computer to “recognize” elements of a scene supplied by its visual sensors, computer scientists typically use millions of images painstakingly labeled by humans. Eslami et al.&nbsp;developed an artificial vision system, dubbed the Generative Query Network (<span class=&quot;smallcaps-auto&quot;>GQN</span>), that has no need for such labeled data. Instead, the <span class=&quot;smallcaps-auto&quot;>GQN</span> first uses images taken from different viewpoints and creates an abstract description of the scene, learning its essentials. Next, on the basis of this representation, the network predicts what the scene would look like from a new, arbitrary viewpoint.</p><p><strong>Abstract</strong>: Scene representation—the process of converting visual sensory data into concise descriptions—is a requirement for intelligent behavior. Recent work has shown that neural networks excel at this task when provided with large, labeled datasets. However, removing the reliance on human labeling remains an important open problem. To this end, we introduce the Generative Query Network (<span class=&quot;smallcaps-auto&quot;>GQN</span>), a framework within which machines learn to represent scenes using only their own sensors. The <span class=&quot;smallcaps-auto&quot;>GQN</span> takes as input images of a scene taken from different viewpoints, constructs an internal representation, and uses this representation to predict the appearance of that scene from previously unobserved viewpoints. The <span class=&quot;smallcaps-auto&quot;>GQN</span> demonstrates representation learning without human labels or domain knowledge, paving the way toward machines that autonomously learn to understand the world around them.</p>"><span>GQN</span></a>/<a href="https://arxiv.org/abs/2002.05867" data-popup-title="Transformers as Soft Reasoners over Language" data-popup-author="Peter Clark, Oyvind Tafjord, Kyle Richardson" data-popup-date="2020-02-23" data-popup-abstract="AI has long pursued the goal of having systems reason over *explicitly provided* knowledge, but building suitable representations has proved challenging. Here we explore whether transformers can similarly learn to reason (or emulate reasoning), but using rules expressed in language, thus bypassing a formal representation. We provide the first demonstration that this is possible, and characterize the extent of this capability. To do this, we use a collection of synthetic datasets that test increasing levels of reasoning complexity (number of rules, presence of negation, and depth of chaining). We find transformers appear to learn rule-based reasoning with high (99%) accuracy on these datasets, and in a way that generalizes to test data requiring substantially deeper chaining than in the training data (95%+ scores). We also demonstrate that the models transfer well to two hand-authored rulebases, and to rulebases paraphrased into more natural language. These findings are significant as it suggests a new role for transformers, namely as a limited &quot;soft theorem prover&quot; operating over explicit theories in language. This in turn suggests new possibilities for explainability, correctability, and counterfactual reasoning in question-answering. All datasets and a live demo are available at http://rule-reasoning.apps.allenai.org/">Transformers</a> with enough samples to force factorization. Training <a href="https://arxiv.org/abs/1910.07113#openai" data-popup-title="Solving Rubik's Cube with a Robot Hand" data-popup-author=" OpenAI, Ilge Akkaya, Marcin Andrychowicz, Maciek Chociej, Mateusz Litwin, Bob McGrew, Arthur Petron, Alex Paino, Matthias Plappert, Glenn Powell, Raphael Ribas, Jonas Schneider, Nikolas Tezak, Jerry Tworek, Peter Welinder, Lilian Weng, Qiming Yuan, Wojcie et al [...]" data-popup-date="2020-06-01" data-popup-abstract="We demonstrate that models trained only in simulation can be used to solve a manipulation problem of unprecedented complexity on a real robot. This is made possible by two key components: a novel algorithm, which we call automatic domain randomization (<span class=&quot;smallcaps-auto&quot;>ADR</span>) and a robot platform built for machine learning. <span class=&quot;smallcaps-auto&quot;>ADR</span> automatically generates a distribution over randomized environments of ever-increasing difficulty. Control policies and vision state estimators trained with <span class=&quot;smallcaps-auto&quot;>ADR</span> exhibit vastly improved sim2real transfer. For control policies, memory-augmented models trained on an <span class=&quot;smallcaps-auto&quot;>ADR</span>-generated distribution of environments show clear signs of emergent meta-learning at test time. The combination of <span class=&quot;smallcaps-auto&quot;>ADR</span> with our custom robot platform allows us to solve a Rubik's cube with a humanoid robot hand, which involves both control and state estimation problems. Videos summarizing our results are available: https://openai.com/blog/solving-rubiks-cube/">Dactyl</a> on millions of domain randomizations induced similar implicit meta-learning where during each runtime invocation, the <span>RNN</span> probes its environment and encodes its understanding of robot hand control into its hidden state; and <a href="https://arxiv.org/abs/1911.00357#facebook" data-popup-title="DD-PPO: Learning Near-Perfect PointGoal Navigators from 2.5 Billion Frames" data-popup-author="Erik Wijmans, Abhishek Kadian, Ari Morcos, Stefan Lee, Irfan Essa, Devi Parikh, Manolis Savva, Dhruv Batra" data-popup-date="2020-01-22" data-popup-abstract="We present Decentralized Distributed Proximal Policy Optimization (DD-<span class=&quot;smallcaps-auto&quot;>PPO</span>), a method for distributed reinforcement learning in resource-intensive simulated environments. DD-<span class=&quot;smallcaps-auto&quot;>PPO</span> is distributed (uses multiple machines), decentralized (lacks a centralized server), and synchronous (no computation is ever stale), making it conceptually simple and easy to implement. In our experiments on training virtual robots to navigate in Habitat-Sim, DD-<span class=&quot;smallcaps-auto&quot;>PPO</span> exhibits near-linear scaling -- achieving a speedup of 107x on 128 <span class=&quot;smallcaps-auto&quot;>GPU</span>s over a serial implementation. We leverage this scaling to train an agent for 2.5 Billion steps of experience (the equivalent of 80 years of human experience) -- over 6 months of <span class=&quot;smallcaps-auto&quot;>GPU</span>-time training in under 3 days of wall-clock time with 64 <span class=&quot;smallcaps-auto&quot;>GPU</span>s.   This massive-scale training not only sets the state of art on Habitat Autonomous Navigation Challenge 2019, but essentially solves the task --near-perfect autonomous navigation in an unseen environment without access to a map, directly from an <span class=&quot;smallcaps-auto&quot;>RGB</span>-D camera and a <span class=&quot;smallcaps-auto&quot;>GPS</span>+Compass sensor. Fortuitously, error vs computation exhibits a power-law-like distribution; thus, 90% of peak performance is obtained relatively early (at 100 million steps) and relatively cheaply (under 1 day with 8 <span class=&quot;smallcaps-auto&quot;>GPU</span>s). Finally, we show that the scene understanding and navigation policies learned can be transferred to other navigation tasks -- the analog of ImageNet pre-training + task-specific fine-tuning for embodied AI. Our model outperforms ImageNet pre-trained <span class=&quot;smallcaps-auto&quot;>CNN</span>s on these transfer tasks and can serve as a universal resource (all models and code are publicly available).">DD-<span>PPO</span></a> outperforms classical robot planners by scaling 2 orders. Or in <a href="https://openai.com/blog/procgen-benchmark/" data-popup-title="Procgen Benchmark: We’re releasing Procgen Benchmark, 16 simple-to-use procedurally-generated environments which provide a direct measure of how quickly a reinforcement learning agent learns generalizable skills" data-popup-author="Karl Cobbe, Christopher Hesse, Jacob Hilton, John Schulman" data-popup-date="2019-12-03" data-popup-abstract="<p>Announcement of <a href=&quot;https://github.com/openai/procgen&quot; title=&quot;Procgen Benchmark: Procedurally-Generated Game-Like Gym-Environments&quot;>Procgen</a>: <a href=&quot;https://arxiv.org/abs/1912.01588#openai&quot;>“Leveraging Procedural Generation to Benchmark Reinforcement Learning”</a>, Cobbe et al 2019:</p><blockquote><p>In this report, we introduce Procgen Benchmark, a suite of 16 procedurally generated game-like environments designed to benchmark both sample efficiency and generalization in reinforcement learning. We believe that the community will benefit from increased access to high quality training environments, and we provide detailed experimental protocols for using this benchmark. We empirically demonstrate that diverse environment distributions are essential to adequately train and evaluate RL agents, thereby motivating the extensive use of procedural content generation. We then use this benchmark to investigate the effects of scaling model size, finding that larger models significantly improve both sample efficiency and generalization.</p></blockquote><p>…We want the best of both worlds: a benchmark comprised of many diverse environments, each of which fundamentally requires generalization. To fulfill this need, we have created Procgen Benchmark. <a href=&quot;https://openai.com/blog/quantifying-generalization-in-reinforcement-learning/&quot; title=&quot;Quantifying Generalization in Reinforcement Learning: We’re releasing CoinRun, a training environment which provides a metric for an agent’s ability to transfer its experience to novel situations and has already helped clarify a longstanding puzzle in reinforcement learning. CoinRun strikes a desirable balance in complexity: the environment is simpler than traditional platformer games like Sonic the Hedgehog but still poses a worthy generalization challenge for state of the art algorithms.&quot;>CoinRun</a> [<a href=&quot;https://arxiv.org/abs/1812.02341#openai&quot; title=&quot;Quantifying Generalization in Reinforcement Learning&quot;>Cobbe et al 2018</a>] now serves as the inaugural environment in Procgen Benchmark, contributing its diversity to a greater whole.</p><p>…We’ve found that all of the Procgen environments require training on 500–1000 different levels before they can generalize to new levels, which suggests that standard RL benchmarks need much more diversity within each environment. Procgen Benchmark has become the standard research platform used by the OpenAI RL team, and we hope that it accelerates the community in creating better RL algorithms.</p>">Procgen</a>, training on hundreds of levels trains agents individually, but at thousands of levels, they begin to generalize to unseen levels. <a href="https://www.gwern.net/docs/rl/2018-silver.pdf#deepmind" data-popup-title="A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play" data-popup-author="David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez, Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, Timothy Lillicrap, Karen Simonyan, Demis Hassabis" data-popup-date="2018-12-07" data-popup-doi="10.1126/science.aar6404" data-popup-abstract="<p>The game of chess is the longest-studied domain in the history of artificial intelligence. The strongest programs are based on a combination of sophisticated search techniques, domain-specific adaptations, and handcrafted evaluation functions that have been refined by human experts over several decades. By contrast, the AlphaGo Zero program recently achieved superhuman performance in the game of Go by reinforcement learning from self-play. In this paper, we generalize this approach into a single AlphaZero algorithm that can achieve superhuman performance in many challenging games. Starting from random play and given no domain knowledge except the game rules, AlphaZero convincingly defeated a world champion program in the games of chess and shogi (Japanese chess), as well as Go.</p>">AlphaZero</a> demonstrated truly superhuman Go without ‘delusions’ just by training a bigger model on a richer signal &amp; pro-level play without any search—and <a href="https://arxiv.org/abs/1911.08265#deepmind" data-popup-title="Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model" data-popup-author="Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan, Laurent Sifre, Simon Schmitt, Arthur Guez, Edward Lockhart, Demis Hassabis, Thore Graepel, Timothy Lillicrap, David Silver" data-popup-date="2019-11-21" data-popup-abstract="Constructing agents with planning capabilities has long been one of the main challenges in the pursuit of artificial intelligence. Tree-based planning methods have enjoyed huge success in challenging domains, such as chess and Go, where a perfect simulator is available. However, in real-world problems the dynamics governing the environment are often complex and unknown. In this work we present the MuZero algorithm which, by combining a tree-based search with a learned model, achieves superhuman performance in a range of challenging and visually complex domains, without any knowledge of their underlying dynamics. MuZero learns a model that, when applied iteratively, predicts the quantities most directly relevant to planning: the reward, the action-selection policy, and the value function. When evaluated on 57 different Atari games—the canonical video game environment for testing AI techniques, in which model-based planning approaches have historically struggled—our new algorithm achieved a new state of the art. When evaluated on Go, chess and shogi, without any knowledge of the game rules, MuZero matched the superhuman performance of the AlphaZero algorithm that was supplied with the game rules.">MuZero</a>, for that matter, demonstrated that just training an <span>RNN</span> end-to-end to predict a reward on enough data is enough to obsolete even AlphaZero and learn tree search implicitly (but better). And on and on.</p>
<p>The <em>scaling hypothesis</em> that, once we find a scalable architecture like self-attention or convolutions, we can simply train ever larger NNs and ever more sophisticated behavior will emerge naturally as the easiest way to optimize for all the tasks &amp; data, looks increasingly plausible.</p>
<p><span> Keeping track.</span> <span>GPT-3</span> in 2020 makes as good a point as any to take a look back on the past decade. In 2010, one could easily fit everyone in the world who genuinely believed in deep learning into a moderate-sized conference room (assisted slightly by the fact that 3 of them were busy founding <a href="https://en.wikipedia.org/wiki/DeepMind" data-popup-title="DeepMind" data-popup-author="English Wikipedia" data-popup-abstract="<p><b>DeepMind Technologies</b> is a UK artificial intelligence company founded in September 2010, and acquired by Google in 2014. The company is based in London, with research centres in Canada, France, and the United States. In 2015, it became a wholly owned subsidiary of Alphabet Inc.</p>">DeepMind</a>). Someone interested in machine learning in 2010 <em>might</em> have read about some stuff in recognizing hand-written digits using all of 1–2 million parameters, or some modest neural tweaks to standard hidden Markov model voice-recognition. In 2010, who would have predicted that over the next 10 years, deep learning would undergo a Cambrian explosion causing a mass extinction of alternative approaches throughout machine learning, that models would scale up to 175,000 million parameters, and that these enormous models would just spontaneously develop all these capabilities, aside from a few diehard connectionists written off as willfully-deluded old-school fanatics by the rest of the AI community (never mind the world), such as <a href="https://www.gwern.net/docs/www/jetpress.org/3d313da208f6eac437cf56b4dca0acc49c93da33.html" data-popup-title="When will computer hardware match the human brain?" data-popup-author="Hans Moravec" data-popup-date="1998" data-popup-abstract="This paper describes how the performance of AI machines tends to improve at the same pace that AI researchers get access to faster hardware. The processing power and memory capacity necessary to match general intellectual performance of the human brain are estimated. Based on extrapolation of past trends and on examination of technologies under development, it is predicted that the required hardware will be available in cheap machines in the 2020s...At the present rate, computers suitable for human-like robots will appear in the 2020s. Can the pace be sustained for another three decades?" rel="archived alternate nofollow" data-url-original="https://jetpress.org/volume1/moravec.htm">Moravec</a>, Schmidhuber, Sutskever, Legg, &amp; Amodei?</p>
<p><span> Hindsight is 20/20.</span> Even in 2015, the scaling hypothesis seemed highly dubious: you needed something to scale, after all, and it was all too easy to look at flaws in existing systems and imagine that they would never go away and progress would sigmoid any month now, soon. Like the genomics revolution where a few far-sighted seers extrapolated that the necessary <em>n</em> for <span>GWAS</span>es would increase exponentially &amp; deliver powerful <span>PGS</span>es soon, while sober experts wrung their hands over “missing heritability” &amp; the miraculous complexity of biology &amp; scoff about how such <em>n</em> requirements proved <span>GWAS</span> was a failed paradigm, the future arrived at first slowly and then quickly. Yet, here we are: all honor to the fanatics, and shame and humiliation to the critics!<a href="#sn8" id="fnref8" role="doc-noteref"><sup>8</sup></a> If only one could go back 10 years, or even 5, to watch every AI researchers’ head explode reading this paper… Unfortunately, few heads appear to be exploding now, because human capacity for hindsight &amp; excuses is boundless (“I can get that much with finetuning, anyway I predicted it all along, how boring”) and <a href="https://intelligence.org/2017/10/13/fire-alarm/" data-popup-title="There’s No Fire Alarm for Artificial General Intelligence" data-popup-author="Eliezer Yudkowsky" data-popup-date="2017-10-13" data-popup-abstract="<p>[Meditation on the problem of coordinating reaction to x-risks, and AI risks in particular. To quote Norbert Wiener:</p><blockquote><p>Again and again I have heard the statement that learning machines cannot subject us to any new dangers, because we can turn them off when we feel like it. But can we? To turn a machine off effectively, we must be in possession of information as to whether the danger point has come. The mere fact that we have made the machine does not guarantee we shall have the proper information to do this.</p></blockquote><p>A fire alarm, even if it is not 100% accurate, coordinates human reactions: it becomes permissible to leave the room and investigate, take precautions, and for everyone to evacuate the building. This is because we all agree that fires usually come with smoke and smoke can be objectively detected. But what is the fire alarm for AI? “AI is whatever we can’t do yet”, and whenever AI accomplishes a new feat, people will simply move the goalposts and say that that task turned out to be unexpectedly easy to solve. There is no agreement on what “imminent <span class=&quot;smallcaps-auto&quot;>AGI</span>” <em>looks like</em>. You can ask AI researchers, “how would the world look different if we were in fact heading towards <span class=&quot;smallcaps-auto&quot;>AGI</span> in the near future, the next decade or three?” and they are unable to answer. They do not know what is or is not a ringing alarm bell, the point at which everyone should start taking the prospect very seriously. It was not chess, it was not ImageNet classification, it was not Go…</p><p>AI so far resembles other technologies like airplanes or nuclear bombs where just years before, the physicists who would invent it, eminent physicists, and physicists in general, were highly uncertain or skeptical or outright convinced of their impossibility. This was because progress in nuclear physics looked much the same regardless of whether nuclear bombs were possible and impossible. There was large ineradicable uncertainty, which appears to have neutered any serious effort to prepare. And yet, these matters ought to be dealt with in advance. Things like nuclear bombs or AI should not just arrive with no one having done anything to prepare. Or consider pandemics. Those who tried to warn the world about coronavirus will find this essay eerily apt.]</p><p>Okay, let’s be blunt here. I don’t think most of the discourse about <span class=&quot;smallcaps-auto&quot;>AGI</span> being far away (<em>or</em> that it’s near) is being generated by models of future progress in machine learning. I don’t think we’re looking at wrong models; I think we’re looking at no models.</p><p>I was once at a conference…I got up in Q&amp;amp;A and said, “Okay, you’ve all told us that progress won’t be all that fast. But let’s be more concrete and specific. I’d like to know what’s the <em>least</em> impressive accomplishment that you are very confident <em>cannot</em> be done in the next two years.”</p><p>There was a silence.</p><p>Eventually, 2 people on the panel ventured replies, spoken in a rather more tentative tone than they’d been using to pronounce that <span class=&quot;smallcaps-auto&quot;>AGI</span> was decades out. They named “A robot puts away the dishes from a dishwasher without breaking them”, and Winograd schemas….A few months after that panel, there was unexpectedly a big breakthrough on Winograd schemas. The breakthrough didn’t crack 80%, so three cheers for wide credibility intervals with error margin, but I expect the predictor might be feeling slightly more nervous now with one year left to go…</p><p>But that’s not the point. The point is the silence that fell after my question, and that eventually I only got 2 replies, spoken in tentative tones. When I asked for concrete feats that were impossible in the next two years, I think that that’s when the luminaries on that panel switched to trying to build a mental model of future progress in machine learning, asking themselves what they could or couldn’t predict, what they knew or didn’t know. And to their credit, most of them did know their profession well enough to realize that forecasting future boundaries around a rapidly moving field is actually <em>really hard</em>, that nobody knows what will appear on arXiv next month, and that they needed to put wide credibility intervals with very generous upper bounds on how much progress might take place 24 months’ worth of arXiv papers later. (Also, Demis Hassabis was present, so they all knew that if they named something insufficiently impossible, Demis would have DeepMind go and do it.)</p><p>…When I observe that there’s no fire alarm for <span class=&quot;smallcaps-auto&quot;>AGI</span>, I’m not saying that there’s no possible equivalent of smoke appearing from under a door. What I’m saying rather is that the smoke under the door is always going to be arguable; it is not going to be a clear and undeniable and absolute sign of fire; and so there is never going to be a fire alarm producing common knowledge that action is now due and socially acceptable…There is never going to be a time before the end when you can look around nervously, and see that it is now clearly common knowledge that you can talk about <span class=&quot;smallcaps-auto&quot;>AGI</span> being imminent, and take action and exit the building in an orderly fashion, without fear of looking stupid or frightened.</p>">“there is no fire alarm”</a>. (If you are still <em>certain</em> that there is near-zero probability of <span>AGI</span> in the next few decades, why? Did you predict—in writing—capabilities like <span>GPT-3</span>? Is this how you expect AI failure to look in the decades beforehand? What specific task, what specific number, would convince you otherwise? How would the world look different than it does now if these crude prototype insect-brain-sized DL systems were not on a path to success?)</p>
<p><span> Authority without accountability.</span> What should we think about the experts? Projections of failure were made by eminent, respectable, serious people. They spoke in considered tones of why AI hype was excessive and might trigger an “AI winter”, and the fundamental flaws of fashionable approaches and why brute force could not work. These statements were made routinely in 2014, 2015, 2016… And they were wrong. I am aware of few issuing a <em>mea culpa</em> or reflecting on it.</p>
<p><span> Phatic, not predictive.</span> There is, however, a certain tone of voice the bien pensant all speak in, whose sound is the same whether right or wrong; a tone shared with many statements in January to March of this year; a tone we can also find in a 1940 <em>Scientific American</em> article authoritatively titled, <a href="https://www.gwern.net/docs/xrisks/1940-sciam-harrington-nuclearweapons-dontworryitcanthappen.pdf" data-popup-title="Don't Worry—It Can't Happen" data-popup-author="Jean Harrington (Scientific American)" data-popup-date="1940-05-01" data-popup-doi="10.2307/24988773" data-popup-abstract="<p>…Early last summer, in the midst of all this research, a chilly sensation began tingling up and down the spines of the experimenters. These extra neutrons that were being erupted—could they not in turn become involuntary bullets, flying from one exploding uranium nucleus into the heart of another, causing another fission which would itself cause still others? Wasn’t there a dangerous possibility that the uranium would at last become explosive? That the samples being bombarded in the laboratories at Columbia University, for example, might blow up the whole of New York City? To make matters more ominous, news of fission research from Germany, plentiful in the early part of 1939, mysteriously and abruptly stopped for some months. Had government censorship been placed on what might be a secret of military importance? The press and populace, getting wind of these possibly lethal goings-on, raised a hue and cry. Nothing daunted, however, the physicists worked on to find out whether or not they would be blown up, and the rest of us along with them. Now, a year after the original discovery, word comes from Paris that we don’t have to worry.</p><p>…With typical French—and scientific—caution, they added that this was perhaps true only for the particular conditions of their own experiment, which was carried out on a large mass of uranium under water. But most scientists agreed that it was very likely true in general.</p><p>…Readers made insomnious by “newspaper talk” of terrific atomic war weapons held in reserve by dictators may now get sleep.</p>">“Don’t Worry—It Can’t Happen”</a>, which advised the reader to not be concerned about it any longer “and get sleep”. (‘It’ was the atomic bomb, about which certain scientists had stopped talking, raising public concerns; not only could it happen, the British bomb project had already begun, and 5 years later it did happen.)</p>
<p><span> The iron law of bureaucracy: Cathedral gothic.</span> This tone of voice is the voice of <a href="https://srconstantin.wordpress.com/2016/10/20/ra/" data-popup-title="Ra" data-popup-author="Sarah Constantin" data-popup-date="2016-10-20" data-popup-abstract="<p>[Speculative essay about a specific kind of groupthink and failure mode of institutions: a pursuit of prestige, legitimacy, and respectability detached from reality, a prizing of vagueness and inscrutability and superficial perfection and avoidance of anything that might seem absurd or daring or mockable.]</p><p>The Egyptian god Ra was a symbol of divine kingship, all-powerful and all-seeing. He’s a good metaphor for a certain kind of psychological phenomenon that involves thought distortions around authority and legitimacy…The idea of a malign Establishment is somewhat convergent:</p><ul><li>The Establishment (attributed to Henry Fairlie in 1950’s Britain, talking about an informal social network of power among prominent, well-connected people)</li><li>The Man (e.g.&nbsp;Yippies, Burning Man)</li><li>The Combine (Ken Kesey)</li><li>Moloch (Allen Ginsberg)</li><li>The Beige Dictatorship (Charles Stross)</li><li>The Cathedral (Mencius Moldbug)</li><li>The Mandarins (Megan McArdle)</li></ul><p>Not all of these ideas are coterminous with Ra, or identical to each other.</p><p>What they have in common is that the Establishment is primarily an upper-class phenomenon, that it is more about social and moral legitimacy than mere wealth or raw power, and that it is <em>boringly evil</em>—it produces respectable, normal, right-thinking, mild-mannered people who do things with very bad consequences.</p><p>…Ra is something more like a psychological mindset, that causes people to actually <i>seek</i> corruption and confusion, and to <i>prefer corruption for its own sake</i> — though, of course, it doesn’t feel quite like that from the inside.</p><p>Ra is a specific kind of glitch in intuition, which can roughly be summarized as the drive to <i>idealize vagueness and despise clarity. </i>I’m going to try to define it by extension, using examples from my and others’ personal experiences.</p><p><strong>Ra is about generic superlativity.</strong></p><p>You know how universal gods are praised with formulas that call them glorious, mighty, exalted, holy, righteous, and other suchlike adjectives, all of which are perfectly <i>generic </i>and involve no <i>specific characteristics</i> except wonderfulness? That’s what Ra is all about.</p><p>The worship of Ra involves a preference for stockpiling money, accolades, awards, or other resources, beyond what you can meaningfully consume or make practical use of; a felt sense of wanting to attain that abstract radiance of “bestness”.</p><p>…<strong>Ra defends itself with vagueness, confusion, incoherence — and then anger.</strong></p><p>“Respectability” turns out to be incoherent quite often — i.e.&nbsp;if you have <i>any </i>consistent model of the world you often have to take extreme or novel positions as a logical conclusion from your assumptions. To Ra, disrespectability is damnation, and thus consistent thought is suspect.</p><p>Vagueness, mental fog, “underconfidence”, avoidance, evasion, blanking out, etc. are hallmarks of Ra. If cornered, a person embodying Ra will abruptly switch from blurry vagueness to <i>anger </i>and <i>nihilism</i>…Ra causes persistent brain fog or confusion, especially around economic thinking or cost-benefit analysis or quantitative estimates.</p><p>…Ra promotes the idea that <i>optimal politeness conveys as little information as possible</i>. That you should actively try to hide preferences (because if you shared them, you’d inconvenience others by pressuring them to satisfy your preferences). That all compliments are empty pleasantries. There’s an interpretation of “politeness” that’s anti-cooperative, that avoids probing for opportunities for genuine mutual benefit or connection and just wants to make the mutual defection process go as smoothly as possible. Ra prefers this, because it’s less revealing, commits you less, doesn’t pin you down, allows you to keep all your options open and devote everything to the pursuit of Ra…I’ve had my writing criticized because “when you give your opinion, it sounds like you think you’re smart”.</p>">authority</a>. The voice of authority insists on calm, and people not “panicking” (the chief of sins). The voice of authority assures you that it won’t happen (because it can’t happen). The voice utters simple arguments about why the status quo will prevail, and considers only how the wild new idea could fail (and not all the possible options). The voice is not, and does not deal in, uncertainty; things will either happen or they will not, and since it will not happen, there is no need to take any precautions (and you should not worry because it can’t happen). The voice does not believe in drawing lines on graphs (it is rank numerology). The voice does not issue any numerical predictions (which could be falsified). The voice is opposed to unethical things like randomized experiments on volunteers (but will overlook the insult). The voice does not have a model of the future (because a model implies it does not already know the future). The voice is concerned about its public image (and unkind gossip about it by other speakers of the voice). The voice is always sober, respectable, and credentialed (the voice would be pleased to write an op-ed for your national magazine and/or newspaper). The voice speaks, and is not spoken to (you cannot ask the voice what objective fact would change its mind). The voice never changes its mind (until it does). The voice is never surprised by events in the world (only disappointed). The voice advises you to go back to sleep (right now).</p>
<p>When someone speaks about future possibilities, what is the tone of their voice?</p>
</section>
<section id="media">

<section id="links">
<h2><a href="#links" title="Link to section: §'Links'">Links</a></h2>
<p>AI:</p>
<ul>
<li><p><span>Matters Of Scale</span>:</p>
<ul>
<li><a href="#gpt-3"><span>GPT-3</span></a>: see above</li>
<li><a href="https://arxiv.org/abs/2005.04305#openai" data-popup-title="Measuring the Algorithmic Efficiency of Neural Networks" data-popup-author="Danny Hernandez, Tom B. Brown" data-popup-date="2020-06-01" data-popup-abstract="Three factors drive the advance of AI: algorithmic innovation, data, and the amount of compute available for training. Algorithmic progress has traditionally been more difficult to quantify than compute and data. In this work, we argue that algorithmic progress has an aspect that is both straightforward to measure and interesting: reductions over time in the compute needed to reach past capabilities. We show that the number of floating-point operations required to train a classifier to AlexNet-level performance on ImageNet has decreased by a factor of 44x between 2012 and 2019. This corresponds to algorithmic efficiency doubling every 16 months over a period of 7 years. By contrast, Moore's Law would only have yielded an 11x cost improvement. We observe that hardware and algorithmic efficiency gains multiply and can be on a similar scale over meaningful horizons, which suggests that a good model of AI progress should integrate measures from both.">“Measuring the Algorithmic Efficiency of Neural Networks”</a>, Hernandez &amp; Brown 2020 (<a href="https://openai.com/blog/ai-and-efficiency/" data-popup-title="AI and Efficiency: We’re releasing an analysis showing that since 2012 the amount of compute needed to train a neural net to the same performance on ImageNet classification has been decreasing by a factor of 2 every 16 months. Compared to 2012, it now takes 44 times less compute to train a neural network to the level of AlexNet (by contrast, Moore’s Law3 would yield an 11✕ cost improvement over this period). Our results suggest that for AI tasks with high levels of recent investment, algorithmic progress has yielded more gains than classical hardware efficiency." data-popup-author="Danny Hernandez, Tom Brown (OpenAI)" data-popup-date="2020-05-05" data-popup-abstract="<p>For our analysis, we primarily leveraged open-source re-implementations<sup>19,20,21</sup> to measure progress on AlexNet level performance over a long horizon. We saw a similar rate of training efficiency improvement for ResNet-50 level performance on ImageNet (17-month doubling time).<sup>7,16</sup> We saw faster rates of improvement over shorter timescales in Translation, Go, and DoTA 2:</p><ol type=&quot;1&quot;><li>Within translation, the Transformer<sup>22</sup> surpassed seq2seq<sup>23</sup> performance on English to French translation on <span class=&quot;smallcaps-auto&quot;>WMT</span>’14 with 61✕ less training compute 3 years later.</li><li>We estimate AlphaZero<sup>24</sup> took 8✕ less compute to get to AlphaGo Zero<sup>25</sup> level performance 1 year later.</li><li>OpenAI Five Rerun required 5✕ less training compute to surpass OpenAI Five<sup>26</sup> (which beat the world champions, OG) 3 months later.</li></ol><p>It can be helpful to think of compute in 2012 not being equal to compute in 2019 in a similar way that dollars need to be inflation-adjusted over time. A fixed amount of compute could accomplish more in 2019 than in 2012. One way to think about this is that some types of AI research progress in two stages, similar to the “tick tock” model of development seen in semiconductors; new capabilities (the “tick”) typically require a significant amount of compute expenditure to obtain, then refined versions of those capabilities (the “tock”) become much more efficient to deploy due to process improvements. Increases in algorithmic efficiency allow researchers to do more experiments of interest in a given amount of time and money. In addition to being a measure of overall progress, algorithmic efficiency gains speed up future AI research in a way that’s somewhat analogous to having more compute.</p><p>…We also find increases in inference efficiency in terms of <span class=&quot;smallcaps-auto&quot;>GPU</span> time<sup>32</sup>, parameters<sup>16</sup>, and flops meaningful, but mostly as a result of their economic implications [ Inference costs dominate total costs for successful deployed systems. Inference costs scale with usage of the system, whereas training costs only need to be paid once.] rather than their effect on future research progress. ShuffleNet<sup>13</sup> achieved AlexNet-level performance with an 18✕ inference efficiency increase in 5 years (15-month doubling time), which suggests that training efficiency and inference efficiency might improve at similar rates.</p><p>…For all these reasons, we’re going to start tracking efficiency <span class=&quot;smallcaps-auto&quot;>SOTA</span>s publicly. We’ll start with vision and translation efficiency benchmarks (ImageNet and <span class=&quot;smallcaps-auto&quot;>WMT</span>14), and we’ll consider adding more benchmarks over time. We believe there are efficiency <span class=&quot;smallcaps-auto&quot;>SOTA</span>s on these benchmarks we’re unaware of and encourage the research community to submit them here (we’ll give credit to original authors and collaborators).</p>">blog</a>/<a href="https://80000hours.org/podcast/episodes/danny-hernandez-forecasting-ai-progress/" data-popup-title="Danny Hernandez on forecasting and the drivers of AI progress" data-popup-author="Arden Koehler, Robert Wiblin, Keiran Harris (80,000 Hours)" data-popup-date="2020-05-22" data-popup-abstract="<p>Companies use about 300,000 times more computation training the best AI systems today than they did in 2012 and algorithmic innovations have also made them 25 times more efficient at the same tasks.</p><p>These are the headline results of two recent papers—<a href=&quot;https://openai.com/blog/ai-and-compute/&quot; title=&quot;&amp;#39;We’re releasing an analysis showing that since 2012, the amount of compute used in the largest AI training runs has been increasing exponentially with a 3.4-month doubling time (by comparison, Moore’s Law had a 2-year doubling period). Since 2012, this metric has grown by more than 300,000✕ (a 2-year doubling period would yield only a 7✕ increase). Improvements in compute have been a key component of AI progress, so as long as this trend continues, it’s worth preparing for the implications of systems far outside today’s capabilities&amp;#39;, Amodei et al 2018&quot;>“AI and Compute”</a> and <a href=&quot;https://openai.com/blog/ai-and-efficiency/&quot; title=&quot;&amp;#39;AI and Efficiency: We’re releasing an analysis showing that since 2012 the amount of compute needed to train a neural net to the same performance on ImageNet classification has been decreasing by a factor of 2 every 16 months. Compared to 2012, it now takes 44 times less compute to train a neural network to the level of AlexNet (by contrast, Moore’s Law would yield an 11✕ cost improvement over this period). Our results suggest that for AI tasks with high levels of recent investment, algorithmic progress has yielded more gains than classical hardware efficiency&amp;#39;, Hernandez &amp;amp; Brown 2020&quot;>“AI and Efficiency”</a>—from the Foresight Team at OpenAI. In today’s episode I spoke with one of the authors, Danny Hernandez, who joined OpenAI after helping develop better forecasting methods at Twitch and Open Philanthropy. Danny and I talk about how to understand his team’s results and what they mean (and don’t mean) for how we should think about progress in AI going forward.</p><p>Debates around the future of AI can sometimes be pretty abstract and theoretical. Danny hopes that providing rigorous measurements of some of the inputs to AI progress so far can help us better understand what causes that progress, as well as ground debates about the future of AI in a better shared understanding of the field…In the interview, Danny and I also discuss a range of other topics, including:</p><ul><li>The question of which experts to believe</li><li>Danny’s journey to working at OpenAI</li><li>The usefulness of “decision boundaries”</li><li>The importance of Moore’s law for people who care about the long-term future</li><li>What OpenAI’s Foresight Team’s findings might imply for policy</li><li>The question whether progress in the performance of AI systems is linear</li><li>The safety teams at OpenAI and who they’re looking to hire</li><li>One idea for finding someone to guide your learning</li><li>The importance of hardware expertise for making a positive impact</li></ul><blockquote><p>If you believe AI progress is fast, what would progress look like that would convince you it’s slow? Paint a picture of that five years from now. What does slow progress look like to you? And now you’re like, “Oh yeah, progress is actually slow”. And what could have happened that would convince you that it’s actually fast. But you can make what would update you clear to yourself and others and that for big decisions, this is generally worthwhile.</p></blockquote>">interview</a>; the first prototype is never the best one, but given enough compute &amp; time, you can refine it and figure out how it should have been done all along, and this paper quantifies the neural net hardware overhang just since 2012: “it now takes 44✕ less compute to train…to the level of AlexNet”. Unsurprising—eg the <a href="https://en.wikipedia.org/wiki/Experience_curve_effects" data-popup-title="Experience curve effects" data-popup-author="English Wikipedia" data-popup-abstract="<p>In management, models of the <b>learning curve effect</b> and the closely related <b>experience curve effect</b> express the relationship between equation and efficiency or between efficiency gains and investment in the effort.</p>">experience curve</a> in <a href="https://en.wikipedia.org/wiki/Linear_programming" data-popup-title="Linear programming" data-popup-author="English Wikipedia" data-popup-abstract="<p><b>Linear programming</b> is a method to achieve the best outcome in a mathematical model whose requirements are represented by linear relationships. Linear programming is a special case of mathematical programming.</p>">linear programming</a>: <a href="https://www.gwern.net/docs/ai/2002-bixby.pdf" data-popup-title="Solving Real-World Linear Programs: A Decade and More of Progress" data-popup-author="Robert E. Bixby" data-popup-date="2002-02-01" data-popup-doi="10.1287/opre.50.1.3.17780" data-popup-abstract="<p>This paper is an invited contribution to the 50th anniversary issue of the journal <em>Operations Research</em>, published by the Institute of Operations Research and Management Science (<span class=&quot;smallcaps-auto&quot;>INFORMS</span>). It describes one person’s perspective on the development of computational tools for linear programming. The paper begins with a short personal history, followed by historical remarks covering the some 40 years of linear-programming developments that predate my own involvement in this subject. It concludes with a more detailed look at the evolution of computational linear programming since 1987.</p><p>…In this paper I have focused primarily on one issue, solving larger, more difficult linear programs faster. The numbers presented speak for themselves. 3 orders of magnitude in machine speed and 3 orders of magnitude in algorithmic speed add up to six orders of magnitude in solving power: A model that might have taken a year to solve 10 years ago can now solve in less than 30 seconds. Of course, no one waits 1 year to solve a model, at least no one I know. The real meaning of such an advance is much harder to measure in practice, but it is real nevertheless. There is no doubt that we now have optimization engines at our disposal that dwarf what was available only a few years ago, making possible the solution of real-world models once considered intractable, and opening up whole new domains of application.</p><p>How do these speed improvements fit into the overall picture of linear-programming practice? They are only a part of that picture, though an essential, enabling part. The pervasive availability of powerful, usable desktop computing, the availability of data to feed our models, and the emergence of algebraic modeling languages to represent our models have all combined with the underlying engines to make operations research and linear programming the powerful tools they are today. However, there are still important issues to be solved. In spite of all the advances, the application of linear programming remains primarily the domain of experts. The need for abstraction still stands as a hurdle between technology and solutions. While the existence of this hurdle is disconcerting, it is at least gratifying to know that the benefits from overcoming it are now greater than ever.</p>">Bixby 2002</a>; see <a href="https://www.gwern.net/docs/ai/2013-grace.pdf#miri" data-popup-title="Algorithmic Progress in Six Domains" data-popup-author="Katja Grace" data-popup-date="2013-12-09" data-popup-abstract="<p>We examine evidence of progress in 6 areas of algorithms research [<span class=&quot;smallcaps-auto&quot;>SAT</span>/chess+Go/factoring/physics simulations/linear programming+scheduling/machine learning], with an eye to understanding likely algorithmic trajectories after the advent of artificial general intelligence. Many of these areas appear to experience fast improvement, though the data are often noisy. For tasks in these areas, gains from algorithmic progress have been roughly 50 to 100% as large as those from hardware progress. Improvements tend to be incremental, forming a relatively smooth curve on the scale of years.</p>">Grace 2013</a>/<a href="https://www.gwern.net/docs/ai/2013-yudkowsky.pdf#miri" data-popup-title="Intelligence Explosion Microeconomics" data-popup-author="Eliezer Yudkowsky" data-popup-date="2013-09-13" data-popup-abstract="<p>I. J. Good’s thesis of the “intelligence explosion” states that a sufficiently advanced machine intelligence could build a smarter version of itself, which could in turn build an even smarter version, and that this process could continue to the point of vastly exceeding human intelligence. As Sandberg (2010) correctly notes, there have been several attempts to lay down return on investment formulas intended to represent sharp speedups in economic or technological growth, but very little attempt has been made to deal formally with Good’s intelligence explosion thesis as such.</p><p>I identify the key issue as <em>returns on cognitive reinvestment</em>—the ability to invest more computing power, faster computers, or improved cognitive algorithms to yield cognitive labor which produces larger brains, faster brains, or better mind designs. There are many phenomena in the world which have been argued to be evidentially relevant to this question, from the observed course of hominid evolution, to Moore’s Law, to the competence over time of machine chess-playing systems, and many more. I go into some depth on some debates which then arise on how to interpret such evidence. I propose that the next step in analyzing positions on the intelligence explosion would be to formalize return on investment curves, so that each stance can formally state which possible micro-foundations they hold to be <em>falsified</em> by historical observations. More generally I pose multiple open questions of “returns on cognitive reinvestment” or “intelligence explosion microeconomics.” Although such questions have received little attention thus far, they seem highly relevant to policy choices affecting outcomes for Earth-originating intelligent life.</p>">Yudkowsky 2013</a>. We don’t know how to train the right kind of neural nets and make huge mistakes with the simplest things, as capability jumps like resnets or EfficientNet or <a href="https://www.gwern.net/docs/www/openreview.net/530fe3927c37ebd3ceb4fd60cf872f0e9b27447c.html#deepmind" data-popup-title="Recurrent Experience Replay in Distributed Reinforcement Learning" data-popup-author="Steven Kapturowski, Georg Ostrovski, John Quan, Remi Munos, Will Dabney" data-popup-date="2018-09-27" data-popup-abstract="<p><em>Abstract</em>: Building on the recent successes of distributed training of RL agents, in this paper we investigate the training of RNN-based RL agents from distributed prioritized experience replay. We study the effects of parameter lag resulting in representational drift and recurrent state staleness and empirically derive an improved training strategy. Using a single network architecture and fixed set of hyper-parameters, the resulting agent, Recurrent Replay Distributed DQN (R2D2), quadruples the previous state of the art on Atari-57, and matches the state of the art on DMLab-30. It is the first agent to exceed human-level performance in 52 of the 57 Atari games.</p><p><em>Keywords</em>: RNN, LSTM, experience replay, distributed training, reinforcement learning</p><p><em>TL;DR</em>: Investigation on combining recurrent neural networks and experience replay leading to state-of-the-art agent on both Atari-57 and DMLab-30 using single set of hyper-parameters.</p>" rel="archived alternate nofollow" data-url-original="https://openreview.net/forum?id=r1lyTjAqYX#deepmind">R2D2</a> occasionally remind us.)</li>
<li><a href="https://arxiv.org/abs/2005.08025#microsoft-openai" data-popup-title="IntelliCode Compose: Code Generation Using Transformer" data-popup-author="Alexey Svyatkovskiy, Shao Kun Deng, Shengyu Fu, Neel Sundaresan" data-popup-date="2020-05-21" data-popup-abstract="In software development through integrated development environments (<span class=&quot;smallcaps-auto&quot;>IDE</span>s), code completion is one of the most widely used features. Nevertheless, majority of integrated development environments only support completion of methods and <span class=&quot;smallcaps-auto&quot;>API</span>s, or arguments.   In this paper, we introduce IntelliCode Compose $-$ a general-purpose multilingual code completion tool which is capable of predicting sequences of code tokens of arbitrary types, generating up to entire lines of syntactically correct code. It leverages state-of-the-art generative transformer model trained on 1.2 billion lines of source code in Python, $C#$, JavaScript and TypeScript programming languages. IntelliCode Compose is deployed as a cloud-based web service. It makes use of client-side tree-based caching, efficient parallel implementation of the beam search decoder, and compute graph optimizations to meet edit-time completion suggestion requirements in the Visual Studio Code <span class=&quot;smallcaps-auto&quot;>IDE</span> and Azure Notebook.   Our best model yields an average edit similarity of $86.7%$ and a perplexity of 1.82 for Python programming language.">“IntelliCode Compose: Code Generation Using [<span>GPT-2</span>] Transformer”</a>, Svyatkovskiy et al 2020 (unclear if application of <a href="https://www.microsoft.com/en-us/research/blog/zero-2-deepspeed-shattering-barriers-of-deep-learning-speed-scale/" data-popup-title="ZeRO-2 &amp; DeepSpeed: Shattering barriers of deep learning speed &amp; scale" data-popup-author="DeepSpeed Team (MS)" data-popup-date="2020-05-19" data-popup-abstract="<p>Today, we are happy to share our new findings and results as we introduce the improved ZeRO-2 and further developments with DeepSpeed:</p><ul><li>An <em>order-of-magnitude larger and faster training</em> with ZeRO-2: ZeRO-2 expands the scope of memory optimizations in the original ZeRO by tackling the full spectrum of memory consumption during training. More specifically, ZeRO-2 introduces new technology to reduce the memory footprint of gradients, activation memory, and fragmented memory, in addition to optimizer state memory optimization in the original ZeRO. Altogether, the memory savings empower DeepSpeed to improve the scale and speed of deep learning training by an order of magnitude. More concretely, ZeRO-2 allows training models as large as 170 billion parameters up to 10✕ faster compared to state of the art.</li><li><em>Fastest <span class=&quot;smallcaps-auto&quot;>BERT</span> training</em>: While ZeRO-2 optimizes large models during distributed training, we also introduce new technology to accelerate single <span class=&quot;smallcaps-auto&quot;>GPU</span> performance via kernel optimizations. These optimizations not only create a strong foundation for scaling out large models, but also improve the single <span class=&quot;smallcaps-auto&quot;>GPU</span> performance of highly tuned and moderately sized models like <span class=&quot;smallcaps-auto&quot;>BERT</span> by more than 30%, reaching a staggering performance of 64 teraflops per V100 <span class=&quot;smallcaps-auto&quot;>GPU</span>, which is over 50% of the hardware peak. Using these optimizations as the building block, DeepSpeed achieves the fastest <span class=&quot;smallcaps-auto&quot;>BERT</span> training record: 44 minutes on 1,024 <span class=&quot;smallcaps-auto&quot;>NVIDIA</span> V100 <span class=&quot;smallcaps-auto&quot;>GPU</span>s, compared with the best published result of 67 minutes on the same number and generation of <span class=&quot;smallcaps-auto&quot;>GPU</span>s.</li></ul><p>…ZeRO-2: Training models with 100 billion parameters up to 10✕ faster:</p><ol type=&quot;1&quot;><li><em>Model scale</em>: State-of-the-art large models (trained without using ZeRO) such as OpenAI <span class=&quot;smallcaps-auto&quot;>GPT</span>-2, <span class=&quot;smallcaps-auto&quot;>NVIDIA</span> Megatron-LM, and Google T5 have sizes of 1.5B, 8.3B, and 11B parameters respectively. ZeRO-2 provides system capability to efficiently run models of 170 billion parameters, an order-of-magnitude bigger than these largest models (Figure 2, top left). The tests were conducted using 400 <span class=&quot;smallcaps-auto&quot;>NVIDIA</span> V100 <span class=&quot;smallcaps-auto&quot;>GPU</span>s; with more devices (such as 1,000 <span class=&quot;smallcaps-auto&quot;>GPU</span>s), ZeRO-2 allows us to scale toward 200 billion parameters.</li><li><em>Speed</em>: Improved memory efficiency powers higher throughput and faster training. Figure 2 (bottom left) shows system throughput of ZeRO-2, ZeRO-1, and baseline model parallelism. Here we use a state-of-the-art model parallelism approach, <span class=&quot;smallcaps-auto&quot;>NVIDIA</span> Megatron-LM, as baseline-MP, while ZeRO-2 and ZeRO-1 both combine ZeRO-powered data parallelism with Megatron-LM model parallelism. ZeRO-2 runs 100-billion-parameter models with over 38 teraflops per <span class=&quot;smallcaps-auto&quot;>GPU</span>, 30% of hardware peak, and aggregated performance over 15 petaflops on the cluster with 400 <span class=&quot;smallcaps-auto&quot;>NVIDIA</span> V100 <span class=&quot;smallcaps-auto&quot;>GPU</span>s. For models of the same size, ZeRO-2 is up to 10✕ faster in training speed when compared to the baseline because model parallelism requires high communication bandwidth to be efficient, and models of these sizes require model parallelism across nodes where the communication bandwidth is limited. The memory savings of ZeRO-2 allows us to reduce model parallelism degree and fit the model without requiring inter-node model parallelism, drastically reducing communication cost. ZeRO-2 is also up to 5✕ faster than ZeRO-1 because its additional memory savings help reduce communication further and support even larger batch sizes.</li><li><em>Scalability</em>: We observe superlinear speedup (Figure 2, top right), where the performance more than doubles when the number of <span class=&quot;smallcaps-auto&quot;>NVIDIA</span> <span class=&quot;smallcaps-auto&quot;>GPU</span>s are doubled. ZeRO-2 reduces the memory footprint of the model states as we increase the data parallelism degree, allowing us to fit larger batch sizes per <span class=&quot;smallcaps-auto&quot;>GPU</span> and resulting in better performance.</li><li><em>Democratizing large model training</em>: ZeRO-2 empowers model scientists to train models up to 13 billion parameters efficiently without any model parallelism that typically requires model refactoring (Figure 2, bottom right). 13 billion parameters is larger than most of the largest state-of-the-art models (such as Google T5, with 11 billion parameters). With respect to throughput, we observe an average throughput of 37 teraflops (30% hardware peak) per V100 <span class=&quot;smallcaps-auto&quot;>GPU</span> for model sizes ranging from 2 billion to 13 billion parameters. Model scientists can therefore experiment freely with large models without worrying about model parallelism. In comparison, the implementations of classic data parallelism approaches (such as PyTorch Distributed Data Parallel) run out of memory with 1.4-billion-parameter models, while ZeRO-1 supports up to 6 billion parameters.</li></ol><p>For more details about ZeRO-2, please see the <a href=&quot;https://github.com/microsoft/DeepSpeed&quot;>DeepSpeed GitHub repository</a> and the updated <a href=&quot;https://arxiv.org/abs/1910.02054#microsoft&quot; title=&quot;&amp;#39;ZeRO: Memory Optimizations Toward Training Trillion Parameter Models&amp;#39;, Rajbhandari et al 2019&quot;>ZeRO paper</a>.</p>">ZeRO-2</a>; see also the <span>GPT-3</span> few-shot code completion abilities)</li>
<li><a href="https://www.gwern.net/docs/ai/2020-bell.pdf#facebook" data-popup-title="GrokNet: Unified Computer Vision Model Trunk and Embeddings For Commerce" data-popup-author="Sean Bell, Yiqun Liu, Sami Alsheikh, Yina Tang, Ed Pizzi, M. Henning, Karun Singh, Omkar Parkhi, Fedor Borisyuk" data-popup-date="2020-08-22" data-popup-abstract="<p>In this paper, we present GrokNet, a deployed image recognition system for commerce applications. GrokNet leverages a multi-task learning approach to train a single computer vision trunk. We achieve a 2.1✕ improvement in exact product match accuracy when compared to the previous state-of-the-art Facebook product recognition system. We achieve this by training on 7 datasets across several commerce verticals, using 80 categorical loss functions and 3 embedding losses. We share our experience of combining diverse sources with wide-ranging label semantics and image statistics, including learning from human annotations, user-generated tags, and noisy search engine interaction data. GrokNet has demonstrated gains in production applications and operates at Facebook scale.</p>">“GrokNet: Unified Computer Vision Model Trunk and Embeddings For Commerce”</a> (<a href="https://ai.facebook.com/blog/powered-by-ai-advancing-product-understanding-and-building-new-shopping-experiences" data-popup-title="Powered by AI: Advancing product understanding and building new shopping experiences" data-popup-author="Tamara Berg, Sean Bell, Manohar Paluri, Andrei Chtcherbatchenko, Harry Chen, Francis Ge, Bo Yin" data-popup-date="2020-05-19" data-popup-abstract="<p>Today we’re announcing:</p><ul><li>We’ve built and deployed GrokNet, a universal computer vision system designed for shopping. It can identify fine-grained product attributes across billions of photos—in different categories, such as fashion, auto, and home decor.</li><li>GrokNet is powering new Marketplace features for buyers and sellers today and we’re testing automatic product tagging on Facebook Pages to help make photos shoppable.</li><li>We’re also introducing Rotating View, a state-of-the-art 3D-like photo capability that allows anyone with a camera on their phone to capture multi-dimensional panoramic views of their listings on Marketplace.</li><li>And we’ve advanced research by creating a state of the art technique to predict occluded or layered objects in photos (like a shirt beneath a jacket).</li><li>These advancements are part of the foundation we’re building to develop an entirely new way to shop on our platforms—making it easier for individuals and small businesses to showcase their products to billions of people, and for buyers to find exactly what they’re looking for.</li></ul><p>…We built, trained, and deployed a model with 83 loss functions across seven data sets to combine multiple verticals into a single embedding space. This universal model allows us to leverage many more sources of information, which increases our accuracy and outperforms our single vertical-focused models… In the GrokNet training architecture, a major challenge is managing 7 datasets and 83 loss functions, so that they all perform well simultaneously. To solve this, we adjust the batch sizes and loss weights, using more images per batch and higher loss weights for the challenging tasks. We also use weakly supervised learning to automatically generate additional training data, further improving accuracy.</p><p>…Our long-term vision is to build an all-in-one AI lifestyle assistant that can accurately search and rank billions of products, while personalizing to individual tastes. That same system would make online shopping just as social as shopping with friends in real life. Going one step further, it would advance visual search to make your real-world environment shoppable. If you see something you like (clothing, furniture, electronics, etc.), you could snap a photo of it and the system would find that exact item, as well as several similar ones to purchase right then and there…While these systems are fragmented right now, incorporating everything into one system is the ambitious challenge we’ve set out to achieve. Building these systems across all Facebook platforms would enable shoppers to connect with their friends and family to get an opinion on an automatically generated 360-degree 3D view of an item. These friends can weigh in on which sneakers they like most or which size painting looks best in the shopper’s kitchen. By combining state-of-the-art computer vision with advancements in other AI domains, such as language understanding, personalization, and social-first experiences, we’re well positioned to transform online shopping for everyone.</p>">blog</a>; one model, 7 datasets, 89m images, 83 losses/tasks, and +8% search quality boost worldwide)</li>
</ul></li>
<li><p><a href="https://openreview.net/forum?id=SyxrxR4KPS#deepmind" data-popup-title="Deep neuroethology of a virtual rodent" data-popup-author="Josh Merel, Diego Aldarondo, Jesse Marshall, Yuval Tassa, Greg Wayne, Bence Olveczky (DM/Harvard)" data-popup-date="2020-03-11" data-popup-abstract="<p><em>TL;DR</em>: We built a physical simulation of a rodent, trained it to solve a set of tasks, and analyzed the resulting networks.</p><p><em>Abstract</em>: Parallel developments in neuroscience and deep learning have led to mutually productive exchanges, pushing our understanding of real and artificial neural networks in sensory and cognitive systems. However, this interaction between fields is less developed in the study of motor control. In this work, we develop a virtual rodent as a platform for the grounded study of motor activity in artificial models of embodied control. We then use this platform to study motor activity across contexts by training a model to solve four complex tasks. Using methods familiar to neuroscientists, we describe the behavioral representations and algorithms employed by different layers of the network using a neuroethological approach to characterize motor activity relative to the rodent’s behavior and goals. We find that the model uses two classes of representations which respectively encode the task-specific behavioral strategies and task-invariant behavioral kinematics. These representations are reflected in the sequential activity and population dynamics of neural subpopulations. Overall, the virtual rodent facilitates grounded collaborations between deep reinforcement learning and motor neuroscience. [Keywords: computational neuroscience, motor control, deep RL]</p>">“Deep neuroethology of a virtual rodent”</a>, Merel et al 2019 (<a href="https://spectrum.ieee.org/tech-talk/artificial-intelligence/machine-learning/ai-powered-rat-valuable-new-tool-neuroscience" data-popup-title="AI-Powered Rat Could Be a Valuable New Tool for Neuroscience: Researchers from DeepMind and Harvard are using a virtual rat to see what neural networks can teach us about biology" data-popup-author="Edd Gent (<span class=&quot;smallcaps-auto&quot;>IEEE</span> Spectrum)" data-popup-date="2020-04-27" data-popup-abstract="<p>Can we study AI the same way we study lab rats? Researchers at DeepMind and Harvard University seem to think so. They built an AI-powered virtual rat that can carry out multiple complex tasks. Then, they used neuroscience techniques to understand how its artificial “brain” controls its movements….Now the authors of a new paper due to be presented this week at <span class=&quot;smallcaps-auto&quot;>ICLR</span> have created a biologically accurate 3D model of a rat that can be controlled by a neural network in a simulated environment. They also showed that they could use neuroscience techniques for analyzing biological brain activity to understand how the neural net controlled the rat’s movements.</p><p>…The virtual rodent features muscles and joints based on measurements from real-life rats, as well as vision and a sense of proprioception, which refers to the feedback system that tells animals where their body parts are and how they’re moving. The researchers then trained a neural network to guide the rat through four tasks—jumping over a series of gaps, foraging in a maze, trying to escape a hilly environment, and performing precisely timed pairs of taps on a ball.</p><p>…Because the researchers had built the AI that powered the rat, much of what they found was expected. But one interesting insight they gained was that the neural activity seemed to occur over longer time scales than would be expected if it were directly controlling muscle forces and limb movements, says Diego Aldarondo, a coauthor and graduate student at Harvard. “This implies that the network represents behaviors at an abstract scale of running, jumping, spinning, and other intuitive behavioral categories,” he says, a cognitive model that has previously been proposed to exist in animals.</p><p>The neural network appeared to reuse some such representations across tasks, and the neural activity encoding them often took the form of sequences, a phenomenon that has been observed in both rodents and songbirds.</p>">media</a>)</p></li>
<li><p><a href="https://arxiv.org/abs/2004.12919#uber" data-popup-title="First return then explore" data-popup-author="Adrien Ecoffet, Joost Huizinga, Joel Lehman, Kenneth O. Stanley, Jeff Clune" data-popup-date="2020-05-03" data-popup-abstract="The promise of reinforcement learning is to solve complex sequential decision problems by specifying a high-level reward function only. However, RL algorithms struggle when, as is often the case, simple and intuitive rewards provide sparse and deceptive feedback. Avoiding these pitfalls requires thoroughly exploring the environment, but despite substantial investments by the community, creating algorithms that can do so remains one of the central challenges of the field. We hypothesize that the main impediment to effective exploration originates from algorithms forgetting how to reach previously visited states (&quot;detachment&quot;) and from failing to first return to a state before exploring from it (&quot;derailment&quot;). We introduce Go-Explore, a family of algorithms that addresses these two challenges directly through the simple principles of explicitly remembering promising states and first returning to such states before exploring. Go-Explore solves all heretofore unsolved Atari games (those for which algorithms could not previously outperform humans when evaluated following current community standards) and surpasses the state of the art on all hard-exploration games, with orders of magnitude improvements on the grand challenges Montezuma's Revenge and Pitfall. We also demonstrate the practical potential of Go-Explore on a challenging and extremely sparse-reward robotics task. Additionally, we show that adding a goal-conditioned policy can further improve Go-Explore's exploration efficiency and enable it to handle stochasticity throughout training. The striking contrast between the substantial performance gains from Go-Explore and the simplicity of its mechanisms suggests that remembering promising states, returning to them, and exploring from them is a powerful and general approach to exploration, an insight that may prove critical to the creation of truly intelligent learning agents.">“Go-Explore 2: First return then explore”</a>, Ecoffet et al 2020</p></li>
<li><p><a href="https://arxiv.org/abs/2005.12126#nvidia" data-popup-title="Learning to Simulate Dynamic Environments with GameGAN" data-popup-author="Seung Wook Kim, Yuhao Zhou, Jonah Philion, Antonio Torralba, Sanja Fidler" data-popup-date="2020-05-25" data-popup-abstract="Simulation is a crucial component of any robotic system. In order to simulate correctly, we need to write complex rules of the environment: how dynamic agents behave, and how the actions of each of the agents affect the behavior of others. In this paper, we aim to learn a simulator by simply watching an agent interact with an environment. We focus on graphics games as a proxy of the real environment. We introduce Game<span class=&quot;smallcaps-auto&quot;>GAN</span>, a generative model that learns to visually imitate a desired game by ingesting screenplay and keyboard actions during training. Given a key pressed by the agent, Game<span class=&quot;smallcaps-auto&quot;>GAN</span> &quot;renders&quot; the next screen using a carefully designed generative adversarial network. Our approach offers key advantages over existing work: we design a memory module that builds an internal map of the environment, allowing for the agent to return to previously visited locations with high visual consistency. In addition, Game<span class=&quot;smallcaps-auto&quot;>GAN</span> is able to disentangle static and dynamic components within an image making the behavior of the model more interpretable, and relevant for downstream tasks that require explicit reasoning over dynamic elements. This enables many interesting applications such as swapping different components of the game to build new games that do not exist.">“Learning to Simulate Dynamic Environments with Game<span>GAN</span>”</a>, Kim et al 2020 (<a href="https://nv-tlabs.github.io/gameGAN/" data-popup-title="Learning to Simulate Dynamic Environments with Game<span class=&quot;smallcaps-auto&quot;>GAN" data-popup-author="Seung Wook Kim, Yuhao Zhou, Jonah Philion, Antonio Torralba, Sanja Fidler (Nvidia)" data-popup-date="2020-05" data-popup-abstract="[Project page for Game<span class=&quot;smallcaps-auto&quot;>GAN</span>, a <span class=&quot;smallcaps-auto&quot;>GAN</span> which can learn and model a playable arcade game such as <em>Pac-Man</em>. This landing page includes diagrams, videos demonstrating Game<span class=&quot;smallcaps-auto&quot;>GAN</span> trained on the official version of <em>Pac-Man</em>, Game<span class=&quot;smallcaps-auto&quot;>GAN</span> trained on a custom version of <em>Pac-Man</em> and VizDoom, Game<span class=&quot;smallcaps-auto&quot;>GAN</span> trained with memory module &amp; disentangling rendering engine, and swapping foreground / background.]">project page</a>; an unexpected appearance of a <a href="https://arxiv.org/abs/1410.5401#deepmind" data-popup-title="Neural Turing Machines" data-popup-author="Alex Graves, Greg Wayne, Ivo Danihelka" data-popup-date="2020-06-05" data-popup-abstract="We extend the capabilities of neural networks by coupling them to external memory resources, which they can interact with by attentional processes. The combined system is analogous to a Turing Machine or Von Neumann architecture but is differentiable end-to-end, allowing it to be efficiently trained with gradient descent. Preliminary results demonstrate that Neural Turing Machines can infer simple algorithms such as copying, sorting, and associative recall from input and output examples.">Neural Turing Machine</a>)</p></li>
<li><p><a href="https://distill.pub/2020/bayesian-optimization/" data-popup-title="Exploring Bayesian Optimization: Breaking Bayesian Optimization into small, sizeable chunks" data-popup-author="Apoorv Agnihotri, Nipun Batra (Distill.pub)" data-popup-date="2020-05-05" data-popup-doi="10.23915/distill.00026" data-popup-abstract="<p>[Discussion of Bayesian optimization (BO), a decision-theoretic application of Bayesian statistics (typically using Gaussian processes for flexibility) which tries to model a set of variables to find the maximum or best in the fewest number of collected data points possible. This differs from normal experiment design which tries to simply maximize the overall information about all points given a fixed number of samples, not just the best point, or “active learning”, which tries to select data points which make the model as predictive as possible while collecting samples. The difference can be visualized by watching posterior distributions for simple 2D problems evolve as data is collected according to different BO or active learning or simple grid-search/random baseline strategies. The optimal strategy is usually infeasible to calculate, so various heuristics like “expected improvement” or “Thompson sampling” are used, and their different behavior can be visualized and compared. BO is heavily used in machine learning to find the best combinations of settings for machine learning models.]</p><p>In this article, we looked at Bayesian Optimization for optimizing a black-box function. Bayesian Optimization is well suited when the function evaluations are expensive, making grid or exhaustive search impractical. We looked at the key components of Bayesian Optimization. First, we looked at the notion of using a surrogate function (with a prior over the space of objective functions) to model our black-box function. Next, we looked at the “Bayes” in Bayesian Optimization — the function evaluations are used as data to obtain the surrogate posterior. We look at acquisition functions, which are functions of the surrogate posterior and are optimized sequentially. This new sequential optimization is inexpensive and thus of utility of us. We also looked at a few acquisition functions and showed how these different functions balance exploration and exploitation. Finally, we looked at some practical examples of Bayesian Optimization for optimizing hyper-parameters for machine learning models.</p>">“Exploring Bayesian Optimization: Breaking Bayesian Optimization into small, sizeable chunks”</a>, Agnihotri &amp; Batra 2020</p></li>
<li><p><a href="http://www.thisworddoesnotexist.com/" data-popup-title="This Word Does Not Exist" data-popup-author="Thomas Dimson" data-popup-date="2020-05-13" data-popup-abstract="<p>[<span class=&quot;smallcaps-auto&quot;>GPT</span>-2 samples generated after training on a dictionary and heavily filtered to try to remove existing words (<a href=&quot;https://github.com/turtlesoupy/this-word-does-not-exist&quot;>source</a>). Example:</p><blockquote><p><strong>pellum (noun)</strong></p><p>the highest or most important point or position</p><p><em>“he never shied from the pellum or the right to preach”</em></p></blockquote><p>Discussion: <a href=&quot;https://news.ycombinator.com/item?id=23169962&quot;>HN</a>, <a href=&quot;https://old.reddit.com/r/MachineLearning/comments/gj475j/project_this_word_does_not_exist/&quot;>/r/ML</a>]</p><p>…Most of the project was spent throwing a number of rejection tricks to make good samples, e.g.,</p><ul><li>Rejecting samples that contain words that are in the a training set / blacklist to force generation completely novel words</li><li>Rejecting samples without the use of the word in the example usage</li><li>Running a part of speech tagger on the example usage to ensure they use the word in the correct <span class=&quot;smallcaps-auto&quot;>POS</span></li></ul>">“This Word Does Not Exist”</a> (<span>GPT-2</span>); <a href="https://www.thisfursonadoesnotexist.com/" data-popup-title="This Fursona Does Not Exist (TFDNE)" data-popup-author="<a href=&quot;https://twitter.com/arfafax&quot;>Arfafax</a>" data-popup-date="2020-05-07" data-popup-abstract="<p>A Style<span class=&quot;smallcaps-auto&quot;>GAN</span> 2 showcase: high-quality <span class=&quot;smallcaps-auto&quot;>GAN</span>-generated furry (anthropomorphic animals) faces, trained on <a href=&quot;https://github.com/arfafax/E621-Face-Dataset&quot;><em>n</em>=55k faces</a> cropped from the e621 furry image booru. For higher quality, the creator heavily filtered faces and aligned them, and upscaled using waifu2✕. For display, it reuses Obormot’s <a href=&quot;https://www.obormot.net/demos/these-waifus-do-not-exist-v2-alt&quot;>“These Waifus Do Not Exist”</a> scrolling grid code to display an indefinite number of faces rather than one at a time. (<span class=&quot;smallcaps-auto&quot;>TFDNE</span> is also available on <a href=&quot;https://artbreeder.com&quot;>Artbreeder</a> for interactive editing/crossbreeding, and a <a href=&quot;https://fursona.app&quot;>Google Colab notebook</a> for Ganspace-based editing</a>.)</p><figure><img src=&quot;/images/gan/2020-05-06-stylegan2-arfafax-tfdne-9xgrid.png&quot; alt=&quot;9 random AI generated furry faces&quot; /><figcaption>9 random <span class=&quot;smallcaps-auto&quot;>TFDNE</span> furry face samples in a grid</figcaption></figure><p>Model download mirrors:</p><ul><li><p><a href=&quot;https://drive.google.com/file/d/1t7E8NEqK_gVJwxrWEihR1IcPfekaBc1d/view&quot;>Google Drive</a></p></li><li><p><a href=&quot;https://mega.nz/file/Wa4EFQRA#XL9X5tGNrlp1bTdafPWK_Kg65RW3J5-CR9biGEfFm_g&quot;>Mega</a></p></li><li><p>Rsync:</p><div class=&quot;sourceCode&quot; id=&quot;cb1&quot;><pre class=&quot;sourceCode Bash&quot;><code class=&quot;sourceCode bash&quot;><span id=&quot;cb1-1&quot;><a href=&quot;#cb1-1&quot;></a><span class=&quot;fu&quot;>rsync</span> --verbose rsync://78.46.86.149:873/biggan/2020-05-06-arfa-stylegan2-e621-r-512-3194880.pkl.xz ./</span></code></pre></div></li></ul>">“This Fursona Does Not Exist (<span>TFDNE</span>)”</a> <a href="https://fursona.app/" data-popup-title="This Fursona Does Not Exist—Fursona Editor (Tensorflow Version)" data-popup-author="Arfafax" data-popup-date="2020-06-01" data-popup-abstract="[Google Colab notebook for interactive editing faces generated by the <span class=&quot;smallcaps-auto&quot;>TFDNE</span>.com furry face Style<span class=&quot;smallcaps-auto&quot;>GAN</span> 2 model, using <a href=&quot;https://github.com/harskish/ganspace&quot;>Ganspace</a> to reverse-engineer the latent encoding and allow control of specific visual attributes of faces.]">editor</a> (a simple but high-quality <a href="https://www.gwern.net/Faces" data-popup-title="Making Anime Faces With StyleGAN" data-popup-author="Gwern Branwen" data-popup-date="4 Feb 2019" data-popup-abstract="<p>Generative neural networks, such as <span class=&quot;smallcaps-auto&quot;>GAN</span>s, have <a href=&quot;#why-dont-gans-work&quot;>struggled for years</a> to generate decent-quality anime faces, despite their great success with photographic imagery such as real human faces. The task has now been effectively solved, for anime faces as well as many other domains, by the development of a new generative adversarial network, <a href=&quot;https://arxiv.org/abs/1812.04948&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;A Style-Based Generator Architecture for Generative Adversarial Networks&quot; data-popup-author=&quot;Tero Karras, Samuli Laine, Timo Aila&quot; data-popup-date=&quot;2019-08-26&quot; data-popup-abstract=&quot;We propose an alternative generator architecture for generative adversarial networks, borrowing from style transfer literature. The new architecture leads to an automatically learned, unsupervised separation of high-level attributes (e.g., pose and identity when trained on human faces) and stochastic variation in the generated images (e.g., freckles, hair), and it enables intuitive, scale-specific control of the synthesis. The new generator improves the state-of-the-art in terms of traditional distribution quality metrics, leads to demonstrably better interpolation properties, and also better disentangles the latent factors of variation. To quantify interpolation quality and disentanglement, we propose two new, automated methods that are applicable to any generator architecture. Finally, we introduce a new, highly varied and high-quality dataset of human faces.&quot; title=&quot;&amp;#39;A Style-Based Generator Architecture for Generative Adversarial Networks&amp;#39;, Karras et al 2018&quot;><em>Style<span class=&quot;smallcaps-auto&quot;>GAN</span></em></a>, whose <a href=&quot;https://github.com/NVlabs/stylegan&quot;>source code</a> was released in February 2019.</p><p>I <a href=&quot;#examples&quot;>show off</a> my Style<span class=&quot;smallcaps-auto&quot;>GAN</span> 1/2 CC-0-licensed anime faces &amp;amp; videos, provide downloads for the final models &amp;amp; <a href=&quot;./Crops#danbooru2019-portraits&quot;>anime portrait face dataset</a>, provide the ‘missing manual’ &amp;amp; explain how I trained them based on <a href=&quot;./Danbooru2018&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;Danbooru2018: A Large-Scale Crowdsourced and Tagged Anime Illustration Dataset&quot; data-popup-author=&quot;Gwern Branwen&quot; data-popup-date=&quot;15 Dec 2015&quot; data-popup-abstract=&quot;&amp;lt;p&amp;gt;Deep learning for computer revision relies on large annotated datasets. Classification/categorization has benefited from the creation of ImageNet, which classifies 1m photos into 1000 categories. But classification/categorization is a coarse description of an image which limits application of classifiers, and there is no comparably large dataset of images with many tags or labels which would allow learning and detecting much richer information about images. Such a dataset would ideally be &amp;amp;gt;1m images with at least 10 descriptive tags each which can be publicly distributed to all interested researchers, hobbyists, and organizations. There are currently no such public datasets, as ImageNet, Birds, Flowers, and MS COCO fall short either on image or tag count or restricted distribution. I suggest that the “image -boorus” be used. The image boorus are longstanding web databases which host large numbers of images which can be ‘tagged’ or labeled with an arbitrary number of textual descriptions; they were developed for and are most popular among fans of anime, who provide detailed annotations.&amp;lt;/p&amp;gt;&amp;lt;p&amp;gt;The best known booru, with a focus on quality, is &amp;lt;a href=&amp;quot;https://danbooru.donmai.us/&amp;quot; class=&amp;quot;docMetadata&amp;quot; data-popup-image-height=&amp;quot;768&amp;quot; data-popup-image-width=&amp;quot;768&amp;quot;&amp;gt;Danbooru&amp;lt;/a&amp;gt;. We create &amp;amp;amp; provide a torrent which contains ~2.5tb of 3.33m images with 92.7m tag instances (of 365k defined tags, ~27.8/image) covering Danbooru from 24 May 2005 through 31 December 2018 (final ID: #3,368,713), providing the image files &amp;amp;amp; a JSON export of the metadata. We also provide a smaller torrent of SFW images downscaled to 512x512px JPGs (241GB; 2,232,462 images) for convenience.&amp;lt;/p&amp;gt;&amp;lt;p&amp;gt;Our hope is that a Danbooru2018 dataset can be used for rich large-scale classification/tagging &amp;amp;amp; learned embeddings, test out the transferability of existing computer vision techniques (primarily developed using photographs) to illustration/anime-style images, provide an archival backup for the Danbooru community, feed back metadata improvements &amp;amp;amp; corrections, and serve as a testbed for advanced techniques such as conditional image generation or style transfer.&amp;lt;/p&amp;gt;&quot;>Danbooru2017/2018</a> with source code for the <a href=&quot;#data-preparation&quot;>data preprocessing</a>, document <a href=&quot;#installation&quot;>installation</a> &amp;amp; <a href=&quot;#configuration&quot;>configuration</a> &amp;amp; <a href=&quot;#running&quot;>training tricks</a>.</p><p>For application, I document various scripts for generating <a href=&quot;#sampling&quot;>images &amp;amp; videos</a>, briefly <a href=&quot;#twdne&quot;>describe the website</a> <a href=&quot;https://www.thiswaifudoesnotexist.net&quot;>“This Waifu Does Not Exist”</a> <a href=&quot;./TWDNE&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;This Waifu Does Not Exist&quot; data-popup-author=&quot;Gwern Branwen&quot; data-popup-date=&quot;19 Feb 2019&quot; data-popup-abstract=&quot;&amp;lt;p&amp;gt;Generating high-quality anime faces has long been a task neural networks struggled with. The invention of StyleGAN in 2018 has effectively solved this task and I have trained a StyleGAN model which can generate high-quality anime faces at 512px resolution. To show off the recent progress, I made a website, &amp;lt;a href=&amp;quot;https://www.thiswaifudoesnotexist.net/&amp;quot; class=&amp;quot;docMetadata&amp;quot; data-popup-title=&amp;quot;ThisWaifuDoesNotExist.net&amp;quot; data-popup-author=&amp;quot;Gwern Branwen&amp;quot; data-popup-date=&amp;quot;2019-02-19&amp;quot; data-popup-abstract=&amp;quot;&amp;amp;lt;p&amp;amp;gt;&amp;amp;lt;a href=&amp;amp;quot;https://www.thiswaifudoesnotexist.net/&amp;amp;quot;&amp;amp;gt;&amp;amp;lt;code&amp;amp;gt;ThisWaifuDoesNotExist.net&amp;amp;lt;/code&amp;amp;gt;&amp;amp;lt;/a&amp;amp;gt; (&amp;amp;lt;a href=&amp;amp;quot;https://www.gwern.net/TWDNE&amp;amp;quot;&amp;amp;gt;TWDNE&amp;amp;lt;/a&amp;amp;gt;) is a static website which uses JS to display random &amp;amp;lt;a href=&amp;amp;quot;https://www.gwern.net/Faces&amp;amp;quot;&amp;amp;gt;anime faces generated by StyleGAN&amp;amp;lt;/a&amp;amp;gt; neural networks, along with &amp;amp;lt;a href=&amp;amp;quot;https://www.gwern.net/GPT-2&amp;amp;quot;&amp;amp;gt;GPT-2&amp;amp;lt;/a&amp;amp;gt;-generated &amp;amp;#39;anime plot summaries&amp;amp;#39;.&amp;amp;lt;/p&amp;amp;gt;&amp;amp;lt;p&amp;amp;gt;&amp;amp;lt;figure&amp;amp;gt;&amp;amp;lt;img src=&amp;amp;quot;/images/gan/thiswaifudoesnotexist.png&amp;amp;quot; alt=&amp;amp;quot;A screenshot of “This Waifu Does Not Exist” (TWDNE) showing a random StyleGAN-generated anime face and a random GPT-2-117M text sample conditioned on anime keywords/phrases.&amp;amp;quot; /&amp;amp;gt;&amp;amp;lt;figcaption&amp;amp;gt;A screenshot of &amp;amp;lt;q&amp;amp;gt;“This Waifu Does Not Exist”&amp;amp;lt;/q&amp;amp;gt; (TWDNE) showing a random StyleGAN-generated anime face and a random GPT-2-117M text sample conditioned on anime keywords/phrases.&amp;amp;lt;/figcaption&amp;amp;gt;&amp;amp;lt;/figure&amp;amp;gt;&amp;amp;lt;/p&amp;amp;gt;&amp;quot;&amp;gt;“This Waifu Does Not Exist”&amp;lt;/a&amp;gt; for displaying random StyleGAN faces. TWDNE displays a different neural-net-generated face &amp;amp;amp; plot summary every 15s. The site was popular and went viral online, especially in China. The model can also be used interactively for exploration &amp;amp;amp; editing in the &amp;lt;a href=&amp;quot;http://artbreeder.com/&amp;quot; class=&amp;quot;docMetadata&amp;quot; data-popup-image-height=&amp;quot;768&amp;quot; data-popup-image-width=&amp;quot;768&amp;quot;&amp;gt;Artbreederonline service&amp;lt;/a&amp;gt;.&amp;lt;/p&amp;gt;&amp;lt;p&amp;gt;TWDNE faces have been used as screensavers, user avatars, character art for game packs or &amp;lt;a href=&amp;quot;https://klimaleksus.github.io/FindTwin/&amp;quot; class=&amp;quot;docMetadata&amp;quot; data-popup-image-height=&amp;quot;768&amp;quot; data-popup-image-width=&amp;quot;768&amp;quot; title=&amp;quot;Find Twin v1.0, by Kly_Men_COmpany: This is a simple game, where you need to find the same image among other similar images.&amp;quot;&amp;gt;online&amp;lt;/a&amp;gt; &amp;lt;a href=&amp;quot;https://github.com/darabos/high-five-trading&amp;quot; class=&amp;quot;docMetadata&amp;quot; data-popup-image-height=&amp;quot;768&amp;quot; data-popup-image-width=&amp;quot;768&amp;quot; title=&amp;quot;Action stock exchange game for Repl.it Game Jam 2019&amp;quot;&amp;gt;games&amp;lt;/a&amp;gt;, uploaded to Pixiv, and used in a research paper (&amp;lt;a href=&amp;quot;https://arxiv.org/abs/1904.01774&amp;quot; class=&amp;quot;docMetadata&amp;quot; data-popup-title=&amp;quot;Image Generation from Small Datasets via Batch Statistics Adaptation&amp;quot; data-popup-author=&amp;quot;Atsuhiro Noguchi, Tatsuya Harada&amp;quot; data-popup-date=&amp;quot;2019-08-26&amp;quot; data-popup-abstract=&amp;quot;Thanks to the recent development of deep generative models, it is becoming possible to generate high-quality images with both fidelity and diversity. However, the training of such generative models requires a large dataset. To reduce the amount of data required, we propose a new method for transferring prior knowledge of the pre-trained generator, which is trained with a large dataset, to a small dataset in a different domain. Using such prior knowledge, the model can generate images leveraging some common sense that cannot be acquired from a small dataset. In this work, we propose a novel method focusing on the parameters for batch statistics, scale and shift, of the hidden layers in the generator. By training only these parameters in a supervised manner, we achieved stable training of the generator, and our method can generate higher quality images compared to previous methods without collapsing even when the dataset is small (~100). Our results show that the diversity of the filters acquired in the pre-trained generator is important for the performance on the target domain. By our method, it becomes possible to add a new class or domain to a pre-trained generator without disturbing the performance on the original domain.&amp;quot; title=&amp;quot;Image Generation from Small Datasets via Batch Statistics Adaptation&amp;quot;&amp;gt;Noguchi &amp;amp;amp; Harada 2019&amp;lt;/a&amp;gt;). TWDNE results also helped inspired Sizigi Studio’s online interactive waifu GAN, &amp;lt;a href=&amp;quot;https://waifulabs.com/&amp;quot; class=&amp;quot;docMetadata&amp;quot; data-popup-image-height=&amp;quot;768&amp;quot; data-popup-image-width=&amp;quot;768&amp;quot;&amp;gt;Waifu Labs&amp;lt;/a&amp;gt;, which generates even better anime faces than my StyleGAN results.&amp;lt;/p&amp;gt;&quot;>I set up</a> as a public demo (see also <a href=&quot;https://artbreeder.com&quot;>Artbreeder</a>), discuss how the trained models can be <a href=&quot;#transfer-learning&quot;>used for transfer learning</a> such as generating high-quality faces of anime characters with small datasets (eg <a href=&quot;#holo&quot;>Holo</a> or <a href=&quot;#asuka&quot;>Asuka Souryuu Langley</a>), and touch on <a href=&quot;#reversing-stylegan-to-control-modify-images&quot;>more advanced Style<span class=&quot;smallcaps-auto&quot;>GAN</span> applications</a> like encoders &amp;amp; controllable generation.</p><p>The <a href=&quot;#appendix&quot;>appendix</a> gives samples of my failures with earlier <span class=&quot;smallcaps-auto&quot;>GAN</span>s for anime face generation, and I provide samples &amp;amp; model from a relatively large-scale <a href=&quot;#biggan&quot;>Big<span class=&quot;smallcaps-auto&quot;>GAN</span></a> training run suggesting that Big<span class=&quot;smallcaps-auto&quot;>GAN</span> may be the next step forward to generating full-scale anime images.</p><p>A minute of reading could save an hour of debugging!</p>">Style<span>GAN</span> 2 face model</a> of <a href="https://en.wikipedia.org/wiki/Furry_fandom" data-popup-title="Furry fandom" data-popup-author="English Wikipedia" data-popup-abstract="<p>The <b>furry fandom</b> is a subculture interested in anthropomorphic animal characters with human personalities and characteristics. Examples of anthropomorphic attributes include exhibiting human intelligence and facial expressions, speaking, walking on two legs, and wearing clothes. The term &quot;furry fandom&quot; is also used to refer to the community of people who gather on the internet and at furry conventions.</p>">furries</a>, also available on <a href="https://artbreeder.com/" data-popup-title="Artbreeder" data-popup-author="Joel Simon" data-popup-date="2019-09-09" data-popup-abstract="[Artbreeder is an interactive <span class=&quot;smallcaps-auto&quot;>GAN</span> generator website. Originally named &quot;Ganbreeder&quot; and providing only the 256px Big<span class=&quot;smallcaps-auto&quot;>GAN</span> generator, it now provides a variety of Big<span class=&quot;smallcaps-auto&quot;>GAN</span> &amp; Style<span class=&quot;smallcaps-auto&quot;>GAN</span> models, including the anime portrait Style<span class=&quot;smallcaps-auto&quot;>GAN</span> model. (It is more general than the similar Waifu Labs, but my anime model is not as good.) Users can generate random samples and explore slight variants of them to gradually explore the &quot;latent space&quot; and find interesting images, but they can also edit images more directly, upload existing images to find the most similar image produced by the model, etc. A popular website, it has generated >56m images from September 2019 to January 2020.]">Artbreeder</a>; interesting for how <a href="https://www.gizmodo.com.au/2020/05/the-internet-furry-drama-raising-big-questions-about-artificial-intelligence/" data-popup-title="The Internet Furry Drama Raising Big Questions About Artificial Intelligence" data-popup-author="Whitney Kimball (Gizmodo)" data-popup-date="2020-05-17" data-popup-abstract="<p>Much of the fun of internet drama comes from its frivolousness, but sometimes an online shitfest points to something bigger. Last week, the AI-powered furry art site <a href=&quot;https://www.thisfursonadoesnotexist.com/&quot;>This Fursona Does Not Exist</a> did just that, igniting a fandom firestorm while also highlighting an important debate about digital art. Trained on more than 55,000 images pulled (without permission) from a furry art forum, the algorithm was a simple case of art theft to some. For others, it was a chance to break out the popcorn. But legal scholars who spoke with Gizmodo said the conflict raises thorny questions about ownership in the age of AI—questions that may ultimately have to be answered in court.</p><p>…At least one person tried (and failed) to find proof that the algorithm was copying images from e621.net outright. And within days, the entire site was slapped with a <span class=&quot;smallcaps-auto&quot;>DMCA</span> copyright infringement complaint. (The company whose name the <span class=&quot;smallcaps-auto&quot;>DMCA</span> was issued in, according to Arfa, denied filing the notice and requested it be withdrawn.) Some degree of backlash is understandable. Furry fandom has long been a close-knit community of independent creators supported by individual commissions. A project aimed at mass-producing fursonas—using original art as training material, no less—could be seen as a threat to creators’ livelihood. Some commenters accused Arfa of disrespect and asked for the choice to opt out of the project. Others complained that their work had been uploaded to e621 without their permission in the first place.</p>">the fur flew</a> due to legal fuzziness &amp; some artists acting like animals, howling about ‘theft’ &amp; free fursonas being a wolf in sheep’s clothing upsetting their pecking order<a href="#sn9" id="fnref9" role="doc-noteref"><sup>9</sup></a>—though the creator has outfoxed the paper tiger threats, these kittlesome questions will dog ML as DL models multiply like rabbits)</p></li>
</ul>
<p>Genetics:</p>
<ul>
<li><p><span>Everything Is Heritable</span>:</p>
<ul>
<li><a href="https://www.biorxiv.org/content/10.1101/2020.05.08.084475v1" data-popup-title="Local genetic correlation analysis reveals heterogeneous etiologic sharing of complex traits" data-popup-author="Yiliang Zhang, Qiongshi Lu, Yixuan Ye, Kunling Huang, Wei Liu, Yuchang Wu, Xiaoyuan Zhong, Boyang Li, Zhaolong Yu, Brittany G. Travers, Donna M. Werling, James J. Li, Hongyu Zhao" data-popup-date="2020-05-10" data-popup-doi="10.1101/2020.05.08.084475" data-popup-abstract="<p>Local genetic correlation quantifies the genetic similarity of complex traits in specific genomic regions, which could shed unique light on etiologic sharing and provide additional mechanistic insights into the genetic basis of complex traits compared to global genetic correlation. However, accurate estimation of local genetic correlation remains challenging, in part due to extensive linkage disequilibrium in local genomic regions and pervasive sample overlap across studies. We introduce <span class=&quot;smallcaps-auto&quot;>SUPERGNOVA</span>, a unified framework to estimate both global and local genetic correlations using summary statistics from genome-wide association studies. Through extensive simulations and analyses of 30 complex traits, we demonstrate that <span class=&quot;smallcaps-auto&quot;>SUPERGNOVA</span> substantially outperforms existing methods and identifies 150 trait pairs with significant local genetic correlations. In particular, we show that the positive, consistently-identified, yet paradoxical genetic correlation between autism spectrum disorder and cognitive performance could be explained by two etiologically-distinct genetic signatures with bidirectional local genetic correlations. We believe that statistically-rigorous local genetic correlation analysis could accelerate progress in complex trait genetics research.</p>">“Local genetic correlation analysis reveals heterogeneous etiologic sharing of complex traits”</a>, Zhang et al 2020 (“autism/IQ <em>r<sub>g</sub></em>…could be explained by 2 etiologically-distinct genetic signatures w/bidirectional local genetic correlations”)</li>
<li><a href="https://www.biorxiv.org/content/10.1101/2020.05.06.081273v1" data-popup-title="Identification of 370 loci for age at onset of sexual and reproductive behaviour, highlighting common aetiology with reproductive biology, externalizing behaviour and longevity" data-popup-author="Melinda C. Mills, Felix C. Tropf, David M. Brazel, Natalie van Zuydam, Ahmad Vaez, e<span class=&quot;smallcaps-auto&quot;>QTLG</span>en Consortium, <span class=&quot;smallcaps-auto&quot;>BIOS</span> Consortium, Tune H. Pers, Harold Snieder, John R.B. Perry, Ken K. Ong, Marce et al [...]" data-popup-date="2020-05-07" data-popup-doi="10.1101/2020.05.06.081273" data-popup-abstract="<p>The timing of reproductive behaviour, age at first sexual intercourse (<span class=&quot;smallcaps-auto&quot;>AFS</span>) and age at first birth (<span class=&quot;smallcaps-auto&quot;>AFB</span>), has implications for reproductive health, adolescent development and evolutionary fitness. In the largest genome-wide association study to date (<span class=&quot;smallcaps-auto&quot;>AFS</span>, N=387,338; <span class=&quot;smallcaps-auto&quot;>AFB</span>, N=542,901), we identify 370 independent signals, 11 which are sex-specific, with a 5-6% polygenic score prediction. Heritability shifted from 10% for those born in 1940 to 23% for the 1965 birth cohort. Using Genomic <span class=&quot;smallcaps-auto&quot;>SEM</span>, we show that signals are largely driven by the genetics of reproductive biology and externalizing behaviour. This is supported by extensive biological follow-up that isolates key genes related to follicle stimulating hormone (<span class=&quot;smallcaps-auto&quot;>FSHB</span>), implantation (<span class=&quot;smallcaps-auto&quot;>ESR</span>1), infertility (endometriosis, spontaneous abortion) and spermatid differentiation, morphogenesis and binding (<span class=&quot;smallcaps-auto&quot;>KLF</span>17, <span class=&quot;smallcaps-auto&quot;>ZPBP</span>).  Later <span class=&quot;smallcaps-auto&quot;>AFB</span> is protective against later-life disease (type 2 diabetes, cardiovascular) and associated with longevity. Those from higher childhood socioeconomic circumstances and polygenic scores in the highest deciles (90%+) experience markedly later reproductive onset. Results are relevant for interventions in teenage sexual, reproductive and mental health, deepen our understanding of the drivers of later-life health and longevity, and fuel infertility and functional follow-up experiments.</p>">“Identification of 370 loci for age at onset of sexual and reproductive behaviour, highlighting common aetiology with reproductive biology, externalizing behaviour and longevity”</a>, Mills et al 2020</li>
<li><a href="https://www.biorxiv.org/content/10.1101/2020.05.09.075226v2" data-popup-title="Genome-wide association study of school grades identifies a genetic overlap between language ability, psychopathology and creativity" data-popup-author="Veera M. Rajagopal, Andrea Ganna, Jonathan R. I. Coleman, Andrea G. Allegrini, Georgios Voloudakis, Jakob Grove, Thomas D. Als, Henriette T. Horsdal, Liselotte Petersen, Vivek Appadurai, Andrew Schork, Alfonso Buil, Cynthia M. Bulik, Jonas Bybjerg-Grauholm et al [...]" data-popup-date="2020-05-12" data-popup-doi="10.1101/2020.05.09.075226" data-popup-abstract="<p>Individuals with psychiatric disorders perform differently in school compared to the general population. Genetic factors contribute substantially to such differences. It is however unclear if differential performance is seen across all cognitive domains such as math and language. Here we report a genome-wide association study (<span class=&quot;smallcaps-auto&quot;>GWAS</span>) of school grades in 30,982 individuals (18,495 with and 12,487 without one or more of six major psychiatric disorders) and a replication study in 4,547 individuals. <span class=&quot;smallcaps-auto&quot;>GWAS</span> of overall school performance yielded results that were highly similar to the results of a previous <span class=&quot;smallcaps-auto&quot;>GWAS</span> of educational attainment. Analyzing subject specific grades, we observed that math performance was severely affected whereas language performance (Danish and English) was relatively unaffected or enhanced in those with psychiatric disorders compared to controls. We found that the genetic variants associated with poor math performance, but better language performance were also associated with increased risk for multiple psychiatric disorders. The same variants were also associated with creativity, which we show through a polygenic score analysis of 2953 creative professionals and 164,622 controls. The results overall suggest that risk for psychiatric disorders, language ability and creativity might have overlapping genetic roots.</p>">“Genome-wide association study of school grades identifies a genetic overlap between language ability, psychopathology and creativity”</a>, Rajagopal et al 2020 (“math performance was severely affected whereas language performance (Danish and English) was relatively unaffected or enhanced in those with psychiatric disorders”)</li>
<li><a href="https://www.medrxiv.org/content/10.1101/2020.05.18.20100685v2" data-popup-title="<span class=&quot;smallcaps-auto&quot;>GWAS of Depression Phenotypes in the Million Veteran Program and Meta-analysis in More than 1.2 Million Participants Yields 178 Independent Risk Loci" data-popup-author="Daniel F. Levey, Murray B. Stein, Frank R. Wendt, Gita A. Pathak, Hang Zhou, Mihaela Aslan, Rachel Quaden, Kelly M. Harrington, Gerard Sanacora, Andrew M. McIntosh, John Concato, Renato Polimanti, Joel Gelernter, on behalf of the Million Veteran Program" data-popup-date="2020-05-22" data-popup-doi="10.1101/2020.05.18.20100685" data-popup-abstract="<p>We report a large meta-analysis of depression using data from the Million Veteran Program (<span class=&quot;smallcaps-auto&quot;>MVP</span>), 23andMe Inc., UK Biobank, and FinnGen; including individuals of European ancestry (<em>n</em>=1,154,267; 340,591 cases) and African ancestry (<em>n</em>=59,600; 25,843 cases). We identified 223 and 233 independent <span class=&quot;smallcaps-auto&quot;>SNP</span>s associated with depression in European ancestry and transancestral analysis, respectively. Genetic correlations within the <span class=&quot;smallcaps-auto&quot;>MVP</span> cohort across electronic health records diagnosis, survey self-report of diagnosis, and a 2-item depression screen exceeded 0.81. Using transcriptome-wide association study (<span class=&quot;smallcaps-auto&quot;>TWAS</span>) we found significant associations for gene expression in several brain regions, including hypothalamus (<span class=&quot;smallcaps-auto&quot;>NEGR</span>1, <em>p</em>=3.19✕10<sup>−25</sup>) and nucleus accumbens (<span class=&quot;smallcaps-auto&quot;>DRD</span>2, <em>p</em>=1.87✕10<sup>−20</sup>). 178 genomic risk loci were fine-mapped to find likely causal variants. We identified likely pathogenicity in these variants and overlapping gene expression for 17 genes from our <span class=&quot;smallcaps-auto&quot;>TWAS</span>, including <span class=&quot;smallcaps-auto&quot;>TRAF</span>3. This study sheds light on the genetic architecture of depression and provides new insight into the interrelatedness of complex psychiatric traits.</p>">“<span>GWAS</span> of Depression Phenotypes in the Million Veteran Program and Meta-analysis in More than 1.2 Million Participants Yields 178 Independent Risk Loci”</a>, Levey et al 2020</li>
<li><a href="https://www.gwern.net/docs/genetics/editing/2020-yagound.pdf" data-popup-title="A Single Gene Causes Thelytokous Parthenogenesis, the Defining Feature of the Cape Honeybee <em>Apis mellifera capensis</em>" data-popup-author="Boris Yagound, Kathleen A. Dogantzis, Amro Zayed, Julianne Lim, Paul Broekhuyse, Emily J. Remnant, Madeleine Beekman, Michael H. Allsopp, Sarah E. Aamidor, Orly Dim, Gabriele Buchmann, Benjamin P. Oldroyd" data-popup-date="2020-05-07" data-popup-doi="10.1016/j.cub.2020.04.033" data-popup-abstract="<p>In honeybees, the ability of workers to produce daughters asexually, i.e., thelytokous parthenogenesis, is restricted to a single subspecies inhabiting the Cape region of South Africa, <a href=&quot;https://en.wikipedia.org/wiki/Cape_honey_bee&quot;><em>Apis mellifera capensis</em></a>. Thelytoky has unleashed new selective pressures and the evolution of traits such as social parasitism, invasiveness, and social cancer. Thelytoky arises from an abnormal meiosis that results in the fusion of two maternal pronuclei, restoring diploidy in newly laid eggs. The genetic basis underlying thelytoky is disputed. To resolve this controversy, we generated a backcross between thelytokous <em>A. m. capensis</em> and non-thelytokous <em>A. m. scutellata</em> from the neighboring population and looked for evidence of genetic markers that co-segregated with thelytokous reproduction in 49 backcross females. We found that markers associated with the gene GB45239 on chromosome 11, including non-synonymous variants, showed consistent co-segregation with thelytoky, whereas no other region did so. Alleles associated with thelytoky were present in all <em>A. m. capensis</em> genomes examined but were absent from all other honeybees worldwide including <em>A. m. scutellata</em>. GB45239 is derived in <em>A. m. capensis</em> and has a putative role in chromosome segregation. It is expressed in ovaries and is downregulated in thelytokous bees, likely because of polymorphisms in the promoter region. Our study reveals how mutations affecting the sequence and/or expression of a single gene can change the reproductive mode of a population. [Keywords: thelytoky, honeybee, meiosis, reproductive mode]</p>">“A Single Gene Causes Thelytokous Parthenogenesis, the Defining Feature of the Cape Honeybee <em>Apis mellifera capensis</em>”</a>, Yagound et al 2020</li>
<li><a href="https://www.biorxiv.org/content/10.1101/2020.05.12.090555v1" data-popup-title="Insights into the genetic architecture of the human face" data-popup-author="Julie D. White, Karlijne Indencleef, Sahin Naqvi, Ryan J. Eller, Jasmien Roosenboom, Myoung Keun Lee, Jiarui Li, Jaaved Mohammed, Stephen Richmond, Ellen E. Quillen, Heather L. Norton, Eleanor Feingold, Tomek Swigut, Mary L. Marazita, Hilde Peeters, Greet  et al [...]" data-popup-date="2020-05-14" data-popup-doi="10.1101/2020.05.12.090555" data-popup-abstract="<p>The human face is complex and multipartite, and characterization of its genetic architecture remains intriguingly challenging. Applying <span class=&quot;smallcaps-auto&quot;>GWAS</span> to multivariate shape phenotypes, we identified 203 genomic regions associated with normal-range facial variation, 117 of which are novel. The associated regions are enriched for both genes relevant to craniofacial and limb morphogenesis and enhancer activity in cranial neural crest cells and craniofacial tissues. Genetic variants grouped by their contribution to similar aspects of facial variation show high within-group correlation of enhancer activity, and four <span class=&quot;smallcaps-auto&quot;>SNP</span> pairs display evidence of epistasis, indicating potentially coordinated actions of variants within the same cell types or tissues. In sum, our analyses provide new insights for understanding how complex morphological traits are shaped by both individual and coordinated genetic actions.</p>">“Insights into the genetic architecture of the human face”</a>, White et al 2020</li>
</ul></li>
<li><p><span>Recent Evolution</span>:</p>
<ul>
<li><a href="https://www.biorxiv.org/content/10.1101/2020.05.26.116111v1" data-popup-title="Sex-biased reduction in reproductive success drives selective constraint on human genes" data-popup-author="Eugene J. Gardner, Matthew D. C. Neville, Kaitlin E. Samocha, Kieron Barclay, Martin Kolk, Mari E. K. Niemi, George Kirov, Hilary C. Martin, Matthew E. Hurles" data-popup-date="2020-05-28" data-popup-doi="10.1101/2020.05.26.116111" data-popup-abstract="<p>Genome-wide sequencing of human populations has revealed substantial variation among genes in the intensity of purifying selection acting on damaging genetic variants. While genes under the strongest selective constraint are highly enriched for Mendelian disorders, most of these genes are not associated with disease and therefore the nature of the selection acting on them is not known. Here we show that genetic variants that damage these genes reduce reproductive success substantially in males but much less so in females. We present evidence that this reduction is mediated by cognitive and behavioural traits, which renders male carriers of such variants less likely to find mating partners. Our findings represent strong genetic evidence that Darwin9s theory of sexual selection is shaping the gene pool of contemporary human populations. Furthermore, our results suggest that sexual selection can account for about a quarter of all purifying selection acting on human genes.</p>">“Sex-biased reduction in reproductive success drives selective constraint on human genes”</a>, Gardner et al 2020; <a href="https://www.biorxiv.org/content/10.1101/2020.05.19.104455v1" data-popup-title="Genome-wide analysis identifies genetic effects on reproductive success and ongoing natural selection at the FADS locus" data-popup-author="Iain Mathieson, Felix R. Day, Nicola Barban, Felix C. Tropf, David M. Brazel, e<span class=&quot;smallcaps-auto&quot;>QTLG</span>en Consortium, <span class=&quot;smallcaps-auto&quot;>BIOS</span> Consortium, Ahmad Vaez, Natalie van Zuydam, Bárbara D. Bitarello, Harold Snieder, et al [...]" data-popup-date="2020-05-22" data-popup-doi="10.1101/2020.05.19.104455" data-popup-abstract="<p>Identifying genetic determinants of reproductive success may highlight mechanisms underlying fertility and also identify alleles under present-day selection. Using data in 785,604 individuals of European ancestry, we identify 43 genomic loci associated with either number of children ever born (<span class=&quot;smallcaps-auto&quot;>NEB</span>) or childlessness. These loci span diverse aspects of reproductive biology across the life course, including puberty timing, age at first birth, sex hormone regulation and age at menopause. Missense alleles in <span class=&quot;smallcaps-auto&quot;>ARHGAP</span>27 were associated with increased <span class=&quot;smallcaps-auto&quot;>NEB</span> but reduced reproductive lifespan, suggesting a trade-off between reproductive ageing and intensity. As <span class=&quot;smallcaps-auto&quot;>NEB</span> is one component of evolutionary fitness, our identified associations indicate loci under present-day natural selection. Accordingly, we find that <span class=&quot;smallcaps-auto&quot;>NEB</span>-increasing alleles have increased in frequency over the past two generations. Furthermore, integration with data from ancient selection scans identifies a unique example of an allele—<span class=&quot;smallcaps-auto&quot;>FADS</span>1/2 gene locus—that has been under selection for thousands of years and remains under selection today. Collectively, our findings demonstrate that diverse biological mechanisms contribute to reproductive success, implicating both neuro-endocrine and behavioural influences.</p>">“Genome-wide analysis identifies genetic effects on reproductive success and ongoing natural selection at the <em><span>FADS</span></em> locus”</a>, Mathieson et al 2020 (previously: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5695684/" data-popup-title="Genome-wide analysis identifies 12 loci influencing human reproductive behavior." data-popup-author="Barban, Nicola Jansen, Rick de Vlaming, Ronald Vaez, Ahmad Mandemakers, Jornt J Tropf, Felix C Shen, Xia Wilson, James F Chasman, Daniel I Nolte, Ilja M Tragante, Vinicius van der Laan, Sander W Perry, John R B Kong, Augustine Ahluwalia, Tarunveer S Albrec et al [...]" data-popup-date="2016" data-popup-doi="10.1038/ng.3698" data-popup-abstract="The genetic architecture of human reproductive behavior-age at first birth (<span class=&quot;smallcaps-auto&quot;>AFB</span>) and number of children ever born (<span class=&quot;smallcaps-auto&quot;>NEB</span>)-has a strong relationship with fitness, human development, infertility and risk of neuropsychiatric disorders. However, very few genetic loci have been identified, and the underlying mechanisms of <span class=&quot;smallcaps-auto&quot;>AFB</span> and <span class=&quot;smallcaps-auto&quot;>NEB</span> are poorly understood. We report a large genome-wide association study of both sexes including 251,151 individuals for <span class=&quot;smallcaps-auto&quot;>AFB</span> and 343,072 individuals for <span class=&quot;smallcaps-auto&quot;>NEB</span>. We identified 12 independent loci that are significantly associated with <span class=&quot;smallcaps-auto&quot;>AFB</span> and/or <span class=&quot;smallcaps-auto&quot;>NEB</span> in a <span class=&quot;smallcaps-auto&quot;>SNP</span>-based genome-wide association study and 4 additional loci associated in a gene-based effort. These loci harbor genes that are likely to have a role, either directly or by affecting non-local gene expression, in human reproduction and infertility, thereby increasing understanding of these complex traits.">Barban et al 2016</a>/<a href="https://www.biorxiv.org/content/early/2016/05/02/049163" data-popup-title="Mega-analysis of 31,396 individuals from 6 countries uncovers strong gene-environment interaction for human fertility" data-popup-author="Felix C. Tropf, Renske M. Verweij, Peter J. van der Most, Gert Stulp, Andrew Bakshi, Daniel A. Briley, Matthew Robinson, Anastasia Numan, Tõnu Esko, Andres Metspalu, Sarah E. Medland, Nicholas G. Martin, Harold Snieder, S. Hong Lee, Melinda C. Mills" data-popup-date="2016-05-02" data-popup-doi="10.1101/049163" data-popup-abstract="<p>Family and twin studies suggest that up to 50% of individual differences in human fertility within a population might be heritable. However, it remains unclear whether the genes associated with fertility outcomes such as number of children ever born (<span class=&quot;smallcaps-auto&quot;>NEB</span>) or age at first birth (<span class=&quot;smallcaps-auto&quot;>AFB</span>) are the same across geographical and historical environments. By not taking this into account, previous genetic studies implicitly assumed that the genetic effects are constant across time and space. We conduct a mega-analysis applying whole genome methods on 31,396 unrelated men and women from six Western countries. Across all individuals and environments, common single-nucleotide polymorphisms (<span class=&quot;smallcaps-auto&quot;>SNP</span>s) explained only ~4% of the variance in <span class=&quot;smallcaps-auto&quot;>NEB</span> and <span class=&quot;smallcaps-auto&quot;>AFB</span>. We then extend these models to test whether genetic effects are shared across different environments or unique to them. For individuals belonging to the same population and demographic cohort (born before or after the 20<sup>th</sup> century fertility decline), <span class=&quot;smallcaps-auto&quot;>SNP</span>-based heritability was almost five times higher at 22% for <span class=&quot;smallcaps-auto&quot;>NEB</span> and 19% for <span class=&quot;smallcaps-auto&quot;>AFB</span>. We also found no evidence suggesting that genetic effects on fertility are shared across time and space. Our findings imply that the environment strongly modifies genetic effects on the tempo and quantum of fertility, that currently ongoing natural selection is heterogeneous across environments, and that gene-environment interactions may partly account for missing heritability in fertility. Future research needs to combine efforts from genetic research and from the social sciences to better understand human fertility.</p><h3>Authors Summary</h3><p>Fertility behavior – such as age at first birth and number of children – varies strongly across historical time and geographical space. Yet, family and twin studies, which suggest that up to 50% of individual differences in fertility are heritable, implicitly assume that the genes important for fertility are the same across both time and space. Using molecular genetic data (<span class=&quot;smallcaps-auto&quot;>SNP</span>s) from over 30,000 unrelated individuals from six different countries, we show that different genes influence fertility in different time periods and different countries, and that the genetic effects consistently related to fertility are presumably small. The fact that genetic effects on fertility appear not to be universal could have tremendous implications for research in the area of reproductive medicine, social science and evolutionary biology alike.</p>">Tropf et al 2016</a>/<a href="https://www.nature.com/articles/ejhg2017105" data-popup-title="Sexual dimorphism in the genetic influence on human childlessness" data-popup-author="Renske M. Verweij, Melinda C. Mills, Felix C. Tropf, René Veenstra, Anastasia Nyman, Harold Snieder" data-popup-date="2017-07-05" data-popup-doi="10.1038/ejhg.2017.105" data-popup-abstract="<p>Previous research has found a genetic component of human reproduction and childlessness. Others have argued that the heritability of reproduction is counterintuitive due to a frequent misinterpretation that additive genetic variance in reproductive fitness should be close to zero. Yet it is plausible that different genetic loci operate in male and female fertility in the form of sexual dimorphism and that these genes are passed on to the next generation. This study examines the extent to which genetic factors influence childlessness and provides an empirical test of genetic sexual dimorphism. Data from the Swedish Twin Register (<em>N</em>=9942) is used to estimate a classical twin model, a genomic-relatedness-matrix restricted maximum likelihood (<span class=&quot;smallcaps-auto&quot;>GREML</span>) model on twins and estimates polygenic scores of age at first birth on childlessness. Results show that the variation in individual differences in childlessness is explained by genetic differences for 47% in the twin model and 59% for women and 56% for men using the <span class=&quot;smallcaps-auto&quot;>GREML</span> model. Using a polygenic score (<span class=&quot;smallcaps-auto&quot;>PGS</span>) of age at first birth (<span class=&quot;smallcaps-auto&quot;>AFB</span>), the odds of remaining childless are around 1.25 higher for individuals with 1 SD higher score on the <span class=&quot;smallcaps-auto&quot;>AFB</span> <span class=&quot;smallcaps-auto&quot;>PGS</span>, but only for women. We find that different sets of genes influence childlessness in men and in women. These findings provide insight into why people remain childless and give evidence of genetic sexual dimorphism.</p>">Verweij et al 2017</a>)</li>
<li><a href="https://www.biorxiv.org/content/10.1101/2020.05.07.083402v1" data-popup-title="Disentangling selection on genetically correlated polygenic traits using whole-genome genealogies" data-popup-author="Aaron J. Stern, Leo Speidel, Noah A. Zaitlen, Rasmus Nielsen" data-popup-date="2020-05-08" data-popup-doi="10.1101/2020.05.07.083402" data-popup-abstract="<p>We present a full-likelihood method to estimate and quantify polygenic adaptation from contemporary <span class=&quot;smallcaps-auto&quot;>DNA</span> sequence data. The method combines population genetic <span class=&quot;smallcaps-auto&quot;>DNA</span> sequence  data and <span class=&quot;smallcaps-auto&quot;>GWAS</span> summary statistics from up to thousands of nucleotide sites in a joint likelihood function to estimate the strength of transient directional selection acting on a polygenic trait. Through population genetic simulations of polygenic trait architectures and <span class=&quot;smallcaps-auto&quot;>GWAS</span>, we show that the method substantially improves power over current methods. We examine the robustness of the method under uncorrected <span class=&quot;smallcaps-auto&quot;>GWAS</span> stratification, uncertainty and ascertainment bias in the <span class=&quot;smallcaps-auto&quot;>GWAS</span> estimates of <span class=&quot;smallcaps-auto&quot;>SNP</span> effects, uncertainty in the identification of causal <span class=&quot;smallcaps-auto&quot;>SNP</span>s, allelic heterogeneity, negative selection, and low <span class=&quot;smallcaps-auto&quot;>GWAS</span> sample size. The method can quantify selection acting on correlated traits, fully controlling for pleiotropy even among traits with strong genetic correlation (|r_g|  = 80%; c.f. schizophrenia and bipolar disorder) while retaining high power to attribute selection to the causal trait. We apply the method to study 56 human polygenic traits for signs of recent adaptation. We find signals of directional selection on pigmentation (tanning, sunburn, hair, P=5.5e-15, 1.1e-11, 2.2e-6, respectively), life history traits (age at first birth, EduYears, P=2.5e-4, 2.6e-4, respectively), glycated hemoglobin (HbA1c, P=1.2e-3), bone mineral density (P=1.1e-3), and neuroticism (P=5.5e-3). We also conduct joint testing of 137 pairs of genetically correlated traits. We find evidence of widespread correlated response acting on these traits (2.6-fold enrichment over the null expectation, P=1.5e-7). We find that for several traits previously reported as adaptive, such as educational attainment and hair color, a significant proportion of the signal of selection on these traits can be attributed to correlated response, vs direct selection (P=2.9e-6, 1.7e-4, respectively). Lastly, our joint test uncovers antagonistic selection that has acted to increase type 2 diabetes (T2D) risk and decrease HbA1c (P=1.5e-5).</p>">“Disentangling selection on genetically correlated polygenic traits using whole-genome genealogies”</a>, Stern et al 2020</li>
</ul></li>
<li><p><span>Engineering</span>:</p>
<ul>
<li><a href="https://www.cambridge.org/core/journals/animal/article/review-recent-advances-in-bovine-in-vitro-embryo-production-reproductive-biotechnology-history-and-methods/4C4A7C008A6014ADBFDECCFED12FAE13/core-reader" data-popup-title="Recent advances in bovine <em>in vitro</em> embryo production: reproductive biotechnology history and methods" data-popup-author="L. B. Ferré, M. E. Kjelland, L. B. Strøbech, P. Hyttel, P. Mermillod, P. J. Ross" data-popup-date="2020-05" data-popup-doi="10.1017/S1751731119002775" data-popup-abstract="<p><em>In vitro</em> production (<strong><em><span class=&quot;smallcaps-auto&quot;>IVP</span></em></strong>) of embryos and associated technologies in cattle have shown significant progress in recent years, in part driven by a better understanding of the full potential of these tools by end users. The combination of <span class=&quot;smallcaps-auto&quot;>IVP</span> with sexed semen (<strong><em>SS</em></strong>) and genomic selection (<strong><em>GS</em></strong>) is being successfully and widely used in North America, South America and Europe. The main advantages offered by these technologies include a higher number of embryos and pregnancies per unit of time, and a wider range of potential female donors from which to retrieve oocytes (including open cyclic females and ones up to 3 months pregnant), including high index genomic calves, a reduced number of sperm required to produce embryos and increased chances of obtaining the desired sex of offspring. However, there are still unresolved aspects of <span class=&quot;smallcaps-auto&quot;>IVP</span> of embryos that limit a wider implementation of the technology, including potentially reduced fertility from the use of SS, reduced oocyte quality after <em>in vitro</em> oocyte maturation and lower embryo cryotolerance, resulting in reduced pregnancy rates compared to <em>in vivo</em>–produced embryos. Nevertheless, promising research results have been reported, and work is in progress to address current deficiencies. The combination of GS, <span class=&quot;smallcaps-auto&quot;>IVP</span> and SS has proven successful in the commercial field in several countries assisting practitioners and cattle producers to improve reproductive performance, efficiency and genetic gain.</p>">“Recent advances in bovine <em>in vitro</em> embryo production: reproductive biotechnology history and methods”</a>, Ferré et al 2020</li>
<li><a href="https://en.wikipedia.org/wiki/Atomic_gardening" data-popup-title="Atomic gardening" data-popup-author="English Wikipedia" data-popup-abstract="<p><b>Atomic gardening</b> is a form of mutation breeding where plants are exposed to radioactive sources, typically cobalt-60, in order to generate mutations, some of which turned out to be useful.</p>">Atomic garden</a> <a href="https://en.wikipedia.org/wiki/Mutation_breeding" data-popup-title="Mutation breeding" data-popup-author="English Wikipedia" data-popup-abstract="<p><b>Mutation breeding</b>, sometimes referred to as &quot;<b>variation breeding</b>&quot;, is the process of exposing seeds to chemicals or radiation in order to generate mutants with desirable traits to be bred with other cultivars. Plants created using mutagenesis are sometimes called mutagenic plants or mutagenic seeds. From 1930 to 2014 more than 3200 mutagenic plant varieties were released that have been derived either as direct mutants (70%) or from their progeny (30%). Crop plants account for 75% of released mutagenic species with the remaining 25% ornamentals or decorative plants. However, although the FAO/IAEA reported in 2014 that over 1,000 mutant varieties of major staple crops were being grown worldwide, it is unclear how many of these varieties are currently used in agriculture or horticulture around the world, as these seeds are not always identified or labeled as having a mutagenic provenance.</p>">mutation breeding</a></li>
</ul></li>
</ul>
<p>Statistics/meta-science/mathematics:</p>
<ul>
<li><a href="https://www.gwern.net/docs/statistics/bias/2020-botviniknezer.pdf" data-popup-title="Variability in the analysis of a single neuroimaging dataset by many teams" data-popup-author="Rotem Botvinik-Nezer, Felix Holzmeister, Colin F. Camerer, Anna Dreber, Juergen Huber, Magnus Johannesson, Michael Kirchler, Roni Iwanir, Jeanette A. Mumford, R. Alison Adcock, Paolo Avesani, Blazej M. Baczkowski, Aahana Bajracharya, Leah Bakst, Sheryl Bal et al [...]" data-popup-date="2020-05-20" data-popup-doi="10.1038/s41586-020-2314-9" data-popup-abstract="<p>Data analysis workflows in many scientific domains have become increasingly complex and flexible. Here we assess the effect of this flexibility on the results of functional magnetic resonance imaging by asking 70 independent teams to analyse the same dataset, testing the same 9 ex-ante hypotheses<sup>1</sup>. The flexibility of analytical approaches is exemplified by the fact that no two teams chose identical workflows to analyse the data. This flexibility resulted in sizeable variation in the results of hypothesis tests, even for teams whose statistical maps were highly correlated at intermediate stages of the analysis pipeline. Variation in reported results was related to several aspects of analysis methodology. Notably, a meta-analytical approach that aggregated information across teams yielded a significant consensus in activated regions. Furthermore, prediction markets of researchers in the field revealed an overestimation of the likelihood of significant findings, even by researchers with direct knowledge of the dataset<sup>2,3,4,5</sup>. Our findings show that analytical flexibility can have substantial effects on scientific conclusions, and identify factors that may be related to variability in the analysis of functional magnetic resonance imaging. The results emphasize the importance of validating and sharing complex analysis workflows, and demonstrate the need for performing and reporting multiple analyses of the same data. Potential approaches that could be used to mitigate issues related to analytical variability are discussed.</p>">“Variability in the analysis of a single neuroimaging dataset by many teams”</a>, Botvinik-Nezer et al 2020</li>
<li><a href="http://raganwald.com/2020/05/03/fractran.html" data-popup-title="Remembering John Conway's FRACTRAN, a ridiculous, yet surprisingly deep language" data-popup-author="Reginald Braithwaite" data-popup-date="2020-05-03" data-popup-abstract="<p>[Memorial for beloved mathematician John Horton Conway, who died in 2020 of coronavirus.</p><p>One of his many playful creations was the esoteric programming language <a href=&quot;https://en.wikipedia.org/wiki/FRACTRAN&quot;><span class=&quot;smallcaps-auto&quot;>FRACTRAN</span></a>: a Turing-complete language implemented as simply multiplying numbers against a list repeatedly. How can this implement even the Fibonacci function, much less all computable functions, how could one come up with said implementation, and why would Conway think of this in the first place?</p><p>Braithwaite explains <span class=&quot;smallcaps-auto&quot;>FRACTRAN</span> and traces its evolution from <a href=&quot;https://en.wikipedia.org/wiki/Register_machine&quot;>Minsky machines</a>: by starting with a fairly understandable model of computation and repeatedly simplifying it to an equivalent computer, one winds up with <span class=&quot;smallcaps-auto&quot;>FRACTRAN</span>, and <span class=&quot;smallcaps-auto&quot;>FRACTRAN</span> turns out to take the same form as the famous unsolved Collatz conjecture—and since each step is Turing-complete (they are all equivalent), that means questions about functions like the Collatz conjecture involving repeated multiplication are <em>undecidable</em> (because we have shown they are all equivalent to full-blown computer programs), and so the Collatz conjecture itself may be undecidable! And that was the serious goal of the whimsical <a href=&quot;https://www.gwern.net/docs/cs/1987-conway.pdf&quot; title=&quot;FRACTRAN: A Simple Universal Programming Language for Arithmetic&quot;>Conway 1987</a>.]</p>">“Remembering John Conway’s <span>FRACTRAN</span>, a ridiculous, yet surprisingly deep language”</a>, Reginald Braithwaite (how does the recently-deceased <a href="https://en.wikipedia.org/wiki/John_Horton_Conway" data-popup-title="John Horton Conway" data-popup-author="English Wikipedia" data-popup-abstract="<p><b>John Horton Conway</b> was an English mathematician active in the theory of finite groups, knot theory, number theory, combinatorial game theory and coding theory. He also made contributions to many branches of recreational mathematics, most notably the invention of the cellular automaton called the Game of Life.</p>">John Conway</a>’s <a href="https://www.gwern.net/docs/cs/1987-conway.pdf" data-popup-title="FRACTRAN: A Simple Universal Programming Language for Arithmetic" data-popup-author="John H. Conway" data-popup-date="1987-01-01" data-popup-doi="10.1007/978-1-4612-4808-8_2" data-popup-abstract="<p>To play the fraction game corresponding to a given list</p><p><em>f</em><sub>1</sub>, <em>f</em><sub>2</sub>, …, <em>f</em><sub>k</sub></p><p>of fractions and starting integer <em>N</em>, you repeatedly multiply the integer you have at any stage (initially <em>N</em>) by the earliest <em>f</em><sub>i</sub> in the list for which the answer is integral. Whenever there is no such <em>f</em><sub>i</sub>, the game <em>stops</em>.</p>">1980</a> <a href="https://en.wikipedia.org/wiki/FRACTRAN" data-popup-title="FRACTRAN" data-popup-author="English Wikipedia" data-popup-abstract="<p><b><span class=&quot;smallcaps-auto&quot;>FRACTRAN</span></b> is a Turing-complete esoteric programming language invented by the mathematician John Conway. A <span class=&quot;smallcaps-auto&quot;>FRACTRAN</span> program is an ordered list of positive fractions together with an initial positive integer input <i>n</i>. The program is run by updating the integer <i>n</i> as follows:</p><ol><li>for the first fraction <i>f</i> in the list for which <i>nf</i> is an integer, replace <i>n</i> by <i>nf</i></li>
<li>repeat this rule until no fraction in the list produces an integer when multiplied by <i>n</i>, then halt.</li></ol>">esolang</a> lead to the <a href="https://en.wikipedia.org/wiki/Collatz_conjecture" data-popup-title="Collatz conjecture" data-popup-author="English Wikipedia" data-popup-abstract="<p>The <b>Collatz conjecture</b> is a conjecture in mathematics that concerns a sequence defined as follows: start with any positive integer <span class=&quot;texhtml mvar&quot; style=&quot;font-style:italic&quot;>n</span>. Then each term is obtained from the previous term as follows: if the previous term is even, the next term is one half of the previous term. If the previous term is odd, the next term is 3 times the previous term plus 1. The conjecture is that no matter what value of <span class=&quot;texhtml mvar&quot; style=&quot;font-style:italic&quot;>n</span>, the sequence will always reach 1.</p>">Collatz conjecture</a>?)</li>
<li><a href="https://www.gwern.net/docs/science/1995-matthews.pdf" data-popup-title="Tumbling toast, Murphy's Law and the fundamental constants" data-popup-author="Robert A. J. Matthews" data-popup-date="1995-07-18" data-popup-doi="10.1088/0143-0807/16/4/005" data-popup-abstract="<p>We investigate the dynamics of toast tumbling from a table to the floor. Popular opinion is that the final state is usually butter-side down, and constitutes <em>prima facie</em> evidence of Murphy’s Law (‘If it can go wrong, it will’). The orthodox view, in contrast, is that the phenomenon is essentially random, with a 50/50 split of possible outcomes. We show that toast does indeed have an inherent tendency to land butter-side down for a wide range of conditions. Furthermore, we show that this outcome is ultimately ascribable to the values of the fundamental constants. As such, this manifestation of Murphy’s Law appears to be an ineluctable feature of our universe.</p>">“Tumbling toast, Murphy’s Law and the fundamental constants”</a>, Matthews 1995 (<a href="https://www.gwern.net/docs/statistics/bias/1997-matthews.pdf" data-popup-title="The Science of Murphy’s Law: Life’s little annoyances are not as random as they seem: the awful truth is that the universe is against you" data-popup-author="Robert A. J. Matthews" data-popup-date="1997-04-01" data-popup-doi="10.2307/24993707" data-popup-abstract="<p>[Popularization of Matthews’s other articles on physics &amp;amp; statistics and what truth there is to <a href=&quot;https://en.wikipedia.org/wiki/Murphy%27s_law&quot;>“Murphy’s law”</a>:</p><ul><li><p><em>Toast</em> falling butter side up: true, because tables are not high enough for toast to be likely to complete one or more rotations before landing given the tilt &amp;amp; falling off edges, therefore toast will in fact tend to land on its top half.</p></li><li><p><em>Maps</em> putting things on edges: true—printed paper maps tend to be hard to use because the place one wants to go will tend to be toward an edge; this is simply a geometric fact due to most of the area of a volume being towards the edge.</p></li><li><p><em>Other Checkout Lines Being Faster</em>: true, because of anthropics, as most waiting time is spent in the slowest line; even if equally loaded, order statistics points out there is only a 1 in <em>n</em> chance that one picked the fastest line out of <em>n</em> lanes</p></li><li><p><em>Mismatched Socks</em>: also true, simply because there are many more ways for socks in a pair to go missing than to go missing in pairs or match up</p></li><li><p><em>Raining</em>: forecasts are fairly accurate, but this ignores base-rates and that much of that accuracy is due to predicting <em>non</em>-rain. It’s a version of the diagnostics/screening paradox:</p><blockquote><p>For example, suppose that the hourly base rate of rain is 0.1, meaning that it is 10 times more likely not to rain during your hour-long stroll. Probability theory then shows that even an 80% accurate forecast of rain is twice as likely to prove wrong as right during your walk—and you’ll end up taking an umbrella unnecessarily. The fact is that even today’s apparently highly accurate forecasts are still not good enough to predict rare events reliably.</p></blockquote></li></ul><p>Keywords: socks, maps, umbrellas, family law, combinatorics, weather forecasting, technology law, mathematical constants, physics, probability theory]</p>">overview</a>; <a href="https://en.wikipedia.org/wiki/Anthropic_principle" data-popup-title="Anthropic principle" data-popup-author="English Wikipedia" data-popup-abstract="<p>The <b>anthropic principle</b> is a philosophical consideration that observations of the universe must be compatible with the conscious and sapient life that observes it. Some proponents of the anthropic principle reason that it explains why this universe has the age and the fundamental physical constants necessary to accommodate conscious life. As a result, they believe it is unremarkable that this universe has fundamental constants that happen to fall within the narrow range thought to be compatible with life.
The <b>strong anthropic principle</b> (<b><span class=&quot;smallcaps-auto&quot;>SAP</span></b>), as explained by John D. Barrow and Frank Tipler, states that this is all the case because the universe is in some sense compelled to eventually have conscious and sapient life emerge within it. Some critics of the <span class=&quot;smallcaps-auto&quot;>SAP</span> argue in favor of a <b>weak anthropic principle</b> (<b><span class=&quot;smallcaps-auto&quot;>WAP</span></b>) similar to the one defined by Brandon Carter, which states that the universe's ostensible fine tuning is the result of selection bias : i.e., only in a universe capable of eventually supporting life will there be living beings capable of observing and reflecting on the matter. Most often such arguments draw upon some notion of the multiverse for there to be a statistical population of universes to select from and from which selection bias could occur.</p>">anthropics</a> size argument from <a href="https://www.gwern.net/docs/science/1980-press.pdf" data-popup-title="Man’s size in terms of fundamental constants" data-popup-author="William H. Press" data-popup-date="1980-01-01" data-popup-doi="10.1119/1.12326" data-popup-abstract="<p>Why are we the size we are, instead of some very different size? Simple physical scaling laws and three “requirements” dictate that our size be of order (h/<sup>2</sup>/<i>m</i><sub><i>e</i></sub><i>e</i><sup>2</sup>)(<i>e</i><sup>2</sup>/<i>G</i><i>m</i><sub><i>p</i></sub> <sup>2</sup>)<sup>1/4</sup>. They also “predict” the mass and radius of the Earth. The three requirements are: (i) We are made of complicated molecules; (ii) we breathe an evolved planetary atmosphere; (iii) we are about as big as we can be without breaking.</p>">Press 1980</a>; see also <a href="https://www.gwern.net/docs/science/2001-bacon.pdf" data-popup-title="A closer look at tumbling toast" data-popup-author="M. E. Bacon, George Heald, Matt James" data-popup-date="2001-01-01" data-popup-doi="10.1119/1.1289213" data-popup-abstract="<p>The study of the mechanics of tumbling toast provides an informative and entertaining project for undergraduates. The relatively recent introduction of software packages to facilitate the analysis of video recordings, and the numerical solution of complex differential equations, makes such a study an attractive candidate for inclusion in an experimental physics course at the undergraduate level. In the study reported here it is found that the experimentally determined free fall angular velocity of a board, tumbling off the edge of a table, can only be predicted at all accurately if slipping is taken into account. The size and shape of the board used in the calculations and in the experiments were roughly the same as that of a piece of toast. In addition, it is found that the board, tumbling from a standard table of height 76 cm, will land butter-side down (neglecting any bounce) for two ranges of overhang (δ<sub>0</sub>). δ<sub>0</sub> is defined as the initial distance from the table edge to a vertical line drawn through the center of mass when the board is horizontal. For our board (length 10.2 cm) the approximate ranges of overhang are 0–0.8 and 2.7–5.1 cm. The importance of the 0–0.8 cm (only 2% of all possible overhangs for which tumbling is possible) favoring a butter-side down landing should not be underestimated when pondering the widely held belief that toast, tumbling from a table, usually falls butter-side down.</p>">Bacon et al 2001</a>/<a href="https://www.gwern.net/docs/science/2012-borghi.pdf" data-popup-title="On the tumbling toast problem" data-popup-author="Riccardo Borghi" data-popup-date="2012-08-01" data-popup-doi="10.1088/0143-0807/33/5/1407" data-popup-abstract="<p>A didactical revisitation of the so-called tumbling toast problem is presented here. The numerical solution of the related Newton’s equations has been found in the space domain, without resorting to the complete time-based law of motion, with a considerable reduction of the mathematical complexity of the problem. This could allow the effect of the different physical mechanisms ruling the overall dynamics to be appreciated in a more transparent way, even by undergraduates. Moreover, the availability from the literature of experimental investigations carried out on tumbling toast allows us to propose different theoretical models of growing complexity in order to show the corresponding improvement of the agreement between theory and observation.</p>">Borghi 2012</a>)</li>
</ul>
<p>Politics/religion:</p>
<ul>
<li><a href="https://www.goodreads.com/review/show/2671118186" data-popup-title="Review of 'The Cultural Revolution: A People's History, 1962-1976' by Frank Dikötter" data-popup-author="Gwern Branwen" data-popup-date="2019-01-14" data-popup-abstract="Dikötter's history of the Cultural Revolution offers a broad overview of the multiple failures and follies of Maoism, which culminated in some of the most destructive and disastrous events in human history: the Cultural Revolution, the Great Leap Forward, and the Third Front. The Cultural Revolution was not prompted by any extraordinary famine, or invasion or genuine threat of invasion, or civil war, or disaster of any kind. How then could it have happened? The Cultural Revolution was sponsored by Mao as a way to purge the middle and upper ranks of the Communist Party of doubters, who might do to him what the Soviets had just done to Stalin: reveal his monstrous crimes to the world. But Mao didn't realize the forces he unleashed. Maoism had benefited from taking credit for post-<span class=&quot;smallcaps-auto&quot;>WWII</span> recovery and the defeat of Japan, but the more its policies were implemented and it tightened its grip, the greater the gap between its utopian promises and the grim impoverished Chinese reality became. Because its theories were radically and systematically wrong, any honest attempt to implement them was doomed to fail, and anyone pragmatic would necessarily betray the system. Old systems and 'inequities' reasserted themselves, to the frustration of true believers. The only ideologically-permissible explanations were excuses like saboteurs and spies and corrupt officials. Usually kept in check, when given Mao's imprimatur and active egging on, mass social resentment and ideological frustration boiled over, leading to a frenzy of virtue-signaling, denunciations, preference falsification spirals, murders, cannibalism, and eventually outright civil war and pandemic. Finally, Mao decided enough purging had happened and his position was secure, and brought it to an end. As strange and awful as it was, the Cultural Revolution offers food for thought on how politics can go viciously wrong, and dangerous aspects of human psychology.">Review of <em>The Cultural Revolution</em></a></li>
<li><a href="https://www.gwern.net/docs/sociology/2019-sommers.pdf" data-popup-title="The Voluntariness of Voluntary Consent: Consent Searches and the Psychology of Compliance" data-popup-author="Roseanna Sommers, Vanessa K. Bohns" data-popup-date="2019-05-01" data-popup-abstract="<p>Consent-based searches are by far the most ubiquitous form of search undertaken by police. A key legal inquiry in these cases is whether consent was granted voluntarily. This Essay suggests that fact finders’ assessments of voluntariness are likely to be impaired by a systematic bias in social perception. Fact finders are likely to underappreciate the degree to which suspects feel pressure to comply with police officers’ requests to perform searches.</p><p>In 2 preregistered laboratory studies, we approached a total of 209 participants (“Experiencers”) with a highly intrusive request: to unlock their password-protected smartphones and hand them over to an experimenter to search through while they waited in another room. A separate 194 participants (“Forecasters”) were brought into the lab and asked whether a reasonable person would agree to the same request if hypothetically approached by the same researcher. Both groups then reported how free they felt, or would feel, to refuse the request.</p><ol type=&quot;1&quot;><li>Study 1 found that whereas most Forecasters believed a reasonable person would refuse the experimenter’s request, most Experiencers—100 out of 103 people—promptly unlocked their phones and handed them over. Moreover, Experiencers reported feeling significantly less free to refuse than did Forecasters contemplating the same situation hypothetically.</li><li>Study 2 tested an intervention modeled after a commonly proposed reform of consent searches, in which the experimenter explicitly advises participants that they have the right to withhold consent. We found that this advisory did not significantly reduce compliance rates or make Experiencers feel more free to say no. At the same time, the gap between Experiencers and Forecasters remained significant.</li></ol><p>These findings suggest that decision makers judging the voluntariness of consent consistently underestimate the pressure to comply with intrusive requests. This is problematic because it indicates that a key justification for suspicionless consent searches—that they are voluntary—relies on an assessment that is subject to bias. The results thus provide support to critics who would like to see consent searches banned or curtailed, as they have been in several states.</p><p>The results also suggest that a popular reform proposal—requiring police to advise citizens of their right to refuse consent—may have little effect. This corroborates previous observational studies that find negligible effects of <em>Miranda</em> warnings on confession rates among interrogees, and little change in rates of consent once police start notifying motorists of their right to refuse vehicle searches. We suggest that these warnings are ineffective because they fail to address the psychology of compliance. The reason people comply with police, we contend, is <em>social</em>, not informational. The social demands of police-citizen interactions persist even when people are informed of their rights. It is time to abandon the myth that notifying people of their rights makes them feel empowered to exercise those rights.</p>">“The Voluntariness of Voluntary Consent: Consent Searches and the Psychology of Compliance”</a>, Sommers &amp; Bohns 2019 (people are bad at predicting resistance to police requests; see also <a href="https://www.gwern.net/docs/technology/2012-christin.pdf" data-popup-title="It’s All about the Benjamins: An Empirical Study on Incentivizing Users to Ignore Security Advice" data-popup-author="Nicolas Christin, Serge Egelman, Timothy Vidas, Jens Grossklags" data-popup-date="2012-01-01" data-popup-doi="10.1007/978-3-642-27576-0_2" data-popup-abstract="<p>We examine the cost for an attacker to pay users to execute arbitrary code—potentially malware. We asked users at home to download and run an executable we wrote without being told what it did and without any way of knowing it was harmless. Each week, we increased the payment amount. Our goal was to examine whether users would ignore common security advice—not to run untrusted executables—if there was a direct incentive, and how much this incentive would need to be. We observed that for payments as low as $0.01, 22% of the people who viewed the task ultimately ran our executable. Once increased to $1.00, this proportion increased to 43%. We show that as the price increased, more and more users who understood the risks ultimately ran the code. We conclude that users are generally unopposed to running programs of unknown provenance, so long as their incentives exceed their inconvenience.</p>">Christin et al 2012</a>)</li>
<li><a href="https://en.wikipedia.org/wiki/Operation_INFEKTION" data-popup-title="Operation INFEKTION" data-popup-author="English Wikipedia" data-popup-abstract="<p><b>Operation <span class=&quot;smallcaps-auto&quot;>INFEKTION</span></b> was the popular name given to a disinformation campaign run by the <span class=&quot;smallcaps-auto&quot;>KGB</span> in the 1980s to plant the idea that the United States had invented <span class=&quot;smallcaps-auto&quot;>HIV</span>/<span class=&quot;smallcaps-auto&quot;>AIDS</span> as part of a biological weapons research project at Fort Detrick, Maryland. Historian Thomas Boghardt popularized the codename &quot;<span class=&quot;smallcaps-auto&quot;>INFEKTION</span>&quot; based on the claims of former East German Ministry of State Security (Stasi) officer Günther Bohnsack, who claimed that the Stasi codename for the campaign was either &quot;<span class=&quot;smallcaps-auto&quot;>INFEKTION</span>&quot; or perhaps also &quot;<span class=&quot;smallcaps-auto&quot;>VORW</span>Ä<span class=&quot;smallcaps-auto&quot;>RTS</span> II&quot;. However, historians Christopher Nehring and Douglas Selvage found in the former Stasi and Bulgarian State Security archives materials that prove the actual Stasi codename for the <span class=&quot;smallcaps-auto&quot;>AIDS</span> disinformation campaign was Operation &quot;<span class=&quot;smallcaps-auto&quot;>DENVER</span>&quot;.</p>">Operation <span>INFEKTION</span></a> (see also <a href="https://www.gwern.net/docs/iq/1997-gordon.pdf#page=47" data-popup-title="Everyday Life as an Intelligence Test: Effects of Intelligence and Intelligence Context: Conspiracy Rumors in Everyday Life" data-popup-author="Robert A. Gordon" data-popup-date="1997-01-01" data-popup-doi="10.1016/S0160-2896(97)90017-9" data-popup-abstract="<p>This major section extends the IQ model into the domain of public opinion concerning what is to be accepted as factual. The domain of factual opinion often lacks the constraints imposed on performance outcomes by real consequences, and it adds the element of irreducible uncertainty that characterizes events not personally witnessed by all. Such considerations can work for or against success of the model. Uncertainty calls forth judgment, but loosening of practical constraints can add unpredictability to outcomes. Surveys concerning belief in conspiracy rumors and key beliefs about the O.J. Simpson trial provide the main data.</p><p><strong>Conspiracy Rumors in Everyday Life</strong></p><p>Although conspiracy themes among Whites have long been studied (Cohn, 1966; Harrington, 1996; <a href=&quot;https://en.wikipedia.org/wiki/The_Paranoid_Style_in_American_Politics&quot;>Hofstadter, 1965</a>), the many rumors centered around conspiracies, some quite elaborate, that have been afloat in the African-American community at one time or another have just recently drawn scholarly attention (Turner, 1993). Often, those rumors concern consumer products manufactured by White corporations and their imagined sponsorship by the Ku Klux Klan (e.g., Kool cigarettes because of the K) or supposed adulteration of fast foods aimed at sterilizing Black males. Others target the government. Black celebrities, authority figures, academics, and media outlets sometimes lend credence to the rumors, and peers sometimes punish better informed individuals for defying the messages.</p><p>…Black organ donations are few because of distrust of the medical system and belief in myths (“Why More,” 1995), fostered, for example, by <a href=&quot;https://en.wikipedia.org/wiki/Nation_of_Islam&quot;>Nation of Islam</a> leader <a href=&quot;https://en.wikipedia.org/wiki/Louis_Farrakhan&quot;>Louis Farrakhan</a>…“Public-health workers say that the discussion of <a href=&quot;https://en.wikipedia.org/wiki/Operation_INFEKTION&quot;><span class=&quot;smallcaps-auto&quot;>AIDS</span> as a plot against blacks</a> has eroded the credibility of <span class=&quot;smallcaps-auto&quot;>AIDS</span> prevention campaigns and has made some blacks suspicious of <span class=&quot;smallcaps-auto&quot;>AIDS</span> tests and treatments”…The director of a Black studies program stated that they diverted attention “from the resolution of pressing social problems” (Myers, 1990, p.&nbsp;A19). Andrew Cooper, publisher of a Black newspaper in Brooklyn, warned that the rumors were dysfunctional for all, observing, “It is a danger to America to have a large group of its citizens believe that its government is in a conspiracy to eliminate them”…Rumor experts offer no account for why, if a rumor is believed by a certain percentage of Blacks, it will also be believed by a certain, but smaller, percentage of Whites….What did account for the more than 40-point race split in public opinion over <a href=&quot;https://en.wikipedia.org/wiki/O._J._Simpson_murder_case&quot;>O.J. Simpson’s guilt</a>?…the <a href=&quot;https://en.wikipedia.org/wiki/Tawana_Brawley_rape_allegations&quot;>Brawley case</a> dominated the news…Finally, a 7-month grand jury investigation concluded that the entire affair was a hoax, originally contrived by Brawley and her mother to cover up the girl’s 4-day absence from home.</p>">Gordon 1997</a>)</li>
<li><a href="https://progressstudies.school/" data-popup-title="Progress Studies for Aspiring Young Scholars: An online summer program in the history of technology for high school students" data-popup-author="Progress Studies for Young Scholars" data-popup-date="2020-05" data-popup-abstract="<p>[2020] Progress Studies for Young Scholars is an online program of guided self-study in the history of industrial civilization for high school students.</p><p>This program will explore: what problems, challenges and hardships in life and work were faced by people in earlier generations and centuries? And how did we solve those problems through science, technology, and invention?</p><p>Learn about manufacturing from blacksmiths to assembly lines; about power from water wheels to combustion to electricity; about food from famine to industrial agriculture and genetically modified crops; about disease from basic sanitation to scientific medicine—and the struggles and circumstances of the men and women who worked to bend the arc of humanity upward.</p><p>Your learning will be supported by instructors who will help you develop your reasoning and research skills. You’ll also have the chance to engage ideas with a community of like-minded peers…A six-week course with daily reading, audio or video content. Go through it on your own, or join a study group with an instructor for daily online discussions and Q&amp;amp;A.</p><p>Speaker series: Danica Remy · Deirdre Nansen McCloskey · Adam Mossoff · Anton Howes · Joel Mokyr · Laura Mazer · Manjari Narayan · Matt Bateman · Max Roser · Sarah Constantin · Tyler Cowen · Jason Crawford · Jerry Neumann · Michael Dearing · Michael Strong · Noor Siddiqui · Patrick Collison · Samo Burja</p>">“Progress Studies for Aspiring Young Scholars”</a> (experimental online summer class for high school students by <a href="https://jasoncrawford.org/">Jason Crawford</a> on development)</li>
</ul>
<p>Psychology/biology:</p>
<ul>
<li><a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5711578/" data-popup-title="Understanding Immunity through the Lens of Disease Ecology." data-popup-author="Hedrick, Stephen M" data-popup-date="2017" data-popup-doi="10.1016/j.it.2017.08.001" data-popup-abstract="As we describe the immune system in ever more exquisite detail, we might find that no matter how successful, this approach will not be sufficient to understand the spread of infectious agents, their susceptibility to vaccine therapy, and human disease resistance. Compared with the strict reductionism practiced as a means of characterizing most biological processes, I propose that the progression and outcome of disease-causing host-parasite interactions will be more clearly understood through a focus on disease ecology.">“Understanding immunity through the lens of disease ecology”</a>, Hedrick 2017<a href="#sn10" id="fnref10" role="doc-noteref"><sup>10</sup></a> (“…for the past few thousand years, we human beings have been the most diseased species on earth”; followup to <a href="https://www.cell.com/immunity/fulltext/S1074-7613(04)00307-3" data-popup-title="The Acquired Immune System: A Vantage from Beneath" data-popup-author="Stephen M. Hedrick" data-popup-date="2004-11-01" data-popup-doi="10.1016/j.immuni.2004.08.020" data-popup-abstract="<p>The immunity exhibited by plants and animals is often viewed as the evolutionary response to the problem of infectious agents. In this respect, the combination of the innate immune system and the acquired immune system has been characterized as the “optimal solution.” In this essay, I propose that there is no possibility of an optimal solution to the problem of parasitism. Regardless of the immunological mechanisms evolved, infectious agents establish a dynamic interaction with common strains of their host species, weighing virulence against transmissibility. In the endless host-parasite coevolution, the immune system can never gain an upper hand on the millions of parasitic microbes and viruses. Rather, evolution of the immune system is driven, most importantly, by the small advantages conferred as a result of host variation. By selecting for ever-more-devious parasites, the immune system is the cause of its own necessity.</p>">Hedrick 2004</a>)</li>
<li><a href="https://rootsofprogress.org/draining-the-swamp" data-popup-title="Draining the swamp: How sanitation conquered disease long before vaccines or antibiotics" data-popup-author="Jason Crawford" data-popup-date="2020-01-28" data-popup-abstract="[Public health history review: as famous as vaccines and antibiotics were, deaths from infectious diseases had been declining for centuries before hand, and vaccines/antibiotics merely helped continue the decline without representing a major trend break. Such trends date back to long before the vindication of germ theory, as incorrect theories like miasmas nevertheless led to effective sanitation and cleanliness interventions which <em>did</em> reduce disease: &quot;the mortality data points to a large and easy-to-underappreciate role of pest control, water sanitation, food handling, and general hygiene.&quot;]">“How sanitation conquered disease long before vaccines or antibiotics”</a>, Jason Crawford</li>
<li><a href="https://www.gwern.net/docs/iq/1997-gordon.pdf" data-popup-title="Everyday Life as an Intelligence Test: Effects of Intelligence and Intelligence Context" data-popup-author="Robert A. Gordon" data-popup-date="1997-01-01" data-popup-doi="10.1016/S0160-2896(97)90017-9" data-popup-abstract="<p>To show why the importance of intelligence is often misperceived, an analogy between single test items and single nontest actions in everyday life is drawn. 3 requirements of good test items are restated, and the analogy is employed to account for underrecognition of the importance of general intelligence in everyday actions, which often fail to meet the requirements and thus fail as intelligence measures for reasons that have little to do with their dependence on intelligence. A new perspective on the role of intelligence in nontest actions is introduced by considering its operation at 3 levels: that of the individual, that of the near context of the individual, and that of entire populations. Social scientists have misunderstood the operation and impact of IQ in populations by confining attention to the individual level. A population-IQ-outcome model is explained that tests for the pooled effects of intelligence at all 3 levels on differences between 2 populations in prevalences of certain outcomes. When the model fits, the difference between 2 populations in the outcome measured is found commensurate with the difference in their IQ or general intelligence distributions. The model is tested on and found to fit prevalences of juvenile delinquency, adult crime, single parenthood, <span class=&quot;smallcaps-auto&quot;>HIV</span> infection, poverty, belief in conspiracy rumors, and key opinions from polls about the O.J. Simpson trial and the earlier Tawana Brawley case. A deviance principle is extracted from empirical findings to indicate kinds of outcome the model will not fit. Implications for theories of practical and multiple intelligences are discussed. To understand the full policy implications of intelligence, such a fundamentally new perspective as that presented here will be needed.</p>">“Everyday Life as an Intelligence Test: Effects of Intelligence and Intelligence Context”</a>, Gordon 1997</li>
<li><a href="https://www.gwern.net/docs/psychology/2020-danese.pdf" data-popup-title="Objective and subjective experiences of child maltreatment and their relationships with psychopathology" data-popup-author="Andrea Danese, Cathy Spatz Widom" data-popup-date="2020-05-18" data-popup-doi="10.1038/s41562-020-0880-3" data-popup-abstract="<p>Does psychopathology develop as a function of the objective or subjective experience of childhood maltreatment? To address this question, we studied a unique cohort of 1,196 children with both objective, court-documented evidence of maltreatment and subjective reports of their childhood maltreatment histories made once they reached adulthood, along with extensive psychiatric assessment. We found that, even for severe cases of childhood maltreatment identified through court records, risk of psychopathology linked to objective measures was minimal in the absence of subjective reports. In contrast, risk of psychopathology linked to subjective reports of childhood maltreatment was high, whether or not the reports were consistent with objective measures. These findings have important implications for how we study the mechanisms through which child maltreatment affects mental health and how we prevent or treat maltreatment-related psychopathology. Interventions for psychopathology associated with childhood maltreatment can benefit from deeper understanding of the subjective experience.</p>">“Objective and subjective experiences of child maltreatment and their relationships with psychopathology”</a>, Danese &amp; Widom 2020 (nothing in psychology makes sense except in the light of individual-differences)</li>
<li><a href="https://www.gwern.net/docs/biology/2015-beekman.pdf" data-popup-title="Brainless but Multi-Headed: Decision Making by the Acellular Slime Mould <em>Physarum polycephalum</em>" data-popup-author="Madeleine Beekman, Tanya Latty" data-popup-date="2015-11-20" data-popup-doi="10.1016/j.jmb.2015.07.007" data-popup-abstract="<ul><li>Can you make decisions if you are brainless?</li><li>Here we use the acellular slime mould <a href=&quot;https://en.wikipedia.org/wiki/Physarum_polycephalum&quot;><em>P. polycephalum</em></a> to study decision making.</li><li>We use foraging and network construction as experimental paradigms.</li><li>Our work reveals the underlying basic mechanisms that organisms use to make decisions.</li><li>We think that the slime mould can be developed further to function as a “model brain”.</li></ul><p>Because of its peculiar biology and the ease with which it can be cultured, the acellular slime mould <a href=&quot;https://en.wikipedia.org/wiki/Physarum_polycephalum&quot;><em>Physarum polycephalum</em></a> has long been a model organism in a range of disciplines. Due to its macroscopic, syncytial nature, it is no surprise that it has been a favourite amongst cell biologists. Its inclusion in the experimental tool kit of behavioural ecologists is much more recent. These recent studies have certainly paid off. They have shown that, for an organism that lacks a brain or central nervous system, <em>P. polycephalum</em> shows rather complex behaviour. For example, it is capable of finding the shortest path through a maze, it can construct networks as efficient as those designed by humans, it can solve computationally difficult puzzles, it makes multi-objective foraging decisions, it balances its nutrient intake and it even behaves irrationally. Are the slime mould’s achievements simply “cute”, worthy of mentioning in passing but nothing to take too seriously? Or do they hint at the fundamental processes underlying all decision making? We will address this question after reviewing the decision-making abilities of the slime mould. [Keywords: acellular slime mould, decision-making, foraging decisions, optimal foraging, trade-offs]</p>">“Brainless but Multi-Headed: Decision Making by the Acellular Slime Mould <em>Physarum polycephalum</em>”</a>, Beekman &amp; Latty 2015</li>
<li><a href="https://www.gwern.net/docs/psychology/2019-curran.pdf" data-popup-title="I'm paid biweekly, just not by leprechauns: Evaluating valid-but-incorrect response rates to attention check items" data-popup-author="Paul G. Curran, Kelsey A. Hauser" data-popup-date="2019-10-01" data-popup-doi="10.1016/j.jrp.2019.103849" data-popup-abstract="<ul><li>Carelessness in self-report data can be detected with many methods.</li><li>Embedding items in a scale with presumed ‘correct’ responses is one of these.</li><li>Properties of these items can impact their usefulness.</li><li>Individuals can provide valid justification for ‘incorrect’ responses.</li><li>Researchers should know their items, and know the risk of not knowing those items.</li></ul><p>Participant carelessness is a source of invalidity in psychological data (Huang, Liu, &amp;amp; Bowling, 2015), and many methods have been created to screen for this carelessness (Curran, 2016; Johnson, 2005). These include items that researchers presume thoughtful individuals will answer in a given way (e.g., disagreement with “I am paid biweekly by leprechauns”, Meade &amp;amp; Craig, 2012). This paper reports on two samples in which individuals spoke aloud a series of these questions, and found that (a) individuals do occasionally report valid justifications for presumed invalid responses, (b) there is relatively high variance in this behavior over different items, and (c) items developed for this specific purpose tend to work better than those drawn from other sources or created ad-hoc. [Keywords: Carelessness, Data cleaning, Insufficient effort responding, Verbal protocol, Self-report data]</p><p>…</p><table><colgroup><col style=&quot;width: 31%&quot; /><col style=&quot;width: 68%&quot; /></colgroup><thead><tr class=&quot;header&quot;><th>Check</th><th>Justifications</th></tr></thead><tbody><tr class=&quot;odd&quot;><td>“All my friends are aliens”</td><td>“‘Aliens’ is a relative term; I don’t actually know for sure” · “What does that even mean, we’re all aliens if there’s other life out there”</td></tr><tr class=&quot;even&quot;><td>“I am interested in…parabanjology”</td><td>“Might be real so don’t want to disagree” · “It sounds like it could be interesting”</td></tr><tr class=&quot;odd&quot;><td>“I work twenty-eight hours in a typical work day.”</td><td>“It feels like that sometimes”</td></tr><tr class=&quot;even&quot;><td>“I am familiar with geological terms such as jpg and firewall.”</td><td>“I know what those are, but don’t know that they’re geological”</td></tr><tr class=&quot;odd&quot;><td>“I am fluent in combinatorial English”</td><td>“I’m fluent in English”</td></tr><tr class=&quot;even&quot;><td>“I am able to read the minds of others” · “I can see into the future”</td><td>“Understand general idea of what others are thinking” · “Close friends know each other” · “Can plan and expect future events”</td></tr><tr class=&quot;odd&quot;><td>“I sleep less than one hour per night”</td><td>“When I’m pulling an all-nighter I do” · “I sleep very few hours each night”</td></tr><tr class=&quot;even&quot;><td>“All my friends say I would make a great poodle”</td><td>“They say I’m like a puppy” · “They say I’d make a great koala” · “Friends say I share dog-like personality” · “Friends have said my hair looks like a poodle” · “Have been told I’d make a good dog” · “Don’t know, I’ve never asked them”</td></tr><tr class=&quot;odd&quot;><td>“I eat cement occasionally”</td><td>“There was cement in my braces, sure that I ate some” · “There are a lot of things that are in cement in a lot of foods, so maybe eating parts of it”</td></tr><tr class=&quot;even&quot;><td>“Answer with ‘Disagree’ for this item”</td><td>“Item doesn’t say how much to disagree (picked ‘Strongly disagree’)”</td></tr><tr class=&quot;odd&quot;><td>“I am paid biweekly by leprechauns”</td><td>“I am paid biweekly, just not by leprechauns”</td></tr><tr class=&quot;even&quot;><td>&quot; “I can run 2 miles in 2 min”</td><td>“It doesn’t say run with your feet, can do it in my mind”</td></tr><tr class=&quot;odd&quot;><td>“I have been to every country in the world”</td><td>“I’ve been to a lot of countries” · “I have probably been to more countries than most people”</td></tr><tr class=&quot;even&quot;><td>“I can teleport across time and space”</td><td>“Well, time passes, and I can move places, so that’s sort of true” · “Is walking a type of teleportation?” · “In my dreams I can because one of my life goals is to be the doctor’s companion”</td></tr></tbody></table><p>Table 2: Selected examples of valid justifications for ‘incorrect’ answers.</p>">“I’m paid biweekly, just not by leprechauns: Evaluating valid-but-incorrect response rates to attention check items”</a>, Curran &amp; Hauser 2019 (how do “lizardman constant” responders justify it? Or, ‘free response is the devil’)</li>
</ul>
<p>Technology:</p>
<ul>
<li><a href="https://www.gwern.net/docs/design/2014-bigelow.pdf" data-popup-title="Reflections on How Designers Design with Data" data-popup-author="Alex Bigelow, Steven Mark Drucker, Danyel Fisher, Miriah D. Meyer" data-popup-date="2014-05-27" data-popup-doi="10.1145/2598153.2598175" data-popup-abstract="<p>In recent years many popular data visualizations have emerged that are created largely by designers whose main area of expertise is not computer science. Designers generate these visualizations using a handful of design tools and environments. To better inform the development of tools intended for designers working with data, we set out to understand designers’ challenges and perspectives. We interviewed professional designers, conducted observations of designers working with data in the lab, and observed designers working with data in team settings in the wild. A set of patterns emerged from these observations from which we extract a number of themes that provide a new perspective on design considerations for visualization tool creators, as well as on known engineering problems.</p><p>…<strong>Patterns</strong>: In our observational studies we observed all of the designers initially sketching visual representations of data on paper, on a whiteboard, or in Illustrator. In these sketches, <strong>the designers would first draw high-level elements of their design such as the layout and axes, followed by a sketching in of data points based on their perceived ideas of data behavior (P1)</strong>. An example is shown in Figure 3. The designers often relied on their understanding of the semantics of data to infer how the data might look, such as F1 anticipating that Fitbit data about walking would occur in short spurts over time while sleep data would span longer stretches. However, <strong>the designers’ inferences about data behavior were often inaccurate (P2)</strong>. This tendency was acknowledged by most of the designers: after her inference from data semantics, F1 indicated that to work effectively, she would need “<em>a better idea of the behavior of each attribute</em>.” Similarly, B1 did not anticipate patterns in how software bugs are closed, prompting a reinterpretation and redesign of her team’s visualization much later in the design process once data behavior was explicitly explored. In the time travel studies, T3 misinterpreted one trip that later caused a complete redesign.</p><p>Furthermore, <strong>the designers’ inferences about data structure were often separated from the actual data (P3)</strong>. In brainstorming sessions at the hackathon, the designers described data that would be extremely difficult or impossible to gather or derive. In working with the <span class=&quot;smallcaps-auto&quot;>HBO</span> dataset, H1 experienced frustration after he spent time writing a formula in Excel only to realize that he was recreating data he had already seen in the aggregate table…Not surprisingly, <strong>the amount of data exploration and manipulation was related to the level of a designer’s experience working with data (P4)</strong>.</p>">“Reflections on How Designers Design with Data”</a>, Bigelow et al 2014 (why are data visualizations so bad—superficially pretty but misleading or useless? Because many designers don’t look at the data, avoid automation &amp; create manually so they can focus on pretty shapes/colors &amp; enjoying fiddling with it, and ignore readers)</li>
<li><a href="https://arxiv.org/abs/2005.06840" data-popup-title="Do Ads Harm News Consumption?" data-popup-author="Shunyao Yan, Klaus M. Miller, Bernd Skiera" data-popup-date="2020-05-21" data-popup-abstract="Many online news publishers finance their websites by displaying ads alongside content. Yet, remarkably little is known about how exposure to such ads impacts users' news consumption. We examine this question using 3.1 million anonymized browsing sessions from 79,856 users on a news website and the quasi-random variation created by ad blocker adoption. We find that seeing ads has a robust negative effect on the quantity and variety of news consumption: Users who adopt ad blockers subsequently consume 20% more news articles corresponding to 10% more categories. The effect persists over time and is largely driven by consumption of &quot;hard&quot; news. The effect is primarily attributable to a learning mechanism, wherein users gain positive experience with the ad-free site; a cognitive mechanism, wherein ads impede processing of content, also plays a role. Our findings open an important discussion on the suitability of advertising as a monetization model for valuable digital content.">“Do Ads Harm News Consumption?”</a>, Yan et al 2020 (“Users who adopt ad blockers subsequently consume 20% more news articles corresponding to 10% more categories. The effect persists over time…”; see <a href="https://www.gwern.net/Ads" data-popup-title="Banner Ads Considered Harmful" data-popup-author="Gwern Branwen" data-popup-date="8 Jan 2017" data-popup-abstract="<p>One source of complexity &amp;amp; JavaScript use on <code>gwern.net</code> is the use of Google AdSense advertising to insert banner ads. In considering design &amp;amp; usability improvements, removing the banner ads comes up every time as a possibility, as readers do not like ads, but such removal comes at a revenue loss and it’s unclear whether the benefit outweighs the cost, suggesting I run an A/B experiment. However, ads might be expected to have broader effects on traffic than individual page reading times/bounce rates, affecting <em>total</em> site traffic instead through long-term effects on or spillover mechanisms between readers (eg social media behavior), rendering the usual A/B testing method of per-page-load/session randomization incorrect; instead it would be better to analyze total traffic as a time-series experiment.</p><p>Design: A decision analysis of revenue vs readers yields an maximum acceptable total traffic loss of ~3%. Power analysis of historical <code>gwern.net</code> traffic data demonstrates that the high autocorrelation yields low statistical power with standard tests &amp;amp; regressions but acceptable power with <span class=&quot;smallcaps-auto&quot;>ARIMA</span> models. I design a long-term Bayesian <code><span class=&quot;smallcaps-auto&quot;>ARIMA</span>(4,0,1)</code> time-series model in which an A/B-test running January–October 2017 in randomized paired 2-day blocks of ads/no-ads uses client-local JS to determine whether to load &amp;amp; display ads, with total traffic data collected in Google Analytics &amp;amp; ad exposure data in Google AdSense. The A/B test ran from 1 January 2017 to 15 October 2017, affecting 288 days with collectively 380,140 pageviews in 251,164 sessions.</p><p>Correcting for a flaw in the randomization, the final results yield a surprisingly large estimate of an expected traffic loss of −9.7% (driven by the subset of users without adblock), with an implied −14% traffic loss if all traffic were exposed to ads (95% credible interval: −13–16%), exceeding my decision threshold for disabling ads &amp;amp; strongly ruling out the possibility of acceptably small losses which might justify further experimentation.</p><p>Thus, banner ads on <code>gwern.net</code> appear to be harmful and AdSense has been removed. If these results generalize to other blogs and personal websites, an important implication is that many websites may be harmed by their use of banner ad advertising without realizing it.</p>">my ad page</a>)</li>
<li><a href="https://www.gwern.net/docs/cs/2020-troise.pdf" data-popup-title="The 1-Bit Instrument: The Fundamentals of 1-Bit Synthesis, Their Implementational Implications, and Instrumental Possibilities" data-popup-author="Blake Troise" data-popup-date="2020-01-01" data-popup-doi="10.1525/jsmg.2020.1.1.44" data-popup-abstract="<p>The 1-bit sonic environment (perhaps most famously musically employed on the ZX Spectrum) is defined by extreme limitation. Yet, belying these restrictions, there is a surprisingly expressive instrumental versatility. This article explores the theory behind the primary, idiosyncratically 1-bit techniques available to the composer-programmer, those that are essential when designing “instruments” in 1-bit environments. These techniques include pulse width modulation for timbral manipulation and means of generating virtual polyphony in software, such as the pin pulse and pulse interleaving techniques. These methodologies are considered in respect to their compositional implications and instrumental applications. [Keywords: chiptune, 1-bit, one-bit, ZX Spectrum, pulse pin method, pulse interleaving, timbre, polyphony, history]</p>">“The 1-Bit Instrument: The Fundamentals of 1-Bit Synthesis, Their Implementational Implications, and Instrumental Possibilities”</a>, Troise 2020</li>
</ul>
<p>Economics:</p>
<ul>
<li><a href="https://www.nytimes.com/2020/04/09/us/politics/amish-coronavirus-ohio.html" data-popup-title="In Ohio, the Amish Take On the Coronavirus: A famously traditional community has mobilized to help hospitals with medical supplies, even as it struggles with reconciling its communal way of life with the dictates of social distancing." data-popup-author="Elizabeth Williamson (<span class=&quot;smallcaps-auto&quot;>NYT</span>)" data-popup-date="2020-04-09" data-popup-abstract="<p>On April 1, John Miller, a manufacturer here with deep connections to the close-knit Amish community of Central Ohio, got a call from Cleveland Clinic. The hospital system was struggling to find protective face masks for its 55,000 employees, plus visitors. Could his team sew 12,000 masks in two days?</p><p>He appealed to Abe Troyer with Keim, a local lumber mill and home goods business and a leader in the Amish community: “Abe, make a sewing frolic.” A frolic, Mr.&nbsp;Miller explained, “is a colloquial term here that means, ‘Get a bunch of people. Throw a bunch of people at this.’” A day later, Mr.&nbsp;Troyer had signed up 60 Amish home seamstresses, and the Cleveland Clinic sewing frolic was on.</p><p>…the pandemic has idled hundreds of Amish seamstresses, craftsmen and artisans, and Amish people do not apply for federal unemployment benefits…Almost overnight, a group of local industry, community and church leaders has mobilized to sustain Amish households by pivoting to work crafting thousands of face masks and shields, surgical gowns and protective garments from medical-grade materials. When those run scarce, they switch to using gaily printed quilting fabric and waterproof Tyvek house wrap.</p><p>…Berlin Gardens, which normally makes garden furniture from recycled plastic milk jugs, completed their first order of 20,000 plastic face shields for Yale New Haven Hospital last month. “We’re close to 100,000 a day,” Sam Yoder, the current owner of Berlin Gardens, said last Friday. “It almost covers our payroll. Not quite.”…Cleveland Clinic has since increased its order to 10,000 masks a day, Ms.&nbsp;Sandhu said, and has also ordered protective gowns…From her sunny sewing room outside Charm, Gladys Beachy will coordinate 9 women, including her widowed mother, who will sew 500 masks each. She can’t help thinking that holding “a quilting” would make the repetitive job more interesting for all of them.</p>">“In Ohio, the Amish Take On the Coronavirus”</a> (supply and demand: masks can be easily made anywhere <em>if</em> prices are allowed to rise &amp; they are not illegal to sell)</li>
<li><a href="https://www.gq.com/story/the-great-paper-caper" data-popup-title="The Great Paper Caper: Years of running drugs and boosting cars left Frank Bourassa thinking: There’s got to be an easier way to earn a dishonest living. That’s when he nerved up the idea to make his fortune. (Literally.) Which is how Frank became the most prolific counterfeiter in American history—a guy with more than $200 million in nearly flawless fake twenties stuffed in a garage. How he got away with it all, well, that’s even crazier." data-popup-author="Wells Tower (GQ)" data-popup-date="2014-11-01" data-popup-abstract="<p>Finally, when he was fairly certain that the cops weren’t onto him, Frank says he called another friend of his who showed up with scanners and radio wands to check the shipment for bugs. The crew opened the truck. On five wooden pallets sat the future of Frank’s criminal enterprise. It was paper of a special kind, made with the same rare cotton-and-linen recipe used for printing American currency. It also bore watermarked images of Andrew Jackson’s face and security strips reading <span class=&quot;smallcaps-auto&quot;>USA</span> <span class=&quot;smallcaps-auto&quot;>TWENTY</span> in minuscule type. The paper was the essential ingredient for fabricating high-grade counterfeit bills that the Canadian police would later describe as “basically undetectable” from the real thing. As soon as the security sweep pronounced the shipment clean, Frank welled up with optimism. “There was no way to stop me from there. I knew I was rich,” Frank recalled. “It was the best day of my life.” Frank now had what he needed to print hundreds of millions of dollars’ worth of fake U.S. currency—and to soon become the most prolific counterfeiter in the history of the trade.</p><p>…The recipe for the rag paper U.S. notes are printed on is deceptively simple—75% cotton and 25% linen—a distinctive composition every American unconsciously knows by feel. Simple though it may be, the recipe is also so widely known that dialing a paper mill and asking for a batch of 75/25 is a speedy way to get raided by the Secret Service (which was created expressly to bust counterfeiters—<span class=&quot;smallcaps-auto&quot;>POTUS</span> tending came later). And even if you <em>could</em> somehow chef up a few reams of the cotton-linen blend, you’d still need to add to it a whole host of security elements: the watermark—the translucent face of Jackson, Franklin, et al.—which appears when you hold the bill up to the light; the security strip; the tiny red and blue fibers embedded throughout the paper; and so on.</p><p>…In the fall of 2008, Frank says he began reaching out to paper mills across Europe and Asia under the alias Thomas Moore, an employee of The Letter Shop, a fictitious Quebec stationery concern. He purported to have a special client who wanted some special paper manufactured. What kind of paper? Well, rag paper with cotton, maybe some linen thrown in there. “Cotton and linen? Like, for currency?” suspicious papermakers would often respond, and Thomas Moore would be heard from no more.</p><p>But Frank had faith that somewhere—maybe in Poland, Slovakia, or Bulgaria—his avatar could flush out a papermaker stupid or crooked enough to make his recipe. In January 2009, he says, his search ended at the Artoz paper company headquartered in Lenzburg, Switzerland. By now, Frank had adopted the <em>nom de plume</em> Jackson Maxwell, of the Keystone Investment and Trading Company, a securities firm whose letterhead, suspiciously, bore no street address.</p><p>In correspondence included in court documents that Frank shared with me, Maxwell told his mark that Keystone was looking to print bond certificates on secure rag paper—customized with one or two security measures designed to, um, foil counterfeiters. Frank says that after Artoz accepted the basics of his bond-brokerage story, he tweaked and refined his order over many months, nudging one felonious tidbit after another onto the papermaker’s plate. He got them to add linen to the recipe. He asked them to mix in chemicals to thwart security pens and black-light tests. He persuaded them to sew in a security strip reading, in near microscopic print, <span class=&quot;smallcaps-auto&quot;>USA</span> <span class=&quot;smallcaps-auto&quot;>TWENTY</span>. (“I told them it was, you know, for a $20 bond.”) Artoz, he says, also agreed to imprint his paper with a watermark, an image etched into a cylindrical printing drum and pressed into the paper while the pulp is still wet. To get the equipment Artoz would need to do this, Frank paid $15,000, routed under a surrogate’s name through a Swiss bank account, to a company in Düren, Germany, that manufactured a drum etched with the likenesses of Andrew Jackson’s face. How did he manage that, exactly? “It was easy,” said Frank. “To you, he’s Andrew Jackson. To some guy in Germany, who the fuck is it? Some guy’s face. He doesn’t know.”</p>">“The Story of America’s Most Prolific Counterfeiter”</a> (how Frank Bourassa tricked a Swiss mill into selling him the unique <a href="https://en.wikipedia.org/wiki/Federal_Reserve_Note" data-popup-title="Federal Reserve Note" data-popup-author="English Wikipedia" data-popup-abstract="<p><b>Federal Reserve Notes</b>, also <b>United States banknotes</b>, are the banknotes currently used in the United States of America. Denominated in United States dollars, Federal Reserve Notes are printed by the United States Bureau of Engraving and Printing on paper made by Crane &amp;amp; Co. of Dalton, Massachusetts. Federal Reserve Notes are the only type of U.S. banknote currently produced. Federal Reserve Notes are authorized by Section 16 of the Federal Reserve Act of 1913 and are issued to the Federal Reserve Banks at the discretion of the Board of Governors of the Federal Reserve System. The notes are then put into circulation by the Federal Reserve Banks, at which point they become liabilities of the Federal Reserve Banks and obligations of the United States.</p>">U.S. dollar</a> linen-paper to create <span data-originalyear="2012" data-originalamount="250" data-currentyear="2020" data-currentamount="317" title="Inflation-adjusted currency: from $250 in 2012 → $317 in 2020">$317<span><span><span aria-label="Equation"></span></span></span></span>m in perfect <a href="https://en.wikipedia.org/wiki/Counterfeit_money" data-popup-title="Counterfeit money" data-popup-author="English Wikipedia" data-popup-abstract="<p><b>Counterfeit money</b> is imitation currency produced without the legal sanction of the state or government, usually in a deliberate attempt to imitate that currency and so as to deceive its recipient. Producing or using counterfeit money is a form of fraud or forgery. The business of counterfeiting money is almost as old as money itself: plated copies have been found of Lydian coins which are thought to be among the first Western coins. Before the introduction of paper money, the most prevalent method of counterfeiting involved mixing base metals with pure gold or silver. Another form of counterfeiting is the production of documents by legitimate printers in response to fraudulent instructions. During World War II, the Nazis forged British pounds and American dollars. Today some of the finest counterfeit banknotes are called <i>Superdollars</i> because of their high quality and likeness to the real US dollar. There has been significant counterfeiting of Euro banknotes and coins since the launch of the currency in 2002, but considerably less than for the US dollar.</p>">counterfeit money</a> &amp; mostly got away with it)</li>
</ul>
<p>Fiction:</p>
<ul>
<li><a href="https://slatestarcodex.com/2020/05/26/my-immortal-as-alchemical-allegory/" data-popup-title="<em>My Immortal</em> As Alchemical Allegory" data-popup-author="Scott Alexander (<span class=&quot;smallcaps-auto&quot;>SSC</span>)" data-popup-date="2020-05-26" data-popup-abstract="<p>[Mock-serious literary exegesis by the author of <a href=&quot;http://unsongbook.com/&quot;><em>Unsong</em></a> of infamously-bad <em>Harry Potter</em> fanfiction <a href=&quot;https://en.wikipedia.org/wiki/My_Immortal_(fan_fiction)&quot;>“My Immortal”</a>; Alexander sets out to try to explain the plot as a cunningly-concealed allegory of a <a href=&quot;https://en.wikipedia.org/wiki/Rosicrucianism&quot;>Rosicrucian</a> initiate’s failure to carry through the great work of <a href=&quot;https://en.wikipedia.org/wiki/Alchemy&quot;>alchemy</a> and a revision of <a href=&quot;https://en.wikipedia.org/wiki/Johann_Wolfgang_von_Goethe&quot;>Goethe’s</a> <a href=&quot;https://en.wikipedia.org/wiki/Faust,_Part_Two&quot;><em>Faust Part II</em></a>, drawing on <a href=&quot;https://en.wikipedia.org/wiki/Carl_Jung&quot;>Carl Jung’s</a> interpretation of alchemy as a metaphor for spiritual transformation. As such, this explains the heavy color symbolism, the protagonist’s failure to consummate her relationship with Draco Malfoy, the author’s inability to distinguish Harry Potter from Rubeus Hagrid, the fourth-wall-breaking towards the end, and the ending itself, in which the protagonist, a self-insert of the author, escapes death and is reborn as the author herself.]</p>">“<em>My Immortal</em> As Alchemical Allegory”</a>, <span>SSC</span></li>
</ul>
<p>Misc:</p>
<ul>
<li><a href="https://www.damninteresting.com/dead-reckoning/" data-popup-title="Dead Reckoning: The 18th century misadventures of HMS <em>Wager</em> and her reluctant crew" data-popup-author="Alan Bellows (Damn Interesting)" data-popup-date="2019-09-12" data-popup-abstract="[Narrative account of a English wreck at the tip of South America; ill-fated and ill-prepared for their mission raiding Spanish properties in the New World as part of the <a href=&quot;https://en.wikipedia.org/wiki/War_of_Jenkins%27_Ear&quot;>War of Jenkin&amp;#39;s Ear&quot;</a>, the crew would undergo the most brutal conditions and mutiny against their cruel incompetent captain. The survivors gradually navigated their way back to England, and the caption to Chile, eventually triggering a mutiny trial. The trial and publicity and published books offer an extremely detailed account from the survivors, and public sentiment turned against the captain and in favor of the crew, who were spared.]">“Dead Reckoning: The 18th century misadventures of <span>HMS</span> <em>Wager</em> and her reluctant crew”</a> (the <a href="https://en.wikipedia.org/wiki/HMS_Wager_(1739)" data-popup-title="HMS Wager (1739)" data-popup-author="English Wikipedia" data-popup-abstract="<p><b><span class=&quot;smallcaps-auto&quot;>HMS</span> <i>Wager</i></b> was a square-rigged sixth-rate Royal Navy ship of 28 guns. She was built as an East Indiaman in about 1734 and made two voyages to India for the East India Company before the Royal Navy purchased her in 1739. She formed part of a squadron under Commodore George Anson and was wrecked on the south coast of Chile on 14 May 1741. The wreck of <i>Wager</i> became famous for the subsequent adventures of the survivors who found themselves marooned on a desolate island in the middle of a Patagonian winter, and in particular because of the Wager Mutiny that followed.</p>"><em>Wager</em></a> <a href="https://en.wikipedia.org/wiki/Wager_Mutiny" data-popup-title="Wager Mutiny" data-popup-author="English Wikipedia" data-popup-abstract="<p>The <b><i>Wager</i> Mutiny</b> was the mutiny of the crew of the British war ship <span><span class=&quot;smallcaps-auto&quot;>HMS</span>&nbsp;<i>Wager</i></span> after she was wrecked on a desolate island off the south coast of Chile in 1741. The ship was part of a squadron commanded by George Anson and bound to attack Spanish interests in the Pacific. <i>Wager</i> lost contact with the squadron while rounding Cape Horn, ran aground, and wrecked on the island in May 1741. The main body of the crew mutinied against Captain David Cheap, abandoned him and his loyal crew members, and returned to England in a modified open boat via the Strait of Magellan. Most of the crew died on the journey, but the mutiny's ring-leaders were among those who survived.</p>">Mutiny</a>)</li>
</ul>
</section>
<section id="books">
<h2><a href="#books" title="Link to section: §'Books'">Books</a></h2>
<p>Fiction:</p>
<ul>
<li><a href="https://www.amazon.com/gp/product/1589881427/?tag=gwernnet-20"><em>The Battle Between the Frogs and the Mice: A Tiny Homeric Epic</em></a>, translated Stallings 2009 (<a href="https://www.goodreads.com/review/show/3231032913" data-popup-title="Review of <em>The Battle Between the Frogs and the Mice: A Tiny Homeric Epic</em>, Stallings 2019" data-popup-author="Gwern Branwen" data-popup-date="2020-05-30" data-popup-abstract="<p>Review of new translation of a well-known but now-neglected <a href=&quot;https://en.wikipedia.org/wiki/Batrachomyomachia&quot;>ancient Greek satirical poem</a> parodying the Homeric epics. Stallings’s rhymed-couplet translation is winsome and charming, preserving the too-cute names and bathos, and pairs well with Grant Silverstein’s energetic pencil drawings. A light and enjoyable read.</p>">review</a>)</li>
</ul>
</section>
<section id="music">
<h2><a href="#music" title="Link to section: §'Music'">Music</a></h2>
<p><a href="https://old.reddit.com/r/TOUHOUMUSIC/search?q=author%3Agwern&amp;sort=new&amp;restrict_sr=on&amp;t=all">Touhou</a>:</p>
<ul>
<li><a href="https://www.youtube.com/watch?v=5OmnD4zRrfw">“Sept Jours sans Elle (Vocal)”</a> (Raven’s Jig; <a href="https://www.youtube.com/watch?v=H6iMYgpnqBU"><em>Une Semaine chez les Écarlates</em></a> {2018}) [classical]</li>
<li><a href="https://www.youtube.com/watch?v=bSjqmUMqH58">“Un Jour Joueur”</a> (Raven’s Jig; <em>Une Semaine chez les Écarlates</em> {2018}) [classical]</li>
<li><a href="https://www.youtube.com/watch?v=xiY5HFHD4Js">“Bons et mauvais Jours”</a> (Raven’s Jig; <em>Une Semaine chez les Écarlates</em> {2018}) [classical]</li>
</ul>
<p><span>MLP</span>:</p>
<ul>
<li><a href="https://www.youtube.com/watch?v=gaHzrCMWlwY">“Morning in Baltimare”</a> (Mane in Green; <em>II. The Journey [The Quest of the Lost Sapphire—Ep. 2]</em> {2017}) [instrumental rock]</li>
<li><a href="https://www.youtube.com/watch?v=M_sViQ2rHnQ">“Love and Reflection”</a> (Dionte George; <a href="https://poniesatdawn.bandcamp.com/album/ignite"><em>Ignite</em></a> {2020}) [jazz]</li>
<li><a href="https://www.youtube.com/watch?v=gJ-bLIjz1jY">“Second Prances (Vocal <span>VIP</span>)”</a> (Etherium Apex ft. Nicole Carino {2020}) [electronic]</li>
<li><a href="https://www.youtube.com/watch?v=tEsDbLYym-w">“Spun”</a> (The Wasteland Wailers feat. Brittany Church &amp; Haymaker; <em>Ignite</em> {2020}) [country]</li>
<li><a href="https://www.youtube.com/watch?v=rOsHR7x9eTA">“Equiterian Empire”</a> (Carbon Maestro; <em>Celestial Divide <span>OST</span></em>) [orchestral]</li>
<li><a href="https://www.youtube.com/watch?v=Uj852XOZyEA">“The Storm Is Coming <span>VIP</span> [Single Purpose Remix]”</a> (UndreamedPanic feat. Metajoker; <em>Ignite</em> {2020}) [rock]</li>
<li><a href="https://www.youtube.com/watch?v=CFOwIOJt4KI">“Mare Cognitum”</a> (Idyllia feat. Velvet R. Wings; <em>Ignite</em> {2020}) [orchestral rock]</li>
<li><a href="https://poniesatdawn.bandcamp.com/track/fire-city-day-night">“Fire City (Day &amp; Night)”</a> (Wandering Artist; <em>Ignite</em> {2020}) [orchestral]</li>
<li><a href="https://www.youtube.com/watch?v=5-r-dY0mzdQ">“What Remains”</a> (Totalspark; <em>Ignite</em> {2020}) [Liquid Drum &amp; Brass]</li>
</ul>
<p>Doujin:</p>
<ul>
<li><a href="https://www.youtube.com/watch?v=wRMkOY9OrbU">“Come, Sweet Death [<em>Komm, süsser Tod</em>]”</a> (Platina Jazz feat. Niklas Gabrielsson; <a href="https://www.amazon.com/Anime-Standards-Vol-Platina-Jazz/dp/B07YG536TT?tag=gwernnet-20"><em>Anime Standards Vol. 6</em></a> {2019}) [jazz]</li>
<li><a href="https://www.youtube.com/watch?v=Bp12S4uM5Fs">“Hope”</a> (Simpsonill {2017}) [electronic]</li>
</ul>
</section>
</section>
<section role="doc-endnotes">
<hr>
<ol>
<li id="fn1" role="doc-endnote"><p>On implicit <a href="https://old.reddit.com/r/reinforcementlearning/search/?q=flair%3AMetaRL&amp;include_over_18=on&amp;restrict_sr=on&amp;sort=top">meta-learning</a>, see: <a href="https://www.cell.com/trends/cognitive-sciences/fulltext/S1364-6613(19)30061-0#deepmind" data-popup-title="Reinforcement Learning, Fast and Slow" data-popup-author="Matthew Botvinick, Sam Ritter, Jane X. Wang, Zeb Kurth-Nelson, Charles Blundell, Demis Hassabis" data-popup-date="2019-05-16" data-popup-doi="10.1016/j.tics.2019.02.006" data-popup-abstract="<p>Recent AI research has given rise to powerful techniques for deep reinforcement learning. In their combination of representation learning with reward-driven behavior, deep reinforcement learning would appear to have inherent interest for psychology and neuroscience.</p><p>One reservation has been that deep reinforcement learning procedures demand large amounts of training data, suggesting that these algorithms may differ fundamentally from those underlying human learning.</p><p>While this concern applies to the initial wave of deep RL techniques, subsequent AI work has established methods that allow deep RL systems to learn more quickly and efficiently. Two particularly interesting and promising techniques center, respectively, on episodic memory and meta-learning. Alongside their interest as AI techniques, deep RL methods leveraging episodic memory and meta-learning have direct and interesting implications for psychology and neuroscience. One subtle but critically important insight which these techniques bring into focus is the fundamental connection between fast and slow forms of learning.</p><p>Deep reinforcement learning (RL) methods have driven impressive advances in artificial intelligence in recent years, exceeding human performance in domains ranging from Atari to Go to no-limit poker. This progress has drawn the attention of cognitive scientists interested in understanding human learning. However, the concern has been raised that deep RL may be too sample-inefficient—that is, it may simply be too slow—to provide a plausible model of how humans learn. In the present review, we counter this critique by describing recently developed techniques that allow deep RL to operate more nimbly, solving problems much more quickly than previous methods. Although these techniques were developed in an AI context, we propose that they may have rich implications for psychology and neuroscience. A key insight, arising from these AI methods, concerns the fundamental connection between fast RL and slower, more incremental forms of learning.</p>">Botvinick et al 2019</a>, <a href="https://arxiv.org/abs/1905.10985#uber" data-popup-title="AI-GAs: AI-generating algorithms, an alternate paradigm for producing general artificial intelligence" data-popup-author="Jeff Clune" data-popup-date="2020-02-25" data-popup-abstract="Perhaps the most ambitious scientific quest in human history is the creation of general artificial intelligence, which roughly means AI that is as smart or smarter than humans. The dominant approach in the machine learning community is to attempt to discover each of the pieces required for intelligence, with the implicit assumption that some future group will complete the Herculean task of figuring out how to combine all of those pieces into a complex thinking machine. I call this the &quot;manual AI approach&quot;. This paper describes another exciting path that ultimately may be more successful at producing general AI. It is based on the clear trend in machine learning that hand-designed solutions eventually are replaced by more effective, learned solutions. The idea is to create an AI-generating algorithm (AI-GA), which automatically learns how to produce general AI. Three Pillars are essential for the approach: (1) meta-learning architectures, (2) meta-learning the learning algorithms themselves, and (3) generating effective learning environments. I argue that either approach could produce general AI first, and both are scientifically worthwhile irrespective of which is the fastest path. Because both are promising, yet the ML community is currently committed to the manual approach, I argue that our community should increase its research investment in the AI-GA approach. To encourage such research, I describe promising work in each of the Three Pillars. I also discuss AI-GA-specific safety and ethical considerations. Because it it may be the fastest path to general AI and because it is inherently scientifically interesting to understand the conditions in which a simple algorithm can produce general AI (as happened on Earth where Darwinian evolution produced human intelligence), I argue that the pursuit of AI-GAs should be considered a new grand challenge of computer science research.">Clune 2019</a>, <a href="https://arxiv.org/abs/1511.09249" data-popup-title="On Learning to Think: Algorithmic Information Theory for Novel Combinations of Reinforcement Learning Controllers and Recurrent Neural World Models" data-popup-author="Juergen Schmidhuber" data-popup-date="2019-08-25" data-popup-abstract="This paper addresses the general problem of reinforcement learning (RL) in partially observable environments. In 2013, our large RL recurrent neural networks (<span class=&quot;smallcaps-auto&quot;>RNN</span>s) learned from scratch to drive simulated cars from high-dimensional video input. However, real brains are more powerful in many ways. In particular, they learn a predictive model of their initially unknown environment, and somehow use it for abstract (e.g., hierarchical) planning and reasoning. Guided by algorithmic information theory, we describe <span class=&quot;smallcaps-auto&quot;>RNN</span>-based AIs (<span class=&quot;smallcaps-auto&quot;>RNNAI</span>s) designed to do the same. Such an <span class=&quot;smallcaps-auto&quot;>RNNAI</span> can be trained on never-ending sequences of tasks, some of them provided by the user, others invented by the <span class=&quot;smallcaps-auto&quot;>RNNAI</span> itself in a curious, playful fashion, to improve its <span class=&quot;smallcaps-auto&quot;>RNN</span>-based world model. Unlike our previous model-building <span class=&quot;smallcaps-auto&quot;>RNN</span>-based RL machines dating back to 1990, the <span class=&quot;smallcaps-auto&quot;>RNNAI</span> learns to actively query its model for abstract reasoning and planning and decision making, essentially &quot;learning to think.&quot; The basic ideas of this report can be applied to many other cases where one <span class=&quot;smallcaps-auto&quot;>RNN</span>-like system exploits the algorithmic information content of another. They are taken from a grant proposal submitted in Fall 2014, and also explain concepts such as &quot;mirror neurons.&quot; Experimental results will be described in separate papers.">Schmidhuber 2015</a>/<a href="https://arxiv.org/abs/1802.08864" data-popup-title="One Big Net For Everything" data-popup-author="Juergen Schmidhuber" data-popup-date="2019-08-26" data-popup-abstract="I apply recent work on &quot;learning to think&quot; (2015) and on PowerPlay (2011) to the incremental training of an increasingly general problem solver, continually learning to solve new tasks without forgetting previous skills. The problem solver is a single recurrent neural network (or similar general purpose computer) called <span class=&quot;smallcaps-auto&quot;>ONE</span>. <span class=&quot;smallcaps-auto&quot;>ONE</span> is unusual in the sense that it is trained in various ways, e.g., by black box optimization / reinforcement learning / artificial evolution as well as supervised / unsupervised learning. For example, <span class=&quot;smallcaps-auto&quot;>ONE</span> may learn through neuroevolution to control a robot through environment-changing actions, and learn through unsupervised gradient descent to predict future inputs and vector-valued reward signals as suggested in 1990. User-given tasks can be defined through extra goal-defining input patterns, also proposed in 1990. Suppose <span class=&quot;smallcaps-auto&quot;>ONE</span> has already learned many skills. Now a copy of <span class=&quot;smallcaps-auto&quot;>ONE</span> can be re-trained to learn a new skill, e.g., through neuroevolution without a teacher. Here it may profit from re-using previously learned subroutines, but it may also forget previous skills. Then <span class=&quot;smallcaps-auto&quot;>ONE</span> is retrained in PowerPlay style (2011) on stored input/output traces of (a) <span class=&quot;smallcaps-auto&quot;>ONE</span>'s copy executing the new skill and (b) previous instances of <span class=&quot;smallcaps-auto&quot;>ONE</span> whose skills are still considered worth memorizing. Simultaneously, <span class=&quot;smallcaps-auto&quot;>ONE</span> is retrained on old traces (even those of unsuccessful trials) to become a better predictor, without additional expensive interaction with the enviroment. More and more control and prediction skills are thus collapsed into <span class=&quot;smallcaps-auto&quot;>ONE</span>, like in the chunker-automatizer system of the neural history compressor (1991). This forces <span class=&quot;smallcaps-auto&quot;>ONE</span> to relate partially analogous skills (with shared algorithmic information) to each other, creating common subroutines in form of shared subnetworks of <span class=&quot;smallcaps-auto&quot;>ONE</span>, to greatly speed up subsequent learning of additional, novel but algorithmically related skills.">2018</a>, <a href="https://lilianweng.github.io/lil-log/2018/11/30/meta-learning.html#openai" data-popup-title="Meta-Learning: Learning to Learn Fast" data-popup-author="Lilian Weng" data-popup-date="2019-11-30" data-popup-abstract="<p>Meta-learning, also known as “learning to learn”, intends to design models that can learn new skills or adapt to new environments rapidly with a few training examples. There are three common approaches: 1. learn an efficient distance metric (metric-based); 2. use (recurrent) network with external or internal memory (model-based); 3. optimize the model parameters explicitly for fast learning (optimization-based).</p><p>…We expect a good meta-learning model capable of well adapting or generalizing to new tasks and new environments that have never been encountered during training time. The adaptation process, essentially a mini learning session, happens during test but with a limited exposure to the new task configurations. Eventually, the adapted model can complete new tasks. This is why meta-learning is also known as <a href=&quot;https://science.sciencemag.org/content/350/6266/1332/&quot; title=&quot;&amp;#39;Human-level concept learning through probabilistic program induction&amp;#39;, Lake et al 2015&quot;>learning to learn</a>.</p><p>Define the Meta-Learning Problem · A Simple View · Training in the Same Way as Testing · Learner and Meta-Learner · Common Approaches · Metric-Based · Convolutional Siamese Neural Network · Matching Networks · Simple Embedding · Full Context Embeddings · Relation Network · Prototypical Networks · Model-Based · Memory-Augmented Neural Networks · <span class=&quot;smallcaps-auto&quot;>MANN</span> for Meta-Learning · Addressing Mechanism for Meta-Learning · Meta Networks · Fast Weights · Model Components · Training Process · Optimization-Based · <span class=&quot;smallcaps-auto&quot;>LSTM</span> Meta-Learner · Why <span class=&quot;smallcaps-auto&quot;>LSTM</span>? · Model Setup · <span class=&quot;smallcaps-auto&quot;>MAML</span> · First-Order <span class=&quot;smallcaps-auto&quot;>MAML</span> · Reptile · The Optimization Assumption · Reptile vs <span class=&quot;smallcaps-auto&quot;>FOMAML</span> · Reference</p>">Weng 2018</a>/<a href="https://lilianweng.github.io/lil-log/2019/06/23/meta-reinforcement-learning.html#openai" data-popup-title="Meta Reinforcement Learning" data-popup-author="Lilian Weng" data-popup-date="2019-06-23" data-popup-abstract="<p>[Review/discussion] Meta-RL is meta-learning on reinforcement learning tasks. After trained over a distribution of tasks, the agent is able to solve a new task by developing a new RL algorithm with its internal activity dynamics. This post starts with the origin of meta-RL and then dives into three key components of meta-RL…, a good meta-learning model is expected to generalize to new tasks or new environments that have never been encountered during training. The adaptation process, essentially a <em>mini learning session</em>, happens at test with limited exposure to the new configurations. Even without any explicit fine-tuning (no gradient backpropagation on trainable variables), the meta-learning model autonomously adjusts internal hidden states to learn.</p><ul class=&quot;table-of-content&quot; id=&quot;markdown-toc&quot;><li><a href=&quot;https://lilianweng.github.io/lil-log/2019/06/23/meta-reinforcement-learning.html#on-the-origin-of-meta-rl&quot; id=&quot;markdown-toc-on-the-origin-of-meta-rl&quot;>On the Origin of Meta-RL</a><ul><li><a href=&quot;https://lilianweng.github.io/lil-log/2019/06/23/meta-reinforcement-learning.html#back-in-2001&quot; id=&quot;markdown-toc-back-in-2001&quot;>Back in 2001</a></li><li><a href=&quot;https://lilianweng.github.io/lil-log/2019/06/23/meta-reinforcement-learning.html#proposal-in-2016&quot; id=&quot;markdown-toc-proposal-in-2016&quot;>Proposal in 2016</a></li></ul></li><li><a href=&quot;https://lilianweng.github.io/lil-log/2019/06/23/meta-reinforcement-learning.html#define-meta-rl&quot; id=&quot;markdown-toc-define-meta-rl&quot;>Define Meta-RL</a><ul><li><a href=&quot;https://lilianweng.github.io/lil-log/2019/06/23/meta-reinforcement-learning.html#formulation&quot; id=&quot;markdown-toc-formulation&quot;>Formulation</a></li><li><a href=&quot;https://lilianweng.github.io/lil-log/2019/06/23/meta-reinforcement-learning.html#main-differences-from-rl&quot; id=&quot;markdown-toc-main-differences-from-rl&quot;>Main Differences from RL</a></li><li><a href=&quot;https://lilianweng.github.io/lil-log/2019/06/23/meta-reinforcement-learning.html#key-components&quot; id=&quot;markdown-toc-key-components&quot;>Key Components</a></li></ul></li><li><a href=&quot;https://lilianweng.github.io/lil-log/2019/06/23/meta-reinforcement-learning.html#meta-learning-algorithms-for-meta-rl&quot; id=&quot;markdown-toc-meta-learning-algorithms-for-meta-rl&quot;>Meta-Learning Algorithms for Meta-RL</a><ul><li><a href=&quot;https://lilianweng.github.io/lil-log/2019/06/23/meta-reinforcement-learning.html#optimizing-model-weights-for-meta-learning&quot; id=&quot;markdown-toc-optimizing-model-weights-for-meta-learning&quot;>Optimizing Model Weights for Meta-learning</a></li><li><a href=&quot;https://lilianweng.github.io/lil-log/2019/06/23/meta-reinforcement-learning.html#meta-learning-hyperparameters&quot; id=&quot;markdown-toc-meta-learning-hyperparameters&quot;>Meta-learning Hyperparameters</a></li><li><a href=&quot;https://lilianweng.github.io/lil-log/2019/06/23/meta-reinforcement-learning.html#meta-learning-the-loss-function&quot; id=&quot;markdown-toc-meta-learning-the-loss-function&quot;>Meta-learning the Loss Function</a></li><li><a href=&quot;https://lilianweng.github.io/lil-log/2019/06/23/meta-reinforcement-learning.html#meta-learning-the-exploration-strategies&quot; id=&quot;markdown-toc-meta-learning-the-exploration-strategies&quot;>Meta-learning the Exploration Strategies</a></li><li><a href=&quot;https://lilianweng.github.io/lil-log/2019/06/23/meta-reinforcement-learning.html#episodic-control&quot; id=&quot;markdown-toc-episodic-control&quot;>Episodic Control</a></li></ul></li><li><a href=&quot;https://lilianweng.github.io/lil-log/2019/06/23/meta-reinforcement-learning.html#training-task-acquisition&quot; id=&quot;markdown-toc-training-task-acquisition&quot;>Training Task Acquisition</a><ul><li><a href=&quot;https://lilianweng.github.io/lil-log/2019/06/23/meta-reinforcement-learning.html#task-generation-by-domain-randomization&quot; id=&quot;markdown-toc-task-generation-by-domain-randomization&quot;>Task Generation by Domain Randomization</a></li><li><a href=&quot;https://lilianweng.github.io/lil-log/2019/06/23/meta-reinforcement-learning.html#evolutionary-algorithm-on-environment-generation&quot; id=&quot;markdown-toc-evolutionary-algorithm-on-environment-generation&quot;>Evolutionary Algorithm on Environment Generation</a></li><li><a href=&quot;https://lilianweng.github.io/lil-log/2019/06/23/meta-reinforcement-learning.html#learning-with-random-rewards&quot; id=&quot;markdown-toc-learning-with-random-rewards&quot;>Learning with Random Rewards</a></li></ul></li><li><a href=&quot;https://lilianweng.github.io/lil-log/2019/06/23/meta-reinforcement-learning.html#references&quot; id=&quot;markdown-toc-references&quot;>References</a></li></ul>">Weng 2019</a>.<a href="#fnref1" role="doc-backlink">↩︎</a></p></li>
<li id="fn2" role="doc-endnote"><p><span>GPT-3</span> hardly costs more than a few million dollars of compute (now) and is cheap to run (pg39: “Even with the full <span>GPT-3</span> 175B, generating 100 pages of content from a trained model can cost on the order of 0.4 kW-hr, or only a few cents in energy costs.”), while <span>IBM</span>’s (otherwise useless) Deep Blue AI project reputedly cost &gt;<span data-originalyear="1997" data-originalamount="5" data-currentyear="2020" data-currentamount="10" title="Inflation-adjusted currency: from $5 in 1997 → $10 in 2020">$10<span><span><span aria-label="Equation"></span></span></span></span>m for the final iteration (reports of <span data-originalyear="1997" data-originalamount="100" data-currentyear="2020" data-currentamount="192" title="Inflation-adjusted currency: from $100 in 1997 → $192 in 2020">$192<span><span><span aria-label="Equation"></span></span></span></span>m appear to be a confusion with the estimated value of <em>publicity</em> mentioned in pg187 of Hsu’s <em>Behind Deep Blue</em>) and Big Science projects like <a href="https://en.wikipedia.org/wiki/ITER" data-popup-title="ITER" data-popup-author="English Wikipedia" data-popup-abstract="<p><b><span class=&quot;smallcaps-auto&quot;>ITER</span></b> is an international nuclear fusion research and engineering megaproject, which will be the world's largest magnetic confinement plasma physics experiment. It is an experimental tokamak nuclear fusion reactor that is being built next to the Cadarache facility in Saint-Paul-lès-Durance, in Provence, southern France.</p>"><span>ITER</span></a> blow &gt;5000✕ the funding to mostly fail. <span>GPT-3</span> could have been done decades ago with global computing resources &amp; scientific budgets; what could be done with today’s hardware &amp; budgets that we just don’t know or care to do? There <em>is</em> a hardware overhang. (See also the <a href="https://www.fhi.ox.ac.uk/brain-emulation-roadmap-report.pdf" title="Sandberg &amp; Bostrom 2008"><em>Whole Brain Emulation Roadmap</em></a> &amp; <a href="https://aiimpacts.org/2019-recent-trends-in-gpu-price-per-flops/" data-popup-title="2019 recent trends in GPU price per FLOPS" data-popup-author="Asya Bergal (AI Impacts)" data-popup-date="2020-03-25" data-popup-abstract="<figure class=&quot;wp-block-table&quot;><table class=&quot;has-fixed-layout&quot;><tbody><tr><td></td><td><strong>Release Prices</strong></td><td><strong>95th-percentile Active Prices</strong></td><td><strong>95th-percentile Active Prices</strong> <strong>(pre-crypto price rise)</strong></td></tr><tr><td></td><td><em>11/2007 – 1/2010</em></td><td><em>3/2011 – 1/2020</em></td><td><em>3/2011 – 12/2016 </em></td></tr><tr><td><strong>$ / single-precision <span class=&quot;smallcaps-auto&quot;>FLOPS</span></strong></td><td>12.5</td><td>17</td><td>16</td></tr><tr><td></td><td><em>9/2014 – 1/2020</em></td><td><em>1/2015 – 1/2020</em></td><td><em>1/2015 – 12/2016 </em></td></tr><tr><td><strong>$ / half-precision <span class=&quot;smallcaps-auto&quot;>FLOPS</span></strong></td><td>8</td><td>10</td><td>8</td></tr><tr><td><strong>$ / half-precision <span class=&quot;smallcaps-auto&quot;>FMA</span> <span class=&quot;smallcaps-auto&quot;>FLOPS</span></strong></td><td>4</td><td>4.5</td><td>—</td></tr></tbody></table></figure><p>Release price data seems to generally support the trends we found in active prices, with the notable exception of trends in <span class=&quot;smallcaps-auto&quot;>GPU</span> price / single-precision <span class=&quot;smallcaps-auto&quot;>FLOPS</span>, which cannot be explained solely by the different start dates.<span id=&quot;easy-footnote-58-2316&quot; class=&quot;easy-footnote-margin-adjust&quot;></span><span class=&quot;easy-footnote&quot;><a href=&quot;https://aiimpacts.org/2019-recent-trends-in-gpu-price-per-flops/#easy-footnote-bottom-58-2316&quot; title=&quot;See our analysis in &amp;lt;a href=&amp;quot;#single-precision-analysis&amp;quot;&amp;gt;this section&amp;lt;/a&amp;gt; above.&quot;><sup>58</sup></a></span> We think the best estimate of the overall trend for prices at which people recently bought <span class=&quot;smallcaps-auto&quot;>GPU</span>s is the 95th-percentile active price data from 2011 – 2020, since release price data does not account for existing <span class=&quot;smallcaps-auto&quot;>GPU</span>s becoming cheaper over time. The pre-crypto trends are similar to the overall trends, suggesting that the trends we are seeing are not anomalous due to cryptocurrency.<br></p><p>Given that, we guess that <span class=&quot;smallcaps-auto&quot;>GPU</span> prices as a whole have fallen at rates that would yield an order of magnitude over roughly:</p><ul><li>17 years for single-precision <span class=&quot;smallcaps-auto&quot;>FLOPS</span></li><li>10 years for half-precision <span class=&quot;smallcaps-auto&quot;>FLOPS</span></li><li>5 years for half-precision fused multiply-add <span class=&quot;smallcaps-auto&quot;>FLOPS</span></li></ul><p>Half-precision <span class=&quot;smallcaps-auto&quot;>FLOPS</span> seem to have become cheaper substantially faster than single-precision in recent years. This may be a “catching up” effect as more of the space on <span class=&quot;smallcaps-auto&quot;>GPU</span>s was allocated to half-precision computing, rather than reflecting more fundamental technological progress.</p><p>[Keywords: AI Timelines, Featured Articles, Hardware and AI Timelines, Hardware progress]</p>">“2019 recent trends in <span>GPU</span> price per <span>FLOPS</span>”</a>.)<a href="#fnref2" role="doc-backlink">↩︎</a></p></li>
<li id="fn3" role="doc-endnote"><p>Eg no use of <a href="https://www.gwern.net/docs/www/old.reddit.com/52b1364dcfce9965e94dbba4277a74045e9627d1.html" data-popup-title="<span class=&quot;smallcaps-auto&quot;>WBE and <span class=&quot;smallcaps-auto&quot;>DRL: a Middle Way of imitation learning from the human brain" data-popup-author="Gwern Branwen" data-popup-date="2018-10-20" data-popup-abstract="<p>Description of emerging machine learning paradigm identified by commentator starspawn0: discussions of building artificial brains typically presume either learning a brain architecture &amp;amp; parameters from scratch (<span class=&quot;smallcaps-auto&quot;>AGI</span>) or laboriously ‘scanning’ and reverse-engineering a biological brain in its entirety to get a functioning artificial brain.</p><p>However, the rise of deep learning’s transfer learning &amp;amp; meta-learning shows a wide variety of intermediate approaches, where ‘side data’ from natural brains can be used as scaffolding to guide &amp;amp; constrain standard deep learning methods. Such approaches do not seek to ‘upload’ or ‘emulate’ any specific brain, they merely seek to imitate an average brain. A simple example would be training a <span class=&quot;smallcaps-auto&quot;>CNN</span> to imitate <a href=&quot;https://en.wikipedia.org/wiki/Eye_tracking&quot;>eyetracking</a> saliency data: what a human looks at while playing a video game or driving is the important part of a scene, and the <span class=&quot;smallcaps-auto&quot;>CNN</span> doesn’t have to learn importance from scratch. A more complex example would be using <span class=&quot;smallcaps-auto&quot;>EEG</span> as a ‘description’ of music in addition to the music itself. f<span class=&quot;smallcaps-auto&quot;>MRI</span> data could be used to guide a NN to have a similar modularized architecture with similar activation patterns given a particular stimulus as a human brain, which presumably is related to human abilities to zero-shot/few-shot learn and generalize.</p><p>While a highly marginal approach at the moment compared to standard approaches like scaling up models &amp;amp; datasets, it is largely untapped, and progress in VR <a href=&quot;https://en.wikipedia.org/wiki/Virtual_reality_headset&quot;>headsets</a> with eyetracking capabilities (intended for <a href=&quot;https://en.wikipedia.org/wiki/Foveated_rendering&quot;>foveated rendering</a> but usable for many other purposes), brain imaging methods &amp;amp; <a href=&quot;https://en.wikipedia.org/wiki/Brain%E2%80%93computer_interface&quot;><span class=&quot;smallcaps-auto&quot;>BCI</span>s</a> has been more rapid than generally appreciated—in part thanks to breakthroughs using DL itself, suggesting the potential for a positive feedback loop where a <span class=&quot;smallcaps-auto&quot;>BCI</span> breakthrough enables a better NN for <span class=&quot;smallcaps-auto&quot;>BCI</span>s and so on.</p>" rel="archived alternate nofollow" data-url-original="https://old.reddit.com/r/reinforcementlearning/comments/9pwy2f/wbe_and_drl_a_middle_way_of_imitation_learning/">brain imitation learning</a> or neural architecture search to try to tailor the model, or even decide basic hyperparameters like widths (which as <a href="https://arxiv.org/abs/1905.11946#google" data-popup-title="EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks" data-popup-author="Mingxing Tan, Quoc V. Le" data-popup-date="2020-02-23" data-popup-abstract="Convolutional Neural Networks (ConvNets) are commonly developed at a fixed resource budget, and then scaled up for better accuracy if more resources are available. In this paper, we systematically study model scaling and identify that carefully balancing network depth, width, and resolution can lead to better performance. Based on this observation, we propose a new scaling method that uniformly scales all dimensions of depth/width/resolution using a simple yet highly effective compound coefficient. We demonstrate the effectiveness of this method on scaling up MobileNets and ResNet.   To go even further, we use neural architecture search to design a new baseline network and scale it up to obtain a family of models, called EfficientNets, which achieve much better accuracy and efficiency than previous ConvNets. In particular, our EfficientNet-B7 achieves state-of-the-art 84.4% top-1 / 97.1% top-5 accuracy on ImageNet, while being 8.4x smaller and 6.1x faster on inference than the best existing ConvNet. Our EfficientNets also transfer well and achieve state-of-the-art accuracy on <span class=&quot;smallcaps-auto&quot;>CIFAR</span>-100 (91.7%), Flowers (98.8%), and 3 other transfer learning datasets, with an order of magnitude fewer parameters. Source code is at https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet.">EfficientNet</a> shows, can make quite a different even in “well-understood and hand-optimized vanilla architectures”).<a href="#fnref3" role="doc-backlink">↩︎</a></p></li>
<li id="fn4" role="doc-endnote"><p>Not even <span>PDF</span>s—so no Google Books, no Arxiv, no Libgen, no Sci-Hub…<a href="#fnref4" role="doc-backlink">↩︎</a></p></li>
<li id="fn5" role="doc-endnote"><p>‘A man is at the doctor’s office, and the doctor tells him, “I’ve got some good news and some bad news for you.” / The man says, “Well, I can’t take the bad news right now, so give me the good news first.” / The doctor says, “Well, the good news is that you have an 18-inch penis.” / The man looks stunned for a moment, and then asks, “What’s the bad news?” / The doctor says, “Your brain’s in your dick.”’<a href="#fnref5" role="doc-backlink">↩︎</a></p></li>
<li id="fn6" role="doc-endnote"><p>Specifically: <a href="https://arxiv.org/abs/1707.02968#google" data-popup-title="Revisiting Unreasonable Effectiveness of Data in Deep Learning Era" data-popup-author="Chen Sun, Abhinav Shrivastava, Saurabh Singh, Abhinav Gupta" data-popup-date="2020-06-05" data-popup-abstract="The success of deep learning in vision can be attributed to: (a) models with high capacity; (b) increased computational power; and (c) availability of large-scale labeled data. Since 2012, there have been significant advances in representation capabilities of the models and computational capabilities of <span class=&quot;smallcaps-auto&quot;>GPU</span>s. But the size of the biggest dataset has surprisingly remained constant. What will happen if we increase the dataset size by 10x or 100x? This paper takes a step towards clearing the clouds of mystery surrounding the relationship between `enormous data' and visual deep learning. By exploiting the <span class=&quot;smallcaps-auto&quot;>JFT</span>-300M dataset which has more than 375M noisy labels for 300M images, we investigate how the performance of current vision tasks would change if this data was used for representation learning. Our paper delivers some surprising (and some expected) findings. First, we find that the performance on vision tasks increases logarithmically based on volume of training data size. Second, we show that representation learning (or pre-training) still holds a lot of promise. One can improve performance on many vision tasks by just training a better base model. Finally, as expected, we present new state-of-the-art results for different vision tasks including image classification, object detection, semantic segmentation and human pose estimation. Our sincere hope is that this inspires vision community to not undervalue the data and develop collective efforts in building larger datasets.">Sun et al 2017</a>, <a href="https://arxiv.org/abs/1712.00409#baidu" data-popup-title="Deep Learning Scaling is Predictable, Empirically" data-popup-author="Joel Hestness, Sharan Narang, Newsha Ardalani, Gregory Diamos, Heewoo Jun, Hassan Kianinejad, Md. Mostofa Ali Patwary, Yang Yang, Yanqi Zhou" data-popup-date="2020-02-24" data-popup-abstract="Deep learning (DL) creates impactful advances following a virtuous recipe: model architecture search, creating large training data sets, and scaling computation. It is widely believed that growing training sets and models should improve accuracy and result in better products. As DL application domains grow, we would like a deeper understanding of the relationships between training set size, computational scale, and model accuracy improvements to advance the state-of-the-art.   This paper presents a large scale empirical characterization of generalization error and model size growth as training sets grow. We introduce a methodology for this measurement and test four machine learning domains: machine translation, language modeling, image processing, and speech recognition. Our empirical results show power-law generalization error scaling across a breadth of factors, resulting in power-law exponents---the &quot;steepness&quot; of the learning curve---yet to be explained by theoretical work. Further, model improvements only shift the error but do not appear to affect the power-law exponent. We also show that model size scales sublinearly with data size. These scaling relationships have significant implications on deep learning research, practice, and systems. They can assist model debugging, setting accuracy targets, and decisions about data set growth. They can also guide computing system design and underscore the importance of continued computational scaling.">Hestness et al 2017</a>, <a href="https://arxiv.org/abs/1811.03600#google" data-popup-title="Measuring the Effects of Data Parallelism on Neural Network Training" data-popup-author="Christopher J. Shallue, Jaehoon Lee, Joseph Antognini, Jascha Sohl-Dickstein, Roy Frostig, George E. Dahl" data-popup-date="2020-02-23" data-popup-abstract="Recent hardware developments have dramatically increased the scale of data parallelism available for neural network training. Among the simplest ways to harness next-generation hardware is to increase the batch size in standard mini-batch neural network training algorithms. In this work, we aim to experimentally characterize the effects of increasing the batch size on training time, as measured by the number of steps necessary to reach a goal out-of-sample error. We study how this relationship varies with the training algorithm, model, and data set, and find extremely large variation between workloads. Along the way, we show that disagreements in the literature on how batch size affects model quality can largely be explained by differences in metaparameter tuning and compute budgets at different batch sizes. We find no evidence that larger batch sizes degrade out-of-sample performance. Finally, we discuss the implications of our results on efforts to train neural networks much faster in the future. Our experimental data is publicly available as a database of 71,638,836 loss measurements taken over the course of training for 168,160 individual models across 35 workloads.">Shallue et al 2018</a>, <a href="https://arxiv.org/abs/1812.06162#openai" data-popup-title="An Empirical Model of Large-Batch Training" data-popup-author="Sam McCandlish, Jared Kaplan, Dario Amodei, OpenAI Dota Team" data-popup-date="2020-06-05" data-popup-abstract="In an increasing number of domains it has been demonstrated that deep learning models can be trained using relatively large batch sizes without sacrificing data efficiency. However the limits of this massive data parallelism seem to differ from domain to domain, ranging from batches of tens of thousands in ImageNet to batches of millions in RL agents that play the game Dota 2. To our knowledge there is limited conceptual understanding of why these limits to batch size differ or how we might choose the correct batch size in a new domain. In this paper, we demonstrate that a simple and easy-to-measure statistic called the gradient noise scale predicts the largest useful batch size across many domains and applications, including a number of supervised learning datasets (MNIST, SVHN, CIFAR-10, ImageNet, Billion Word), reinforcement learning domains (Atari and Dota), and even generative model training (autoencoders on SVHN). We find that the noise scale increases as the loss decreases over a training run and depends on the model size primarily through improved model performance. Our empirically-motivated theory also describes the tradeoff between compute-efficiency and time-efficiency, and provides a rough model of the benefits of adaptive batch-size training.">McCandlish et al 2018</a>, <a href="https://arxiv.org/abs/1909.12673" data-popup-title="A Constructive Prediction of the Generalization Error Across Scales" data-popup-author="Jonathan S. Rosenfeld, Amir Rosenfeld, Yonatan Belinkov, Nir Shavit" data-popup-date="2020-02-20" data-popup-abstract="The dependency of the generalization error of neural networks on model and dataset size is of critical importance both in practice and for understanding the theory of neural networks. Nevertheless, the functional form of this dependency remains elusive. In this work, we present a functional form which approximates well the generalization error in practice. Capitalizing on the successful concept of model scaling (e.g., width, depth), we are able to simultaneously construct such a form and specify the exact models which can attain it across model/data scales. Our construction follows insights obtained from observations conducted over a range of model/data scales, in various model types and datasets, in vision and language tasks. We show that the form both fits the observations well across scales, and provides accurate predictions from small- to large-scale models and data.">Rosenfeld et al 2019</a>, <a href="https://arxiv.org/abs/2002.11794" data-popup-title="Train Large, Then Compress: Rethinking Model Size for Efficient Training and Inference of Transformers" data-popup-author="Zhuohan Li, Eric Wallace, Sheng Shen, Kevin Lin, Kurt Keutzer, Dan Klein, Joseph E. Gonzalez" data-popup-date="2020-03-06" data-popup-abstract="Since hardware resources are limited, the objective of training deep learning models is typically to maximize accuracy subject to the time and memory constraints of training and inference. We study the impact of model size in this setting, focusing on Transformer models for <span class=&quot;smallcaps-auto&quot;>NLP</span> tasks that are limited by compute: self-supervised pretraining and high-resource machine translation. We first show that even though smaller Transformer models execute faster per iteration, wider and deeper models converge in significantly fewer steps. Moreover, this acceleration in convergence typically outpaces the additional computational overhead of using larger models. Therefore, the most compute-efficient training strategy is to counterintuitively train extremely large models but stop after a small number of iterations.   This leads to an apparent trade-off between the training efficiency of large Transformer models and the inference efficiency of small Transformer models. However, we show that large models are more robust to compression techniques such as quantization and pruning than small models. Consequently, one can get the best of both worlds: heavily compressed, large models achieve higher accuracy than lightly compressed, small models.">Li et al 2020</a>, <a href="https://arxiv.org/abs/2001.08361#openai" data-popup-title="Scaling Laws for Neural Language Models" data-popup-author="Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, Dario Amodei" data-popup-date="2020-01-24" data-popup-abstract="We study empirical scaling laws for language model performance on the cross-entropy loss. The loss scales as a power-law with model size, dataset size, and the amount of compute used for training, with some trends spanning more than seven orders of magnitude. Other architectural details such as network width or depth have minimal effects within a wide range. Simple equations govern the dependence of overfitting on model/dataset size and the dependence of training speed on model size. These relationships allow us to determine the optimal allocation of a fixed compute budget. Larger models are significantly more sample-efficient, such that optimally compute-efficient training involves training very large models on a relatively modest amount of data and stopping significantly before convergence.">Kaplan et al 2020</a>, <a href="https://arxiv.org/abs/2004.13637#facebook" data-popup-title="Recipes for building an open-domain chatbot" data-popup-author="Stephen Roller, Emily Dinan, Naman Goyal, Da Ju, Mary Williamson, Yinhan Liu, Jing Xu, Myle Ott, Kurt Shuster, Eric M. Smith, Y-Lan Boureau, Jason Weston" data-popup-date="2020-04-29" data-popup-abstract="Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, and displaying knowledge, empathy and personality appropriately, while maintaining a consistent persona. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter models, and make our models and code publicly available under the collective name Blender. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.">Roller et al 2020</a>. It is noteworthy that the pursuit of large models is driven almost exclusively by OpenAI &amp; industry entities (the latter of which are content with far smaller models), and that academia has evinced an almost total disinterest (disgust, even). For all that the scaling hypothesis is ‘obvious’ and scaling is ‘predicted’, there is remarkably little interest in actually <em>doing</em> it. Perhaps we should pay more attention to what people do rather than what they say.<a href="#fnref6" role="doc-backlink">↩︎</a></p></li>
<li id="fn7" role="doc-endnote"><p>One interesting aspect of image scaling experiments is that even when performance is ‘plateauing’ on the original task &amp; approaching label error, the transfer learning continues to improve. Apparently the internal representations, even when adequate for mere classification and so the score cannot increase more than a small percentage, become more human-like—encoding <a href="https://arxiv.org/abs/1503.02531#google" data-popup-title="Distilling the Knowledge in a Neural Network" data-popup-author="Geoffrey Hinton, Oriol Vinyals, Jeff Dean" data-popup-date="2020-06-01" data-popup-abstract="A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on <span class=&quot;smallcaps-auto&quot;>MNIST</span> and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel.">dark knowledge</a>? I’ve noticed with language models, the final fractions of a loss appear to make a substantial difference to generated sample quality, perhaps because it is only after all the easier modeling is finished that the lazy language model is forced to squeeze out the next bit of performance by more correctly modeling more sophisticated things like logic, objects, world-knowledge, etc.<a href="#fnref7" role="doc-backlink">↩︎</a></p></li>
<li id="fn8" role="doc-endnote"><p>Now that <span>GPT-3</span>’s few-shot and <a href="https://arxiv.org/abs/2003.08380#google" data-popup-title="TTTTTackling WinoGrande Schemas" data-popup-author="Sheng-Chieh Lin, Jheng-Hong Yang, Rodrigo Nogueira, Ming-Feng Tsai, Chuan-Ju Wang, Jimmy Lin" data-popup-date="2020-06-06" data-popup-abstract="We applied the T5 sequence-to-sequence model to tackle the AI2 WinoGrande Challenge by decomposing each example into two input text strings, each containing a hypothesis, and using the probabilities assigned to the &quot;entailment&quot; token as a score of the hypothesis. Our first (and only) submission to the official leaderboard yielded 0.7673 AUC on March 13, 2020, which is the best known result at this time and beats the previous state of the art by over five points.">T5 finetuning</a> have begun to make people like Gary Marcus feel slightly nervous about WinoGrande, they have begun <a href="https://arxiv.org/abs/2004.13831" data-popup-title="A Review of Winograd Schema Challenge Datasets and Approaches" data-popup-author="Vid Kocijan, Thomas Lukasiewicz, Ernest Davis, Gary Marcus, Leora Morgenstern" data-popup-date="2020-06-06" data-popup-abstract="The Winograd Schema Challenge is both a commonsense reasoning and natural language understanding challenge, introduced as an alternative to the Turing test. A Winograd schema is a pair of sentences differing in one or two words with a highly ambiguous pronoun, resolved differently in the two sentences, that appears to require commonsense knowledge to be resolved correctly. The examples were designed to be easily solvable by humans but difficult for machines, in principle requiring a deep understanding of the content of the text and the situation it describes. This paper reviews existing Winograd Schema Challenge benchmark datasets and approaches that have been published since its introduction.">preparing their excuses</a> for why Winograd schemas <a href="https://www.gwern.net/Modus" data-popup-title="One Man's Modus Ponens" data-popup-author="Gwern Branwen" data-popup-date="1 May 2012" data-popup-abstract="<p>A logically-valid argument which takes the form of a <a href=&quot;https://en.wikipedia.org/wiki/modus_ponens&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;Modus ponens&quot; data-popup-author=&quot;English Wikipedia&quot; data-popup-abstract=&quot;&amp;lt;p&amp;gt;In propositional logic, &amp;lt;i&amp;gt;&amp;lt;b&amp;gt;modus ponens&amp;lt;/b&amp;gt;&amp;lt;/i&amp;gt; is a rule of inference. It can be summarized as &amp;quot;&amp;lt;i&amp;gt;P implies Q&amp;lt;/i&amp;gt; and &amp;lt;i&amp;gt;P&amp;lt;/i&amp;gt; is asserted to be true, therefore &amp;lt;i&amp;gt;Q&amp;lt;/i&amp;gt; must be true.&amp;quot;&amp;lt;/p&amp;gt;&quot; title=&quot;Wikipedia: modus ponens&quot;>modus ponens</a> may be interpreted in several ways; a major one is to interpret it as a kind of <em><a href=&quot;https://en.wikipedia.org/wiki/reductio_ad_absurdum&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;Reductio ad absurdum&quot; data-popup-author=&quot;English Wikipedia&quot; data-popup-abstract=&quot;&amp;lt;p&amp;gt;In logic, &amp;lt;b&amp;gt;&amp;lt;i lang=&amp;quot;la&amp;quot; title=&amp;quot;Latin language text&amp;quot;&amp;gt;reductio ad absurdum&amp;lt;/i&amp;gt;&amp;lt;/b&amp;gt;, also known as &amp;lt;b&amp;gt;&amp;lt;i lang=&amp;quot;la&amp;quot; title=&amp;quot;Latin language text&amp;quot;&amp;gt;argumentum ad absurdum&amp;lt;/i&amp;gt;&amp;lt;/b&amp;gt;, &amp;lt;i&amp;gt;&amp;lt;b&amp;gt;apagogical arguments&amp;lt;/b&amp;gt;&amp;lt;/i&amp;gt; or the &amp;lt;i&amp;gt;&amp;lt;b&amp;gt;appeal to extremes&amp;lt;/b&amp;gt;&amp;lt;/i&amp;gt;, is a form of argument that attempts either to disprove a statement by showing it inevitably leads to a ridiculous, absurd, or impractical conclusion, or to prove one by showing that if it were not true, the result would be absurd or impossible. Traced back to classical Greek philosophy in Aristotle&amp;#39;s &amp;lt;i&amp;gt;Prior Analytics&amp;lt;/i&amp;gt;, this technique has been used throughout history in both formal mathematical and philosophical reasoning, as well as in debate.&amp;lt;/p&amp;gt;&quot; title=&quot;Wikipedia: reductio ad absurdum&quot;>reductio ad absurdum</a></em>, where by ‘proving’ a conclusion believed to be false, one might instead take it as a <a href=&quot;https://en.wikipedia.org/wiki/modus_tollens&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;Modus tollens&quot; data-popup-author=&quot;English Wikipedia&quot; data-popup-abstract=&quot;&amp;lt;p&amp;gt;In propositional logic, &amp;lt;i&amp;gt;&amp;lt;b&amp;gt;modus tollens&amp;lt;/b&amp;gt;&amp;lt;/i&amp;gt; is a valid argument form and a rule of inference. It is an application of the general truth that if a statement is true, then so is its contrapositive.&amp;lt;/p&amp;gt;&quot; title=&quot;Wikipedia: modus tollens&quot;>modus tollens</a> which proves that one of the <em>premises</em> is false. This “Moorean shift” is aphorized as the snowclone, “One man’s modus ponens is another man’s modus tollens”. The Moorean shift is a powerful counter-argument which has been deployed against many skeptical &amp;amp; metaphysical claims in philosophy, where often the conclusion is extremely unlikely and little evidence can be provided for the premises used in the proofs; and it is relevant to many other debates, particularly methodological ones.</p>">weren’t <em>really</em></a> good measures of commonsense reasoning/intelligence (because intelligence, of course, is whatever AI can’t do yet).<a href="#fnref8" role="doc-backlink">↩︎</a></p></li>
<li id="fn9" role="doc-endnote"><p>Don’t worry: we already have short-shorts &amp; ear-<a href="https://en.wikipedia.org/wiki/United_States_Treasury_security#TIPS" data-popup-title="United States Treasury security" data-popup-author="English Wikipedia" data-popup-abstract="<p><b>United States Treasury securities</b> are government debt instruments issued by the United States Department of the Treasury to finance government spending as an alternative to taxation. Treasury securities are often referred to simply as <b>Treasurys</b>. Since 2012, U.S. government debt has been managed by the Bureau of the Fiscal Service, succeeding the Bureau of the Public Debt.</p>"><span>TIPS</span></a> to hedge against fursona inflation. That said, we advise taking a large position in equineties image macro funds to benefit from a flight to quality and herding: it’ll be a bear market for kinky bonds—and that’s no bull.<a href="#fnref9" role="doc-backlink">↩︎</a></p></li>
<li id="fn10" role="doc-endnote"><p>Some interesting references:</p>
<ul>
<li><p><span>Coevolution Of Virulence</span>:</p>
<ul>
<li><a href="https://www.gwern.net/docs/genetics/selection/1936-greenwood-experimentalepidemiology.pdf" data-popup-title="<em>Experimental Epidemiology</em>" data-popup-author="M. Greenwood, A. Bradford Hill, W. W. C. Topley, J. Wilson" data-popup-date="1936-01-01" data-popup-abstract="<p>The studies outlined in the report above have been in progress for some 15 years and they form an attempt to place the science of epidemiology on an experimental basis. They are laborious and costly and the authors justify both the labour and the expense involved in the introduction. Although it is well known that animal hosts and their microbial parasites vary in resistance and infectivity respectively, and that many other factors play their part in the form which an epidemic disease takes, when all the odd pieces of knowledge are added together the answer is only a working hypothesis and not a conclusion. In other words, the many questions regarding epidemics can only be answered by finding out actually what happens in an infected herd, not by deducing what might happen from knowledge of what occurs in individual hosts. The herd must be the universe of study.</p><p>The experiments on which the report is based have involved the use of between 100,000 and 200,000 mice, and a brief outline of the general methods of experiment are given. It has been possible to maintain herds for months or years infected with bacterial parasites such as <a href=&quot;https://en.wikipedia.org/wiki/Salmonella_enterica_subsp._enterica&quot;><em>Salmonella typhimurium</em></a> and <a href=&quot;https://en.wikipedia.org/wiki/Pasteurellosis&quot;><em>Pasteurella muriseptica</em></a> without any cross-infection and to watch the effect of various methods of interference on the spread of infection. Experiments have also been made with herds infected with the virus disease <a href=&quot;https://en.wikipedia.org/wiki/Ectromelia_virus&quot;><em>ectromelia</em></a>.</p><p>From statistical analyses of the results, it is concluded that in herds of mice living in close and continuous contact and subject to the continuous or intermittent immigration of susceptibles, the disease will never normally die out. It might happen that the disease would become extinct but such an event would be a mere accident of small numbers. The form of the mortality curve depends mainly upon the rate of immigration and the equilibrium between hosts and parasites is fundamentally unstable and, when disturbed, the system tends to pass through a period of violent fluctuations before equilibrium is again established. The average resistance of surviving mice increases with survival in the herd but never becomes absolute. The great majority eventually succumb to the reigning disease.</p><p>Selection, both by death of the more susceptible, and by natural immunization, plays a part in the increased resistance displayed by surviving mice, and the latter is probably the more important. An infected herd is a highly complex system, consisting of mice suffering from a fatal infection, others in a state of infection-equilibrium that ends in death or recovery at some later period, others undergoing natural immunization by an infection of slighter degree, and a small minority not yet infected. The differences in the form which epidemics display are due to the state of equilibrium established in this complex system, which may be shifting or temporarily stabilized.</p><p>The level of mortality in a herd, the proportion of immunizing to fatal infections, and the degree to which infection occurs, are largely determined by the characters of the bacterial strain with which the epidemic is initiated. It is considered that virulence and infectivity may vary, a highly potent &quot;epidemic&quot; strain possessing both these characters.</p><p>Apart from changes in the conditions of contact, the only significant method of interfering with the normal course of events in the infected herds is artificial immunization. It has not, however, under the conditions of these experiments, approached the successes recorded from the field. As with natural immunization, so artificial immunization has appeared to be more effective against the virus disease (<em>ectromelia</em>) than against the bacterial disease (mouse typhoid). In no case, however, is the immunity attained complete, the immunized mice eventually dying from the prevailing disease. Infection of immunized animals is common and in <em>ectromelia</em>, and probably in the bacterial diseases, many of the immunized and infected mice are infective for normal animals. It is, therefore, unlikely that, even if it were possible to devise a method of immunization more effective in lowering mortality than those employed by the authors, infection could be eliminated from the herds and so render safe the admission of susceptible immigrants.</p><p>As stated in the preface: “the experimental epidemic affords a more natural, and more severe, method of testing the value of any prophylactic procedure than assays carried out by more artificial tests on individual animals. It can never, of course, replace field observations made under completely natural conditions; but it may well indicate possible solutions to many of the more important practical problems, and so direct the field epidemiologist along the most fruitful lines of inquiry.”</p>"><em>Experimental Epidemiology</em></a>, Greenwood et al 1936 (<a href="https://www.gwern.net/docs/genetics/selection/1936-jama.pdf" data-popup-title="Experimental Epidemiology" data-popup-author="<span class=&quot;smallcaps-auto&quot;>JAMA</span> editors" data-popup-date="1936-12-01" data-popup-doi="10.1001/jama.1936.02770500037013" data-popup-abstract="<p>The study of experimental epidemics recently reported by <a href=&quot;https://www.gwern.net/docs/genetics/selection/1936-greenwood-experimentalepidemiology.pdf&quot;>Greenwood, Hill, Topley and Wilson<sup>1</sup></a> involves observations extending over some fifteen years and the use of between 100,000 and 200,000 mice. Their methods were adequately controlled and ably presented. In fact, so carefully was their technic developed that it usually proved possible to maintain herds of mice for months or years without the accidental introduction of any extraneous infection.</p><p>In one series of observations, six different epidemics of <a href=&quot;https://en.wikipedia.org/wiki/Pasteurellosis&quot;>pasteurellosis</a> were under simultaneous observation. In the long continued epidemics under these experimental conditions, no tendency for periods of high or low mortality to recur at definite seasons of the year was noted. Uncontaminated animals were introduced to many of their herds of infected mice at stated intervals. The great majority of such mice were infected shortly after entrance, so that the reacting system at any moment contained a relatively small proportion of animals presenting a virgin soil. After the first wave of disease and death that always follows the aggregation of an infected herd, the epidemics settled into a state of unstable equilibrium. With a small number of daily uninfected immigrants, the mortality curves tended to show relatively wide and relatively regular fluctuations.</p>">editorial</a>)</li>
<li><a href="https://www.gwern.net/docs/genetics/selection/1979-anderson.pdf" data-popup-title="Population biology of infectious diseases: Part I" data-popup-author="Roy M. Anderson, Robert M. May" data-popup-date="1979-08-01" data-popup-doi="10.1038/280361a0" data-popup-abstract="<p>If the host population is taken to be a dynamic variable (rather than constant, as conventionally assumed), a wider understanding of the population biology of infectious diseases emerges. In this first part of a two-part article, mathematical models are developed, shown to fit data from laboratory experiments, and used to explore the evolutionary relations among transmission parameters. In the <a href=&quot;/docs/genetics/selection/1979-may-2.pdf&quot;>second part of the article</a>, to be published in next week’s issue, the models are extended to include indirectly transmitted infections, and the general implications for infectious diseases are considered.</p><p>…The effects of micro-parasitic infections on the dynamics of animal populations depend on the ecology of the interactions between host and parasite. These patterns of disease behaviour involve 4 principal factors, namely: the host providing a habitat for the parasite; the degree to which the parasite induces host mortality (or diminishes the reproductive capability of the host); the extent to which the host acquires immunity; and the necessity of transmission from one host to the next. Overlaid on these factors are many biological complications, specific to individual host—parasite associations, whose sequential action is determined by lifecycle structure.</p><p>In the second part of this article, we show how a common set of factors are involved in the dynamics of all infectious diseases, whether they are caused by viral or helmintic agents, and whether they are transmitted directly or indirectly between hosts.</p>">“Population biology of infectious diseases: Part I”</a>/<a href="https://www.gwern.net/docs/genetics/selection/1979-may-2.pdf" data-popup-title="Population biology of infectious diseases: Part II" data-popup-author="Robert M. May, Roy M. Anderson" data-popup-date="1979-08-01" data-popup-doi="10.1038/280455a0" data-popup-abstract="<p>In the first part of this two-part article (<a href=&quot;/docs/genetics/selection/1979-anderson.pdf&quot;><em>Nature</em> 280, 361–367</a>), mathematical models of directly transmitted microparasitic infections were developed, taking explicit account of the dynamics of the host population. The discussion is now extended to both microparasites (viruses, bacteria and protozoa) and macroparasites (helminths and arthropods), transmitted either directly or indirectly via one or more intermediate hosts. Consideration is given to the relation between the ecology and evolution of the transmission processes and the overall dynamics, and to the mechanisms that can produce cyclic patterns, or multiple stable states, in the levels of infection in the host population.</p><p>…This 2-part article has blended some new theoretical studies and new analysis of existing laboratory data with a review and synthesis of past and present models for the overall transmission dynamics of parasitic infections. We have defined ‘parasite’ broadly to include viruses, bacteria and protozoans along with the more conventional helminth and arthropod parasites, and we have concentrated attention upon the circumstances under which the infection may significantly alter the growth rate of its host population.</p><p>Some of the theoretical conclusions can be pleasingly supported by hard data, while others remain more speculative. On the whole, our main goal is to help elevate the study of host—parasite population dynamics to its proper place in ecological thinking: parasites (broadly defined) are probably at least as important as the more usually-studied predators and insect parasitoids in regulating natural populations.</p>">“Part II”</a>, Anderson &amp; May 1979</li>
<li><a href="https://www.gwern.net/docs/genetics/selection/1982-anderson.pdf" data-popup-title="Coevolution of hosts and parasites" data-popup-author="R. M. Anderson, R. M. May" data-popup-date="1982" data-popup-doi="10.1017/s0031182000055360" data-popup-abstract="<p>The present paper aims to take a line that is somewhat more empirical than most of the previous theoretical work. Defining parasites broadly to include viruses, bacteria, protozoans and helminths, we observe that the virulence of a parasite (the rate at which it induces host mortality) is usually coupled with the transmission rate and with the time taken to recover by those hosts for whom the infection is not lethal. Specifically, in mice, men and other vertebrates (Fenner &amp;amp; Ratcliffe, 1965; Burnet &amp;amp; White, 1972) and in many invertebrates (Maramorosch &amp;amp; Shope, 1975; Anderson &amp;amp; May, 1981) low virulence is generally associated with effective immunological or non-specific responses which tend to suppress pathogen replication, with a concomitant reduction in transmissibility. Using data for the epidemiological parameters characterizing the various grades of <a href=&quot;https://en.wikipedia.org/wiki/Myxoma_virus&quot;>myxoma virus</a> <a href=&quot;https://en.wikipedia.org/wiki/Myxomatosis&quot;>infecting rabbits in Australia</a>, we show how in this particular case virulence maybe expected to evolve to an intermediate value; the analysis appears to accord with the observed facts. Other examples are discussed in a more qualitative way. <em>In general, we conclude that the complicated interplay between virulence and transmissibility of parasites leaves room for many coevolutionary paths to be followed,with many endpoints.</em></p>">“Coevolution of hosts and parasites”</a>, Anderson &amp; May 1982</li>
</ul></li>
<li><p><span>Passaging</span>:</p>
<ul>
<li><a href="https://www.gwern.net/docs/genetics/selection/1998-ebert.pdf" data-popup-title="Experimental Evolution of Parasites" data-popup-author="Dieter Ebert" data-popup-date="1998-11-20" data-popup-doi="10.1126/science.282.5393.1432" data-popup-abstract="<p>Serial passage experiments are a form of experimental evolution that is frequently used in applied sciences; for example, in vaccine development. During these experiments, molecular and phenotypic evolution can be monitored in real time, providing insights into the causes and consequences of parasite evolution. Within-host competition generally drives an increase in a parasite’s virulence in a new host, whereas the parasite becomes avirulent to its former host, indicating a trade-off between parasite fitnesses on different hosts. Understanding why parasite virulence seldom escalates similarly in natural populations could help us to manage virulence and deal with emerging diseases.</p>">“Experimental Evolution of Parasites”</a>, Ebert 1998</li>
<li><a href="https://www.gwern.net/docs/genetics/selection/1973-sabin.pdf" data-popup-title="History of Sabin attenuated poliovirus oral live vaccine strains" data-popup-author="A. B. Sabin, L. R. Boulger" data-popup-date="1973-01-01" data-popup-doi="10.1016/0092-1157(73)90048-6" data-popup-abstract="<p>The full data concerning the history of attenuated poliovirus strains developed by one of us (Sabin, 1965) for vaccine production do not appear in a single journal. Over the past few years we have had frequent requests for the details such as isolation and attenuation and accordingly we felt that bringing the data together in the report below would be both helpful and informative to those involved in the production and control of poliovirus vaccine (oral) prepared from these strains.</p>">“History of Sabin attenuated poliovirus oral live vaccine strains”</a>, Sabin &amp; Boulger 1973 (making <a href="https://en.wikipedia.org/wiki/Albert_Sabin" data-popup-title="Albert Sabin" data-popup-author="English Wikipedia" data-popup-abstract="<p><b>Albert Bruce Sabin</b> was a Polish American medical researcher, best known for developing the oral polio vaccine, which has played a key role in nearly eradicating the disease. In 1969–72, he served as the President of the Weizmann Institute of Science in Israel.</p>">Sabin’s</a> <a href="https://en.wikipedia.org/wiki/Polio_vaccine#Attenuated_2" data-popup-title="Polio vaccine" data-popup-author="English Wikipedia" data-popup-abstract="<p><b>Polio vaccines</b> are vaccines used to prevent poliomyelitis (polio). Two types are used: an inactivated poliovirus given by injection (<span class=&quot;smallcaps-auto&quot;>IPV</span>) and a weakened poliovirus given by mouth (<span class=&quot;smallcaps-auto&quot;>OPV</span>). The World Health Organization (<span class=&quot;smallcaps-auto&quot;>WHO</span>) recommends all children be fully vaccinated against polio. The two vaccines have eliminated polio from most of the world, and reduced the number of cases reported each year from an estimated 350,000 in 1988 to 33 in 2018.</p>">polio vaccine</a> by dozens of passages through monkeys &amp; monkey tissues)</li>
</ul></li>
</ul>
<a href="#fnref10" role="doc-backlink">↩︎</a></li>
</ol>
</section></div></div></div>
    </div>
    <footer>
        <div>created by buildstarted &copy; 2020 <a href="/about">about</a></div>
        <div>Share this page on social media: copy and paste this url https://linksfor.dev/</div>
        <div>If you prefer RSS: <a href="https://linksfor.dev/feed.xml">https://linksfor.dev/feed.xml</a></div>
    </footer>
    
    <script async defer>
        _dna = window._dna || {};
        _dna.siteId = "linksfor.devs";
        _dna.outlink = true;

        (function () {
            let dna = document.createElement('script');
            dna.type = 'text/javascript';
            dna.async = true;
            dna.src = '//dna.buildstarted.com/t.js';
            let s = document.getElementsByTagName('script')[0];
            s.parentNode.insertBefore(dna, s);
        })();
    </script>
    <noscript><img src="//dna.buildstarted.com/g?siteId=linksfor.devs"/></noscript>
</body>
</html>