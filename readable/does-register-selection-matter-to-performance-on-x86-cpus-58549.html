<!DOCTYPE html>
<html lang="en">
<head>
    <title>
Does register selection matter to performance on x86 CPUs? - linksfor.dev(s)    </title>
    <meta charset="utf-8">
    <link rel="alternate" type="application/rss+xml" title="Linksfor.dev(s) feed" href="https://linksfor.dev/feed.rss" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta name="google" value="notranslate">
    <link rel="stylesheet" type="text/css" href="/style.css" />
    <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon">
    <link rel="icon" href="/favicon.ico" type="image/x-icon">
    <meta property="og:title" content="Does register selection matter to performance on x86 CPUs? - linksfor.dev(s)"/>
    <meta property="article:author" content="fiigii.com"/>
    <meta property="og:description" content="Instruction selection is a critical portion in compilers, which different instructions could cause significantly performance difference even though the semantics not changed. Does register selection a"/>
    <meta property="og:type" content="website"/>
    <meta property="og:url" content="https://fiigii.com/2020/02/16/Does-register-selection-matter-to-performance-on-x86-CPUs/"/>

<meta property="og:site_name" content="linksfor.dev(s)" />
</head>
<body>
    <div class="devring" style="background: #222">
        <div style="text-align:center">Explore other dev related sites in this ring. If you would like to join this ring <a href="https://devring.club">click here</a>.</div>
        <div class="grid">
            <div style="display: grid; grid-template-columns: .5fr 1fr 1fr 1fr; text-align: center;">
                <span class="devring-title"><a href="https://devring.club/">devring.club</a></span>
                <a href="https://devring.club/sites/1/prev" class="devring-previous">Previous</a>
                <a href="https://devring.club/random" class="devring-random">Random</a>
                <a href="https://devring.club/sites/1/next" class="devring-next">Next</a>
            </div>
        </div>
    </div>
    <div class="grid">
        <h1>
<a href="/" style="color:inherit">linksfor.dev(s)</a>
        </h1>
        <title>linksfor.dev(s) - Does register selection matter to performance on x86 CPUs?</title>
<div class="readable">
        <h1>Does register selection matter to performance on x86 CPUs?</h1>
            <div>by fiigii.com</div>
            <div>Reading time: 9-12 minutes</div>
        <div>Posted here: 19 Feb 2020</div>
        <p><a href="https://fiigii.com/2020/02/16/Does-register-selection-matter-to-performance-on-x86-CPUs/">https://fiigii.com/2020/02/16/Does-register-selection-matter-to-performance-on-x86-CPUs/</a></p>
        <hr/>
<div id="readability-page-1" class="page"><div itemprop="articleBody">
    <p>Instruction selection is a critical portion in compilers, which different instructions could cause significantly performance difference even though the semantics not changed. Does register selection also matter to performance (assume the register selection does not lead to less or more register spills)? Honestly, I never intentionally thought of this question until I came across it on Zhihu (a Chinese Q&amp;A website). But this is a really interesting topic that reflects many tricks of assembly programming and compiler code generation. So, that deserves a blog to refresh my memory and give a share :) </p>
<p>In other words, the question is equivalent to </p>
<blockquote>
<p>Is one of the instructions below faster than another one?<br><img src="https://fiigii.com/2020/02/16/Does-register-selection-matter-to-performance-on-x86-CPUs/ADD.png" alt="">  </p>
</blockquote>
<p>And the question can be extended to any instruction of x86/x86-64 ISAs (not only on <code>ADD</code>).</p>
<p>From undergraduate classes in CS departments, we know modern computer architectures usually have a pipeline stage called <strong>register renaming</strong> that assigns real physical registers to the named logic register referred to in an assembly instruction. For example, the following code uses <code>EAX</code> twice but the two usages are not related to each other.<br></p><figure><table><tbody><tr><td><pre><span>1</span><br><span>2</span><br></pre></td><td><pre><span>ADD EDX, EAX</span><br><span>ADD EAX, EBX</span><br></pre></td></tr></tbody></table></figure>
<p>Assume this code is semantically correct. In practice, CPUs usually assign different physical registers to the two <code>EAX</code> for breaking <a href="https://en.wikipedia.org/wiki/Data_dependency#Anti-dependency" target="_blank" rel="noopener">anti-dependency</a>. So they can parallel execute on pipelined superscalar CPUs, and <code>ADD EAX, EBX</code> does not have to worry about if writing over <code>EAX</code> impacts <code>ADD EDX, EAX</code>. <strong>Therefore, we usually suppose different register names in assembly code do NOT cause performance difference on modern x86 CPUs.</strong></p>
<p>Is the story over? No.</p>
<p>The above statements only hold for general cases. There are a lot of conner cases existing in the real world that our college courses never reached. CPUs are pearls of modern engineering and industry, which also have many conner cases breaking our common sense. So, different register names will impact performance a lot, sometimes. I collected these conner cases in four categories. </p>
<blockquote>
<p>Note, the rest of the article only talks about Intel micro-architectures. </p>
</blockquote>
<h2 id="Special-Instructions"><a href="#Special-Instructions" title="Special Instructions"></a>Special Instructions</h2><p>A few instructions are executing slower with certain logic registers due to micro-architecture limitations. The most famous one is <code>LEA</code>.<br><code>LEA</code> was designed to leverage the complex and powerful x86 addressing-mode in wider areas, such as arithmetic computations. <code>LEA</code> could be executed on AGU (Address Generation Unit) and save registers from intermediate results. However, certain forms of <code>LEA</code> only can be executed on port1, which those <code>LEA</code> forms with lower ILP and higher latency are called <strong>slow LEA</strong>. According to the Intel optimization manual, using <code>EBP</code>, <code>RBP</code>, or <code>R13</code> as the base address will make LEA slower.</p>
<p><img src="https://fiigii.com/2020/02/16/Does-register-selection-matter-to-performance-on-x86-CPUs/LEA.png" alt=""></p>
<p>Although compilers could assign other registers to the base address variables, sometimes that is impossible in register allocations and there are more forms of slow LEAs that cannot be improved by register selection. Hence, in general, compilers avoid generating slow LEAs by (1) replacing <code>LEA</code> by equivalent instruction sequences that may need more temporary registers or (2) folding <code>LEA</code> into its user instructions’ addressing modes.</p>
<h2 id="Partial-Register-Stall"><a href="#Partial-Register-Stall" title="Partial Register Stall"></a>Partial Register Stall</h2><p>Most kinds of x86 registers (e.g., general-purpose registers, FLAGS, and SIMD registers, etc.) can be accessed by multiple granularities. For instance, <code>RAX</code> can be partially accessed via <code>EAX</code>, <code>AX</code>, <code>AH</code>, and <code>AL</code>. Accessing <code>AL</code> is independent of <code>AH</code> on Intel CPUs, but reading <code>EAX</code> content that written though <code>AL</code> has significant performance degradation (5-6 cycles of latency). Consequently, Intel suggests always using registers with sizes of 32- or 64-bit.</p>
<p><img src="https://fiigii.com/2020/02/16/Does-register-selection-matter-to-performance-on-x86-CPUs/PRS1.png" alt=""><br></p><figure><table><tbody><tr><td><pre><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br><span>6</span><br></pre></td><td><pre><span>MOV   AL,  BYTE PTR [RDI]</span><br><span>MOV   EBX, EAX </span><br><span></span><br><span>MOVZX EAX, BYTE PTR [RDI]</span><br><span>AND   EAX, <span>0xFFFFFF00</span></span><br><span>MOV   EBX, EAX </span><br></pre></td></tr></tbody></table></figure>
<p>Partial register stall is relatively easy to detect on general-purpose registers, but similar problems could happen on FLAGS registers and that is pretty covert. Certain instructions like <code>CMP</code> update all bits of FLAGS as the execution results, but <code>INC</code> and <code>DEC</code> write into FLAGS except <code>CF</code>. So, if <code>JCC</code> directly use FLAGS content from <code>INC</code>/<code>DEC</code>, <code>JCC</code> would possibly have false dependency from unexpected instructions.<br></p><figure><table><tbody><tr><td><pre><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br></pre></td><td><pre><span>CMP EDX, DWORD PTR [EBP]</span><br><span>...</span><br><span>INC ECX</span><br><span>JBE LBB_XXX </span><br></pre></td></tr></tbody></table></figure>
<p>Consequently, on certain Intel architectures, compilers usually do not generate <code>INC</code>/<code>DEC</code> for loop count updating (i.e., <code>i++</code> of <code>for (int i = N; i != 0; i--)</code>) or reuse the <code>INC</code>/<code>DEC</code> produced FLAGS on <code>JCC</code>. On the flip side, that would increase the code size and make I-cache issues. Fortunately, Intel has fixed the partial register stall on FLAGS since <a href="https://en.wikipedia.org/wiki/Sandy_Bridge" target="_blank" rel="noopener">SandyBridge</a>. But that still exists on most of the mainstream ATOM CPUs.</p>
<p><img src="https://fiigii.com/2020/02/16/Does-register-selection-matter-to-performance-on-x86-CPUs/PRS2.png" alt=""></p>
<p>So far, you may already think of SIMD registers. Yes, the partial register stall also occurs on SIMD registers. </p>
<p><img src="https://fiigii.com/2020/02/16/Does-register-selection-matter-to-performance-on-x86-CPUs/PRS3.png" alt=""></p>
<p>But partial SIMD/FLAGS register stall is an instruction selection issue instead of register selection. Let’s finish this section and move on.</p>
<h2 id="Architecture-Bugs"><a href="#Architecture-Bugs" title="Architecture Bugs"></a>Architecture Bugs</h2><p>Certain Intel architectures (SandyBridge, Haswell, and Skylake) have <a href="https://stackoverflow.com/questions/21390165/why-does-breaking-the-output-dependency-of-lzcnt-matter" target="_blank" rel="noopener">a bug</a> on three instructions - <code>LZCNT</code>, <code>TZCNT</code>, and <code>POPCNT</code>. These three instructions all have 2 operands (1 source register and 1 destination register), but they are different from most of the other 2-operand instructions like <code>ADD</code>. <code>ADD</code> reads its source <strong>and</strong> destination, and stores the result back to the destination register, which ADD-like instructions are called RMW (Read-Modify-Write). <code>LZCNT</code>, <code>TZCNT</code>, and <code>POPCNT</code> are not RWM that just read the source and write back to the destination. Due to some unknown reason, those Intel architectures incorrectly treat <code>LZCNT</code>, <code>TZCNT</code>, and <code>POPCNT</code> as the normal RWM instructions, which the <code>LZCNT</code>, <code>TZCNT</code>, and <code>POPCNT</code> have to wait for the computing results in both operands. Actually, only waiting for the source register getting done is enough.<br></p><figure><table><tbody><tr><td><pre><span>1</span><br><span>2</span><br><span>3</span><br></pre></td><td><pre><span>POPCNT  RCX, QWORD PTR [RDI]</span><br><span>...</span><br><span>POPCNT  RCX, QWORD PTR [RDI+<span>8</span>]</span><br></pre></td></tr></tbody></table></figure>
<p>Assume the above code is compiled from an unrolled loop that iteratively computes bit-count on an array. Since each <code>POPCNT</code> operates over a non-overlapped <code>Int64</code> element, so the two <code>POPCNT</code> should execute totally in parallel. In other words, unrolling the loop by <code>2</code> iterations can make it at least 2x faster. However, that does not happen because Intel CPUs think that the second <code>POPCNT</code> needs to read <code>RCX</code> that written by the first <code>POPCNT</code>. So, the two <code>POPCNT</code> never gets parallel running.</p>
<p>To solve this problem, we can change the <code>POPCNT</code> to use a dependency-free register as the destination, but that usually complicates the compiler’s register allocation too much. A simpler solution is to force triggering register renaming on the destination register via zeroing it.<br></p><figure><table><tbody><tr><td><pre><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br></pre></td><td><pre><span>XOR     RCX, RCX </span><br><span>POPCNT  RCX, QWORD PTR [RDI]</span><br><span>...</span><br><span>XOR     RCX, RCX </span><br><span>POPCNT  RCX, QWORD PTR [RDI+<span>8</span>]</span><br></pre></td></tr></tbody></table></figure>
<p>Zeroing <code>RCX</code> by <code>XOR RCX, RXC</code> or <code>SUB RCX, RCX</code> does not actually execute <code>XOR</code> or <code>SUB</code> operations that instructions just trigger register renaming to assign an empty register to <code>RCX</code>. Therefore, <code>XOR REG1, REG1</code> and <code>SUB REG1, REG1</code> do not reach the CPU pipeline stages behind register renaming, which makes the zeroing very cheap even though that increases CPU front-end pressures a bit.</p>
<h2 id="SIMD-Registers"><a href="#SIMD-Registers" title="SIMD Registers"></a>SIMD Registers</h2><p>Intel fulfills really awesome SIMD acceleration via SSE/AVX/AVX-512 ISA families. But there are more tricks on SIMD code generation than the scalar side. Most of the issues are not only about instruction/register selections but also impacted by instruction encoding, calling conventions, and hardware optimizations, etc.</p>
<p>Intel introduced <code>VEX</code> encoding with AVX that allows instructions to have an additional register to make the destination non-destructive. That is really good for register allocation on new SIMD instructions. However, Intel made a <code>VEX</code> counterpart for every old SSE instruction even though non-SIMD floating-point instructions. Then something gets messed up.<br></p><figure><table><tbody><tr><td><pre><span>1</span><br><span>2</span><br><span>3</span><br></pre></td><td><pre><span>MOVAPS  XMM0, XMMWORD PTR [RDI]</span><br><span>...</span><br><span>VSQRTSS XMM0, XMM0, XMM1 </span><br></pre></td></tr></tbody></table></figure>
<p><code>SQRTSS XMM0, XMM1</code> computes the square root of the floating point number in <code>XMM1</code> and writes the result into <code>XMM0</code>. The VEX version <code>VSQRTSS</code> requires 3 register operands, which copies the upper 64-bit of the second operand to the result. That makes <code>VSQRTSS</code> has additional dependencies on the second operand. For example,  in the above code, <code>VSQRTSS XMM0, XMM0, XMM1</code> has to wait for loading data into <code>XMM0</code> but that is useless for scalar floating-point code. You may think that we can let compilers always reuse the 3rd register at the 2nd position, <code>VSQRTSS XMM0, XMM1, XMM1</code>, to break the dependency. However, that does not work when the 3rd operand directly from a memory location, like <code>VSQRTSS XMM0, XMM1, XMMWORD PTR [RDI]</code>. In that situation, a better solution would insert <code>XOR</code> to trigger the register renaming for dst.</p>
<p>Usually, programmers think that using 256-bit YMM registers should get 2x faster than 128-bit XMM registers. Actually, that is not always true. Windows x64 calling conventions define <a href="https://docs.microsoft.com/en-us/cpp/build/x64-calling-convention?view=vs-2019#callercallee-saved-registers" target="_blank" rel="noopener">XMM0-XMM15 as callee saved registers</a>, so using YMM0-YMM15 would cause more caller saving code than XMM registers. Moreover, Intel only implemented store forwarding for registers &lt;= 128-bit, so that spilling YMM register could be more expensive than XMM. These additional overheads could reduce the benefits of using YMM.</p>
<h2 id="One-More-Thing"><a href="#One-More-Thing" title="One More Thing"></a>One More Thing</h2><p>Look back at the very beginning code of this post, that seems not to fall into the above categories. But the 2 lines of code still may run in different performances. In the code section below, the comments show the instruction encoding, which means the binary representation of instructions in memory. We can see using <code>ADD</code> with <code>EAX</code> as dst register is 1-byte short than another, so that has higher code density and better cache-friendly.<br></p><figure><table><tbody><tr><td><pre><span>1</span><br><span>2</span><br></pre></td><td><pre><span>ADD EAX, <span>0xffff0704</span> </span><br><span>ADD EBX, <span>0xffff0704</span> </span><br></pre></td></tr></tbody></table></figure>
<p>Consequently, even though selecting <code>EAX</code> or other registers (like <code>EBX</code>, <code>ECX</code>, <code>R8D</code>, etc.) does not directly change <code>ADD</code>‘s latency/throughput, it is also possible to impact the whole program performance.</p>

  </div></div></div>
    </div>
    <footer>
        <div>created by buildstarted &copy; 2020 <a href="/about">about</a></div>
        <div>Share this page on social media: copy and paste this url https://linksfor.dev/</div>
        <div>If you prefer RSS: <a href="https://linksfor.dev/feed.xml">https://linksfor.dev/feed.xml</a></div>
        <div>Customer satisfaction guaranteed to be optional.</div>
    </footer>
    
    <script async defer>
        _dna = window._dna || {};
        _dna.siteId = "linksfor.devs";
        _dna.outlink = true;

        (function() {
            let dna = document.createElement('script');
            dna.type = 'text/javascript';
            dna.async = true;
            dna.src = '//dna.buildstarted.com/t.js';
            let s = document.getElementsByTagName('script')[0];
            s.parentNode.insertBefore(dna, s);
        })();
    </script>
    <noscript><img src="//dna.buildstarted.com/g?siteId=linksfor.devs"/></noscript>
</body>
</html>