<!DOCTYPE html>
<html lang="en">
<head>
    <title>
r/MachineLearning - [Discussion] OpenAI should now change their name to ClosedAI - linksfor.dev(s)    </title>
    <meta charset="utf-8">
    <link rel="alternate" type="application/rss+xml" title="Linksfor.dev(s) feed" href="https://linksfor.dev/feed.rss" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta name="google" value="notranslate">
    <link rel="stylesheet" type="text/css" href="/style.css" />
    <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon">
    <link rel="icon" href="/favicon.ico" type="image/x-icon">
    <meta property="og:title" content="r/MachineLearning - [Discussion] OpenAI should now change their name to ClosedAI - linksfor.dev(s)"/>
    <meta property="og:description" content="620 votes and 222 comments so far on Reddit"/>
    <meta property="og:type" content="website"/>
    <meta property="og:url" content="https://www.reddit.com/r/MachineLearning/comments/aqwcyx/discussion_openai_should_now_change_their_name_to/"/>

<meta property="og:site_name" content="linksfor.dev(s)" />
</head>
<body>
    <div class="devring" style="background: #222">
        <div style="text-align:center">Explore other dev related sites in this ring. If you would like to join this ring <a href="https://devring.club">click here</a>.</div>
        <div class="grid">
            <div style="display: grid; grid-template-columns: .5fr 1fr 1fr 1fr; text-align: center;">
                <span class="devring-title"><a href="https://devring.club/">devring.club</a></span>
                <a href="https://devring.club/sites/1/prev" class="devring-previous">Previous</a>
                <a href="https://devring.club/random" class="devring-random">Random</a>
                <a href="https://devring.club/sites/1/next" class="devring-next">Next</a>
            </div>
        </div>
    </div>
    <div class="grid">
        <h1>
<a href="/" style="color:inherit">linksfor.dev(s)</a>
        </h1>
        <title>linksfor.dev(s) - r/MachineLearning - [Discussion] OpenAI should now change their name to ClosedAI</title>
<div class="readable">
        <h1>r/MachineLearning - [Discussion] OpenAI should now change their name to ClosedAI</h1>
            <div>Reading time: 5-6 minutes</div>
        <div>Posted here: 20 Jul 2020</div>
        <p><a href="https://www.reddit.com/r/MachineLearning/comments/aqwcyx/discussion_openai_should_now_change_their_name_to/">https://www.reddit.com/r/MachineLearning/comments/aqwcyx/discussion_openai_should_now_change_their_name_to/</a></p>
        <hr/>
<div id="readability-page-1" class="page"><div><div data-click-id="text"><div theme="[object Object]"><p>EDIT:</p><p>Someone replied to the issue, this is what was said:</p><blockquote><p>It looks like what's going on is: The layers currently enter a 'functional api construction' mode only if all of the inputs in the first argument come from other Keras layers. However, you have None included in the inputs in the first positional arg, so it's not triggering functional api construction.</p></blockquote><blockquote><p>That causes the layer to get 'inlined' in the outer functional model rather than correctly included. You should be able to work around this by changing the layer api so Nones should not get passed in.</p></blockquote><blockquote><p>We have a major cleanup/refactoring of the Functional API mostly done that make the functional api triggering much clearer (if any symbolic values appear in the inputs) &amp; sort out a number of other issues w/ it. But, that will only land in 2.4. It's not immediately obvious if we can squeeze a fix into tf 2.3 as the RC is already out.</p></blockquote><p>If you look at the notebooks, the inputs to some of the lines look like this:</p><p><code>P_outputs = P_trans11((inputHiddenVals, None, None, None))[0]</code></p><p>It looks like the issue is that the  are extra <code>None</code>s are causing disappearing variables issue, and a workaround could be just to have</p><p><code>P_outputs = P_trans11(inputHiddenVals)[0]</code></p><hr><p>tl'dr: For anyone who has used the functional api with custom layers, it might be worth running</p><pre><code>for i, var in enumerate(model.trainable_variables):
    print(model.trainable_variables[i].name)
</code></pre><p>so see if all your weights are there.</p><hr><p>Using custom layers with the functional API results in missing weights in the <code>trainable_variables</code>. Those weights are not in the <code>non_trainable_variables</code> either.</p><p>But if those weights aren't in <code>trainable_variables</code>they are essential frozen, since it is only those weights that receive gradient updates, as seen in the Keras model training code below:</p><p><a href="https://github.com/tensorflow/tensorflow/blob/1fb8f4988d69237879aac4d9e3f268f837dc0221/tensorflow/python/keras/engine/training.py#L2729" rel="noopener nofollow ugc" target="_blank">https://github.com/tensorflow/tensorflow/blob/1fb8f4988d69237879aac4d9e3f268f837dc0221/tensorflow/python/keras/engine/training.py#L2729</a></p><pre><code>  gradients = tape.gradient(loss, trainable_variables)

  # Whether to aggregate gradients outside of optimizer. This requires support
  # of the optimizer and doesn't work with ParameterServerStrategy and
  # CentralStroageStrategy.
  aggregate_grads_outside_optimizer = (
      optimizer._HAS_AGGREGATE_GRAD and  # pylint: disable=protected-access
      not isinstance(strategy.extended,
                     parameter_server_strategy.ParameterServerStrategyExtended))

  if aggregate_grads_outside_optimizer:
    # We aggregate gradients before unscaling them, in case a subclass of
    # LossScaleOptimizer all-reduces in fp16. All-reducing in fp16 can only be
    # done on scaled gradients, not unscaled gradients, for numeric stability.
    gradients = optimizer._aggregate_gradients(zip(gradients,  # pylint: disable=protected-access
                                                   trainable_variables))
  if isinstance(optimizer, lso.LossScaleOptimizer):
    gradients = optimizer.get_unscaled_gradients(gradients)
  gradients = optimizer._clip_gradients(gradients)  # pylint: disable=protected-access
  if trainable_variables:
    if aggregate_grads_outside_optimizer:
      optimizer.apply_gradients(
          zip(gradients, trainable_variables),
          experimental_aggregate_gradients=False)
    else:
      optimizer.apply_gradients(zip(gradients, trainable_variables))
</code></pre><p>The bug can be seen in this Colab gist</p><p><a href="https://colab.research.google.com/gist/Santosh-Gupta/40c54e5b76e3f522fa78da6a248b6826/missingtrainablevarsinference_var.ipynb" rel="noopener nofollow ugc" target="_blank">https://colab.research.google.com/gist/Santosh-Gupta/40c54e5b76e3f522fa78da6a248b6826/missingtrainablevarsinference_var.ipynb</a></p><p>This gist uses the transformers library to create the models so its easy to see the bug. For an in-depth look, the colab gist below creates all the custom layers from scratch</p><p><a href="https://colab.research.google.com/gist/Santosh-Gupta/aa34086a72956600910976e4f7ebe323/model_weight_debug_scratch_public_inference_var.ipynb" rel="noopener nofollow ugc" target="_blank">https://colab.research.google.com/gist/Santosh-Gupta/aa34086a72956600910976e4f7ebe323/model_weight_debug_scratch_public_inference_var.ipynb</a></p><p>As you can see in the notebooks, a workaround is to create models using keras subclassing instead; model subclassing results in all the weights appearing in <code>trainable_variables</code>. To be absolutely sure that the functional API and subclasses models are exactly the same, I ran inference on them using the same input at the bottom of each notebook; the outputs for the models were exactly the same. But training using the functional API model would treat many of the weights as frozen (and there's no way to make them unfrozen since those weights aren't registered in the <code>non_trainable_variables</code> either).</p><p>I've been looking at this for about a month, as far as I can tell, I don't think there was anything unique about the transformer layer I created; it may be the case that Any Keras model using custom sublayers and the functional API is prone to this.</p><p>I put up a Github issue 24 days ago, but I can't tell if this is something being worked on.</p><p><a href="https://github.com/tensorflow/tensorflow/issues/40638" rel="noopener nofollow ugc" target="_blank">https://github.com/tensorflow/tensorflow/issues/40638</a></p><p>If anyone else has been using the Keras functional API with custom layer, would love to hear if you're also getting the same issue when you check the trainable variables.</p></div></div></div></div></div>
    </div>
    <footer>
        <div>created by buildstarted &copy; 2020 <a href="/about">about</a></div>
        <div>Share this page on social media: copy and paste this url https://linksfor.dev/</div>
        <div>If you prefer RSS: <a href="https://linksfor.dev/feed.xml">https://linksfor.dev/feed.xml</a></div>
        <div>Customer satisfaction guaranteed to be optional.</div>
    </footer>
    
    <script async defer>
        _dna = window._dna || {};
        _dna.siteId = "linksfor.devs";
        _dna.outlink = true;

        (function () {
            let dna = document.createElement('script');
            dna.type = 'text/javascript';
            dna.async = true;
            dna.src = '//dna.buildstarted.com/t.js';
            let s = document.getElementsByTagName('script')[0];
            s.parentNode.insertBefore(dna, s);
        })();
    </script>
    <noscript><img src="//dna.buildstarted.com/g?siteId=linksfor.devs" /></noscript>
</body>
</html>