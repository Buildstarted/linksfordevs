<!DOCTYPE html>
<html lang="en">
<head>
    <title>
Still Why No HTTPS? -
linksfor.dev(s)
    </title>
	<link rel="alternate" type="application/rss+xml" title="Linksfor.dev(s) feed" href="https://linksfor.dev/feed.rss" />
    <meta charset="utf-8">
    <meta name="Description" content="A curated source of links that devs might find interesting. Updated around the clock." />
    <meta name="google" value="notranslate">
    <link rel="stylesheet" type="text/css" href="/style.css" />
</head>
<body>
    <div class="grid">
        
<div class="readable">
    <h1>Still Why No HTTPS?</h1>
    <section class="article_text post"> <p>Back in July last year, <a href="https://www.troyhunt.com/why-no-https-heres-the-worlds-largest-websites-not-redirecting-insecure-requests/">Scott Helme and I shipped a little pet project that tracked the world&apos;s largest websites not implementing HTTPS by default</a>. We called it <a href="https://whynohttps.com/">Why No HTTPS?</a> and it gave people a way to see the largest websites not taking transport layer security seriously. We also broke the list down on a country-by-country basis and it quickly became a means of highlighting security gaps and serving as a &quot;list of shame&quot;. I&apos;ve had many organisations reach out and ask to be removed once they&apos;d done their TLS things properly so clearly, the site is driving the right behaviour. Today, we&apos;re happy to share the first update since November last year.</p><h3 id="the-web-is-more-secure-more-of-the-time">The Web is More Secure More of the Time</h3><p>Let&apos;s start with the good news: since the first release of this little project, <a href="https://letsencrypt.org/stats/">HTTPS adoption has steadily trended upwards</a>:</p><p><img src="https://www.troyhunt.com/content/images/2019/12/HTTPS-Adoption.png" alt></p>
<p>We&apos;ve gone from 70% of all HTTP requests being over the secure scheme to 80% which is a pretty good effort in a relatively short amount of time. But, of course, it&apos;s the websites serving that remaining 20% of traffic that I want to focus on here. Let&apos;s being with where we source the list of top sites from and that&apos;s something we&apos;ve changed for this release.</p><h3 id="bye-bye-alexa-hello-tranco">Bye Bye Alexa, Hello Tranco</h3><p>When we launched the site, the list was based on the Alexa Top 1M. However, this list was becoming somewhat tricky to use reliably <a href="https://scotthelme.co.uk/top-1-million-analysis-september-2019/">as Scott explained in October</a>:</p><blockquote>I used to use the Alexa Top 1 Million for this research but I&apos;ve been having issues with the list. They tried to remove access at one point and while I managed to have it restored, there are other issues too. The accuracy of the data has been called into question and also the list itself has been having weird issues recently like not returning 1 million entries... Yep, that&apos;s right, the Alexa Top 1 Million list has been returning, in some cases, only ~650,000 entries recently, which is of course a problem.</blockquote><p>Consequently, there are some differences in the way sites are ranked and as a result, there are some unexpected appearances. For example, the 21st largest site on the global list is <a href="http://googletagmanager.com/">googletagmanager.com</a>. Now obviously this isn&apos;t a website in the sense that folks go there looking for useful content (many would argue quite the contrary), but based on the Tranco data it&apos;s one of the most traffic&apos;d websites in the world so it&apos;s within scope of this project.</p><p>So that&apos;s our starting point in terms of identifying which sites we assess, let&apos;s move onto the methodology around how a site ultimately makes our list. </p><h3 id="methodology-and-false-positives">Methodology and False-Positives</h3><p>A quick recap on out methodology first: Scott runs a service which indexes a whole bunch of security things on the world&apos;s top million websites each day. He publishes the results of that effort <a href="https://crawler.ninja/files/json/">via his free crawler.ninja website</a> (really Scott, .ninja?!) and I then roll the HTTP sites and HTTPS sites list into the Why No HTTPS? website. In that regard, it&apos;s quite simple. Except it&apos;s not really...</p><p>As I explained in <a href="https://www.troyhunt.com/why-no-https-questions-answered-new-data-path-forward/">this Q&amp;A blog post last year</a>, there are a whole bunch of reasons why a site that <em>you</em> see apparently doing things right might still be on our list. If you&apos;re going to chime in here with a bit of &quot;But [blah].com loads over HTTPS by default for me&quot;, do please start by reading that blog post.</p><p>Read the post? Good! What we&apos;re left with pretty much boils down to an expectation that a site responds to an HTTP request over the insecure scheme with either a 301 or 302 (ideally the former so it&apos;s a <em>permanent </em>redirect) to a secure URL (multi-hop is also ok: a 301 to an HTTP address that then 301s to an HTTPS address is fine). If I make an insecure curl request from here in Australia, for example, and I get an HTTP 401 then the site goes onto the list. There has been some dissatisfaction over this methodology due to how much website behaviour can vary from location to location, so in this update we&apos;ve added a means of getting a &quot;free pass&quot; that will automatically exclude a site from the list.</p><p>Preloaded HSTS is awesome (<a href="https://www.troyhunt.com/understanding-http-strict-transport/">here&apos;s an old blog post that explains why</a>). Once a site is pinned into the browser&apos;s static list of HSTS sites, insecure requests will <em>always</em> be upgraded and the 301 / 302 done by the website becomes redundant. Further, check out <a href="https://hstspreload.org/">the requirements to be preloaded in the first place</a>, in particular, this one:</p><blockquote><strong>Redirect</strong> from HTTP to HTTPS on the same host, if you are listening on port 80.</blockquote><p>What this means is that if a site is in <a href="https://cs.chromium.org/codesearch/f/chromium/src/net/http/transport_security_state_static.json">the preload list</a>, we&apos;re comfortable excluding it from our list of shame. A great example of this is the domain I mentioned earlier - <a href="http://googletagmanager.com/">googletagmanager.com</a>. When I curl that address insecurely, here&apos;s what happens:</p><figure class="kg-card kg-image-card"><img src="https://www.troyhunt.com/content/images/2019/12/image-5.png" class="kg-image"></figure><p>Arguably, this should keep the site in scope of being on our list but because it&apos;s been successfully preloaded and the browser simply won&apos;t allow an insecure request, it gets a free pass. Other notable &quot;free pass&quot; sites include hyatt.com (a curl for me just 301s to a www prefixed address served insecurely) and... haveibeenpwned.com:</p><figure class="kg-card kg-image-card"><img src="https://www.troyhunt.com/content/images/2019/12/image-6.png" class="kg-image"></figure><p>Over many years I&apos;ve carefully honed a bunch of Cloudflare firewall rules to identify non-browser traffic that doesn&apos;t adhere to expected norms. The response above serves a body containing anti-automation (CAPTCHA) over the same scheme the request was made to (a Cloudflare behaviour). You <em>shouldn&apos;t</em> ever get that response in an actual browser but if you did, the fact that HSTS has been preloaded for the domain for years means the request would automatically be upgraded hence HIBP is really a false positive.</p><p>This practice of giving HSTS preloaded sites a free pass is something we hope will drive <em>more </em>websites in this direction. The next time someone reaches out and claims their site is incorrectly categorised that&apos;s going to be my first response - preload your domain then the next update to the site will keep you excluded.</p><h3 id="check-the-diffs-on-github">Check the Diffs on GitHub</h3><p>Lastly, if you&apos;d like to see exactly what&apos;s changed in the data set, <a href="https://github.com/troyhunt/Why-No-HTTPS-Data">check out the public GitHub repository</a>. You&apos;ll see all the input data and all the output data, the latter being precisely the files that drive the Why No HTTPS? website. I personally find it interesting to look at diffs on files such as the <a href="https://github.com/troyhunt/Why-No-HTTPS-Data/blob/master/Output%20Files/top50-au.json">top50-au.json</a> one as it gives me a really good sense of what&apos;s changed. I&apos;ve ordered these files by domain name rather than rank to make things a little easier, but of course with ranks regularly changing anyway then the move from Alexa to Tranco there&apos;s going to be a heap of changes from last time even if the HTTPS status hasn&apos;t changed. At the very least though, it makes it super easy to see which sites have now dropped off the list altogether.</p><p>There&apos;s always a bunch of feedback on these releases and people often find really interesting things in the data. Do chime in below, keeping in mind the earlier point about reading <a href="https://www.troyhunt.com/why-no-https-questions-answered-new-data-path-forward/">the Q&amp;A blog post</a> first. And, of course, please continue to use this site as leverage to move more organisations in the &quot;secure by default&quot; direction.</p> <section class="article-open_tag"> <a class="tag" href="/tag/ssl/">SSL</a> <a class="tag" href="/tag/security/">Security</a> </section> </section>
</div>
    </div>
    <footer>
        <div>created by buildstarted &copy; 2019 <a href="/about">about</a></div>
        <div>Share this page on social media: copy and paste this url https://linksfor.dev/</div>
    </footer>
    
    <script>
        (function() {
            var COLLECT_URL = "https://dna.buildstarted.com/t";
            var SITE_ID = "linksfor.devs";
            var GLOBAL_VAR_NAME = "__DNA__";

            window[GLOBAL_VAR_NAME] = {};

            window[GLOBAL_VAR_NAME].sendPageView = function() {
                var path = location.pathname;
                var referrer = document.referrer;

                var url = COLLECT_URL + "?siteid=" + SITE_ID + "&p=" + encodeURIComponent(path) + "&r=" + encodeURIComponent(referrer);

                try { fetch(url, { method: "GET" }); } catch (e) {}
            };

            window[GLOBAL_VAR_NAME].sendPageView();
        })();
    </script>
</body>
</html>