<!DOCTYPE html>
<html lang="en">
<head>
    <title>
Open Source Virtual Background | BenTheElder - linksfor.dev(s)    </title>
    <meta charset="utf-8">
    <link rel="alternate" type="application/rss+xml" title="Linksfor.dev(s) feed" href="https://linksfor.dev/feed.rss" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta name="google" value="notranslate">
    <link rel="stylesheet" type="text/css" href="/style.css" />
    <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon">
    <link rel="icon" href="/favicon.ico" type="image/x-icon">
    <meta property="og:title" content="Open Source Virtual Background | BenTheElder - linksfor.dev(s)"/>
    <meta property="og:description" content="Linux:movie-camera:"/>
    <meta property="og:type" content="website"/>
    <meta property="og:url" content="https://elder.dev/posts/open-source-virtual-background/"/>

<meta property="og:site_name" content="linksfor.dev(s)" />
</head>
<body>
    <div class="devring" style="background: #222">
        <div style="text-align:center">Explore other dev related sites in this ring. If you would like to join this ring <a href="https://devring.club">click here</a>.</div>
        <div class="grid">
            <div style="display: grid; grid-template-columns: .5fr 1fr 1fr 1fr; text-align: center;">
                <span class="devring-title"><a href="https://devring.club/">devring.club</a></span>
                <a href="https://devring.club/sites/1/prev" class="devring-previous">Previous</a>
                <a href="https://devring.club/random" class="devring-random">Random</a>
                <a href="https://devring.club/sites/1/next" class="devring-next">Next</a>
            </div>
        </div>
    </div>
    <div class="grid">
        <h1>
<a href="/" style="color:inherit">linksfor.dev(s)</a>
        </h1>
        <title>linksfor.dev(s) - Open Source Virtual Background | BenTheElder</title>
<div class="readable">
        <h1>Open Source Virtual Background | BenTheElder</h1>
            <div>Reading time: 16-21 minutes</div>
        <div>Posted here: 10 Apr 2020</div>
        <p><a href="https://elder.dev/posts/open-source-virtual-background/">https://elder.dev/posts/open-source-virtual-background/</a></p>
        <hr/>
<div id="readability-page-1" class="page"><div id="content">
    <div>
      <p><span>Linux</span><span><span title=":movie-camera:">:movie-camera:</span></span></p><p>April 9th, 2020</p>
      <hr>
      <p>With many of us around the globe under <a href="https://www.sfchronicle.com/local-politics/article/Bay-Area-must-shelter-in-place-Only-15135014.php">shelter in place</a> due to <a href="https://www.cdc.gov/coronavirus/2019-ncov/index.html">COVID-19</a>
video calls have become a lot more common. In particular, <a href="https://zoom.us/">ZOOM</a> has
<a href="https://en.wikipedia.org/wiki/Zoom_Video_Communications#Criticism">controversially</a> become very popular. Arguably Zoom‚Äôs most interesting feature
is the ‚Äú<a href="https://support.zoom.us/hc/en-us/articles/210707503-Virtual-Background">Virtual Background</a>‚Äù support which allows users to replace
the background behind them in their webcam video feed with any image (or video).</p>
<p>I‚Äôve been using Zoom for a long time at work for Kubernetes open source meetings,
usually from my company laptop. With daily ‚Äúwork from home‚Äù I‚Äôm now inclined to
use my more powerful and ergonomic personal desktop for some of my open source work.</p>
<p>Unfortunately, Zoom‚Äôs linux client only supports the ‚Äú<a href="https://en.wikipedia.org/wiki/Chroma_key">chroma-key</a>‚Äù A.K.A. ‚Äú<a href="https://en.wikipedia.org/wiki/Green_screen_(disambiguation)">green screen</a>‚Äù
background removal method. This method requires a solid color backdrop, ideally
a green screen with uniform lighting.</p>
<p>Since I do not have a green screen I decided to simply implement my own background
removal, which was obviously better than cleaning my apartment or just using
my laptop all the time. <span title=":grin:">:grin:</span></p>
<p>It turns out we can actually get pretty decent results with off the shelf, open source
components and just a little of our own code.</p>
<h2 id="reading-the-camera">Reading The Camera<a href="#reading-the-camera" arialabel="Anchor"> üîóÔ∏é</a> </h2>
<p>First thing‚Äôs first: How are we going to get the video feed from our webcam for
processing?</p>
<p>Since I use Linux on my personal desktop (when not playing PC games) I chose to
use the <a href="https://opencv.org/">OpenCV</a> <a href="https://pypi.org/project/opencv-python/">python bindings</a> as I‚Äôm already familiar with them and they include
useful image processing primatives in addition to <a href="https://en.wikipedia.org/wiki/Video4Linux">V4L2</a> bindings for reading from
webcams.</p>
<p>Reading a frame from the webcam with <code>python-opencv</code> is very simple:</p>
<div><pre><code data-lang="python"><span>1</span><span>import</span> <span>cv2</span>
<span>2</span><span>cap</span> <span>=</span> <span>cv2</span><span>.</span><span>VideoCapture</span><span>(</span><span></span><span>'</span><span>/dev/video0</span><span>'</span><span>)</span>
<span>3</span><span>success</span><span>,</span> <span>frame</span> <span>=</span> <span>cap</span><span>.</span><span>read</span><span>(</span><span>)</span>
</code></pre></div><p>For better results with my camera before capturing set:</p>
<div><pre><code data-lang="python"><span>1</span><span># configure camera for 720p @ 60 FPS</span>
<span>2</span><span>height</span><span>,</span> <span>width</span> <span>=</span> <span>720</span><span>,</span> <span>1280</span>
<span>3</span><span>cap</span><span>.</span><span>set</span><span>(</span><span>cv2</span><span>.</span><span>CAP_PROP_FRAME_WIDTH</span> <span>,</span><span>width</span><span>)</span>
<span>4</span><span>cap</span><span>.</span><span>set</span><span>(</span><span>cv2</span><span>.</span><span>CAP_PROP_FRAME_HEIGHT</span><span>,</span><span>height</span><span>)</span>
<span>5</span><span>cap</span><span>.</span><span>set</span><span>(</span><span>cv2</span><span>.</span><span>CAP_PROP_FPS</span><span>,</span> <span>60</span><span>)</span>
</code></pre></div><p>Most video conferencing software seems to cap video to 720p @ 30 FPS or lower,
but we won‚Äôt necessarily read every frame anyhow, this sets an upper limit.</p>
<p>Put the frame capture in a loop and we‚Äôve got our video feed!</p>
<div><pre><code data-lang="python"><span>1</span><span>while</span> <span>True</span><span>:</span>
<span>2</span>    <span>success</span><span>,</span> <span>frame</span> <span>=</span> <span>cap</span><span>.</span><span>read</span><span>(</span><span>)</span>
</code></pre></div><p>We can save a test frame with just:</p>
<div><pre><code data-lang="python"><span>1</span><span>cv2</span><span>.</span><span>imwrite</span><span>(</span><span></span><span>"</span><span>test.jpg</span><span>"</span><span>,</span> <span>frame</span><span>)</span>
</code></pre></div><p>And now we can see that our camera works. Success!</p>
<figure>
    <img src="https://elder.dev/posts/open-source-virtual-background/2020-04-09-031036.jpg" alt="don&amp;rsquo;t mind my corona beard"> <figcaption>
            <p><em>don‚Äôt mind my corona beard</em></p>
        </figcaption>
</figure>

<h2 id="finding-the-background">Finding The Background<a href="#finding-the-background" arialabel="Anchor"> üîóÔ∏é</a> </h2>
<p>OK, now that we have a video feed, how do we identify the background so we can
replace it? This is the tricky part ‚Ä¶</p>
<p>While Zoom doesn‚Äôt seem to have commented anywhere about how they implemented
this, the way it behaves makes me suspect that a neural network is involved,
it‚Äôs hard to explain but the results look like one.
Additionally, I found an article about <a href="https://en.wikipedia.org/wiki/Microsoft_Teams">Microsoft Teams</a> implementing <a href="https://en.pingwest.com/a/1579">background blur</a> with a <a href="https://en.wikipedia.org/wiki/Convolutional_neural_network">convolutional neural network</a>.</p>
<p>Creating our own network wouldn‚Äôt be too hard in principle ‚Äì There are many
articles and papers on the topic of <a href="https://en.wikipedia.org/wiki/Image_segmentation">image segmentation</a> and plenty of open
source libraries and tools, but we need a fairly specialized dataset to get
good results.</p>
<p>Specifically we‚Äôd need lots of webcam like images with the ideal
human foreground marked pixel by pixel versus the background.</p>
<p>Building this sort of dataset in prepartion for training a neural net probably would
be a <em>lot</em> of work. Thankfully a research team at Google has already done all of this hard
work and open sourced a pre-trained neural network for ‚Äúperson segmentation‚Äù
called <a href="https://blog.tensorflow.org/2019/11/updated-bodypix-2.html">BodyPix</a> that works pretty well! ‚ù§Ô∏è</p>
<p>BodyPix is currently only available in <a href="https://www.tensorflow.org/js">TensorFlow.js</a> form, so the easiest
way to use it is from the <a href="https://www.npmjs.com/package/body-pix-node">body-pix-node</a> library.</p>
<p>To get faster <a href="https://blogs.nvidia.com/blog/2016/08/22/difference-deep-learning-training-inference-ai/">inference</a> (prediction) in the browser a <a href="https://en.wikipedia.org/wiki/WebGL">WebGL</a> backend is preferred, but in
<a href="https://nodejs.org/en/">node</a> we can use the <a href="https://www.tensorflow.org/install/gpu">Tensorflow GPU backend</a>
(NOTE: this requires a <a href="https://www.nvidia.com/en-us/">NVIDIA</a> Graphics Card, which I have).</p>
<p>To make this easier to setup, we‚Äôll start by setting up a small containerized
tensorflow-gpu + node environment / project. Using this with <a href="https://github.com/NVIDIA/nvidia-docker">nvidia-docker</a> is
much easier than getting all of the right dependencies setup on your host, it
only requires docker and an up-to-date GPU driver on the host.</p>

<div><pre><code data-lang="json"><span>1</span><span>{</span>
<span>2</span>    <span>"name"</span><span>:</span> <span>"bodypix"</span><span>,</span>
<span>3</span>    <span>"version"</span><span>:</span> <span>"0.0.1"</span><span>,</span>
<span>4</span>    <span>"dependencies"</span><span>:</span> <span>{</span>
<span>5</span>        <span>"@tensorflow-models/body-pix"</span><span>:</span> <span>"^2.0.5"</span><span>,</span>
<span>6</span>        <span>"@tensorflow/tfjs-node-gpu"</span><span>:</span> <span>"^1.7.1"</span>
<span>7</span>    <span>}</span>
<span>8</span><span>}</span>
</code></pre></div>
<div><p>bodypix/Dockerfile<span>Dockerfile</span></p></div>
<div><pre><code data-lang="Dockerfile"><span> 1</span><span># Base image with TensorFlow GPU requirements</span><span>
</span><span> 2</span><span></span><span>FROM</span><span> nvcr.io/nvidia/cuda:10.0-cudnn7-runtime-ubuntu18.04</span><span>
</span><span> 3</span><span></span><span># Install node</span><span>
</span><span> 4</span><span></span><span>RUN</span> apt update <span>&amp;&amp;</span> apt install -y curl make build-essential <span>
</span><span> 5</span><span></span>    <span>&amp;&amp;</span> curl -sL https://deb.nodesource.com/setup_12.x <span>|</span> bash - <span>
</span><span> 6</span><span></span>    <span>&amp;&amp;</span> apt-get -y install nodejs <span>
</span><span> 7</span><span></span>    <span>&amp;&amp;</span> mkdir /.npm <span>
</span><span> 8</span><span></span>    <span>&amp;&amp;</span> chmod <span>777</span> /.npm<span>
</span><span> 9</span><span></span><span># Ensure we can get enough GPU memory</span><span>
</span><span>10</span><span></span><span># Unfortunately tfjs-node-gpu exposes no gpu configuration :(</span><span>
</span><span>11</span><span></span><span>ENV</span> <span>TF_FORCE_GPU_ALLOW_GROWTH</span><span>=</span><span>true</span>
<span>12</span><span># Install node package dependencies</span><span>
</span><span>13</span><span></span><span>WORKDIR</span><span> /src</span><span>
</span><span>14</span><span></span><span>COPY</span> package.json /src/<span>
</span><span>15</span><span></span><span>RUN</span> npm install<span>
</span><span>16</span><span></span><span># Setup our app as the entrypoint</span><span>
</span><span>17</span><span></span><span>COPY</span> app.js /src/<span>
</span><span>18</span><span></span><span>ENTRYPOINT</span> node /src/app.js<span>
</span></code></pre></div><p>Now to serve the results‚Ä¶ <strong>WARNING</strong>: I am not a node expert! This is just
my quick evening hack, bear with me :-)</p>
<p>The following simple script replies to an HTTP <code>POST</code>ed image with a binary mask
(an 2d array of binary pixels, where zero pixels are the background).</p>






<div><pre><code data-lang="javascript"><span> 1</span><span>const</span> <span>tf</span> <span>=</span> <span>require</span><span>(</span><span>'@tensorflow/tfjs-node-gpu'</span><span>)</span><span>;</span>
<span> 2</span><span>const</span> <span>bodyPix</span> <span>=</span> <span>require</span><span>(</span><span>'@tensorflow-models/body-pix'</span><span>)</span><span>;</span>
<span> 3</span><span>const</span> <span>http</span> <span>=</span> <span>require</span><span>(</span><span>'http'</span><span>)</span><span>;</span>
<span> 4</span><span>(</span><span>async</span> <span>(</span><span>)</span> <span>=&gt;</span> <span>{</span>
<span> 5</span>    <span>const</span> <span>net</span> <span>=</span> <span>await</span> <span>bodyPix</span><span>.</span><span>load</span><span>(</span><span>{</span>
<span> 6</span>        <span>architecture</span><span>:</span> <span>'MobileNetV1'</span><span>,</span>
<span> 7</span>        <span>outputStride</span><span>:</span> <span>16</span><span>,</span>
<span> 8</span>        <span>multiplier</span><span>:</span> <span>0.75</span><span>,</span>
<span> 9</span>        <span>quantBytes</span><span>:</span> <span>2</span><span>,</span>
<span>10</span>    <span>}</span><span>)</span><span>;</span>
<span>11</span>    <span>const</span> <span>server</span> <span>=</span> <span>http</span><span>.</span><span>createServer</span><span>(</span><span>)</span><span>;</span>
<span>12</span>    <span>server</span><span>.</span><span>on</span><span>(</span><span>'request'</span><span>,</span> <span>async</span> <span>(</span><span>req</span><span>,</span> <span>res</span><span>)</span> <span>=&gt;</span> <span>{</span>
<span>13</span>        <span>var</span> <span>chunks</span> <span>=</span> <span>[</span><span>]</span><span>;</span>
<span>14</span>        <span>req</span><span>.</span><span>on</span><span>(</span><span>'data'</span><span>,</span> <span>(</span><span>chunk</span><span>)</span> <span>=&gt;</span> <span>{</span>
<span>15</span>            <span>chunks</span><span>.</span><span>push</span><span>(</span><span>chunk</span><span>)</span><span>;</span>
<span>16</span>        <span>}</span><span>)</span><span>;</span>
<span>17</span>        <span>req</span><span>.</span><span>on</span><span>(</span><span>'end'</span><span>,</span> <span>async</span> <span>(</span><span>)</span> <span>=&gt;</span> <span>{</span>
<span>18</span>            <span>const</span> <span>image</span> <span>=</span> <span>tf</span><span>.</span><span>node</span><span>.</span><span>decodeImage</span><span>(</span><span>Buffer</span><span>.</span><span>concat</span><span>(</span><span>chunks</span><span>)</span><span>)</span><span>;</span>
<span>19</span>            <span>segmentation</span> <span>=</span> <span>await</span> <span>net</span><span>.</span><span>segmentPerson</span><span>(</span><span>image</span><span>,</span> <span>{</span>
<span>20</span>                <span>flipHorizontal</span><span>:</span> <span>false</span><span>,</span>
<span>21</span>                <span>internalResolution</span><span>:</span> <span>'medium'</span><span>,</span>
<span>22</span>                <span>segmentationThreshold</span><span>:</span> <span>0.7</span><span>,</span>
<span>23</span>            <span>}</span><span>)</span><span>;</span>
<span>24</span>            <span>res</span><span>.</span><span>writeHead</span><span>(</span><span>200</span><span>,</span> <span>{</span> <span>'Content-Type'</span><span>:</span> <span>'application/octet-stream'</span> <span>}</span><span>)</span><span>;</span>
<span>25</span>            <span>res</span><span>.</span><span>write</span><span>(</span><span>Buffer</span><span>.</span><span>from</span><span>(</span><span>segmentation</span><span>.</span><span>data</span><span>)</span><span>)</span><span>;</span>
<span>26</span>            <span>res</span><span>.</span><span>end</span><span>(</span><span>)</span><span>;</span>
<span>27</span>            <span>tf</span><span>.</span><span>dispose</span><span>(</span><span>image</span><span>)</span><span>;</span>
<span>28</span>        <span>}</span><span>)</span><span>;</span>
<span>29</span>    <span>}</span><span>)</span><span>;</span>
<span>30</span>    <span>server</span><span>.</span><span>listen</span><span>(</span><span>9000</span><span>)</span><span>;</span>
<span>31</span><span>}</span><span>)</span><span>(</span><span>)</span><span>;</span>
</code></pre></div>
<p>We can use <a href="https://numpy.org/">numpy</a> and <a href="https://requests.readthedocs.io/en/master/">requests</a> to convert a frame to a mask from our
python script with the following method:</p>
<div><pre><code data-lang="python"><span> 1</span><span>def</span> <span>get_mask</span><span>(</span><span>frame</span><span>,</span> <span>bodypix_url</span><span>=</span><span></span><span>'</span><span>http://localhost:9000</span><span>'</span><span>)</span><span>:</span>
<span> 2</span>    <span>_</span><span>,</span> <span>data</span> <span>=</span> <span>cv2</span><span>.</span><span>imencode</span><span>(</span><span></span><span>"</span><span>.jpg</span><span>"</span><span>,</span> <span>frame</span><span>)</span>
<span> 3</span>    <span>r</span> <span>=</span> <span>requests</span><span>.</span><span>post</span><span>(</span>
<span> 4</span>        <span>url</span><span>=</span><span>bodypix_url</span><span>,</span>
<span> 5</span>        <span>data</span><span>=</span><span>data</span><span>.</span><span>tobytes</span><span>(</span><span>)</span><span>,</span>
<span> 6</span>        <span>headers</span><span>=</span><span>{</span><span></span><span>'</span><span>Content-Type</span><span>'</span><span>:</span> <span></span><span>'</span><span>application/octet-stream</span><span>'</span><span>}</span><span>)</span>
<span> 7</span>    <span># convert raw bytes to a numpy array</span>
<span> 8</span>    <span># raw data is uint8[width * height] with value 0 or 1</span>
<span> 9</span>    <span>mask</span> <span>=</span> <span>np</span><span>.</span><span>frombuffer</span><span>(</span><span>r</span><span>.</span><span>content</span><span>,</span> <span>dtype</span><span>=</span><span>np</span><span>.</span><span>uint8</span><span>)</span>
<span>10</span>    <span>mask</span> <span>=</span> <span>mask</span><span>.</span><span>reshape</span><span>(</span><span>(</span><span>frame</span><span>.</span><span>shape</span><span>[</span><span>0</span><span>]</span><span>,</span> <span>frame</span><span>.</span><span>shape</span><span>[</span><span>1</span><span>]</span><span>)</span><span>)</span>
<span>11</span>    <span>return</span> <span>mask</span>
</code></pre></div><p>Which gives us a result something like:</p>
<p><img src="https://elder.dev/posts/open-source-virtual-background/mask.jpg"></p><p>While I was working on this, I spotted this tweet:</p>
<blockquote><p lang="en" dir="ltr">This is definitely the BEST background for video calls. üíØ <a href="https://t.co/Urz62Kg32k">pic.twitter.com/Urz62Kg32k</a></p>
<img src="https://pbs.twimg.com/media/EUn1cBxUwAAi6gv?format=jpg&amp;name=large">
‚Äî Ashley Willis (McNamara) (@ashleymcnamara) <a href="https://twitter.com/ashleymcnamara/status/1245796639408701440?ref_src=twsrc%5Etfw">April 2, 2020</a></blockquote>
<p>Now that we have the foreground / background mask, it will be easy to replace
the background.</p>
<p>After grabbing the awesome ‚ÄúVirtual Background‚Äù picture from that twitter thread and
cropping it to a 16:9 ratio image ‚Ä¶</p>
<p><img src="https://elder.dev/posts/open-source-virtual-background/background.jpg"></p><p>‚Ä¶ we can do the following:</p>
<div><pre><code data-lang="python"><span> 1</span><span># read in a "virtual background" (should be in 16:9 ratio)</span>
<span> 2</span><span>replacement_bg_raw</span> <span>=</span> <span>cv2</span><span>.</span><span>imread</span><span>(</span><span></span><span>"</span><span>background.jpg</span><span>"</span><span>)</span>
<span> 3</span>
<span> 4</span><span># resize to match the frame (width &amp; height from before)</span>
<span> 5</span><span>width</span><span>,</span> <span>height</span> <span>=</span> <span>720</span><span>,</span> <span>1280</span>
<span> 6</span><span>replacement_bg</span> <span>=</span> <span>cv2</span><span>.</span><span>resize</span><span>(</span><span>replacement_bg_raw</span><span>,</span> <span>(</span><span>width</span><span>,</span> <span>height</span><span>)</span><span>)</span>
<span> 7</span>
<span> 8</span><span># combine the background and foreground, using the mask and its inverse</span>
<span> 9</span><span>inv_mask</span> <span>=</span> <span>1</span><span>-</span><span>mask</span>
<span>10</span><span>for</span> <span>c</span> <span>in</span> <span>range</span><span>(</span><span>frame</span><span>.</span><span>shape</span><span>[</span><span>2</span><span>]</span><span>)</span><span>:</span>
<span>11</span>    <span>frame</span><span>[</span><span>:</span><span>,</span><span>:</span><span>,</span><span>c</span><span>]</span> <span>=</span> <span>frame</span><span>[</span><span>:</span><span>,</span><span>:</span><span>,</span><span>c</span><span>]</span><span>*</span><span>mask</span> <span>+</span> <span>replacement_bg</span><span>[</span><span>:</span><span>,</span><span>:</span><span>,</span><span>c</span><span>]</span><span>*</span><span>inv_mask</span>
</code></pre></div><p>Which gives us:</p>
<p><img src="https://elder.dev/posts/open-source-virtual-background/masked.jpg"></p><p>The raw mask is clearly not tight enough due to the performance trade-offs
we made with our BodyPix parameters but .. so far so good!</p>
<p>This background gave me an idea ‚Ä¶</p>
<h2 id="making-it-fun">Making It Fun<a href="#making-it-fun" arialabel="Anchor"> üîóÔ∏é</a> </h2>
<p>Now that we have the masking done, what can we do to make it look better?</p>
<p>The first obvious step is to smooth the mask out, with something like:</p>
<div><pre><code data-lang="python"><span>1</span><span>def</span> <span>post_process_mask</span><span>(</span><span>mask</span><span>)</span><span>:</span>
<span>2</span>    <span>mask</span> <span>=</span> <span>cv2</span><span>.</span><span>dilate</span><span>(</span><span>mask</span><span>,</span> <span>np</span><span>.</span><span>ones</span><span>(</span><span>(</span><span>10</span><span>,</span><span>10</span><span>)</span><span>,</span> <span>np</span><span>.</span><span>uint8</span><span>)</span> <span>,</span> <span>iterations</span><span>=</span><span>1</span><span>)</span>
<span>3</span>    <span>mask</span> <span>=</span> <span>cv2</span><span>.</span><span>erode</span><span>(</span><span>mask</span><span>,</span> <span>np</span><span>.</span><span>ones</span><span>(</span><span>(</span><span>10</span><span>,</span><span>10</span><span>)</span><span>,</span> <span>np</span><span>.</span><span>uint8</span><span>)</span> <span>,</span> <span>iterations</span><span>=</span><span>1</span><span>)</span>
<span>4</span>    <span>return</span> <span>mask</span>
</code></pre></div><p>This can help a bit, but it‚Äôs pretty minor and just replacing the background
is a little boring, since we‚Äôve hacked this up ourselves we can do anything
instead of just a basic background removal ‚Ä¶</p>
<p>Given that we‚Äôre using a Star Wars ‚Äúvirtual background‚Äù I decided to create
hologram effect to fit in better. This also lets lean into blurring the mask.</p>
<p>First update the post processing to:</p>
<div><pre><code data-lang="python"><span>1</span><span>def</span> <span>post_process_mask</span><span>(</span><span>mask</span><span>)</span><span>:</span>
<span>2</span>    <span>mask</span> <span>=</span> <span>cv2</span><span>.</span><span>dilate</span><span>(</span><span>mask</span><span>,</span> <span>np</span><span>.</span><span>ones</span><span>(</span><span>(</span><span>10</span><span>,</span><span>10</span><span>)</span><span>,</span> <span>np</span><span>.</span><span>uint8</span><span>)</span> <span>,</span> <span>iterations</span><span>=</span><span>1</span><span>)</span>
<span>3</span>    <span>mask</span> <span>=</span> <span>cv2</span><span>.</span><span>blur</span><span>(</span><span>mask</span><span>.</span><span>astype</span><span>(</span><span>float</span><span>)</span><span>,</span> <span>(</span><span>30</span><span>,</span><span>30</span><span>)</span><span>)</span>
<span>4</span>    <span>return</span> <span>mask</span>
</code></pre></div><p>Now the edges are blurry which is good, but we need to start building the hologram
effect.</p>
<p>Hollywood holograms typically have the following properties:</p>
<ul>
<li>washed out / monocrhomatic color, as if done with a bright laser</li>
<li>scan lines or a grid like effect, as if many beams created the image</li>
<li>‚Äúghosting‚Äù as if the projection is done in layers or imperfectly reaching the correct distance</li>
</ul>
<p>We can add these step by step.</p>
<p>First for the blue tint we just need to apply an OpenCV colormap:</p>
<div><pre><code data-lang="python"><span>1</span><span># map the frame into a blue-green colorspace</span>
<span>2</span><span>holo</span> <span>=</span> <span>cv2</span><span>.</span><span>applyColorMap</span><span>(</span><span>frame</span><span>,</span> <span>cv2</span><span>.</span><span>COLORMAP_WINTER</span><span>)</span>
</code></pre></div><p>Then we can add the scan lines with a halftone-like effect:</p>
<div><pre><code data-lang="python"><span>1</span><span># for every bandLength rows darken to 10-30% brightness,</span>
<span>2</span><span># then don't touch for bandGap rows.</span>
<span>3</span><span>bandLength</span><span>,</span> <span>bandGap</span> <span>=</span> <span>2</span><span>,</span> <span>3</span>
<span>4</span><span>for</span> <span>y</span> <span>in</span> <span>range</span><span>(</span><span>holo</span><span>.</span><span>shape</span><span>[</span><span>0</span><span>]</span><span>)</span><span>:</span>
<span>5</span>    <span>if</span> <span>y</span> <span>%</span> <span>(</span><span>bandLength</span><span>+</span><span>bandGap</span><span>)</span> <span>&lt;</span> <span>bandLength</span><span>:</span>
<span>6</span>        <span>holo</span><span>[</span><span>y</span><span>,</span><span>:</span><span>,</span><span>:</span><span>]</span> <span>=</span> <span>holo</span><span>[</span><span>y</span><span>,</span><span>:</span><span>,</span><span>:</span><span>]</span> <span>*</span> <span>np</span><span>.</span><span>random</span><span>.</span><span>uniform</span><span>(</span><span>0.1</span><span>,</span> <span>0.3</span><span>)</span>
</code></pre></div><p>Next we can add some ghosting by adding weighted copies of the current effect,
shifted along an axis:</p>
<div><pre><code data-lang="python"><span> 1</span><span># shift_img from: https://stackoverflow.com/a/53140617</span>
<span> 2</span><span>def</span> <span>shift_img</span><span>(</span><span>img</span><span>,</span> <span>dx</span><span>,</span> <span>dy</span><span>)</span><span>:</span>
<span> 3</span>    <span>img</span> <span>=</span> <span>np</span><span>.</span><span>roll</span><span>(</span><span>img</span><span>,</span> <span>dy</span><span>,</span> <span>axis</span><span>=</span><span>0</span><span>)</span>
<span> 4</span>    <span>img</span> <span>=</span> <span>np</span><span>.</span><span>roll</span><span>(</span><span>img</span><span>,</span> <span>dx</span><span>,</span> <span>axis</span><span>=</span><span>1</span><span>)</span>
<span> 5</span>    <span>if</span> <span>dy</span><span>&gt;</span><span>0</span><span>:</span>
<span> 6</span>        <span>img</span><span>[</span><span>:</span><span>dy</span><span>,</span> <span>:</span><span>]</span> <span>=</span> <span>0</span>
<span> 7</span>    <span>elif</span> <span>dy</span><span>&lt;</span><span>0</span><span>:</span>
<span> 8</span>        <span>img</span><span>[</span><span>dy</span><span>:</span><span>,</span> <span>:</span><span>]</span> <span>=</span> <span>0</span>
<span> 9</span>    <span>if</span> <span>dx</span><span>&gt;</span><span>0</span><span>:</span>
<span>10</span>        <span>img</span><span>[</span><span>:</span><span>,</span> <span>:</span><span>dx</span><span>]</span> <span>=</span> <span>0</span>
<span>11</span>    <span>elif</span> <span>dx</span><span>&lt;</span><span>0</span><span>:</span>
<span>12</span>        <span>img</span><span>[</span><span>:</span><span>,</span> <span>dx</span><span>:</span><span>]</span> <span>=</span> <span>0</span>
<span>13</span>    <span>return</span> <span>img</span>
<span>14</span>
<span>15</span><span># the first one is roughly: holo * 0.2 + shifted_holo * 0.8 + 0</span>
<span>16</span><span>holo2</span> <span>=</span> <span>cv2</span><span>.</span><span>addWeighted</span><span>(</span><span>holo</span><span>,</span> <span>0.2</span><span>,</span> <span>shift_img</span><span>(</span><span>holo1</span><span>.</span><span>copy</span><span>(</span><span>)</span><span>,</span> <span>5</span><span>,</span> <span>5</span><span>)</span><span>,</span> <span>0.8</span><span>,</span> <span>0</span><span>)</span>
<span>17</span><span>holo2</span> <span>=</span> <span>cv2</span><span>.</span><span>addWeighted</span><span>(</span><span>holo2</span><span>,</span> <span>0.4</span><span>,</span> <span>shift_img</span><span>(</span><span>holo1</span><span>.</span><span>copy</span><span>(</span><span>)</span><span>,</span> <span>-</span><span>5</span><span>,</span> <span>-</span><span>5</span><span>)</span><span>,</span> <span>0.6</span><span>,</span> <span>0</span><span>)</span>
</code></pre></div><p>Last: We‚Äôll want to keep <em>some</em> of the original color, so let‚Äôs combine
the holo effect with the original frame similar to how we added the ghosting:</p>
<div><pre><code data-lang="python"><span>1</span><span>holo_done</span> <span>=</span> <span>cv2</span><span>.</span><span>addWeighted</span><span>(</span><span>img</span><span>,</span> <span>0.5</span><span>,</span> <span>holo2</span><span>,</span> <span>0.6</span><span>,</span> <span>0</span><span>)</span>
</code></pre></div><p>A frame with the hologram effect now looks like:</p>
<p><img src="https://elder.dev/posts/open-source-virtual-background/holo-frame.jpg"></p><p>On it‚Äôs own this looks pretty <span title=":shrug:">:shrug:</span></p>
<p>But combined with our virtual background it looks more like:</p>
<p><img src="https://elder.dev/posts/open-source-virtual-background/holo-masked.jpg"></p><p>There we go! <span title=":tada:">:tada:</span> (I promise it looks cooler with motion / video <span title=":upside_down_face:">:upside_down_face:</span>)</p>
<h2 id="outputting-video">Outputting Video<a href="#outputting-video" arialabel="Anchor"> üîóÔ∏é</a> </h2>
<p>Now we‚Äôre just missing one thing ‚Ä¶ We can‚Äôt actually use this in a call yet.</p>
<p>To fix that, we‚Äôre going to use <a href="https://github.com/jremmons/pyfakewebcam">pyfakewebcam</a> and <a href="https://github.com/umlaeute/v4l2loopback">v4l2loopback</a> to create a fake webcam device.</p>
<p>We‚Äôre also going to actually wire this all up with docker.</p>
<p>First create a <code>requirements.txt</code> with our dependencies:</p>





<div><p>fakecam/requirements.txt<span>Dockerfile</span></p></div>
<div><pre><code data-lang="Dockerfile"><span>1</span><span>numpy</span><span>=</span><span>=</span>1.18.2<span>
</span><span>2</span><span></span>opencv-python<span>=</span><span>=</span>4.2.0.32<span>
</span><span>3</span><span></span><span>requests</span><span>=</span><span>=</span>2.23.0<span>
</span><span>4</span><span></span><span>pyfakewebcam</span><span>=</span><span>=</span>0.1.0</code></pre></div>
<p>And then the <code>Dockerfile</code> for the fake camera app:</p>





<div><p>fakecam/Dockerfile<span>Dockerfile</span></p></div>
<div><pre><code data-lang="Dockerfile"><span> 1</span><span>FROM</span><span> python:3-buster</span><span>
</span><span> 2</span><span></span><span># ensure pip is up to date</span><span>
</span><span> 3</span><span></span><span>RUN</span> pip install --upgrade pip<span>
</span><span> 4</span><span></span><span># install opencv dependencies</span><span>
</span><span> 5</span><span></span><span>RUN</span> apt-get update <span>&amp;&amp;</span> <span>
</span><span> 6</span><span></span>    apt-get install -y <span>
</span><span> 7</span><span></span>      <span>`</span><span># opencv requirements` </span>
<span> 8</span>      libsm6 libxext6 libxrender-dev <span>
</span><span> 9</span><span></span>      <span>`</span><span># opencv video opening requirements` </span>
<span>10</span>      libv4l-dev<span>
</span><span>11</span><span></span><span># install our requirements</span><span>
</span><span>12</span><span></span><span>WORKDIR</span><span> /src</span><span>
</span><span>13</span><span></span><span>COPY</span> requirements.txt /src/<span>
</span><span>14</span><span></span><span>RUN</span> pip install --no-cache-dir -r /src/requirements.txt<span>
</span><span>15</span><span></span><span># copy in the virtual background</span><span>
</span><span>16</span><span></span><span>COPY</span> background.jpg /data/<span>
</span><span>17</span><span></span><span># run our fake camera script (with unbuffered output for easier debug)</span><span>
</span><span>18</span><span></span><span>COPY</span> fake.py /src/<span>
</span><span>19</span><span></span><span>ENTRYPOINT</span> python -u fake.py</code></pre></div>
<p>We‚Äôre going to need to install <code>v4l2loopback</code> from a shell:</p>
<div><pre><code data-lang="shell"><span>1</span>sudo apt install v4l2loopback-dkms
</code></pre></div><p>And then configure a fake camera device:</p>
<div><pre><code data-lang="shell"><span>1</span>sudo modprobe -r v4l2loopback
<span>2</span>sudo modprobe v4l2loopback <span>devices</span><span>=</span><span>1</span> <span>video_nr</span><span>=</span><span>20</span> <span>card_label</span><span>=</span><span>"v4l2loopback"</span> <span>exclusive_caps</span><span>=</span><span>1</span>
</code></pre></div><p>We need the <code>exclusive_caps</code> setting for some apps (chrome, zoom) to work, the label
is just for our convenience when selecting the camera in apps, and the video number
just makes this <code>/dev/video20</code> if available, which is unlikely to be already in use.</p>
<p>Now we can update our script to create the fake camera:</p>
<div><pre><code data-lang="python"><span>1</span><span># again use width, height from before</span>
<span>2</span><span>fake</span> <span>=</span> <span>pyfakewebcam</span><span>.</span><span>FakeWebcam</span><span>(</span><span></span><span>'</span><span>/dev/video20</span><span>'</span><span>,</span> <span>width</span><span>,</span> <span>height</span><span>)</span>
</code></pre></div><p>We also need to note that <code>pyfakewebcam</code> expects images in <code>RGB</code> (red, green, blue)
while our OpenCV operations are in <code>BGR</code> (blue, green, red) channel order.</p>
<p>We can fix this before outputting and then send a frame with:</p>
<div><pre><code data-lang="python"><span>1</span><span>frame</span> <span>=</span> <span>cv2</span><span>.</span><span>cvtColor</span><span>(</span><span>frame</span><span>,</span> <span>cv2</span><span>.</span><span>COLOR_BGR2RGB</span><span>)</span>
<span>2</span><span>fake</span><span>.</span><span>schedule_frame</span><span>(</span><span>frame</span><span>)</span>
</code></pre></div><p>All together the script looks like:</p>






<div><pre><code data-lang="Python"><span> 1</span><span>import</span> <span>os</span>
<span> 2</span><span>import</span> <span>cv2</span>
<span> 3</span><span>import</span> <span>numpy</span> <span>as</span> <span>np</span>
<span> 4</span><span>import</span> <span>requests</span>
<span> 5</span><span>import</span> <span>pyfakewebcam</span>
<span> 6</span>
<span> 7</span><span>def</span> <span>get_mask</span><span>(</span><span>frame</span><span>,</span> <span>bodypix_url</span><span>=</span><span></span><span>'</span><span>http://localhost:9000</span><span>'</span><span>)</span><span>:</span>
<span> 8</span>    <span>_</span><span>,</span> <span>data</span> <span>=</span> <span>cv2</span><span>.</span><span>imencode</span><span>(</span><span></span><span>"</span><span>.jpg</span><span>"</span><span>,</span> <span>frame</span><span>)</span>
<span> 9</span>    <span>r</span> <span>=</span> <span>requests</span><span>.</span><span>post</span><span>(</span>
<span>10</span>        <span>url</span><span>=</span><span>bodypix_url</span><span>,</span>
<span>11</span>        <span>data</span><span>=</span><span>data</span><span>.</span><span>tobytes</span><span>(</span><span>)</span><span>,</span>
<span>12</span>        <span>headers</span><span>=</span><span>{</span><span></span><span>'</span><span>Content-Type</span><span>'</span><span>:</span> <span></span><span>'</span><span>application/octet-stream</span><span>'</span><span>}</span><span>)</span>
<span>13</span>    <span>mask</span> <span>=</span> <span>np</span><span>.</span><span>frombuffer</span><span>(</span><span>r</span><span>.</span><span>content</span><span>,</span> <span>dtype</span><span>=</span><span>np</span><span>.</span><span>uint8</span><span>)</span>
<span>14</span>    <span>mask</span> <span>=</span> <span>mask</span><span>.</span><span>reshape</span><span>(</span><span>(</span><span>frame</span><span>.</span><span>shape</span><span>[</span><span>0</span><span>]</span><span>,</span> <span>frame</span><span>.</span><span>shape</span><span>[</span><span>1</span><span>]</span><span>)</span><span>)</span>
<span>15</span>    <span>return</span> <span>mask</span>
<span>16</span>
<span>17</span><span>def</span> <span>post_process_mask</span><span>(</span><span>mask</span><span>)</span><span>:</span>
<span>18</span>    <span>mask</span> <span>=</span> <span>cv2</span><span>.</span><span>dilate</span><span>(</span><span>mask</span><span>,</span> <span>np</span><span>.</span><span>ones</span><span>(</span><span>(</span><span>10</span><span>,</span><span>10</span><span>)</span><span>,</span> <span>np</span><span>.</span><span>uint8</span><span>)</span> <span>,</span> <span>iterations</span><span>=</span><span>1</span><span>)</span>
<span>19</span>    <span>mask</span> <span>=</span> <span>cv2</span><span>.</span><span>blur</span><span>(</span><span>mask</span><span>.</span><span>astype</span><span>(</span><span>float</span><span>)</span><span>,</span> <span>(</span><span>30</span><span>,</span><span>30</span><span>)</span><span>)</span>
<span>20</span>    <span>return</span> <span>mask</span>
<span>21</span>
<span>22</span><span>def</span> <span>shift_img</span><span>(</span><span>img</span><span>,</span> <span>dx</span><span>,</span> <span>dy</span><span>)</span><span>:</span>
<span>23</span>    <span>img</span> <span>=</span> <span>np</span><span>.</span><span>roll</span><span>(</span><span>img</span><span>,</span> <span>dy</span><span>,</span> <span>axis</span><span>=</span><span>0</span><span>)</span>
<span>24</span>    <span>img</span> <span>=</span> <span>np</span><span>.</span><span>roll</span><span>(</span><span>img</span><span>,</span> <span>dx</span><span>,</span> <span>axis</span><span>=</span><span>1</span><span>)</span>
<span>25</span>    <span>if</span> <span>dy</span><span>&gt;</span><span>0</span><span>:</span>
<span>26</span>        <span>img</span><span>[</span><span>:</span><span>dy</span><span>,</span> <span>:</span><span>]</span> <span>=</span> <span>0</span>
<span>27</span>    <span>elif</span> <span>dy</span><span>&lt;</span><span>0</span><span>:</span>
<span>28</span>        <span>img</span><span>[</span><span>dy</span><span>:</span><span>,</span> <span>:</span><span>]</span> <span>=</span> <span>0</span>
<span>29</span>    <span>if</span> <span>dx</span><span>&gt;</span><span>0</span><span>:</span>
<span>30</span>        <span>img</span><span>[</span><span>:</span><span>,</span> <span>:</span><span>dx</span><span>]</span> <span>=</span> <span>0</span>
<span>31</span>    <span>elif</span> <span>dx</span><span>&lt;</span><span>0</span><span>:</span>
<span>32</span>        <span>img</span><span>[</span><span>:</span><span>,</span> <span>dx</span><span>:</span><span>]</span> <span>=</span> <span>0</span>
<span>33</span>    <span>return</span> <span>img</span>
<span>34</span>
<span>35</span><span>def</span> <span>hologram_effect</span><span>(</span><span>img</span><span>)</span><span>:</span>
<span>36</span>    <span># add a blue tint</span>
<span>37</span>    <span>holo</span> <span>=</span> <span>cv2</span><span>.</span><span>applyColorMap</span><span>(</span><span>img</span><span>,</span> <span>cv2</span><span>.</span><span>COLORMAP_WINTER</span><span>)</span>
<span>38</span>    <span># add a halftone effect</span>
<span>39</span>    <span>bandLength</span><span>,</span> <span>bandGap</span> <span>=</span> <span>2</span><span>,</span> <span>3</span>
<span>40</span>    <span>for</span> <span>y</span> <span>in</span> <span>range</span><span>(</span><span>holo</span><span>.</span><span>shape</span><span>[</span><span>0</span><span>]</span><span>)</span><span>:</span>
<span>41</span>        <span>if</span> <span>y</span> <span>%</span> <span>(</span><span>bandLength</span><span>+</span><span>bandGap</span><span>)</span> <span>&lt;</span> <span>bandLength</span><span>:</span>
<span>42</span>            <span>holo</span><span>[</span><span>y</span><span>,</span><span>:</span><span>,</span><span>:</span><span>]</span> <span>=</span> <span>holo</span><span>[</span><span>y</span><span>,</span><span>:</span><span>,</span><span>:</span><span>]</span> <span>*</span> <span>np</span><span>.</span><span>random</span><span>.</span><span>uniform</span><span>(</span><span>0.1</span><span>,</span> <span>0.3</span><span>)</span>
<span>43</span>    <span># add some ghosting</span>
<span>44</span>    <span>holo_blur</span> <span>=</span> <span>cv2</span><span>.</span><span>addWeighted</span><span>(</span><span>holo</span><span>,</span> <span>0.2</span><span>,</span> <span>shift_image</span><span>(</span><span>holo</span><span>.</span><span>copy</span><span>(</span><span>)</span><span>,</span> <span>5</span><span>,</span> <span>5</span><span>)</span><span>,</span> <span>0.8</span><span>,</span> <span>0</span><span>)</span>
<span>45</span>    <span>holo_blur</span> <span>=</span> <span>cv2</span><span>.</span><span>addWeighted</span><span>(</span><span>holo_blur</span><span>,</span> <span>0.4</span><span>,</span> <span>shift_image</span><span>(</span><span>holo</span><span>.</span><span>copy</span><span>(</span><span>)</span><span>,</span> <span>-</span><span>5</span><span>,</span> <span>-</span><span>5</span><span>)</span><span>,</span> <span>0.6</span><span>,</span> <span>0</span><span>)</span>
<span>46</span>    <span># combine with the original color, oversaturated</span>
<span>47</span>    <span>out</span> <span>=</span> <span>cv2</span><span>.</span><span>addWeighted</span><span>(</span><span>img</span><span>,</span> <span>0.5</span><span>,</span> <span>holo_blur</span><span>,</span> <span>0.6</span><span>,</span> <span>0</span><span>)</span>
<span>48</span>    <span>return</span> <span>out</span>
<span>49</span>
<span>50</span><span>def</span> <span>get_frame</span><span>(</span><span>cap</span><span>,</span> <span>background_scaled</span><span>)</span><span>:</span>
<span>51</span>    <span>_</span><span>,</span> <span>frame</span> <span>=</span> <span>cap</span><span>.</span><span>read</span><span>(</span><span>)</span>
<span>52</span>    <span># fetch the mask with retries (the app needs to warmup and we're lazy)</span>
<span>53</span>    <span># e v e n t u a l l y c o n s i s t e n t</span>
<span>54</span>    <span>mask</span> <span>=</span> <span>None</span>
<span>55</span>    <span>while</span> <span>mask</span> <span>is</span> <span>None</span><span>:</span>
<span>56</span>        <span>try</span><span>:</span>
<span>57</span>            <span>mask</span> <span>=</span> <span>get_mask</span><span>(</span><span>frame</span><span>)</span>
<span>58</span>        <span>except</span><span>:</span>
<span>59</span>            <span>print</span><span>(</span><span></span><span>"</span><span>mask request failed, retrying</span><span>"</span><span>)</span>
<span>60</span>    <span># post-process mask and frame</span>
<span>61</span>    <span>mask</span> <span>=</span> <span>post_process_mask</span><span>(</span><span>mask</span><span>)</span>
<span>62</span>    <span>frame</span> <span>=</span> <span>hologram_effect</span><span>(</span><span>frame</span><span>)</span>
<span>63</span>    <span># composite the foreground and background</span>
<span>64</span>    <span>inv_mask</span> <span>=</span> <span>1</span><span>-</span><span>mask</span>
<span>65</span>    <span>for</span> <span>c</span> <span>in</span> <span>range</span><span>(</span><span>frame</span><span>.</span><span>shape</span><span>[</span><span>2</span><span>]</span><span>)</span><span>:</span>
<span>66</span>        <span>frame</span><span>[</span><span>:</span><span>,</span><span>:</span><span>,</span><span>c</span><span>]</span> <span>=</span> <span>frame</span><span>[</span><span>:</span><span>,</span><span>:</span><span>,</span><span>c</span><span>]</span><span>*</span><span>mask</span> <span>+</span> <span>background_scaled</span><span>[</span><span>:</span><span>,</span><span>:</span><span>,</span><span>c</span><span>]</span><span>*</span><span>inv_mask</span>
<span>67</span>    <span>return</span> <span>frame</span>
<span>68</span>
<span>69</span><span># setup access to the *real* webcam</span>
<span>70</span><span>cap</span> <span>=</span> <span>cv2</span><span>.</span><span>VideoCapture</span><span>(</span><span></span><span>'</span><span>/dev/video0</span><span>'</span><span>)</span>
<span>71</span><span>height</span><span>,</span> <span>width</span> <span>=</span> <span>720</span><span>,</span> <span>1280</span>
<span>72</span><span>cap</span><span>.</span><span>set</span><span>(</span><span>cv2</span><span>.</span><span>CAP_PROP_FRAME_WIDTH</span><span>,</span> <span>width</span><span>)</span>
<span>73</span><span>cap</span><span>.</span><span>set</span><span>(</span><span>cv2</span><span>.</span><span>CAP_PROP_FRAME_HEIGHT</span><span>,</span> <span>height</span><span>)</span>
<span>74</span><span>cap</span><span>.</span><span>set</span><span>(</span><span>cv2</span><span>.</span><span>CAP_PROP_FPS</span><span>,</span> <span>60</span><span>)</span>
<span>75</span>
<span>76</span><span># setup the fake camera</span>
<span>77</span><span>fake</span> <span>=</span> <span>pyfakewebcam</span><span>.</span><span>FakeWebcam</span><span>(</span><span></span><span>'</span><span>/dev/video20</span><span>'</span><span>,</span> <span>width</span><span>,</span> <span>height</span><span>)</span>
<span>78</span>
<span>79</span><span># load the virtual background</span>
<span>80</span><span>background</span> <span>=</span> <span>cv2</span><span>.</span><span>imread</span><span>(</span><span></span><span>"</span><span>/data/background.jpg</span><span>"</span><span>)</span>
<span>81</span><span>background_scaled</span> <span>=</span> <span>cv2</span><span>.</span><span>resize</span><span>(</span><span>background</span><span>,</span> <span>(</span><span>width</span><span>,</span> <span>height</span><span>)</span><span>)</span>
<span>82</span>
<span>83</span><span># frames forever</span>
<span>84</span><span>while</span> <span>True</span><span>:</span>
<span>85</span>    <span>frame</span> <span>=</span> <span>get_frame</span><span>(</span><span>cap</span><span>,</span> <span>background_scaled</span><span>)</span>
<span>86</span>    <span># fake webcam expects RGB</span>
<span>87</span>    <span>frame</span> <span>=</span> <span>cv2</span><span>.</span><span>cvtColor</span><span>(</span><span>frame</span><span>,</span> <span>cv2</span><span>.</span><span>COLOR_BGR2RGB</span><span>)</span>
<span>88</span>    <span>fake</span><span>.</span><span>schedule_frame</span><span>(</span><span>frame</span><span>)</span></code></pre></div>
<p>Now build the images:</p>
<div><pre><code data-lang="shell"><span>1</span>docker build -t bodypix ./bodypix
<span>2</span>docker build -t fakecam ./fakecam
</code></pre></div><p>And run them like:</p>
<div><pre><code data-lang="shell"><span> 1</span><span># create a network</span>
<span> 2</span>docker network create --driver bridge fakecam
<span> 3</span><span># start the bodypix app</span>
<span> 4</span>docker run -d <span>
</span><span> 5</span><span></span>  --name<span>=</span>bodypix <span>
</span><span> 6</span><span></span>  --network<span>=</span>fakecam <span>
</span><span> 7</span><span></span>  --gpus<span>=</span>all --shm-size<span>=</span>1g --ulimit <span>memlock</span><span>=</span>-1 --ulimit <span>stack</span><span>=</span><span>67108864</span> <span>
</span><span> 8</span><span></span>  bodypix
<span> 9</span><span># start the camera, note that we need to pass through video devices,</span>
<span>10</span><span># and we want our user ID and group to have permission to them</span>
<span>11</span><span># you may need to `sudo groupadd $USER video`</span>
<span>12</span>docker run -d <span>
</span><span>13</span><span></span>  --name<span>=</span>fakecam <span>
</span><span>14</span><span></span>  --network<span>=</span>fakecam <span>
</span><span>15</span><span></span>  -p 8080:8080 <span>
</span><span>16</span><span></span>  -u <span>"</span><span>$$</span><span>(id -u):</span><span>$$</span><span>(getent group video | cut -d: -f3)</span><span>"</span> <span>
</span><span>17</span><span></span>  <span>$$</span><span>(</span>find /dev -name <span>'video*'</span> -printf <span>"--device %p "</span><span>)</span> <span>
</span><span>18</span><span></span>  fakecam
</code></pre></div><p>Now make sure to start this <em>before</em> opening the camera with any apps, and
be sure to select the ‚Äúv4l2loopback‚Äù / <code>/dev/video20</code> camera in Zoom etc.</p>
<h2 id="the-finished-result">The Finished Result<a href="#the-finished-result" arialabel="Anchor"> üîóÔ∏é</a> </h2>
<p>Here‚Äôs a quick clip I recorded of this in action:</p>
<video playsinline="" autoplay="" muted="" loop="">
    <source src="./holo-demo.webm" type="video/webm">
    Your browser does not support this video.
</video>
<p>Look! I‚Äôm dialing into the millenium falcon with an open source camera stack!</p>
<p>I‚Äôm pretty happy with how this came out. I‚Äôll definitely be joining all of my meetings this way in the morning. <span title=":grin:">:grin:</span></p>

    </div>
  </div></div></div>
    </div>
    <footer>
        <div>created by buildstarted &copy; 2020 <a href="/about">about</a></div>
        <div>Share this page on social media: copy and paste this url https://linksfor.dev/</div>
        <div>If you prefer RSS: <a href="https://linksfor.dev/feed.xml">https://linksfor.dev/feed.xml</a></div>
        <div>Customer satisfaction guaranteed to be optional.</div>
    </footer>
    
    <script async defer>
        _dna = window._dna || {};
        _dna.siteId = "linksfor.devs";
        _dna.outlink = true;

        (function() {
            let dna = document.createElement('script');
            dna.type = 'text/javascript';
            dna.async = true;
            dna.src = '//dna.buildstarted.com/t.js';
            let s = document.getElementsByTagName('script')[0];
            s.parentNode.insertBefore(dna, s);
        })();
    </script>
    <noscript><img src="//dna.buildstarted.com/g?siteId=linksfor.devs"/></noscript>
</body>
</html>