<!DOCTYPE html>
<html lang="en">
<head>
    <title>
An Introduction to System.Threading.Channels | .NET Blog -
linksfor.dev(s)
    </title>
	<link rel="alternate" type="application/rss+xml" title="Linksfor.dev(s) feed" href="https://linksfor.dev/feed.rss" />
    <meta charset="utf-8">
    <meta name="Description" content="A curated source of links that devs might find interesting. Updated around the clock." />
    <meta name="google" value="notranslate">
    <link rel="stylesheet" type="text/css" href="/style.css" />
</head>
<body>
    <div class="grid">
        
<div class="readable">
    <h1>An Introduction to System.Threading.Channels | .NET Blog</h1>
    <div class="entry-content col-12 sharepostcontent"><div class="row justify-content-center"><div class="col-md-4"><div><img src="https://secure.gravatar.com/avatar/49e67eaea0533a547f3489aa03707bbb?s=58&amp;d=mm&amp;r=g" width="58" alt="Avatar" class="avatar avatar-58 wp-user-avatar wp-user-avatar-58 photo avatar-default"></div></div></div><p>&#x201C;Producer/consumer&#x201D; problems are everywhere, in all facets of our lives. A line cook at a fast food restaurant, slicing tomatoes that are handed off to another cook to assemble a burger, which is handed off to a register worker to fulfill your order, which you happily gobble down. Postal drivers delivering mail all along their routes, and you either seeing a truck arrive and going out to the mailbox to retrieve your deliveries or just checking later in the day when you get home from work. An airline employee offloading suitcases from a cargo hold of a jetliner, placing them onto a conveyer belt, where they&#x2019;re shuttled down to another employee who transfers bags to a van and drives them to yet another conveyer that will take them to you. And a happy engaged couple preparing to send out invites to their wedding, with one partner addressing an envelope and handing it off to the other who stuffs and licks it.</p><p>As software developers, we routinely see happenings from our everyday lives make their way into our software, and &#x201C;producer/consumer&#x201D; problems are no exception. Anyone who&#x2019;s piped together commands at a command-line has utilized producer/consumer, with the stdout from one program being fed as the stdin to another. Anyone who&#x2019;s launched multiple workers to compute discrete values or to download data from multiple sites has utilized producer/consumer, with a consumer aggregating results for display or further processing. Anyone who&#x2019;s tried to parallelize a pipeline has very explicitly employed producer/consumer. And so on.</p><p>All of these scenarios, whether in our real-world or software lives, have something in common: there is some vehicle for handing off the results from the producer to the consumer. The fast food employee places the completed burgers in a stand that the register worker pulls from to fill the customer&#x2019;s bag. The postal worker places mail into a mailbox. The engaged couple&#x2019;s hands meet to transfer the materials from one to the other. In software, such a hand-off requires a data structure of some kind to facilitate the transaction, storage that can used by the producer to transfer a result and potentially buffer more, while also enabling the consumer to be notified that one or more results are available. Enter System.Threading.Channels.</p><h2>What is a Channel?</h2><p>I often find it easiest to understand some technology by implementing a simple version myself. In doing so, I learn about various problems implementers of that technology may have had to overcome, trade-offs they may have had to make, and the best way to utilize the functionality. To that end, let&#x2019;s start learning about System.Threading.Channels by implementing a &#x201C;channel&#x201D; from scratch.</p><p>A channel is simply a data structure that&#x2019;s used to store produced data for a consumer to retrieve, and an appropriate synchronization to enable that to happen safely, while also enabling appropriate notifications in both directions. There is a multitude of possible design decisions involved. Should a channel be able to hold an unbounded number of items? If not, what should happen when it fills up? How critical is performance? Do we need to try to minimize synchronization? Can we make any assumptions about how many producers and consumers are allowed concurrently? For the purposes of quickly writing a simple channel, let&#x2019;s make simplifying assumptions that we don&#x2019;t need to enforce any particular bound and that we don&#x2019;t need to be overly concerned about overheads. We&#x2019;ll also make up a simple API.</p><p>To start, we need our type, to which we&#x2019;ll add a few simple methods:</p><pre><code class="language-c#">public sealed class Channel&lt;T&gt;
{
    public void Write(T value);
    public ValueTask&lt;T&gt; ReadAsync(CancellationToken cancellationToken = default);
}
</code></pre><p>Our <code>Write</code> method gives us a method we can use to produce data into the channel, and our <code>ReadAsync</code> method gives us a method to consume from it. Since we decided our channel is unbounded, producing data into it will always complete successfully and synchronously, just as does calling <code>Add</code> on a <code>List&lt;T&gt;</code>, hence we&#x2019;ve made it non-asynchronous and void-returning. In contrast, our method for consuming is <code>ReadAsync</code>, which is asynchronous because the data we want to consume may not yet be available yet, and thus we&#x2019;ll need to wait for it to arrive if nothing is available to consume at the time we try. And while in our getting-started design we&#x2019;re not overly concerned with performance, we also don&#x2019;t want to have lots of unnecessary overheads. Since we expect to be reading frequently, and for us to often be reading when data is already available to be consumed, our <code>ReadAsync</code> method returns a <code>ValueTask&lt;T&gt;</code> rather than a <code>Task&lt;T&gt;</code>, so that we can make it allocation-free when it completes synchronously.</p><p>Now we just need to implement these two methods. To start, we&#x2019;ll add two fields to our type: one to serve as the storage mechanism, and one to coordinate between the producers and consumers:</p><pre><code class="language-c#">private readonly ConcurrentQueue&lt;T&gt; _queue = new ConcurrentQueue&lt;T&gt;();
private readonly SemaphoreSlim _semaphore = new SemaphoreSlim(0);
</code></pre><p>We use a <code>ConcurrentQueue&lt;T&gt;</code> to store the data, freeing us from needing to do our own locking to protect the buffering data structure, as <code>ConcurrentQueue&lt;T&gt;</code> is already thread-safe for any number of producers and any number of consumers to access concurrently. And we use a <code>SempahoreSlim</code> to help coordinate between producers and consumers and to notify consumers that might be waiting for additional data to arrive.</p><p>Our <code>Write</code> method is simple. It just needs to store the data into the queue and increment the <code>SemaphoreSlim</code>&#x2018;s count by &#x201C;release&#x201D;ing it:</p><pre><code class="language-c#">public void Write(T value)
{
    _queue.Enqueue(value); // store the data
    _semaphore.Release(); // notify any consumers that more data is available
}
</code></pre><p>And our <code>ReadAsync</code> method is almost just as simple. It needs to wait for data to be available and then take it out.</p><pre><code class="language-c#">public async ValueTask&lt;T&gt; ReadAsync(CancellationToken cancellationToken = default)
{
    await _semaphoreSlim.WaitAsync(cancellationToken).ConfigureAwait(false); // wait
    bool gotOne = _queue.TryDequeue(out T item); // retrieve the data
    Debug.Assert(gotOne);
    return item;
}
</code></pre><p>Note that because no other code could be manipulating the semaphore or the queue, we know that once we&#x2019;ve successfully waited on the semaphore, the queue will have data to give us, which is why we can just assert that the <code>TryDequeue</code> method successfully returned one. If those assumptions ever changed, this implementation would need to become more complicated.</p><p>And that&#x2019;s it: we have our basic channel. If all you need are the basic features assumed here, such an implementation is perfectly reasonable. Of course, the requirements are often more significant, both on performance and on APIs necessary to enable more scenarios.</p><p>Now that we understand the basics of what a channel provides, we can switch to looking at the actual System.Threading.Channel APIs.</p><h2>Introducing System.Threading.Channels</h2><p>The core abstractions exposed from the System.Threading.Channels library are a writer:</p><pre><code class="language-c#">public abstract class ChannelWriter&lt;T&gt;
{
    public abstract bool TryWrite(T item);
    public virtual ValueTask WriteAsync(T item, CancellationToken cancellationToken = default);
    public abstract ValueTask&lt;bool&gt; WaitToWriteAsync(CancellationToken cancellationToken = default);
    public void Complete(Exception error);
    public virtual bool TryComplete(Exception error);
}
</code></pre><p>and a reader:</p><pre><code class="language-c#">public abstract class ChannelReader&lt;T&gt;
{
    public abstract bool TryRead(out T item);
    public virtual ValueTask&lt;T&gt; ReadAsync(CancellationToken cancellationToken = default)
    public abstract ValueTask&lt;bool&gt; WaitToReadAsync(CancellationToken cancellationToken = default);
    public virtual IAsyncEnumerable&lt;T&gt; ReadAllAsync([EnumeratorCancellation] CancellationToken cancellationToken = default);
    public virtual Task Completion { get; }
}
</code></pre><p>Having just completed our own simple channel design and implementation, most of this API surface area should feel familiar. <code>ChannelWriter&lt;T&gt;</code> provides a <code>TryWrite</code> method that&#x2019;s very similar to our Write method; however, it&#x2019;s abstract and a Try method that returns a <code>Boolean</code>, to account for the fact that some implementations may be bounded in how many items they can physically store, and if the channel was full such that writing couldn&#x2019;t complete synchronously, <code>TryWrite</code> would need to return <code>false</code> to indicate that writing was unsuccessful. However, <code>ChannelWriter&lt;T&gt;</code> also provides the <code>WriteAsync</code> method; in such a case where the channel is full and writing would need to wait (often referred to as &#x201C;back pressure&#x201D;), <code>WriteAsync</code> can be used, with the producer awaiting the result of <code>WriteAsync</code> and only being allowed to continue when room becomes available.</p><p>Of course, there are situations where code may not want to produce a value immediately; if producing a value is expensive or if a value represents an expensive resource (maybe it&#x2019;s a big object that would take up a lot of memory, or maybe it stores a bunch of open files) and if there&#x2019;s a reasonable chance the producer is running faster than the consumer, the producer may want to delay producing a value until it knows a write will be immediately successful. For that, and related scenarios, there&#x2019;s <code>WaitToWriteAsync</code>. A producer can await for <code>WaitToWriteAsync</code> to return <code>true</code>, and only then choose to produce a value that it then <code>TryWrite</code>s or <code>WriteAsync</code>s to the channel.</p><p>Note that <code>WriteAsync</code> is virtual. Some implementations may choose to provide a more optimized implementation, but with abstract <code>TryWrite</code> and <code>WaitToWriteAsync</code>, the base type can provide a reasonable implementation, which is only slightly more sophisticated than this:</p><pre><code class="language-c#">public async ValueTask WriteAsync(T item, CancellationToken cancellationToken)
{
    while (await WaitToWriteAsync(cancellationToken).ConfigureAwait(false))
        if (TryWrite(item))
            return;

    throw new ChannelCompletedException();
}
</code></pre><p>In addition to showing how <code>WaitToWriteAsync</code> and <code>TryWrite</code> can be used, this highlights a few additional interesting things. First, the while loop is present here because channels by default can be used by any number of producers and any number of consumers concurrently. If a channel had an upper bound on how many items it could store, and if multiple threads are racing to write to the buffer, it&#x2019;s possible for two threads to be told &#x201C;yes, there&#x2019;s space&#x201D; via <code>WaitToWriteAsync</code>, but then for one of them to lose the race condition and have <code>TryWrite</code> return <code>false</code>, hence the need to loop around and try again. This example also highlights why <code>WaitToWriteAsync</code> returns a <code>ValueTask&lt;bool&gt;</code> instead of just <code>ValueTask</code>, as well as situations beyond a full buffer in which <code>TryWrite</code> may return <code>false</code>. Channels support the notion of completion, where a producer can signal to a consumer that there won&#x2019;t be any further items produced, enabling the consumer to gracefully stop trying to consume. This is done via the <code>Complete</code> or <code>TryComplete</code> methods previously shown on <code>ChannelWriter&lt;T&gt;</code> (<code>Complete</code> is just implemented to call <code>TryComplete</code> and throw if it returns <code>false</code>). But if one producer marks the channel as complete, other producers need to know they&#x2019;re no longer welcome to write into the channel; in that case, <code>TryWrite</code> returns <code>false</code>, <code>WaitToWriteAsync</code> also returns <code>false</code>, and <code>WriteAsync</code> throws a ChannelCompletedException.</p><p>Most of the members on <code>ChannelReader&lt;T&gt;</code> are likely self-explanatory as well. <code>TryRead</code> will try to synchronously extract the next element from the channel, returning whether it was successful in doing so. <code>ReadAsync</code> will also extract the next element from the channel, but if an element can&#x2019;t be retrieved synchronously, it will return a task for that element. And <code>WaitToReadAsync</code> returns a <code>ValueTask&lt;bool&gt;</code> that serves as a notification for when an element is available to be consumed. Just as with <code>ChannelWriter&lt;T&gt;</code>&#x2018;s <code>WriteAsync</code>, <code>ReadAsync</code> is virtual, with the base implementation implementable in terms of the abstract <code>TryRead</code> and <code>WaitToReadAsync</code>; this isn&#x2019;t the exact implementation in the base class, but it&#x2019;s close:</p><pre><code class="language-c#">public async ValueTask&lt;T&gt; ReadAsync(CancellationToken cancellationToken)
{
    while (true)
    {
        if (!await WaitToReadAsync(cancellationToken).ConfigureAwait(false))
            throw new ChannelClosedException();

        if (TryRead(out T item))
            return item;
    }
}
</code></pre><p>There are a variety of typical patterns for how one consumes from a <code>ChannelReader&lt;T&gt;</code>. If a channel represents an unending stream of values, one approach is simply to sit in an infinite loop consuming via <code>ReadAsync</code>:</p><pre><code class="language-c#">while (true)
{
    T item = await channelReader.ReadAsync();
    Use(item);
}
</code></pre><p>Of course, if the stream of values isn&#x2019;t infinite and the channel will be marked completed at some point, once consumers have emptied the channel of all its data subsequent attempts to <code>ReadAsync</code> from it will throw. In contrast <code>TryRead</code> will return <code>false</code>, as will <code>WaitToReadAsync</code>. So, a more common consumption pattern is via a nested loop:</p><pre><code class="language-c#">while (await channelReader.WaitToReadAsync())
    while (channelReader.TryRead(out T item))
        Use(item);
</code></pre><p>The inner &#x201C;while&#x201D; could have instead been a simple &#x201C;if&#x201D;, but having the tight inner loop enables a cost-conscious developer to avoid the small additional overheads of <code>WaitToReadAsync</code> when an item is already available such that TryRead will successfully consume an item. In fact, this is the exact pattern employed by the <code>ReadAllAsync</code> method. <code>ReadAllAsync</code> was introduced in .NET Core 3.0, and returns an <code>IAsyncEnumerable&amp;lt;T&amp;gt;</code>. It enables all of the data to be read from a channel using familiar language constructs:</p><pre><code class="language-c#">await foreach (T item in channelReader.ReadAllAsync())
    Use(item);
</code></pre><p>And the base implementation of the virtual method employs the exact pattern nested-loop pattern shown previously with <code>WaitToReadAsync</code> and <code>TryRead</code>:</p><pre><code class="language-c#">public virtual async IAsyncEnumerable&lt;T&gt; ReadAllAsync(
    [EnumeratorCancellation] CancellationToken cancellationToken = default)
{
    while (await WaitToReadAsync(cancellationToken).ConfigureAwait(false))
        while (TryRead(out T item))
            yield return item;
}
</code></pre><p>The final member of <code>ChannelReader&amp;lt;T&amp;gt;</code> is <code>Completion</code>. This simply returns a <code>Task</code> that will complete when the channel reader is completed, meaning the channel was marked for completion by a writer and all data has been consumed.</p><h2>Built-In Channel Implementations</h2><p>Ok, so we know how to write to writers and read from readers&#x2026; but from where do we get those writers and readers?</p><p>The <code>Channel&amp;lt;TWrite, TRead&amp;gt;</code> type exposes a Writer property and a Reader property that returns a <code>ChannelWriter&amp;lt;TWrite&amp;gt;</code> and a <code>ChannelReader&amp;lt;TRead&amp;gt;</code>, respectively:</p><pre><code class="language-c#">public abstract class Channel&lt;TWrite, TRead&gt;
{
    public ChannelReader&lt;TRead&gt; Reader { get;  }
    public ChannelWriter&lt;TWrite&gt; Writer { get; }
}
</code></pre><p>This base abstract class is available for the niche uses cases where a channel may itself transform written data into a different type for consumption, but the vast majority use case has <code>TWrite</code> and <code>TRead</code> being the same, which is why the majority use happens via the derived Channel<t> type, which is nothing more than:</t></p><pre><code class="language-c#">public abstract class Channel&lt;T&gt; : Channel&lt;T, T&gt; { }
</code></pre><p>The non-generic Channel type then provides factories for several implementations of <code>Channel&amp;lt;&amp;T&amp;gt;</code>:</p><pre><code class="language-c#">public static class Channel
{
    public static Channel&lt;T&gt; CreateUnbounded&lt;T&gt;();
    public static Channel&lt;T&gt; CreateUnbounded&lt;T&gt;(UnboundedChannelOptions options);

    public static Channel&lt;T&gt; CreateBounded&lt;T&gt;(int capacity);
    public static Channel&lt;T&gt; CreateBounded&lt;T&gt;(BoundedChannelOptions options);
}
</code></pre><p>The <code>CreateUnbounded</code> method creates a channel with no imposed limit on the number of items that can be stored (of course at some point it might hit the limits of the memory, just as with <code>List&amp;lt;T&amp;gt;</code> and any other collection), very much like the simple Channel-like type we implemented at the beginning of this post. Its <code>TryWrite</code> will always return <code>true</code>, and both its <code>WriteAsync</code> and its <code>WaitToWriteAsync</code> will always complete synchronously.</p><p>In contrast, the <code>CreateBounded</code> method creates a channel with an explicit limit maintained by the implementation. Prior to reaching this capacity, just as with <code>CreateUnbounded</code>, <code>TryWrite</code> will return <code>true</code> and both <code>WriteAsync</code> and <code>WaitToWriteAsync</code> will complete synchronously. But once the channel fills up, <code>TryWrite</code> will return <code>false</code>, and both <code>WriteAsync</code> and <code>WaitToWriteAsync</code> will complete asynchronously, only completing their returned tasks when space is available, or another producer signals the channel&#x2019;s completion. (It should go without saying that all of these APIs that accept a <code>CancellationToken</code> can also be interrupted by cancellation being requested).</p><p>Both <code>CreateUnbounded</code> and <code>CreateBounded</code> have overloads that accept a <code>ChannelOptions</code>-derived type. This base <code>ChannelOptions</code> provides options that can control any channel&#x2019;s behavior. For example, it exposes <code>SingleWriter</code> and <code>SingleReader</code> properties, which allow the creator to indicate constraints they&#x2019;re willing to accept; a creator sets <code>SingleWriter</code> to <code>true</code> to indicate that at most one producer will be accessing the writer at a time, and similarly sets <code>SingleReader</code> to <code>true</code> to indicate that at most one consumer will be accessing the reader at a time. This allows for the factory methods to specialize the implementation that&#x2019;s created, optimizing it based on the supplied options; for example, if the options passed to <code>CreateUnbounded</code> specifies <code>SingleReader</code> as <code>true</code>, it returns an implementation that not only avoids locks when reading, it also avoids interlocked operations when reading, significantly reducing the overheads involved in consuming from the channel. The base <code>ChannelOptions</code> also exposes an <code>AllowSynchronousContinuations</code> property. As with <code>SingleReader</code> and <code>SingleWriter</code>, this defaults to <code>false</code>, and a creator setting it to <code>true</code> means signing up for some optimizations that also have strong implications for how producing and consuming code is written. Specifically, <code>AllowSynchronousContinuations</code> in a sense allows a producer to temporarily become a consumer. Let&#x2019;s say there&#x2019;s no data in a channel and a consumer comes along and calls <code>ReadAsync</code>. By awaiting the task returned from <code>ReadAsync</code>, that consumer is effectively hooking up a callback to be invoked when data is written to the channel. By default, that callback will be invoked asynchronously, with the producer writing the data to the channel and then queueing the invocation of that callback, which allows the producer to concurrently go on its merry way while the consumer is processed by some other thread. However, in some situations it may be advantageous for performance to allow that producer writing the data to also itself process the callback, e.g. rather than <code>TryWrite</code> queueing the invocation of the callback, it simply invokes the callback itself. This can significantly cut down on overheads, but also requires great understanding of the environment, as, for example, if you were holdling a lock while calling <code>TryWrite</code>, with <code>AllowSynchronousContinuations</code> set to <code>true</code>, you might end up invoking the callback while holding your lock, which (depending on what the callback tried to do) could end up observing some broken invariants your lock was trying to maintain.</p><p>The <code>BoundedChannelOptions</code> passed to <code>CreateBounded</code> layers on additional options specific to bounding. In addition to the maximum capacity supported by the channel, it also exposes a <code>BoundedChannelFullMode</code> enum that indicates the behavior writes should experience when the channel is full:</p><pre><code class="language-c#">public enum BoundedChannelFullMode
{
    Wait,
    DropNewest,
    DropOldest,
    DropWrite
}
</code></pre><p>The default is <code>Wait</code>, which has the semantics already discussed: <code>TryWrite</code> on a full channel returns <code>false</code>, <code>WriteAsync</code> will return a task that will only complete when space became available and the write could complete successfully, and similarly <code>WaitToWriteAsync</code> will only complete when space becomes available. The other three modes instead enable writes to always complete synchronously, dropping an element if the channel is full rather than introducing back pressure. <code>DropOldest</code> will remove the &#x201C;oldest&#x201D; item (wall-clock time) from the queue, meaning whichever element would next be dequeued by a consumer. Conversely, <code>DropNewest</code> will remove the newest item, whichever element was most recently written to the channel. And <code>DropWrite</code> drops the item currently being written, meaning for example <code>TryWrite</code> will return <code>true</code> but the item it added will immediately be removed.</p><h3>Performance</h3><p>From an API perspective, that&#x2019;s pretty much it. The abstractions exposed are relatively simple, which is a large part of where the power of the library comes from. Simple abstractions and a few concrete implementations that should meet the 99.9% use cases of developers&#x2019; needs. Of course, the surface area of the library might suggest that the implementation is also simple. In truth, there&#x2019;s a decent amount of complexity in the implementation, mostly focused on enabling great throughput while enabling simple consumption patterns easily used in consuming code. The implementation, for example, goes to great pains to minimize allocations. You may have noticed that many of the methods in the surface area return <code>ValueTask</code> and <code>ValueTask&lt;T&gt;</code> rather than <code>Task</code> and <code>Task&lt;T&gt;</code>. As we saw in our trivial example implementation at the beginning of this article, we can utilize <code>ValueTask&lt;T&gt;</code> to avoid allocations when methods complete synchronously, but the System.Threading.Channels implementation also takes advantage of the advanced <code>IValueTaskSource</code> and <code>IValueTaskSource&lt;T&gt;</code> interfaces to avoid allocations even when the various methods complete asynchronously and need to return tasks.</p><p>Consider this benchmark:</p><pre><code class="language-c#">using BenchmarkDotNet.Attributes;
using BenchmarkDotNet.Running;
using System.Threading.Channels;
using System.Threading.Tasks;

[MemoryDiagnoser]
public class Program
{
    static void Main() =&gt; BenchmarkRunner.Run&lt;Program&gt;();

    private readonly Channel&lt;int&gt; s_channel = Channel.CreateUnbounded&lt;int&gt;();

    [Benchmark]
    public async Task WriteThenRead()
    {
        ChannelWriter&lt;int&gt; writer = s_channel.Writer;
        ChannelReader&lt;int&gt; reader = s_channel.Reader;
        for (int i = 0; i &lt; 10_000_000; i++)
        {
            writer.TryWrite(i);
            await reader.ReadAsync();
        }
    }
}
</code></pre><p>Here we&#x2019;re just testing the throughput and memory allocation on an unbounded channel when writing an element and then reading out that element 10 million times, which means an element will always be available for the read to consume and thus the read will always complete synchronously, yielding the following results on my machine (the 72 bytes shown in the Allocated column is for the single Task returned from WriteThenRead):</p><table><thead><tr><th> Method</th><th> Mean</th><th> Error</th><th> StdDev</th><th> Gen 0</th><th> Gen 1</th><th> Gen 2</th><th> Allocated</th></tr></thead><tbody><tr><td> WriteThenRead</td><td> 527.8 ms</td><td> 2.03 ms</td><td> 1.90 ms</td><td> &#x2013;</td><td> &#x2013;</td><td> &#x2013;</td><td> 72 B</td></tr></tbody></table><p>But now let&#x2019;s change it slightly, first issuing the read and only then writing the element that will satisfy it. In this case, reads will always complete asynchronously because the data to complete them will never be available:</p><pre><code class="language-c#">using BenchmarkDotNet.Attributes;
using BenchmarkDotNet.Running;
using System.Threading.Channels;
using System.Threading.Tasks;

[MemoryDiagnoser]
public class Program
{
    static void Main() =&gt; BenchmarkRunner.Run&lt;Program&gt;();

    private readonly Channel&lt;int&gt; s_channel = Channel.CreateUnbounded&lt;int&gt;();

    [Benchmark]
    public async Task ReadThenWrite()
    {
        ChannelWriter&lt;int&gt; writer = s_channel.Writer;
        ChannelReader&lt;int&gt; reader = s_channel.Reader;
        for (int i = 0; i &lt; 10_000_000; i++)
        {
            ValueTask&lt;int&gt; vt = reader.ReadAsync();
            writer.TryWrite(i);
            await vt;
        }
    }
}
</code></pre><p>which on my machine for 10 million writes and reads yields results like this:</p><table><thead><tr><th> Method</th><th> Mean</th><th> Error</th><th> StdDev</th><th> Gen 0</th><th> Gen 1</th><th> Gen 2</th><th> Allocated</th></tr></thead><tbody><tr><td> ReadThenWrite</td><td> 881.2 ms</td><td> 4.60 ms</td><td> 4.30 ms</td><td> &#x2013;</td><td> &#x2013;</td><td> &#x2013;</td><td> 72 B</td></tr></tbody></table><p>So, there&#x2019;s some more overhead when every read completes asynchronously, but even here we see zero allocations for the 10 million asynchronously-completing reads (again, the 72 bytes shown in the Allocated column is for the Task returned from ReadThenWrite)!</p><h2>Combinators</h2><p>Generally consumption of channels is simple, using one of the approaches shown earlier. But as with IEnumerable<t>s, it&#x2019;s also possible to implement various kinds of operations over channels to accomplish a specific purpose. For example, let&#x2019;s say I want to wait for the first element to arrive from either of two supplied readers; I could write something like this:</t></p><pre><code class="language-c#">public static async ValueTask&lt;ChannelReader&lt;T&gt;&gt; WhenAny&lt;T&gt;(
    ChannelReader&lt;T&gt; reader1, ChannelReader&lt;T&gt; reader2)
{
    var cts = new CancellationTokenSource();
    Task&lt;bool&gt; t1 = reader1.WaitToReadAsync(cts.Token).AsTask();
    Task&lt;bool&gt; t2 = reader2.WaitToReadAsync(cts.Token).AsTask();
    Task&lt;bool&gt; completed = await Task.WhenAny(t1, t2);
    cts.Cancel();
    return completed == t1 ? reader1 : reader2;
}
</code></pre><p>Here we&#x2019;re just calling <code>WaitToReadAsync</code> on both channels, and returning the reader for whichever one completes first. One of the interesting things to note about this example is that, while <code>ChannelReader&lt;T&gt;</code> bears many similarities to <code>IEnumerator&lt;T&gt;</code>, this example can&#x2019;t be implemented well on top of <code>IEnumerator&lt;T&gt;</code> (or <code>IAsyncEnumerator&lt;T&gt;</code>). <code>I{Async}Enumerator&lt;T&gt;</code> exposes a <code>MoveNext{Async}</code> method, which moves the cursor ahead to the next item, which is then exposed from <code>Current</code>. If we tried to implement such a <code>WhenAny</code> on top of <code>IAsyncEnumerator&lt;T&gt;</code>, we would need to invoke <code>MoveNextAsync</code> on each. In doing so, we would potentially move both ahead to their next item. If we then used that method in a loop, we would likely end up missing items from one or both enumerators, because we would potentially have advanced the enumerator that we didn&#x2019;t return from the method.</p><h2>Relationship to the rest of .NET Core</h2><p>System.Threading.Channels is part of the .NET Core shared framework, meaning a .NET Core app can start using it without installing anything additional. It&#x2019;s also available as a separate NuGet package, though the separate implementation doesn&#x2019;t have all of the optimizations that built-in implementation has, in large part because the built-in implementation is able to take advantage of additional runtime and library support in .NET Core.</p><p>It&#x2019;s also used by a variety of other systems in .NET. For example, ASP.NET uses channels as part of SignalR as well as in its Libuv-based Kestrel transport. Channels are also used by the upcoming QUIC implementation currently being developed for .NET 5.</p><p>If you squint, the System.Threading.Channels library also looks a bit similar to the System.Threading.Tasks.Dataflow library that&#x2019;s been available with .NET for years. In some ways, the dataflow library is a superset of the channels library; in particular, the <code>BufferBlock&lt;T&gt;</code> type from the dataflow library exposes much of the same functionality. However, the dataflow library is also focused on a different programming model, one where blocks are linked together such that data flows automatically from one to the next. It also includes advanced functionality that supports, for example, a form of two-phase commit, with multiple blocks linked to the same consumers, and those consumers able to atomically take from multiple blocks without deadlocking. Those mechanisms required to enable that are much more involved, and while more powerful are also more expensive. This is evident just by writing the same benchmark for <code>BufferBlock&lt;T&gt;</code> as we did earlier for Channels.</p><pre><code class="language-c#">using BenchmarkDotNet.Attributes;
using BenchmarkDotNet.Running;
using System.Threading.Channels;
using System.Threading.Tasks;
using System.Threading.Tasks.Dataflow;

[MemoryDiagnoser]
public class Program
{
    static void Main() =&gt; BenchmarkRunner.Run&lt;Program&gt;();

    private readonly Channel&lt;int&gt; _channel = Channel.CreateUnbounded&lt;int&gt;();
    private readonly BufferBlock&lt;int&gt; _bufferBlock = new BufferBlock&lt;int&gt;();

    [Benchmark]
    public async Task Channel_ReadThenWrite()
    {
        ChannelWriter&lt;int&gt; writer = _channel.Writer;
        ChannelReader&lt;int&gt; reader = _channel.Reader;
        for (int i = 0; i &lt; 10_000_000; i++)
        {
            ValueTask&lt;int&gt; vt = reader.ReadAsync();
            writer.TryWrite(i);
            await vt;
        }
    }

    [Benchmark]
    public async Task BufferBlock_ReadThenWrite()
    {
        for (int i = 0; i &lt; 10_000_000; i++)
        {
            Task&lt;int&gt; t = _bufferBlock.ReceiveAsync();
            _bufferBlock.Post(i);
            await t;
        }
    }
}
</code></pre><table><thead><tr><th> Method</th><th> Mean</th><th> Error</th><th> StdDev</th><th> Gen 0</th><th> Gen 1</th><th> Gen 2</th><th> Allocated</th></tr></thead><tbody><tr><td> Channel_ReadThenWrite</td><td> 878.9 ms</td><td> 0.68 ms</td><td> 0.60 ms</td><td> 72 B</td><td> &#x2013;</td><td> &#x2013;</td><td> 72 B</td></tr><tr><td> BufferBlock_ReadThenWrite</td><td> 20,116.4 ms</td><td> 192.82 ms</td><td> 180.37 ms</td><td> 1184000.0000</td><td> 2000.0000</td><td> &#x2013;</td><td> 7360000232 B</td></tr></tbody></table><p>This is in no way meant to suggest that the System.Threading.Tasks.Dataflow library shouldn&#x2019;t be used. It enables developers to express succinctly a large number of concepts, and it can exhibit very good performance when applied to the problems it suits best. However, when all one needs is a hand-off data structure between one or more producers and one or more consumers you&#x2019;ve manually implemented, System.Threading.Channels is a much simpler, leaner bet.</p><h2>What&#x2019;s Next?</h2><p>Hopefully at this point you have a better understanding of the System.Threading.Channels library, enough to see how it might fit into and help improve your applications. Give it a try, and we&#x2019;d love your feedback, suggestions, issues, and PRs to improve it further at <a href="https://github.com/dotnet/runtime">https://github.com/dotnet/runtime</a>. Thanks!</p></div>
</div>
    </div>
    <footer>
        <div>created by buildstarted &copy; 2019 <a href="/about">about</a></div>
        <div>Share this page on social media: copy and paste this url https://linksfor.dev/</div>
    </footer>
    
    <script>
        (function() {
            var COLLECT_URL = "https://dna.buildstarted.com/t";
            var SITE_ID = "linksfor.devs";
            var GLOBAL_VAR_NAME = "__DNA__";

            window[GLOBAL_VAR_NAME] = {};

            window[GLOBAL_VAR_NAME].sendPageView = function() {
                var path = location.pathname;
                var referrer = document.referrer;

                var url = COLLECT_URL + "?siteid=" + SITE_ID + "&p=" + encodeURIComponent(path) + "&r=" + encodeURIComponent(referrer);

                try { fetch(url, { method: "GET" }); } catch (e) {}
            };

            window[GLOBAL_VAR_NAME].sendPageView();
        })();
    </script>
</body>
</html>