<!DOCTYPE html>
<html lang="en">
<head>
    <title>
A Simple GPU Hash&#xA0;Table - linksfor.dev(s)    </title>
    <meta charset="utf-8">
    <link rel="alternate" type="application/rss+xml" title="Linksfor.dev(s) feed" href="https://linksfor.dev/feed.rss" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta name="google" value="notranslate">
    <link rel="stylesheet" type="text/css" href="/style.css" />
    <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon">
    <link rel="icon" href="/favicon.ico" type="image/x-icon">
    <meta property="og:title" content="A Simple GPU Hash&#xA0;Table - linksfor.dev(s)"/>
    <meta property="article:author" content="David Farrell"/>
    <meta property="og:description" content="A Simple GPU Hash&#xA0;Table"/>
    <meta property="og:type" content="website"/>
    <meta property="og:url" content="http://nosferalatu.com/SimpleGPUHashTable.html"/>

<meta property="og:site_name" content="linksfor.dev(s)" />
</head>
<body>
	<div class="devring" style="background: #222">
		<div class="grid">
			<div style="display: grid; grid-template-columns: .5fr 1fr 1fr 1fr; text-align: center;">
				<span class="devring-title">devring.club</span>
				<a href="https://devring.club/site/1/previous" class="devring-previous">Previous</a>
				<a href="https://devring.club/random" class="devring-random">Random</a>
				<a href="https://devring.club/sites/1/next" class="devring-next">Next</a>
			</div>
		</div>
	</div>
    <div class="grid">
        <h1>
<a href="/" style="color:inherit">linksfor.dev(s)</a>
        </h1>
        <title>linksfor.dev(s) - A Simple GPU Hash&#xA0;Table</title>
<div class="readable">
        <h1>A Simple GPU Hash&#xA0;Table</h1>
            <div>by David Farrell</div>
            <div>Reading time: 17-21 minutes</div>
        <div>Posted here: 11 Mar 2020</div>
        <p><a href="http://nosferalatu.com/SimpleGPUHashTable.html">http://nosferalatu.com/SimpleGPUHashTable.html</a></p>
        <hr/>
<div id="readability-page-1" class="page"><div>
    <div>
        <div>
    <section id="content">
        <article>
            
            <div>
                
                <p>I released a new project <a href="https://github.com/nosferalatu/SimpleGPUHashTable">A Simple <span>GPU</span> Hash Table</a> on&nbsp;Github.</p>
<p>It is a simple <span>GPU</span> hash table capable of hundreds of millions of insertions per second. On my laptop’s <span>NVIDIA</span> <span>GTX</span> 1060, the code inserts 64 million randomly generated key/values in about 210 milliseconds, and deletes 32 million of those key/value pairs in about 64&nbsp;milliseconds.</p>
<p>That’s a rate of around 300 million insertions/second and 500 million deletions/second on a&nbsp;laptop.</p>
<p>It’s written in <span>CUDA</span>, although the same technique is just as applicable to <span>HLSL</span> or <span>GLSL</span>.</p>
<p>The hash table implementation has a few constraints that make it work well on a <span>GPU</span>:</p>
<ul>
<li>Only 32 bit keys and 32 bit&nbsp;values</li>
<li>The hash table is a fixed&nbsp;size</li>
<li>The hash table size must be a power of&nbsp;two</li>
</ul>
<p>An empty sentinel must be reserved for both keys and values (0xffffffff in the example&nbsp;code).</p>

<p>The hash table uses open addressing with <a href="https://en.wikipedia.org/wiki/Linear_probing">linear probing</a>, so it’s just an array of keys/values in memory, and has excellent cache performance. This is in contrast to chaining, which involves pointer chasing through a linked list. The hash table is a simple array of KeyValue&nbsp;items:</p>
<div><pre><span></span><span>struct</span> <span>KeyValue</span>
<span>{</span>
    <span>uint32_t</span> <span>key</span><span>;</span>
    <span>uint32_t</span> <span>value</span><span>;</span>
<span>};</span>
</pre></div>


<p>The table uses power-of-two sizes instead of prime numbers because pow2/<span>AND</span> masking is a fast single instruction and the modulo operator is much slower. This is important for linear probing, because as the table is linearly searched, the slot index must be wrapped at each slot. The cost of a modulo operation at each slot quickly adds&nbsp;up.</p>
<p>The table stores only a key and a value for each item, not the hash of the key. Because the table stores 32-bit keys, computing the hash is fairly quick. The example code uses a Murmur3 hash which is only a few shifts, XORs, and&nbsp;multiplies.</p>
<p>The hash table uses a lock free technique that does not depend on any memory ordering. Even if some writes are out of order with other writes, the hash table is still in a valid state. This will be explained more below. This technique works very well for <span>GPU</span>’s, which can have thousands of threads running&nbsp;concurrently.</p>
<p>Both the keys and values of the hash table are initialized to&nbsp;empty.</p>
<p>The code can be modified to use 64 bit keys and/or 64 bit values. Keys require atomic reads, writes, and compare-and-swap operations, and values require atomic read and writes operations. Fortunately <span>CUDA</span> reads/writes of 32 and 64 bit values are atomic as long as they are naturally aligned (see <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses">here</a>), and 64 bit atomic compare-and-swap exist on modern <span>GPU</span>’s. There is of course some performance cost to using 64 bit&nbsp;key/values.</p>

<p>When thinking about the hash table, consider that there are four possible states that each key/value can be&nbsp;in:</p>
<ul>
<li>Both key and value are empty. This is how the hash table is&nbsp;initialized.</li>
<li>The key has been written, but the value has not yet been written. If another thread reads data at this point, then it will return the value of empty (which is fine; it’s the same result as if the other thread had ran slightly earlier, and this is a concurrent data&nbsp;structure).</li>
<li>Both the key and value have been&nbsp;written.</li>
<li>The value is visible to other threads, but the key is not yet visible. This might happen because the <span>CUDA</span> programming model assumes a weakly ordered memory model. This is fine, in any event— the key is still empty, even though the value is&nbsp;not.</li>
</ul>
<p>A subtle but important point is that once a key has been written to a slot, it never moves (even when the key is deleted, as discussed&nbsp;below).</p>
<p>The hash table code works even with weakly ordered memory models, where the order of memory reads and writes is unknown. As we look at the code below for hash table insertion, lookups, and deletion, keep in mind that each key/value is in one of these four&nbsp;states.</p>

<p>The <span>CUDA</span> function that inserts key/value pairs into the hash table looks like&nbsp;this:</p>
<div><pre><span></span><span>void</span> <span>gpu_hashtable_insert</span><span>(</span><span>KeyValue</span><span>*</span> <span>hashtable</span><span>,</span> <span>uint32_t</span> <span>key</span><span>,</span> <span>uint32_t</span> <span>value</span><span>)</span>
<span>{</span>
    <span>uint32_t</span> <span>slot</span> <span>=</span> <span>hash</span><span>(</span><span>key</span><span>);</span>

    <span>while</span> <span>(</span><span>true</span><span>)</span>
    <span>{</span>
        <span>uint32_t</span> <span>prev</span> <span>=</span> <span>atomicCAS</span><span>(</span><span>&amp;</span><span>hashtable</span><span>[</span><span>slot</span><span>].</span><span>key</span><span>,</span> <span>kEmpty</span><span>,</span> <span>key</span><span>);</span>
        <span>if</span> <span>(</span><span>prev</span> <span>==</span> <span>kEmpty</span> <span>||</span> <span>prev</span> <span>==</span> <span>key</span><span>)</span>
        <span>{</span>
            <span>hashtable</span><span>[</span><span>slot</span><span>].</span><span>value</span> <span>=</span> <span>value</span><span>;</span>
            <span>break</span><span>;</span>
        <span>}</span>
        <span>slot</span> <span>=</span> <span>(</span><span>slot</span> <span>+</span> <span>1</span><span>)</span> <span>&amp;</span> <span>(</span><span>kHashTableCapacity</span><span>-</span><span>1</span><span>);</span>
    <span>}</span>
<span>}</span>
</pre></div>


<p>To insert a key, the code iterates through the hash table array starting at the insertion key’s hash. At each hash table array slot, it does an atomic compare-and-swap, which compares the key at that slot to empty, updates the slot’s key with the insertion key if it matches, and returns the slot’s original key. If that slot’s original key was empty or matches the insertion key, then the code found the correct slot for the insertion key, and sets the slot’s value to the insertion&nbsp;value.</p>
<p>If there are many items with the same key in a single kernel invocation of gpu_hashtable_insert(), then any of those values may be written to the key’s slot. This is still correct; one of the key/value writes in the invocation will succeed, but because all of this is happening in parallel across many threads, it is impossible to know which will be the last to write to&nbsp;memory.</p>

<p>The code to find&nbsp;keys:</p>
<div><pre><span></span><span>uint32_t</span> <span>gpu_hashtable_lookup</span><span>(</span><span>KeyValue</span><span>*</span> <span>hashtable</span><span>,</span> <span>uint32_t</span> <span>key</span><span>)</span>
<span>{</span>
        <span>uint32_t</span> <span>slot</span> <span>=</span> <span>hash</span><span>(</span><span>key</span><span>);</span>

        <span>while</span> <span>(</span><span>true</span><span>)</span>
        <span>{</span>
            <span>if</span> <span>(</span><span>hashtable</span><span>[</span><span>slot</span><span>].</span><span>key</span> <span>==</span> <span>key</span><span>)</span>
            <span>{</span>
                <span>return</span> <span>hashtable</span><span>[</span><span>slot</span><span>].</span><span>value</span><span>;</span>
            <span>}</span>
            <span>if</span> <span>(</span><span>hashtable</span><span>[</span><span>slot</span><span>].</span><span>key</span> <span>==</span> <span>kEmpty</span><span>)</span>
            <span>{</span>
                <span>return</span> <span>kEmpty</span><span>;</span>
            <span>}</span>
            <span>slot</span> <span>=</span> <span>(</span><span>slot</span> <span>+</span> <span>1</span><span>)</span> <span>&amp;</span> <span>(</span><span>kHashTableCapacity</span> <span>-</span> <span>1</span><span>);</span>
        <span>}</span>
<span>}</span>
</pre></div>


<p>To find the value of a key stored in the hash table, we iterate through the hash table array starting at the lookup key’s hash. At each slot, we test if the key is the one we’re looking for, and if so we return its value. We also test if the key is empty, and terminate the search if&nbsp;so.</p>
<p>If the lookup fails to find the key, then it returns a value of&nbsp;empty.</p>
<p>These lookups can concurrently happen with inserts and deletes. A thread will see each key/value in the table in one of the four states described&nbsp;above.</p>

<p>The code to delete&nbsp;keys:</p>
<div><pre><span></span><span>void</span> <span>gpu_hashtable_delete</span><span>(</span><span>KeyValue</span><span>*</span> <span>hashtable</span><span>,</span> <span>uint32_t</span> <span>key</span><span>,</span> <span>uint32_t</span> <span>value</span><span>)</span>
<span>{</span>
    <span>uint32_t</span> <span>slot</span> <span>=</span> <span>hash</span><span>(</span><span>key</span><span>);</span>

    <span>while</span> <span>(</span><span>true</span><span>)</span>
    <span>{</span>
        <span>if</span> <span>(</span><span>hashtable</span><span>[</span><span>slot</span><span>].</span><span>key</span> <span>==</span> <span>key</span><span>)</span>
        <span>{</span>
            <span>hashtable</span><span>[</span><span>slot</span><span>].</span><span>value</span> <span>=</span> <span>kEmpty</span><span>;</span>
            <span>return</span><span>;</span>
        <span>}</span>
        <span>if</span> <span>(</span><span>hashtable</span><span>[</span><span>slot</span><span>].</span><span>key</span> <span>==</span> <span>kEmpty</span><span>)</span>
        <span>{</span>
            <span>return</span><span>;</span>
        <span>}</span>
        <span>slot</span> <span>=</span> <span>(</span><span>slot</span> <span>+</span> <span>1</span><span>)</span> <span>&amp;</span> <span>(</span><span>kHashTableCapacity</span> <span>-</span> <span>1</span><span>);</span>
    <span>}</span>
<span>}</span>
</pre></div>


<p>Deleting a key involves something unexpected: we leave the key in the table and mark its value (not the key) as empty. This code is very similar to lookup(), except that if it finds a matching key in the table, it sets its value to&nbsp;empty.</p>
<p>As mentioned earlier, once a key has been written to a slot, it never moves. Even when an item is deleted from the table, the key remains but its value is empty. This means there’s no need to use an atomic operation to write to the slot’s value, because it doesn’t matter if the current value is non-empty or empty— the slot’s value will be updated to empty either&nbsp;way.</p>

<p>Resizing the hash table can be performed by creating a new table of larger size and inserting all non-empty elements of the old table into the new table. I didn’t implement that, as the example code is intended to be simple. Furthermore, this example is in <span>CUDA</span>, and memory allocations in <span>CUDA</span> programs are often done by the host code, not within a <span>CUDA</span>&nbsp;kernel.</p>
<p>Alternatively, <a href="https://web.stanford.edu/class/ee380/Abstracts/070221_LockFreeHash.pdf">A Lock-Free Wait-Free Hash Table</a> describes how to resize this lock free data&nbsp;structure. </p>

<p>In the code snippets above, gpu_hashtable_insert()/_lookup()/_delete() process one key/value at a time. In the example code, though, gpu_hashtable_insert()/_lookup()/_delete() process an array of key/value pairs in parallel, where each key/value is processed by one <span>GPU</span>&nbsp;thread:</p>
<div><pre><span></span><span>// CPU code to invoke the CUDA kernel on the GPU</span>
<span>uint32_t</span> <span>threadblocksize</span> <span>=</span> <span>1024</span><span>;</span>
<span>uint32_t</span> <span>gridsize</span> <span>=</span> <span>(</span><span>numkvs</span> <span>+</span> <span>threadblocksize</span> <span>-</span> <span>1</span><span>)</span> <span>/</span> <span>threadblocksize</span><span>;</span>
<span>gpu_hashtable_insert_kernel</span><span>&lt;&lt;&lt;</span><span>gridsize</span><span>,</span> <span>threadblocksize</span><span>&gt;&gt;&gt;</span><span>(</span><span>hashtable</span><span>,</span> <span>kvs</span><span>,</span> <span>numkvs</span><span>);</span>

<span>// GPU code to process numkvs key/values in parallel</span>
<span>void</span> <span>gpu_hashtable_insert_kernel</span><span>(</span><span>KeyValue</span><span>*</span> <span>hashtable</span><span>,</span> <span>const</span> <span>KeyValue</span><span>*</span> <span>kvs</span><span>,</span> <span>unsigned</span> <span>int</span> <span>numkvs</span><span>)</span>
<span>{</span>
    <span>unsigned</span> <span>int</span> <span>threadid</span> <span>=</span> <span>blockIdx</span><span>.</span><span>x</span><span>*</span><span>blockDim</span><span>.</span><span>x</span> <span>+</span> <span>threadIdx</span><span>.</span><span>x</span><span>;</span>
    <span>if</span> <span>(</span><span>threadid</span> <span>&lt;</span> <span>numkvs</span><span>)</span>
    <span>{</span>
        <span>gpu_hashtable_insert</span><span>(</span><span>hashtable</span><span>,</span> <span>kvs</span><span>[</span><span>threadid</span><span>].</span><span>key</span><span>,</span> <span>kvs</span><span>[</span><span>threadid</span><span>].</span><span>value</span><span>);</span>
    <span>}</span>
<span>}</span>
</pre></div>


<p>The lock free hash table supports concurrent inserts, lookups, and deletes. Because the hash table key/values are always in one of four states, and keys don’t move around, the hash table guarantees correctness even when mixing different kinds of&nbsp;operations.</p>
<p>However, when processing a batch of inserts/deletes in parallel in a kernel, and a kernel’s input array of key/values contains duplicate keys, then it’s unpredictable which of those key/values will “win” and be the last one written to the hash table. For example, imagine that an insertion kernel is called with an input array of key/values is <code>A/0 B/1 A/2 C/3 A/4</code>. When the kernel finishes, the key/values <code>B/1</code> and <code>C/3</code> are guaranteed to be in the table, but any one of <code>A/0</code>, <code>A/2</code>, or <code>A/4</code> will be in the table. Depending on the application, this may or may not be an issue— it might be known that the input array has no duplicate keys, or it might not matter which value is the last to be&nbsp;written.</p>
<p>If this is an issue for an application, then it needs to separate the duplicate key/values into different <span>CUDA</span> kernel calls. In <span>CUDA</span>, all operations within a kernel invocation always finish before the next kernel invocation (at least, within a single stream; kernels in different streams execute in parallel). In the above example, if one kernel is called with <code>A/0 B/1 A/2 C/3</code>, and then another kernel is called with <code>A/4</code>, then the key <code>A</code> will have the value <code>4</code>.</p>
<p>Another consideration is whether the lookup() and delete() functions should use a plain or volatile pointer to the hash table key/value array. The <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#volatile-qualifier"><span>CUDA</span> documentation for volatile</a> states that “The compiler is free to optimize reads and writes to global or shared memory … These optimizations can be disabled using the volatile keyword: … any reference to this variable compiles to an actual memory read or write instruction.” Using volatile is not necessary for correctness; if a thread uses a cached value from an earlier read, it will be using slightly out of date information, but it will still be using information from a valid state of the hash table at some point in that kernel’s invocation. If an application wants to be sure to use the most up to date information, then it can use a volatile pointer, but there is a slight performance hit (in my tests, deleting 32 million elements went from 500 million deletes / second to 450&nbsp;Mdeletes/sec).</p>

<p>In a test that inserts 64 million items and deletes 32 million of them, there’s no contest between std::unordered_map and the <span>GPU</span> hash&nbsp;table:</p>
<p><img alt="performance comparison" src="http://nosferalatu.com/images/stdunorderedmap_vs_simplegpuhashtable.png"></p>
<p>The std::unordered_map took 70691 milliseconds to insert and delete the items, and then freeing the unordered_map (freeing an unordered_map with millions of items consumes a significant amount of time because unordered_map does many memory allocations internally). To be fair, std:unordered_map is operating on a very different set of constraints. It is a single <span>CPU</span> thread, it supports key/values of any size, and it works well with high load factors and has steady performance after many&nbsp;deletions.</p>
<p>The <span>GPU</span> hash table and interop time took 984 milliseconds. That includes time for allocating and deleting the hash table (which is a single memory allocation of 1 <span>GB</span>, which takes some time in <span>CUDA</span>), inserting and deleting the items, and iterating through all items. Also included are all memory copies to/from the <span>GPU</span>.</p>
<p>The <span>GPU</span> hash table took 271 milliseconds. That includes just the <span>GPU</span> time to insert and delete the items, and not the time for memory copies or iterating over the resulting table. If the <span>GPU</span> table is long lived, or the hash table exists entirely on the <span>GPU</span> (for example, to build a hash table on the <span>GPU</span> that is used by other <span>GPU</span> code, not the <span>CPU</span>), then this is the relevant&nbsp;time.</p>
<p>The <span>GPU</span> hash table achieves high performance thanks to the large bandwidth and massive parallelism of <span>GPU</span>’s.</p>

<p>There are a couple of issues with this hash table design that should be kept in&nbsp;mind:</p>
<ul>
<li>Linear probing suffers from clustering, so that keys are often very far from their ideal slots in the&nbsp;table</li>
<li>Keys are not removed from the table by the delete function, and clutter the table over&nbsp;time</li>
</ul>
<p>As a result, performance of the hash table can degrade over time, especially with long-lived hash tables that have had many inserts and deletes. One way to mitigate these issues is to rehash the table into a new table with a sufficiently low load factor and filtering out deleted keys during the&nbsp;rehash.</p>
<p>To illustrate these issues, I used the hash table example code to create a hash table with a capacity of 128 million elements, and inserted 4 million elements into the table in a loop until the table had 124 million elements (a load factor around 0.96). Here’s the timing of the kernel that inserted 4 million elements; every row is a <span>CUDA</span> kernel call that inserts 4 million new items into the same hash&nbsp;table:</p>
<table>
<thead>
<tr>
<th>Load factor &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;</th>
<th>Time to insert 4194304 items</th>
</tr>
</thead>
<tbody>
<tr>
<td>0.00</td>
<td>11.608448 ms (361.314798 million keys/second)</td>
</tr>
<tr>
<td>0.03</td>
<td>11.751424 ms (356.918799 million keys/second)</td>
</tr>
<tr>
<td>0.06</td>
<td>11.942592 ms (351.205515 million keys/second)</td>
</tr>
<tr>
<td>0.09</td>
<td>12.081120 ms (347.178429 million keys/second)</td>
</tr>
<tr>
<td>0.12</td>
<td>12.242560 ms (342.600233 million keys/second)</td>
</tr>
<tr>
<td>0.16</td>
<td>12.396448 ms (338.347235 million keys/second)</td>
</tr>
<tr>
<td>0.19</td>
<td>12.533024 ms (334.660176 million keys/second)</td>
</tr>
<tr>
<td>0.22</td>
<td>12.703328 ms (330.173626 million keys/second)</td>
</tr>
<tr>
<td>0.25</td>
<td>12.884512 ms (325.530693 million keys/second)</td>
</tr>
<tr>
<td>0.28</td>
<td>13.033472 ms (321.810182 million keys/second)</td>
</tr>
<tr>
<td>0.31</td>
<td>13.239296 ms (316.807174 million keys/second)</td>
</tr>
<tr>
<td>0.34</td>
<td>13.392448 ms (313.184256 million keys/second)</td>
</tr>
<tr>
<td>0.37</td>
<td>13.624000 ms (307.861434 million keys/second)</td>
</tr>
<tr>
<td>0.41</td>
<td>13.875520 ms (302.280855 million keys/second)</td>
</tr>
<tr>
<td>0.44</td>
<td>14.126528 ms (296.909756 million keys/second)</td>
</tr>
<tr>
<td>0.47</td>
<td>14.399328 ms (291.284699 million keys/second)</td>
</tr>
<tr>
<td>0.50</td>
<td>14.690304 ms (285.515123 million keys/second)</td>
</tr>
<tr>
<td>0.53</td>
<td>15.039136 ms (278.892623 million keys/second)</td>
</tr>
<tr>
<td>0.56</td>
<td>15.478656 ms (270.973402 million keys/second)</td>
</tr>
<tr>
<td>0.59</td>
<td>15.985664 ms (262.379092 million keys/second)</td>
</tr>
<tr>
<td>0.62</td>
<td>16.668673 ms (251.627968 million keys/second)</td>
</tr>
<tr>
<td>0.66</td>
<td>17.587200 ms (238.486174 million keys/second)</td>
</tr>
<tr>
<td>0.69</td>
<td>18.690048 ms (224.413765 million keys/second)</td>
</tr>
<tr>
<td>0.72</td>
<td>20.278816 ms (206.831789 million keys/second)</td>
</tr>
<tr>
<td>0.75</td>
<td>22.545408 ms (186.038058 million keys/second)</td>
</tr>
<tr>
<td>0.78</td>
<td>26.053312 ms (160.989275 million keys/second)</td>
</tr>
<tr>
<td>0.81</td>
<td>31.895008 ms (131.503463 million keys/second)</td>
</tr>
<tr>
<td>0.84</td>
<td>42.103294 ms (99.619378 million keys/second)</td>
</tr>
<tr>
<td>0.87</td>
<td>61.849056 ms (67.815164 million keys/second)</td>
</tr>
<tr>
<td>0.90</td>
<td>105.695999 ms (39.682713 million keys/second)</td>
</tr>
<tr>
<td>0.94</td>
<td>240.204636 ms (17.461378 million keys/second)</td>
</tr>
</tbody>
</table>
<p>Clearly, performance degrades as the load factor increases. This is not a desirable property of a hash table for most applications. If the application is inserting items into a hash table and then throwing it away (for example, to count the words in a book), this might not be a problem. But if the application has a long-lived hash table (for example, an image editing application that stores the non-empty parts of the image in a table, and where the user causes many inserts and deletes in the table), this could be a real&nbsp;issue.</p>
<p>I measured the probe lengths of the hash table after 64 million inserts (so at a load factor of 0.50). The average probe length was 0.4774, so most keys were either in their best slot or just one slot away. The maximum probe length was&nbsp;60.</p>
<p>I then measured the probe lengths of the table after 124 million inserts (a load factor of 0.97), and found that the average probe length was 10.1757, and the maximum probe length was <em>6474</em> (!!). Linear probing’s performance is not great with high load&nbsp;factors.</p>
<p>The best way to use this hash table is to keep load factors low. However, that trades off memory for performance. Fortunately, with 32 bit keys and values, that might be a reasonable choice for an application. In the example above, if we wanted a load factor of 0.25 with a hash table capacity of 128 million items, then we could insert no more than 32 million items, wasting the other 96 million items— times 8 bytes for each key/value, that’s 768 <span>MB</span> of wasted&nbsp;space.</p>
<p>Note that this is wasted space in <span>GPU</span> <span>VRAM</span>, which is a more precious commodity than the system memory. While most modern desktop <span>GPU</span>’s running <span>CUDA</span> have at least 4 <span>GB</span> of <span>VRAM</span> (and at the time of writing, an <span>NVIDIA</span> 2080 Ti has 11 <span>GB</span>), it’s still not ideal to waste so much&nbsp;memory.</p>
<p>I will be writing more about other ways to build <span>GPU</span> hash tables that do not suffer long probe lengths like linear probing and ways to reuse deleted slots in the&nbsp;table.</p>

<p>To determine the probe length of a key, we can subtract the key’s hash (its ideal index in the table) from the key’s actual index in the&nbsp;table:</p>
<div><pre><span></span><span>// get_key_index() -&gt; index of key in hash table</span>
<span>uint32_t</span> <span>probelength</span> <span>=</span> <span>(</span><span>get_key_index</span><span>(</span><span>key</span><span>)</span> <span>-</span> <span>hash</span><span>(</span><span>key</span><span>))</span> <span>&amp;</span> <span>(</span><span>hashtablecapacity</span><span>-</span><span>1</span><span>);</span>
</pre></div>


<p>Because of the magic of two’s complement numbers, and the fact that the hash table capacity is a power-of-two, this works even when the key’s index has wrapped around the beginning of the table. Consider a key that hashes to 1 but has been inserted at slot 3; then for a hash table of capacity 4, we have <code>(3 - 1) &amp; 3</code>, which equals&nbsp;2.</p>

<p>Contact me at <a href="http://twitter.com/nosferalatu">Nosferalatu on Twitter</a> or open a new issue at <a href="https://github.com/nosferalatu/SimpleGPUHashTable">the example code’s Github repo</a> with any questions or&nbsp;comments.</p>
<p>This code is based on the excellent work&nbsp;at:</p>
<ul>
<li><a href="https://preshing.com/20130605/the-worlds-simplest-lock-free-hash-table/">The World’s Simplest Lock-Free Hash&nbsp;Table</a></li>
<li><a href="https://web.stanford.edu/class/ee380/Abstracts/070221_LockFreeHash.pdf">A Lock-Free Wait-Free Hash&nbsp;Table</a></li>
</ul>
<p>In the future, I will be writing more about <span>GPU</span> hash table implementations and analyzing their performance. I’ve been looking at chaining, Robin Hood hashing, and cuckoo hashing using atomic operations for <span>GPU</span>-friendly data&nbsp;structures.</p>
            </div>
            <!-- /.entry-content -->
        </article>
    </section>

        </div>
        
    </div>
</div></div></div>
    </div>
    <footer>
        <div>created by buildstarted &copy; 2020 <a href="/about">about</a></div>
        <div>Share this page on social media: copy and paste this url https://linksfor.dev/</div>
        <div>If you prefer RSS: <a href="https://linksfor.dev/feed.xml">https://linksfor.dev/feed.xml</a></div>
		<div>Customer satisfaction guaranteed to be optional.</div>
    </footer>
    
    <script async defer>
        _dna = window._dna || {};
        _dna.siteId = "linksfor.devs";
        _dna.outlink = true;

        (function () {
            let dna = document.createElement('script');
            dna.type = 'text/javascript';
            dna.async = true;
            dna.src = '//dna.buildstarted.com/t.js';
            let s = document.getElementsByTagName('script')[0];
            s.parentNode.insertBefore(dna, s);
        })();
    </script>
    <noscript><img src="//dna.buildstarted.com/g?siteId=linksfor.devs"/></noscript>
</body>
</html>