<!DOCTYPE html>
<html lang="en">
<head>
    <title>
Machine Learning of Sets - linksfor.dev(s)    </title>
    <meta charset="utf-8">
    <link rel="alternate" type="application/rss+xml" title="Linksfor.dev(s) feed" href="https://linksfor.dev/feed.rss" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta name="google" value="notranslate">
    <link rel="stylesheet" type="text/css" href="/style.min.css" />
    <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon">
    <link rel="icon" href="/favicon.ico" type="image/x-icon">
    <meta property="og:title" content="Machine Learning of Sets - linksfor.dev(s)"/>
    <meta property="og:description" content="In machine learning, we typically work with input pairs (x, y), and we try to figure out how x and y depend on each other.To do so, we gather many such pairs..."/>
    <meta property="og:type" content="website"/>
    <meta property="og:url" content="http://akosiorek.github.io/ml/2020/08/12/machine_learning_of_sets.html"/>

<meta property="og:site_name" content="linksfor.dev(s)" />
</head>
<body>
    <div class="devring" style="background: #222">
        <div style="text-align:center">Explore other dev related sites in this ring. If you would like to join this ring <a href="https://devring.club">click here</a>.</div>
        <div class="grid">
            <div style="display: grid; grid-template-columns: .5fr 1fr 1fr 1fr; text-align: center;">
                <span class="devring-title"><a href="https://devring.club/">devring.club</a></span>
                <a href="https://devring.club/sites/1/prev" class="devring-previous">Previous</a>
                <a href="https://devring.club/random" class="devring-random">Random</a>
                <a href="https://devring.club/sites/1/next" class="devring-next">Next</a>
            </div>
        </div>
    </div>
    <div class="grid">
        <h1 style="margin: unset">
<a href="/" style="color:inherit">linksfor.dev(s)</a>
        </h1>
        <title>linksfor.dev(s) - Machine Learning of Sets</title>
<div class="readable">
        <h1>Machine Learning of Sets</h1>
            <div>Reading time: 21-26 minutes</div>
        <div>Posted here: 14 Aug 2020</div>
        <p><a href="http://akosiorek.github.io/ml/2020/08/12/machine_learning_of_sets.html">http://akosiorek.github.io/ml/2020/08/12/machine_learning_of_sets.html</a></p>
        <hr/>
<div id="readability-page-1" class="page"><div itemprop="articleBody">
    <p>In machine learning, we typically work with input pairs (x, y), and we try to figure out how x and y depend on each other.
To do so, we gather many such pairs and hope that the dependence will reveal itself if a) we have enough data, b) our model is expressive enough to approximate this dependency, and c) we get the hyperparameters right.
In the simplest case, both x and y are just scalar values (or vectors <span id="MathJax-Element-1-Frame" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi mathvariant=&quot;bold&quot;>x</mi></mrow><mo>,</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi mathvariant=&quot;bold&quot;>y</mi></mrow></math>" role="presentation"><span role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold">x</mi></mrow><mo>,</mo><mrow><mi mathvariant="bold">y</mi></mrow></math></span></span>); for example, given some measurements of a plant’s shape, we might want to predict its species. The measurements here are real vectors <span id="MathJax-Element-2-Frame" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi mathvariant=&quot;bold&quot;>x</mi></mrow><mo>&amp;#x2208;</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi class=&quot;MJX-tex-caligraphic&quot; mathvariant=&quot;script&quot;>X</mi></mrow></math>" role="presentation"><span role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold">x</mi></mrow><mo>∈</mo><mrow><mi mathvariant="script">X</mi></mrow></math></span></span>, where the input space <span id="MathJax-Element-3-Frame" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi class=&quot;MJX-tex-caligraphic&quot; mathvariant=&quot;script&quot;>X</mi></mrow><mo>=</mo><msup><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi mathvariant=&quot;double-struck&quot;>R</mi></mrow><mi>d</mi></msup></math>" role="presentation"><span role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="script">X</mi></mrow><mo>=</mo><msup><mrow><mi mathvariant="double-struck">R</mi></mrow><mi>d</mi></msup></math></span></span> is usually Euclidean, and the species is a label <span id="MathJax-Element-4-Frame" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi mathvariant=&quot;bold&quot;>y</mi></mrow><mo>&amp;#x2208;</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi class=&quot;MJX-tex-caligraphic&quot; mathvariant=&quot;script&quot;>Y</mi></mrow></math>" role="presentation"><span role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold">y</mi></mrow><mo>∈</mo><mrow><mi mathvariant="script">Y</mi></mrow></math></span></span> (usually an integer or a one-hot vector), but it is common for <span id="MathJax-Element-5-Frame" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi mathvariant=&quot;bold&quot;>x</mi></mrow></math>" role="presentation"><span role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold">x</mi></mrow></math></span></span> and <span id="MathJax-Element-6-Frame" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi mathvariant=&quot;bold&quot;>y</mi></mrow></math>" role="presentation"><span role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold">y</mi></mrow></math></span></span> to have more structure.</p>

<p>One of the main assumptions we rely on is that the pairs of (x, y) points are <a href="https://en.wikipedia.org/wiki/Independent_and_identically_distributed_random_variables">independent and identically distributed (i.i.d.) random variables</a>.
Let us unpack this a bit, starting from the end,</p>

<ul>
  <li><code>random variable</code>: there exists some stochastic generative process from which the variables were randomly sampled,</li>
  <li><code>identically</code>: all samples come from the same probability distribution,</li>
  <li><code>independent</code>: the generative process has no memory of generated samples, and hence any generated sample does not change the distribution over future generated samples.</li>
</ul>

<p>Any structure in <span id="MathJax-Element-7-Frame" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi mathvariant=&quot;bold&quot;>x</mi></mrow><mo>,</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi mathvariant=&quot;bold&quot;>y</mi></mrow></math>" role="presentation"><span role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold">x</mi></mrow><mo>,</mo><mrow><mi mathvariant="bold">y</mi></mrow></math></span></span>, or both introduces constraints, and a successful application of an algorithm to a particular problem does heavily depend on whether or not this algorithm takes the relevant constraints into account.
A common constraint in image-related problems is translation equivariance<sup id="fnref:cnnequiv" role="doc-noteref"><a href="#fn:cnnequiv">1</a></sup>—the output of the algorithm should shift with any shifts applied to the image (you can read more about equvariances in <a href="https://fabianfuchsml.github.io/equivariance1of2/">this excellent blog post</a>).
In natural language-related problems, a typical constraint is causality: a token at position t can depend on any previous tokens at position 1:t-1, but it cannot depend on any future tokens<sup id="fnref:languecausality" role="doc-noteref"><a href="#fn:languecausality">2</a></sup>.</p>

<p>In the above examples, the dependencies between points (e.g., autoregressive dependence in NLP) are clear from the context.
However, if a data point is not a vector, matrix, or a sequence of vectors, but it is a <strong>set of vectors</strong>, these dependencies become less clear.
In particular, elements in an input set resemble elements in a dataset (i.e., lack of order), but the critical difference is that they are <strong>not independent</strong>, therefore breaking the i.i.d. assumption.
Accounting for this specific structure in inputs or outputs of an ML model leads to a family of set learning problems, which have recently gained considerable attention in the machine learning community.
I thought it would be useful to delve into the machine learning of sets.
In the following, we will consider set-to-vector, vector-to-set, and set-to-set problems and provide implementations of simple algorithms in <a href="https://github.com/google/jax">JAX</a> and <a href="https://github.com/deepmind/dm-haiku">haiku</a>.</p>

<p>First some imports:</p>

<div><div><pre><code>import jax
import jax.numpy as jnp
import haiku as hk
</code></pre></div></div>


<p>Before we start, it is useful to introduce some notation.
Let <span id="MathJax-Element-8-Frame" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi mathvariant=&quot;bold&quot;>x</mi></mrow><mo>&amp;#x2208;</mo><msup><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi mathvariant=&quot;double-struck&quot;>R</mi></mrow><mi>d</mi></msup></math>" role="presentation"><span role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold">x</mi></mrow><mo>∈</mo><msup><mrow><mi mathvariant="double-struck">R</mi></mrow><mi>d</mi></msup></math></span></span> be an input vector, <span id="MathJax-Element-9-Frame" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi mathvariant=&quot;bold&quot;>y</mi></mrow><mo>&amp;#x2208;</mo><msup><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi mathvariant=&quot;double-struck&quot;>R</mi></mrow><mi>k</mi></msup></math>" role="presentation"><span role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold">y</mi></mrow><mo>∈</mo><msup><mrow><mi mathvariant="double-struck">R</mi></mrow><mi>k</mi></msup></math></span></span> the output vector, and let <span id="MathJax-Element-10-Frame" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>X</mi><mo>=</mo><mo fence=&quot;false&quot; stretchy=&quot;false&quot;>{</mo><msub><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi mathvariant=&quot;bold&quot;>x</mi></mrow><mi>i</mi></msub><msubsup><mo fence=&quot;false&quot; stretchy=&quot;false&quot;>}</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>M</mi></msubsup></math>" role="presentation"><span role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>X</mi><mo>=</mo><mo fence="false" stretchy="false">{</mo><msub><mrow><mi mathvariant="bold">x</mi></mrow><mi>i</mi></msub><msubsup><mo fence="false" stretchy="false">}</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>M</mi></msubsup></math></span></span> and <span id="MathJax-Element-11-Frame" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>Y</mi><mo>=</mo><mo fence=&quot;false&quot; stretchy=&quot;false&quot;>{</mo><msub><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi mathvariant=&quot;bold&quot;>y</mi></mrow><mi>j</mi></msub><msubsup><mo fence=&quot;false&quot; stretchy=&quot;false&quot;>}</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></msubsup></math>" role="presentation"><span role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>Y</mi><mo>=</mo><mo fence="false" stretchy="false">{</mo><msub><mrow><mi mathvariant="bold">y</mi></mrow><mi>j</mi></msub><msubsup><mo fence="false" stretchy="false">}</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></msubsup></math></span></span> be sets of <span id="MathJax-Element-12-Frame" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>M</mi></math>" role="presentation"><span role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>M</mi></math></span></span> and <span id="MathJax-Element-13-Frame" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>N</mi></math>" role="presentation"><span role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>N</mi></math></span></span> elements, respectively.
Note that, until now, <span id="MathJax-Element-14-Frame" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>y</mi></math>" role="presentation"><span role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>y</mi></math></span></span> or <span id="MathJax-Element-15-Frame" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi mathvariant=&quot;bold&quot;>y</mi></mrow></math>" role="presentation"><span role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold">y</mi></mrow></math></span></span> were simply labels.
From now on, however, <span id="MathJax-Element-16-Frame" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi mathvariant=&quot;bold&quot;>x</mi></mrow></math>" role="presentation"><span role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold">x</mi></mrow></math></span></span> and <span id="MathJax-Element-17-Frame" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi mathvariant=&quot;bold&quot;>y</mi></mrow></math>" role="presentation"><span role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold">y</mi></mrow></math></span></span> can live in the same space, and simply be elements of different sets.
I will also use <span id="MathJax-Element-18-Frame" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi class=&quot;MJX-tex-caligraphic&quot; mathvariant=&quot;script&quot;>L</mi></mrow><mo stretchy=&quot;false&quot;>(</mo><mi>X</mi><mo>,</mo><mi>Y</mi><mo stretchy=&quot;false&quot;>)</mo></math>" role="presentation"><span role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="script">L</mi></mrow><mo stretchy="false">(</mo><mi>X</mi><mo>,</mo><mi>Y</mi><mo stretchy="false">)</mo></math></span></span> as a loss function operating on two sets, and <span id="MathJax-Element-19-Frame" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>l</mi><mo stretchy=&quot;false&quot;>(</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi mathvariant=&quot;bold&quot;>x</mi></mrow><mo>,</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi mathvariant=&quot;bold&quot;>y</mi></mrow><mo stretchy=&quot;false&quot;>)</mo></math>" role="presentation"><span role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>l</mi><mo stretchy="false">(</mo><mrow><mi mathvariant="bold">x</mi></mrow><mo>,</mo><mrow><mi mathvariant="bold">y</mi></mrow><mo stretchy="false">)</mo></math></span></span> will be a loss function for pairs of elements.</p>


<p>This is perhaps the simplest set-learning problem since it only requires permutation invariance.
A function <span id="MathJax-Element-20-Frame" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>f</mi></math>" role="presentation"><span role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>f</mi></math></span></span> is invariant to permutations <span id="MathJax-Element-21-Frame" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>&amp;#x03C0;</mi></math>" role="presentation"><span role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>π</mi></math></span></span> if <span id="MathJax-Element-22-Frame" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi mathvariant=&quot;normal&quot;>&amp;#x2200;</mi><mi>&amp;#x03C0;</mi></math>" role="presentation"><span role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi mathvariant="normal">∀</mi><mi>π</mi></math></span></span>: <span id="MathJax-Element-23-Frame" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>f</mi><mo stretchy=&quot;false&quot;>(</mo><mi>X</mi><mo stretchy=&quot;false&quot;>)</mo><mo>=</mo><mi>f</mi><mo stretchy=&quot;false&quot;>(</mo><mi>&amp;#x03C0;</mi><mi>X</mi><mo stretchy=&quot;false&quot;>)</mo></math>" role="presentation"><span role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>f</mi><mo stretchy="false">(</mo><mi>X</mi><mo stretchy="false">)</mo><mo>=</mo><mi>f</mi><mo stretchy="false">(</mo><mi>π</mi><mi>X</mi><mo stretchy="false">)</mo></math></span></span>.
Permutation invariance has always been known in machine learning, as loss functions we use almost never<sup id="fnref:acn" role="doc-noteref"><a href="#fn:acn">3</a></sup> depend on the ordering of elements in our datasets or minibatches.
This is not for the lack of order: to create a minibatch, we stack multiple data elements in an array; this pairs every element in the minibatch with its minibatch index, therefore implicitly creating an order.
Loss functions tend to discard information about the order, usually by taking the mean over data examples.
We can create permutation-invariant functions by following a similar logic.</p>

<p>Examples in a minibatch are processed independently (which reflects their i.i.d. nature), but if each entry in the minibatch contains more than just a single data point (many pixels in an image, points in a point cloud, tokens in a language sentence), then flattening these points into a vector and feeding it into an MLP or a CNN results in different parameters being used for processing different data points, and hence order is used implicitly; feeding the points into an RNN reuses parameters, but introduces an explicit dependence on the order.</p>

<p>A straightforward solution to this issue is to treat points in a single example in the same way we treat examples in the minibatch: treat them independently.
This approach, followed by a permutation-invariant pooling operation such as max or mean pooling, is explored in <a href="https://arxiv.org/abs/1703.06114">Zaheer et al., “Deep Sets”, NeurIPS 2017</a> and is proven to be a universal set-function approximator<sup id="fnref:deepsetdim" role="doc-noteref"><a href="#fn:deepsetdim">4</a></sup>.</p>

<div><div><pre><code>class DeepSet(hk.Module):

  def __init__(self, encoder, decoder):
    super().__init__()
    self._encoder = encoder
    self._decoder = decoder
    
  def __call__(self, x):
    """Compute the DeepSet embedding.

    Args:
      x: Tensor of shape [batch_size, n_elems, n_dim].
    """
    return self._decoder(self._encoder(x).mean(1))
</code></pre></div></div>

<p>While newer approaches with better empirical performance exist, they all draw from the Deep Sets framework<sup id="fnref:setembeddings" role="doc-noteref"><a href="#fn:setembeddings">5</a></sup>.
Another factor contributing to the fact that the set-to-vector problem is quite easy is that pooling operations naturally work with variable-sized sets–there is nothing extra we have to do to handle sets of variable cardinality.
This is not the case in the following two problems, where we have to take the set size into account explicitly.</p>


<p>In vector-to-set, the task is to generate a set of real vectors from some (usually vector-valued) conditioning.</p>

<p>The majority of approaches out there focus on generating ordered sequences instead of unordered sets, and usually of fixed or at least known size.
This allows using MLPs<sup id="fnref:setae" role="doc-noteref"><a href="#fn:setae">6</a></sup> and RNNs<sup id="fnref:order_matters" role="doc-noteref"><a href="#fn:order_matters">7</a></sup> to predict fixed- and variable-length sets, respectively, but at the price of having to learn permutation-equivariance from data.
Learning permutation-equivariance can be induced by data augmentation. It is easy to generate different permutations, but usually comes at a decreased performance and/or longer training times compared to truly permutation-equivariant methods<sup id="fnref:data_augmentation" role="doc-noteref"><a href="#fn:data_augmentation">8</a></sup>.</p>

<div><div><pre><code>  def set_mlp(conditioning, decoder, n_elements):
    """Predicts a set.

    Args:
      conditioning: tensor of shape [batch_size, n_dim].
      decoder: callable, e.g. an MLP.
      n_elements: int.
    """
    z = decoder(conditioning)
    batch_size = conditioning.shape[0]
    # all we can do here is reshape!
    return z.reshape(batch_size, n_elements, -1)

  def set_rnn(conditioning, state, rnn, n_elements):
    """Predicts a set.

    Args:
      conditioning: tensor of shape [batch_size, n_dim].
      state: initial state for the rnn.
      rnn: rnn core.
      n_elements: int.
    """
    zs = []
    for _ in range(n_elements):
      z, state = rnn(conditioning, state)
      zs.append(z[:, None])  # add an axis

    return jnp.concatenate(zs, 1)
</code></pre></div></div>

<h4 id="permutation-invariant-loss-functions">Permutation-Invariant Loss Functions</h4>
<p>Learning to generate sets based on some conditioning typically requires scoring that set against the conditioning.
If we have ground-truth sets at our disposal, we can compare the generated sets against the ground-truth ones for the same conditioning.
This can take the form of supervised learning (think of detecting objects in an image, where we need to generate a set of bounding boxes) or unsupervised learning (autoencoding point-clouds, say).
Since we generally have no guarantee that the generated sets will obey any ordering (why should they?), we have to apply losses invariant to that ordering.
We have two options here:</p>

<ul>
  <li>We can find an optimal matching between two sets<sup id="fnref:bipartite_matching" role="doc-noteref"><a href="#fn:bipartite_matching">9</a></sup>, which comes down to finding a permutation <span id="MathJax-Element-24-Frame" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>&amp;#x03C0;</mi></math>" role="presentation"><span role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>π</mi></math></span></span> of one of the sets that minimizes the computed loss, that is: <span id="MathJax-Element-25-Frame" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msup><mi>&amp;#x03C0;</mi><mo>&amp;#x22C6;</mo></msup><mo>=</mo><mi>arg</mi><mo>&amp;#x2061;</mo><munder><mo movablelimits=&quot;true&quot; form=&quot;prefix&quot;>min</mo><mi>&amp;#x03C0;</mi></munder><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi class=&quot;MJX-tex-caligraphic&quot; mathvariant=&quot;script&quot;>L</mi></mrow><mo stretchy=&quot;false&quot;>(</mo><mi>&amp;#x03C0;</mi><mi>X</mi><mo>,</mo><mi>Y</mi><mo stretchy=&quot;false&quot;>)</mo></math>" role="presentation"><span role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mi>π</mi><mo>⋆</mo></msup><mo>=</mo><mi>arg</mi><mo>⁡</mo><munder><mo movablelimits="true" form="prefix">min</mo><mi>π</mi></munder><mrow><mi mathvariant="script">L</mi></mrow><mo stretchy="false">(</mo><mi>π</mi><mi>X</mi><mo>,</mo><mi>Y</mi><mo stretchy="false">)</mo></math></span></span>, with <span id="MathJax-Element-26-Frame" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi class=&quot;MJX-tex-caligraphic&quot; mathvariant=&quot;script&quot;>L</mi></mrow><mo stretchy=&quot;false&quot;>(</mo><mi>&amp;#x03C0;</mi><mi>X</mi><mo>,</mo><mi>Y</mi><mo stretchy=&quot;false&quot;>)</mo><mo>=</mo><munder><mo>&amp;#x2211;</mo><mi>i</mi></munder><mi>l</mi><mo stretchy=&quot;false&quot;>(</mo><msub><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi mathvariant=&quot;bold&quot;>x</mi></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>&amp;#x03C0;</mi><mo stretchy=&quot;false&quot;>(</mo><mi>i</mi><mo stretchy=&quot;false&quot;>)</mo></mrow></msub><mo>,</mo><msub><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi mathvariant=&quot;bold&quot;>y</mi></mrow><mi>i</mi></msub><mo stretchy=&quot;false&quot;>)</mo></math>" role="presentation"><span role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="script">L</mi></mrow><mo stretchy="false">(</mo><mi>π</mi><mi>X</mi><mo>,</mo><mi>Y</mi><mo stretchy="false">)</mo><mo>=</mo><munder><mo>∑</mo><mi>i</mi></munder><mi>l</mi><mo stretchy="false">(</mo><msub><mrow><mi mathvariant="bold">x</mi></mrow><mrow><mi>π</mi><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msub><mo>,</mo><msub><mrow><mi mathvariant="bold">y</mi></mrow><mi>i</mi></msub><mo stretchy="false">)</mo></math></span></span>. This can be done exactly using the cubic <a href="https://en.wikipedia.org/wiki/Hungarian_algorithm">Hungarian matching</a> algorithm, or approximately using e.g. <a href="https://arxiv.org/abs/1106.1925">optimal-transport</a>- or <a href="https://web.stanford.edu/~bayati/papers/bpmwmIT.pdf">message-passing</a>-based algorithms.</li>
  <li>Instead of finding a matching, we can find a lower bound on what the matched loss would be. A popular choice here is the Chamfer loss<sup id="fnref:chamfer" role="doc-noteref"><a href="#fn:chamfer">10</a></sup>, which computes <span id="MathJax-Element-27-Frame" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><munder><mo>&amp;#x2211;</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>x</mi><mo>&amp;#x2208;</mo><mi>X</mi></mrow></munder><munder><mo movablelimits=&quot;true&quot; form=&quot;prefix&quot;>min</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>y</mi><mo>&amp;#x2208;</mo><mi>Y</mi></mrow></munder><mi>l</mi><mo stretchy=&quot;false&quot;>(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy=&quot;false&quot;>)</mo><mo>+</mo><munder><mo>&amp;#x2211;</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>y</mi><mo>&amp;#x2208;</mo><mi>Y</mi></mrow></munder><munder><mo movablelimits=&quot;true&quot; form=&quot;prefix&quot;>min</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>x</mi><mo>&amp;#x2208;</mo><mi>X</mi></mrow></munder><mi>l</mi><mo stretchy=&quot;false&quot;>(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy=&quot;false&quot;>)</mo></math>" role="presentation"><span role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><munder><mo>∑</mo><mrow><mi>x</mi><mo>∈</mo><mi>X</mi></mrow></munder><munder><mo movablelimits="true" form="prefix">min</mo><mrow><mi>y</mi><mo>∈</mo><mi>Y</mi></mrow></munder><mi>l</mi><mo stretchy="false">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy="false">)</mo><mo>+</mo><munder><mo>∑</mo><mrow><mi>y</mi><mo>∈</mo><mi>Y</mi></mrow></munder><munder><mo movablelimits="true" form="prefix">min</mo><mrow><mi>x</mi><mo>∈</mo><mi>X</mi></mrow></munder><mi>l</mi><mo stretchy="false">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy="false">)</mo></math></span></span>. For every element in one set, it finds the element in the other set that results in the lowest pairwise loss. This loss does not work for multisets as elements can be repeated.</li>
</ul>

<p>If we do not have ground-truth for each conditioning (we have just sets), or if we have many possible sets for each conditioning (e.g., a group of possible sets for one of a few labels), we can instead learn by matching distributions e.g., in the GAN setting.
If we take this approach, we have two problems, really: that of vector-to-set for the generator and set-to-vector for the discriminator. 
Fortunately, we know how to solve the set-to-vector problem with a permutation-invariant neural net, and shortly I am going to describe some permutation-equivariant methods for generation.
This is precisely what we recently explored in <a href="https://oolworkshop.github.io/program/ool_32.html">Stelzner et al., “Generative Adversarial Set Transformers”, ICML 2020 Object-Oriented Learning Workshop</a>.</p>

<p>Coincidentally, sometimes we have to deal with a set of latent variables inside a model. For example in Attend-Infer-Repeat (AIR, <a href="https://papers.nips.cc/paper/6230-attend-infer-repeat-fast-scene-understanding-with-generative-models">paper</a>, <a href="http://akosiorek.github.io/ml/2017/09/03/implementing-air.html">blog</a>), a set of object-centered latent variables was used to render an image.
We did not need to worry about permutations of these variables, though, since the rendering process was permutation-invariant, and any loss applied to the final image carried over to the latent variables in a permutation-invariant way, too!</p>

<h4 id="gradient-descent-to-the-rescue">Gradient Descent to the Rescue!</h4>
<p>Until recently, there was no accepted method able to predict variable-sized sets in a permutation-equivariant manner.
For completness, note that a function g is equivariant to permutations <span id="MathJax-Element-28-Frame" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>&amp;#x03C0;</mi></math>" role="presentation"><span role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>π</mi></math></span></span> if <span id="MathJax-Element-29-Frame" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi mathvariant=&quot;normal&quot;>&amp;#x2200;</mi><mi>&amp;#x03C0;</mi></math>" role="presentation"><span role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi mathvariant="normal">∀</mi><mi>π</mi></math></span></span>: <span id="MathJax-Element-30-Frame" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>&amp;#x03C0;</mi><mi>g</mi><mo stretchy=&quot;false&quot;>(</mo><mi>X</mi><mo stretchy=&quot;false&quot;>)</mo><mo>=</mo><mi>g</mi><mo stretchy=&quot;false&quot;>(</mo><mi>&amp;#x03C0;</mi><mi>X</mi><mo stretchy=&quot;false&quot;>)</mo></math>" role="presentation"><span role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>π</mi><mi>g</mi><mo stretchy="false">(</mo><mi>X</mi><mo stretchy="false">)</mo><mo>=</mo><mi>g</mi><mo stretchy="false">(</mo><mi>π</mi><mi>X</mi><mo stretchy="false">)</mo></math></span></span>.
<a href="https://arxiv.org/abs/1906.06565">Zhang et al., “Deep Set Prediction Networks”, NeurIPS 2019</a> used the well-known (but still pretty cool!) observation that the gradient of a permutation-invariant function (such as the DeepSet embedding) is permutation equivariant to the input set<sup id="fnref:invgrad" role="doc-noteref"><a href="#fn:invgrad">11</a></sup>.
Their introduced model, DSPN, uses a fixed initial set adapted via a nested loop of gradient-descent on a learned loss function.
This loss function compares the currently-generated set and the conditioning, telling us how well the current set and the conditioning match.
DSPN achieved quite good results on point-cloud generation (but only MNIST) and showed proof-of-concept results to object detection in images.</p>

<div><div><pre><code>class DeepSetPredictionNetwork(hk.Module):

  def __init__(self, set_encoder, max_n_points, n_dim,
              n_updates=5, step_size=1., repr_loss_func):
    """Builds the module.

    Args:
      set_encoder: An encoder for sets, e.g. a DeepSet.
      max_n_points: an integer.
      n_dim: dimensionality of the set elements.
      n_updates: The number of gradient updates applied to the initial set.
      step_size: Learning rate for the inner gradient descent loop.
      repr_loss_func: A loss function used to compare the embedding of a
        generated set and an embedding of the conditioning, e.g. squared-error.
    """
    
    super().__init__()
    self._set_encoder = set_encoder
    self._max_n_points = max_n_points
    self._n_dim = n_dim
    self._n_updates = n_updates
    self._step_size = step_size

    self._clip_pres = lambda x: jnp.clip(x, 0., 1.)

    def repr_loss(inputs, target):
      h = self._set_encoder(*inputs)
      # We take a mean over the number of points.
      return repr_loss_func(h, target).mean(1).sum()

    self._repr_loss_grad = hk.grad(repr_loss)

  def __call__(self, z):     
    # create the initial set and presence variables
    current_set = hk.get_parameter('init_set',
                            shape=(self._max_n_points, self._n_dim),
                            init=hk.initializers.RandomUniform(0., 1.)
    )
    
    current_pres = self._clip_pres(hk.get_parameter('init_pres',
                                    shape=(self._max_n_points, 1),
                                    init=hk.initializers.Constant(.5),
    ))

    # DSPN returns the starting set/pres and apparently puts loss on it.
    all_sets, all_pres = [current_set], [current_pres]
    for _ in range(self._n_updates):
      set_grad, pres_grad = self._repr_loss_grad((current_set, current_pres), z)

      current_set = current_set - self._step_size * set_grad
      current_pres = current_pres - self._step_size * pres_grad
      # We need to make sure that the presence is valid after each update.
      current_pres = self._clip_pres(current_pres)

      all_sets.append(current_set)
      all_pres.append(current_pres)

    return all_sets, all_pres
</code></pre></div></div>

<figure id="DSPN_flow">
  <p><img src="http://akosiorek.github.io/resources/DSPN_flow.png">
  </p>

  <figcaption>
    <b>Fig. 1:</b> <a href="https://arxiv.org/abs/1906.06565">DSPN</a> iteratively transforms an initial set (left) into the final prediction (2nd from the right) by gradient descent.
  </figcaption>
</figure>

<p>While a cool idea, the gradient iteration learned by DSPN is a flow field (see <a href="#DSPN_flow">Fig. 1</a>), and it necessarily requires many iterations to reach the final prediction.
Instead, we can learn a permutation-equivariant operator that directly outputs the required set.</p>

<h4 id="attention-is-all-you-need-really">Attention is All You Need, Really</h4>
<p>Not too long ago, <a href="https://arxiv.org/abs/1706.03762">Vaswani et al. showed that we could replace RNNs with attention, causal masking, and position embeddings</a>.
It turns out that discarding causal masking and position embeddings leads to self-attention that is permutation-equivariant, as explored in <a href="https://arxiv.org/abs/1810.00825">Lee et al., “Set Transformer”, ICML 2019</a>.
If this is the case, can we build a model similar to DSPN, but with a transformer instead of the inner gradient-descent inner loop?
Of course, we can!
There are several advantages:</p>
<ul>
  <li>The initial set can be higher-dimensional (in DSPN, it has to be the same dimensionality as the output set), leading to more degrees of freedom.</li>
  <li>Transformer layers can operate on the set of different dimensionality, and they do not have to project it to the output dimensionality between layers. This might seem trivial, but it relaxes the flow-field constraint, and in practice, creates transformations that can hold on to some additional state, akin to RNNs.</li>
  <li>DSPN captures dependencies between individual points only via a pooling operation in its DeepSet encoder. Transformers are all about relational reasoning, and can directly use interdependencies between points to generate the final set.</li>
</ul>

<figure id="tspn">
  <p><img src="http://akosiorek.github.io/resources/tspn.svg">
  </p>

  <figcaption>
    <b>Fig. 2:</b> <a href="https://arxiv.org/abs/2006.16841">TSPN</a> uses a Transformer to directly transform a random point cloud.
  </figcaption>
</figure>

<p>We explored this idea in two recent papers; both published at the <a href="https://oolworkshop.github.io/">ICML 2020 Object-Oriented Learning workshop</a>,</p>

<ul>
  <li><a href="https://arxiv.org/abs/2006.16841">Kosiorek, Kim, and Rezende, “Conditional Set Generation with Transformers”</a>, where we introduce the Transformer Set Prediction Network (TSPN). TSPN uses an MLP to predict the required number of points from a conditioning, samples the required number of points from a base distribution, and transforms them using a Transformer, see <a href="#tspn">Fig. 2</a> for an overview.</li>
  <li><a href="https://oolworkshop.github.io/program/ool_32.html">Stelzner, Kersting, and Kosiorek, “Generative Adversarial Set Transformers”</a> introduces GAST: a similar idea, where a number of points from a base distribution are conditionally-transformed (based on a global noise vector) using a Transormer. We then use a Set Transformer to discriminate between the generated and real sets.</li>
</ul>

<p>The same idea was concurrently explored by at least two other groups<sup id="fnref:other_set_att_papers" role="doc-noteref"><a href="#fn:other_set_att_papers">12</a></sup>.
While details differ, the main finding is that an initial set (randomly-sampled or deterministic and learned) passed through several layers of attention leads to state-of-the-art set generation.
The general architecture is as follows:</p>
<ul>
  <li>Some (big) neural net encoder for processing the conditioning, e.g., a ResNet for images.</li>
  <li>The encoder produces some key-and-value vectors.</li>
  <li>We take either a deterministic or randomly-sampled set of queries and attend over the key-and-value pairs.</li>
  <li>The result might be post-processed by self-attention and/or point-wise MLPs.</li>
  <li>We apply a permutation-invariant loss function, one of the described above. Hungarian matching seems to give the best results.</li>
</ul>

<figure id="slot_attention">
  <p><img src="http://akosiorek.github.io/resources/slot_attention.png">
  </p>

  <figcaption>
    <b>Fig. 3:</b> <a href="https://arxiv.org/abs/2006.15055">Slot Attention</a> induces competition between queries, leading to SOTA unsupervised object segmentation.
  </figcaption>
</figure>

<p>The results of <a href="https://github.com/facebookresearch/detr">Carion et al.’s DETR</a> model are particularly impressive. While it still required quite a bit of engineering, this pure set-prediction approach achieves state-of-the-art on large-scale object detection on COCO!
<a href="https://arxiv.org/abs/2006.15055">Locatello et al.</a> show that the particular form of attention required might depend on the task; in their experiments, they normalize attention across the query axis (instead of the key axis), which leads to competition between queries, and provides superior results for unsupervised object segmentation (<a href="#slot_attention">Fig. 3</a>).</p>

<h4 id="what-about-those-point-processes">What about those Point Processes??!!</h4>
<p>While the above approaches definitely work for generating sets, they make no use of the well-known area of statistics concerned with modeling sets: point processes!
Point processes treat the set size <span id="MathJax-Element-31-Frame" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>k</mi><mo>&amp;#x2208;</mo><msub><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi mathvariant=&quot;double-struck&quot;>N</mi></mrow><mo>+</mo></msub></math>" role="presentation"><span role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>k</mi><mo>∈</mo><msub><mrow><mi mathvariant="double-struck">N</mi></mrow><mo>+</mo></msub></math></span></span> as a random variable and model it jointly with the set membership <span id="MathJax-Element-32-Frame" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>X</mi><mo>&amp;#x2208;</mo><msup><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi class=&quot;MJX-tex-caligraphic&quot; mathvariant=&quot;script&quot;>X</mi></mrow><mi>k</mi></msup></math>" role="presentation"><span role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>X</mi><mo>∈</mo><msup><mrow><mi mathvariant="script">X</mi></mrow><mi>k</mi></msup></math></span></span>, thus modeling the joint density <span id="MathJax-Element-33-Frame" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>p</mi><mo stretchy=&quot;false&quot;>(</mo><mi>X</mi><mo>,</mo><mi>k</mi><mo stretchy=&quot;false&quot;>)</mo></math>" role="presentation"><span role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>p</mi><mo stretchy="false">(</mo><mi>X</mi><mo>,</mo><mi>k</mi><mo stretchy="false">)</mo></math></span></span>.
This is in contrast to some of the previously-described methods; e.g., DSPN uses heuristics to determine the set size, which does or does not work depending on which loss function it is used with (<a href="https://arxiv.org/abs/2006.16841">see our TSPN paper for details</a>).
Our TSPN is not much better in that regard, and casts determining the set size as a classification problem–this works quite well in practice, but it <strong>cannot generalize</strong> to set sizes not seen in training.
While a detailed description of point process would take too much space to fit in this blog, I would like to highlight one notion, which I learned about from an excellent paper by Vu et al. called <a href="https://arxiv.org/abs/1703.02155">“Model-Based Multiple Instance Learning”</a>.</p>

<p>Let <span id="MathJax-Element-34-Frame" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>f</mi><mi>k</mi></msub><mo stretchy=&quot;false&quot;>(</mo><mi>X</mi><mo stretchy=&quot;false&quot;>)</mo><mo>=</mo><msub><mi>f</mi><mi>k</mi></msub><mo stretchy=&quot;false&quot;>(</mo><msub><mi>x</mi><mn>1</mn></msub><mo>,</mo><mo>.</mo><mo>.</mo><mo>.</mo><mo>,</mo><msub><mi>x</mi><mi>i</mi></msub><mo>,</mo><mo>.</mo><mo>.</mo><mo>.</mo><mo>,</mo><msub><mi>x</mi><mi>k</mi></msub><mo stretchy=&quot;false&quot;>)</mo></math>" role="presentation"><span role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>f</mi><mi>k</mi></msub><mo stretchy="false">(</mo><mi>X</mi><mo stretchy="false">)</mo><mo>=</mo><msub><mi>f</mi><mi>k</mi></msub><mo stretchy="false">(</mo><msub><mi>x</mi><mn>1</mn></msub><mo>,</mo><mo>.</mo><mo>.</mo><mo>.</mo><mo>,</mo><msub><mi>x</mi><mi>i</mi></msub><mo>,</mo><mo>.</mo><mo>.</mo><mo>.</mo><mo>,</mo><msub><mi>x</mi><mi>k</mi></msub><mo stretchy="false">)</mo></math></span></span> be a probability density function defined over sets of <span id="MathJax-Element-35-Frame" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>k</mi></math>" role="presentation"><span role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>k</mi></math></span></span> elements, and let this density be invariant to ordering of the elements of the set, that is <span id="MathJax-Element-36-Frame" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi mathvariant=&quot;normal&quot;>&amp;#x2200;</mi><mi>&amp;#x03C0;</mi></math>" role="presentation"><span role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi mathvariant="normal">∀</mi><mi>π</mi></math></span></span>: <span id="MathJax-Element-37-Frame" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>f</mi><mo stretchy=&quot;false&quot;>(</mo><mi>X</mi><mo stretchy=&quot;false&quot;>)</mo><mo>=</mo><mi>f</mi><mo stretchy=&quot;false&quot;>(</mo><mi>&amp;#x03C0;</mi><mi>X</mi><mo stretchy=&quot;false&quot;>)</mo></math>" role="presentation"><span role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>f</mi><mo stretchy="false">(</mo><mi>X</mi><mo stretchy="false">)</mo><mo>=</mo><mi>f</mi><mo stretchy="false">(</mo><mi>π</mi><mi>X</mi><mo stretchy="false">)</mo></math></span></span>.
It turns out that we can use this density to compare sets of the same cardinality with each other in terms of how probable they are (i.e., how high their likelihood is), but, even if we have two such functions for sets of cardinality <span id="MathJax-Element-38-Frame" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>k</mi></math>" role="presentation"><span role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>k</mi></math></span></span> and <span id="MathJax-Element-39-Frame" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>m</mi></math>" role="presentation"><span role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>m</mi></math></span></span>, we simply <strong>cannot use them to compare sets of those different cardinalities</strong>.
Why is that?
Well, comparing sets of two and sets of three elements is a bit like comparing square meters m<span id="MathJax-Element-40-Frame" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msup><mi></mi><mn>2</mn></msup></math>" role="presentation"><span role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mi></mi><mn>2</mn></msup></math></span></span> and cubic meters m<span id="MathJax-Element-41-Frame" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msup><mi></mi><mn>3</mn></msup></math>" role="presentation"><span role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mi></mi><mn>3</mn></msup></math></span></span>, or like comparing apples and oranges.
It is not that we cannot compare sets of different cardinality, but we have to first bring them into the same space, which in this case is dimension-less.
To do that, we have to account for a) the number of possible permutations of each set, and b) the unit volume (in case of metric space and comparing m<span id="MathJax-Element-42-Frame" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msup><mi></mi><mn>2</mn></msup></math>" role="presentation"><span role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mi></mi><mn>2</mn></msup></math></span></span> and m<span id="MathJax-Element-43-Frame" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msup><mi></mi><mn>3</mn></msup></math>" role="presentation"><span role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mi></mi><mn>3</mn></msup></math></span></span>, we need to figure out how big a meter m<span id="MathJax-Element-44-Frame" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msup><mi></mi><mn>1</mn></msup></math>" role="presentation"><span role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mi></mi><mn>1</mn></msup></math></span></span> is).
This leads to the following definition of the probability density function of a set of size <span id="MathJax-Element-45-Frame" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>k</mi></math>" role="presentation"><span role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>k</mi></math></span></span>,</p>

<p><span id="MathJax-Element-46-Frame" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mi>p</mi><mo stretchy=&quot;false&quot;>(</mo><mo fence=&quot;false&quot; stretchy=&quot;false&quot;>{</mo><msub><mi>x</mi><mn>1</mn></msub><mo>,</mo><mo>.</mo><mo>.</mo><mo>.</mo><mo>,</mo><msub><mi>x</mi><mi>k</mi></msub><mo fence=&quot;false&quot; stretchy=&quot;false&quot;>}</mo><mo stretchy=&quot;false&quot;>)</mo><mo>=</mo><mi>p</mi><mo stretchy=&quot;false&quot;>(</mo><mi>X</mi><mo>,</mo><mi>k</mi><mo stretchy=&quot;false&quot;>)</mo><mo>=</mo><msub><mi>p</mi><mi>c</mi></msub><mo stretchy=&quot;false&quot;>(</mo><mi>k</mi><mo stretchy=&quot;false&quot;>)</mo><mi>k</mi><mo>!</mo><msup><mi>U</mi><mi>k</mi></msup><msub><mi>f</mi><mi>k</mi></msub><mo stretchy=&quot;false&quot;>(</mo><msub><mi>x</mi><mn>1</mn></msub><mo>,</mo><mo>.</mo><mo>.</mo><mo>.</mo><mo>,</mo><msub><mi>x</mi><mi>k</mi></msub><mo stretchy=&quot;false&quot;>)</mo><mspace width=&quot;thinmathspace&quot; /><mo>,</mo></math>" role="presentation"><span role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mi>p</mi><mo stretchy="false">(</mo><mo fence="false" stretchy="false">{</mo><msub><mi>x</mi><mn>1</mn></msub><mo>,</mo><mo>.</mo><mo>.</mo><mo>.</mo><mo>,</mo><msub><mi>x</mi><mi>k</mi></msub><mo fence="false" stretchy="false">}</mo><mo stretchy="false">)</mo><mo>=</mo><mi>p</mi><mo stretchy="false">(</mo><mi>X</mi><mo>,</mo><mi>k</mi><mo stretchy="false">)</mo><mo>=</mo><msub><mi>p</mi><mi>c</mi></msub><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo><mi>k</mi><mo>!</mo><msup><mi>U</mi><mi>k</mi></msup><msub><mi>f</mi><mi>k</mi></msub><mo stretchy="false">(</mo><msub><mi>x</mi><mn>1</mn></msub><mo>,</mo><mo>.</mo><mo>.</mo><mo>.</mo><mo>,</mo><msub><mi>x</mi><mi>k</mi></msub><mo stretchy="false">)</mo><mspace width="thinmathspace"></mspace><mo>,</mo></math></span></span></p>

<p>where <span id="MathJax-Element-47-Frame" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>p</mi><mi>c</mi></msub><mo stretchy=&quot;false&quot;>(</mo><mi>k</mi><mo stretchy=&quot;false&quot;>)</mo></math>" role="presentation"><span role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>p</mi><mi>c</mi></msub><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo></math></span></span> is the probability mass function of the set size, <span id="MathJax-Element-48-Frame" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>k</mi><mo>!</mo></math>" role="presentation"><span role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>k</mi><mo>!</mo></math></span></span> accounts for all possible permutations of set elements <span id="MathJax-Element-49-Frame" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi mathvariant=&quot;bold&quot;>x</mi></mrow><mi>i</mi></msub></math>" role="presentation"><span role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mrow><mi mathvariant="bold">x</mi></mrow><mi>i</mi></msub></math></span></span>, <span id="MathJax-Element-50-Frame" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>U</mi><mo>&amp;#x2208;</mo><msub><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi mathvariant=&quot;double-struck&quot;>R</mi></mrow><mo>+</mo></msub></math>" role="presentation"><span role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>U</mi><mo>∈</mo><msub><mrow><mi mathvariant="double-struck">R</mi></mrow><mo>+</mo></msub></math></span></span> is the unit volume expressed as a scalar value, and <span id="MathJax-Element-51-Frame" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>f</mi><mi>k</mi></msub></math>" role="presentation"><span role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>f</mi><mi>k</mi></msub></math></span></span> is the permutation-invariant density of a set of size k.
Interestingly, none of the above set-generation papers take the point-process theory into account when defining their likelihoods over sets.
I would be curious to see if it improves results, as Vu et al. suggest.</p>


<p>Given the knowledge of how to solve set-to-vector and vector-to-set problems, it should be quite clear how to solve a set-to-set problem: we can encode a set into a vector, and then decode that vector into a set using one of the above vector-to-set methods.
While correct, this approach forces us to use a bottleneck in the shape of a single vector.
Perhaps a better option is to encode a set to an intermediate set, possibly of smaller cardinality, and use that smaller set as conditioning when generating the output set.
There are many methods of how this can be done, and I will only mention that we explored some such problems in <a href="https://arxiv.org/abs/1810.00825">Lee et al., “Set Transformer”, ICML 2019</a> and encourage curious readers to look at the paper.</p>


<p>Thank you for reaching this far!
We have covered some basics of set-oriented machine learning by taking a look at set-to-vector, vector-to-set, and set-to-set problems and some approaches to solving them.
I find this area of ML incredibly interesting, for the variety of things that we consider in life as sets is endless.
At the same time, the set-learning models tend to be both theoretically- and architecturally- interesting.
Moving forward, I would like to see more models directly based on the point-process theory.
Another area that I have not mentioned, and one that is extremely applicable, is that of normalizing flows.
You can read about <a href="http://akosiorek.github.io/ml/2018/04/03/norm_flows.html">the basics of normalizing flows in my previous blog post</a>, but in short, they are used to transform a simple probability distribution into a more complicated one.
As such, there is nothing preventing us from using flows to transform a distribution over independent variables into a joint distribution over sets.
While there are some papers that use this idea<sup id="fnref:set_flow_models" role="doc-noteref"><a href="#fn:set_flow_models">13</a></sup> to define permutation-invariant likelihoods, none of them uses point-process theory.
I will leave working out how to combine flows and point processes as an exercise to the reader, and I will be looking out for papers doing that :)</p>


<p>If you want to learn about point processes, I would recommend:</p>
<ul>
  <li>The excellent and yet a very short book <a href="https://global.oup.com/academic/product/poisson-processes-9780198536932?cc=us&amp;lang=en&amp;">“Poisson Process” by J. F. C. Kingman</a>.</li>
  <li><a href="https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-262-discrete-stochastic-processes-spring-2011/">The open MIT course on Discrete Stochastic Processes by Robert Gallager</a>, which provides a very gentle introduction to point processes without any measure theory.</li>
</ul>

<h4 id="footnotes">Footnotes</h4>


<h4 id="acknowledgements">Acknowledgements</h4>
<p>I would like to give huge thanks to Fabian Fuchs, Thomas Kipf, Hyunjik Kim, Yan Zhang, George Papamakarios, and Danilo Rezende for insightful and inspiring discussions about the machine learning of sets. I would also like to thank Hyunjik Kim and Fabian Fuchs for their feedback on the initial version of this post.
This post would not happen if not for Juho Lee, who got me interested in sets in the first place.</p>


  </div></div></div>
    </div>
    <footer>
        <div>created by <a href="https://buildstarted.com">buildstarted</a> &copy; 2020 <a href="/about">about</a></div>
        <div>Share this page on social media: copy and paste this url https://linksfor.dev/</div>
        <div>If you prefer RSS: <a href="https://linksfor.dev/feed.xml">https://linksfor.dev/feed.xml</a></div>
        <div>Customer satisfaction guaranteed to be optional.</div>
    </footer>
    
    <script async defer>
        _dna = window._dna || {};
        _dna.siteId = "linksfor.devs";
        _dna.outlink = true;

        (function() {
            let dna = document.createElement('script');
            dna.type = 'text/javascript';
            dna.async = true;
            dna.src = '//dna.buildstarted.com/t.js';
            let s = document.getElementsByTagName('script')[0];
            s.parentNode.insertBefore(dna, s);
        })();
    </script>
    <noscript><img src="//dna.buildstarted.com/g?siteId=linksfor.devs"/></noscript>
</body>
</html>