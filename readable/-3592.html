<!DOCTYPE html>
<html lang="en">
<head>
    <title>
An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling - linksfor.dev(s)    </title>
    <meta charset="utf-8">
    <link rel="alternate" type="application/rss+xml" title="Linksfor.dev(s) feed" href="https://linksfor.dev/feed.rss" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta name="google" value="notranslate">
    <link rel="stylesheet" type="text/css" href="/style.css" />
    <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon">
    <link rel="icon" href="/favicon.ico" type="image/x-icon">
    <meta property="og:title" content="An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling - linksfor.dev(s)"/>
    <meta property="article:author" content="Authors:Shaojie Bai, J. Zico Kolter, Vladlen Koltun"/>
    <meta property="og:description" content="For most deep learning practitioners, sequence modeling is synonymous with&#xA;recurrent networks. Yet recent results indicate that convolutional&#xA;architectures can outperform recurrent networks on tasks such as audio&#xA;synthesis and machine translation. Given a new sequence modeling task or&#xA;dataset, which architecture should one use? We conduct a systematic evaluation&#xA;of generic convolutional and recurrent architectures for sequence modeling. The&#xA;models are evaluated across a broad range of standard tasks that are commonly&#xA;used to benchmark recurrent networks. Our results indicate that a simple&#xA;convolutional architecture outperforms canonical recurrent networks such as&#xA;LSTMs across a diverse range of tasks and datasets, while demonstrating longer&#xA;effective memory. We conclude that the common association between sequence&#xA;modeling and recurrent networks should be reconsidered, and convolutional&#xA;networks should be regarded as a natural starting point for sequence modeling&#xA;tasks. To assist related work, we have made code available at&#xA;http://github.com/locuslab/TCN ."/>
    <meta property="og:type" content="website"/>
    <meta property="og:url" content="https://arxiv.org/abs/1803.01271"/>

<meta property="og:site_name" content="linksfor.dev(s)" />
</head>
<body>
    <div class="grid">
        <h1>
<a href="/" style="color:inherit">linksfor.dev(s)</a>
        </h1>
        <title>linksfor.dev(s) - An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling</title>
<div class="readable">
        <h1>An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling</h1>
            <div>by Authors:Shaojie Bai, J. Zico Kolter, Vladlen Koltun</div>
            <div>Reading time: 2 minutes</div>
        <div>Posted here: 27 Feb 2019</div>
        <p><a href="https://arxiv.org/abs/1803.01271">https://arxiv.org/abs/1803.01271</a></p>
        <hr/>
<div id="readability-page-1" class="page"><div id="content-inner">
  <div id="abs">

    
    
    <p>
  
  
  
    
  
  
    
    
  

  (Submitted on 4 Mar 2018 (<a href="https://arxiv.org/abs/1803.01271v1">v1</a>), last revised 19 Apr 2018 (this version, v2))</p>
    <blockquote><span>Abstract:</span>  For most deep learning practitioners, sequence modeling is synonymous with
recurrent networks. Yet recent results indicate that convolutional
architectures can outperform recurrent networks on tasks such as audio
synthesis and machine translation. Given a new sequence modeling task or
dataset, which architecture should one use? We conduct a systematic evaluation
of generic convolutional and recurrent architectures for sequence modeling. The
models are evaluated across a broad range of standard tasks that are commonly
used to benchmark recurrent networks. Our results indicate that a simple
convolutional architecture outperforms canonical recurrent networks such as
LSTMs across a diverse range of tasks and datasets, while demonstrating longer
effective memory. We conclude that the common association between sequence
modeling and recurrent networks should be reconsidered, and convolutional
networks should be regarded as a natural starting point for sequence modeling
tasks. To assist related work, we have made code available at
<a href="http://github.com/locuslab/TCN" rel="external noopener nofollow">this http URL</a> .
</blockquote>
    <!--CONTEXT-->
    
  </div>
</div><div>
      <h2>Submission history</h2><p> From: Shaojie Bai [<a href="https://arxiv.org/show-email/e8e738bf/1803.01271">view email</a>]
      <br>
  <strong><a href="https://arxiv.org/abs/1803.01271v1">[v1]</a></strong>
  Sun, 4 Mar 2018 00:20:29 UTC (884 KB)<br><strong>[v2]</strong>
Thu, 19 Apr 2018 14:32:38 UTC (1,108 KB)<br></p></div></div></div>
    </div>
    <footer>
        <div>created by buildstarted &copy; 2020 <a href="/about">about</a></div>
        <div>Share this page on social media: copy and paste this url https://linksfor.dev/</div>
        <div>If you prefer RSS: <a href="https://linksfor.dev/feed.xml">https://linksfor.dev/feed.xml</a></div>
    </footer>
    
    <script async defer>
        _dna = window._dna || {};
        _dna.siteId = "linksfor.devs";
        _dna.outlink = true;

        (function () {
            let dna = document.createElement('script');
            dna.type = 'text/javascript';
            dna.async = true;
            dna.src = '//dna.buildstarted.com/t.js';
            let s = document.getElementsByTagName('script')[0];
            s.parentNode.insertBefore(dna, s);
        })();
    </script>
    <noscript><img src="//dna.buildstarted.com/g?siteId=linksfor.devs"/></noscript>
</body>
</html>