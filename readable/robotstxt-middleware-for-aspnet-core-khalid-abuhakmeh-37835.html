<!DOCTYPE html>
<html lang="en">
<head>
    <title>
Robots.txt Middleware For ASP.NET Core - linksfor.dev(s)    </title>
    <meta charset="utf-8">
    <link rel="alternate" type="application/rss+xml" title="Linksfor.dev(s) feed" href="https://linksfor.dev/feed.rss" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta name="google" value="notranslate">
    <link rel="stylesheet" type="text/css" href="/style.css" />
    <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon">
    <link rel="icon" href="/favicon.ico" type="image/x-icon">
    <meta property="og:title" content="Robots.txt Middleware For ASP.NET Core - linksfor.dev(s)"/>
    <meta property="article:author" content="Khalid Abuhakmeh"/>
    <meta property="og:description" content="As a web developer, it&#x2019;s important to remind ourselves that we push our work onto the internet for the world to see. Our development team utilizes Windows Azure for many environments, and all those environments need to be publicly accessible. There are ways to limit accessibility on Windows Azure, but the eyes looking at our work can vary, so having a hard constraint becomes unmanageable for us."/>
    <meta property="og:type" content="website"/>
    <meta property="og:url" content="https://khalidabuhakmeh.com/robotstxt-middleware-aspnet-core"/>

<meta property="og:site_name" content="linksfor.dev(s)" />
</head>
<body>
    <div class="grid">
        <h1>
<a href="/" style="color:inherit">linksfor.dev(s)</a>
        </h1>
        <title>linksfor.dev(s) - Robots.txt Middleware For ASP.NET Core</title>
<div class="readable">
        <h1>Robots.txt Middleware For ASP.NET Core</h1>
            <div>by Khalid Abuhakmeh</div>
            <div>Reading time: 10-12 minutes</div>
        <div>Posted here: 29 Sep 2019</div>
        <p><a href="https://khalidabuhakmeh.com/robotstxt-middleware-aspnet-core">https://khalidabuhakmeh.com/robotstxt-middleware-aspnet-core</a></p>
        <hr/>
<div id="readability-page-1" class="page"><div>
                <p>As a web developer, it’s important to remind ourselves that we push our work onto the internet for the world to see. Our development team utilizes Windows Azure for many environments, and all those environments need to be publicly accessible. There are ways to limit accessibility on Windows Azure, but the eyes looking at our work can vary, so having a hard constraint becomes unmanageable for us.</p>

<p>The accessibility of our environments helps stakeholders see iterations of our progress faster and provide feedback. In turn, that feedback goes back into our development, making the deliverable better for everyone.</p>

<!--more-->

<p>In practice, we usually have three environments in Windows Azure App Services: Development, Staging, and Production. We want all these to be visible to <em>people</em>, but we all know people don’t just view the web. The majority of web users are automated processes that scan your site, and many other sites to provide search results. I put a generous amount of effort into a previous blog post and proud of it. <a href="https://khalidabuhakmeh.com/search-experiences-for-your-aspnet-core-apps-with-elasticsearch">If you want to learn how to build a great search experience in your ASP.NET Core application, I highly recommend it</a>.</p>

<p>In this post, I’ll explain why a <code>robots.txt</code> is essential for your public-facing ASP.NET Core applications. Additionally, you’ll learn how to write or generate a few variations of the file. Finally, you’ll learn how to serve a specific file based on your current hosting environment.</p>

<p>This post is by no means limited to Windows Azure users and works anywhere you host your ASP.NET Core applications.</p>

<p>If you want the code, you can download it from my GitHub repository. It targets ASP.NET Core 3.0 but could be modified to support lower SDK versions.</p>

<p><strong><a href="https://github.com/khalidabuhakmeh/RobotsTxt">Download the Project</a></strong></p>

<h2 id="what-is-a-robotstxt"><a href="#what-is-a-robotstxt">What is a Robots.txt</a></h2>

<p>As mentioned in the previous section, most of the web’s users are automated processes. These processes are known as Web crawlers and operated by data collection and search companies like Google, Facebook, and Microsoft.</p>

<p>Wikipedia sums it perfectly:</p>

<blockquote>
  <p>A Web crawler, sometimes called a spider or spiderbot and often shortened to crawler, is an Internet bot that systematically browses the World Wide Web, typically for the purpose of Web indexing (web spidering). <cite>–<a href="https://en.wikipedia.org/wiki/Web_crawler">Wikipedia</a></cite></p>
</blockquote>

<p>While algorithms can differ, most crawlers start at your homepage and navigate the links on your site in an attempt to find the most content. However, the exciting fact is, you can talk to these crawlers!</p>

<p>If talking to spiders makes your skin crawl, don’t worry, I promise there won’t be any insects involved. Instead, there is a simple plain text file called a <code>robots.txt</code> file. This file contains instructions for crawlers, telling them where to look in your site for meaningful content, and even where <em>not</em> to look. As reiterated on Wikipedia:</p>

<blockquote>
  <p>The robots exclusion standard, also known as the robots exclusion protocol or simply robots.txt, is a standard used by websites to communicate with web crawlers and other web robots. The standard specifies how to inform the web robot about which areas of the website should not be processed or scanned.<cite>–<a href="https://en.wikipedia.org/wiki/Robots_exclusion_standard">Wikipedia</a></cite></p>
</blockquote>

<p>It’s important to note that the instructions in the file are no guarantee that a crawler obeys your wishes. There are bad bots that crawl sites for many other reasons, especially if your site contains valuable information.</p>

<h2 id="why-it-is-important"><a href="#why-it-is-important">Why It is Important</a></h2>

<p>I’m glad you are asking the question:</p>

<blockquote>
  <p>Why is it essential to have a <code>robots.txt</code> file for my ASP.NET Core application?</p>
</blockquote>

<p>While you may not have content within your application, you still want individuals to find your primary domain. Pointing crawlers to the specific pages to you and your users can make for a more significant impact.</p>

<p><strong>In my case, we have multiple domains that can have very similar content. Crawlers may mistake the variations in domains having similar content as an attempt to create spam sites. The duplication of sites can hurt the overall ranking of a site in search engine results.</strong></p>

<p><strong>Even worse, your non-primary sites could rank higher than your primary! Imagine users entering critical information into your development environment, only for it to be lost. Yikes!</strong></p>

<h2 id="generating-your-own-robotstxt"><a href="#generating-your-own-robotstxt">Generating Your Own Robots.txt</a></h2>

<p>A <code>robots.txt</code> has a few components, but the essential parts are who and what.
Whom do you want to scan your site?
What parts of the site do you want to be indexed?
Let’s take a simple <code>robots.txt</code> file:</p>

<pre><code>User-Agent: *
Allow: /
</code></pre>

<p>The first line of the file defines what <code>User-Agent</code> can scan the site. The <code>User-Agent</code> is a string value passed from the client. For example, Google’s bot passes a value of <code>Googlebot</code>, but they also have an <a href="https://support.google.com/webmasters/answer/1061943?hl%3Den">army of bots</a>.</p>

<p>The second line of the file tells the bot what it can index. The example informs the bot to start at the root path and work its way everywhere it can. You can also <code>Disallow</code> specific paths, but don’t assume this takes the place of securing your site. As mentioned before, bots do not have to respect the <code>robots.txt</code> file.</p>

<p>I recommend generating a few variations using <a href="https://en.ryte.com/free-tools/robots-txt-generator/#allowing">this online tool</a>. As the tool mentions, you can damage your site’s search results if done incorrectly, so be careful and think through the file you’ll be serving.</p>

<p><img src="https://d33wubrfki0l68.cloudfront.net/bafc499dc20d928545cb3fa220f6f4712bb3a0ec/e778e/assets/images/generated/robotstxt/robotstxt_generator-712-b8b6fb.png" alt="serving the https://d33wubrfki0l68.cloudfront.net/05fffa36524262cdffb4f70a41c5bc9fccc2d0b8/fde2c/robots.txt" srcset="https://d33wubrfki0l68.cloudfront.net/27cc790cd77c3730d84e1ed7d18c686ee62c6452/38c94/assets/images/generated/robotstxt/robotstxt_generator-400-b8b6fb.png 400w, https://d33wubrfki0l68.cloudfront.net/066d023c5eaadf2e2277f7a23f560384af42bb5b/fcaa8/assets/images/generated/robotstxt/robotstxt_generator-600-b8b6fb.png 600w, https://d33wubrfki0l68.cloudfront.net/bafc499dc20d928545cb3fa220f6f4712bb3a0ec/e778e/assets/images/generated/robotstxt/robotstxt_generator-712-b8b6fb.png 712w"></p>

<h2 id="the-code"><a href="#the-code">The Code</a></h2>

<p>You’ve learned a lot up to this point:</p>

<ul>
  <li>You know what a crawler is</li>
  <li>How to talk to them via a <code>robots.txt</code> file</li>
  <li>Have generated a few variations</li>
</ul>

<p>In this section, I’ll show you how to write a piece of middleware that takes inspiration for ASP.NET Core’s use of environments. Our goal is to serve a <code>robots.txt</code> file unique to each environment we have.</p>

<p>We want to tell bots to stay away from our development environments while boosting the importance of our production sites.</p>

<h3 id="robotstxt-per-environment"><a href="#robotstxt-per-environment">Robots.txt Per Environment</a></h3>

<p>The first step is to realize that ASP.NET Core provides an <code>Environment</code> mechanism. By default, we get <code>Development</code>, <code>Staging</code>, and <code>Production</code>. We can create any environments, but let’s start here.</p>

<p><img src="https://d33wubrfki0l68.cloudfront.net/991ca8bb4fa634003c36226713b9cf10eae49eac/42eae/assets/images/generated/robotstxt/environments_aspnetcore-800-729527.png" alt="asp.net core environments" srcset="https://d33wubrfki0l68.cloudfront.net/074a033669e3183df1c5ac13c5cb7b25eabb5e67/af1fa/assets/images/generated/robotstxt/environments_aspnetcore-400-729527.png 400w, https://d33wubrfki0l68.cloudfront.net/16818fc90d1e4755210fd2c9df5a7df8ed81707b/17b05/assets/images/generated/robotstxt/environments_aspnetcore-600-729527.png 600w, https://d33wubrfki0l68.cloudfront.net/991ca8bb4fa634003c36226713b9cf10eae49eac/42eae/assets/images/generated/robotstxt/environments_aspnetcore-800-729527.png 800w, https://d33wubrfki0l68.cloudfront.net/9c436d76c1a6e99fbf9672a9c986a177e04c036b/85865/assets/images/generated/robotstxt/environments_aspnetcore-851-729527.png 851w"></p>

<p>Our robots.txt files can match these environments:</p>

<pre><code>robots.txt
robots.Production.txt
robots.Development.txt
</code></pre>

<p>The <code>robots.txt</code> is our fallback, but the other files are specific to the environments. ASP.NET Core reads the current environment value from the environmental variable <code>ASPNETCORE_ENVIRONMENT</code> which can be set at runtime or via launch settings.</p>

<h3 id="robotstxt-middleware-code"><a href="#robotstxt-middleware-code">Robots.txt Middleware Code</a></h3>

<p>The middleware is straight-forward. It scans our directory for the files based on our environments. In my sample, I place the files in the content path of the project, and not the webroot path. You could place it anywhere, but in general, I don’t want the static file middleware to serve the incorrect file accidentally.</p>

<div><div><pre><code><span>public</span> <span>static</span> <span>class</span> <span>RobotsTxtMiddlewareExtensions</span>
<span>{</span>
    <span>public</span> <span>static</span> <span>IApplicationBuilder</span> <span>UseRobotsTxt</span><span>(</span>
        <span>this</span> <span>IApplicationBuilder</span> <span>builder</span><span>,</span>
        <span>IWebHostEnvironment</span> <span>env</span><span>,</span>
        <span>string</span> <span>rootPath</span> <span>=</span> <span>null</span>
    <span>)</span>
    <span>{</span>
        <span>return</span> <span>builder</span><span>.</span><span>MapWhen</span><span>(</span><span>ctx</span> <span>=&gt;</span> <span>ctx</span><span>.</span><span>Request</span><span>.</span><span>Path</span><span>.</span><span>StartsWithSegments</span><span>(</span><span>"/robots.txt"</span><span>),</span> <span>b</span> <span>=&gt;</span>
            <span>b</span><span>.</span><span>UseMiddleware</span><span>&lt;</span><span>RobotsTxtMiddleware</span><span>&gt;(</span><span>env</span><span>.</span><span>EnvironmentName</span><span>,</span> <span>rootPath</span> <span>??</span> <span>env</span><span>.</span><span>ContentRootPath</span> <span>));</span>
    <span>}</span>
<span>}</span>

<span>public</span> <span>class</span> <span>RobotsTxtMiddleware</span>
<span>{</span>
    <span>const</span> <span>string</span> <span>Default</span> <span>=</span>
        <span>@"User-Agent: *\nAllow: /"</span><span>;</span>

    <span>private</span> <span>readonly</span> <span>RequestDelegate</span> <span>next</span><span>;</span>
    <span>private</span> <span>readonly</span> <span>string</span> <span>environmentName</span><span>;</span>
    <span>private</span> <span>readonly</span> <span>string</span> <span>rootPath</span><span>;</span>

    <span>public</span> <span>RobotsTxtMiddleware</span><span>(</span>
        <span>RequestDelegate</span> <span>next</span><span>,</span>
        <span>string</span> <span>environmentName</span><span>,</span>
        <span>string</span> <span>rootPath</span>
    <span>)</span>
    <span>{</span>
        <span>this</span><span>.</span><span>next</span> <span>=</span> <span>next</span><span>;</span>
        <span>this</span><span>.</span><span>environmentName</span> <span>=</span> <span>environmentName</span><span>;</span>
        <span>this</span><span>.</span><span>rootPath</span> <span>=</span> <span>rootPath</span><span>;</span>
    <span>}</span>

    <span>public</span> <span>async</span> <span>Task</span> <span>InvokeAsync</span><span>(</span><span>HttpContext</span> <span>context</span><span>)</span>
    <span>{</span>
        <span>if</span> <span>(</span><span>context</span><span>.</span><span>Request</span><span>.</span><span>Path</span><span>.</span><span>StartsWithSegments</span><span>(</span><span>"/robots.txt"</span><span>))</span>
        <span>{</span>
            <span>var</span> <span>generalRobotsTxt</span> <span>=</span> <span>Path</span><span>.</span><span>Combine</span><span>(</span><span>rootPath</span><span>,</span> <span>"robots.txt"</span><span>);</span>
            <span>var</span> <span>environmentRobotsTxt</span> <span>=</span> <span>Path</span><span>.</span><span>Combine</span><span>(</span><span>rootPath</span><span>,</span> <span>$"robots.</span><span>{</span><span>environmentName</span><span>}</span><span>.txt"</span><span>);</span>
            <span>string</span> <span>output</span><span>;</span>

            <span>// try environment first</span>
            <span>if</span> <span>(</span><span>File</span><span>.</span><span>Exists</span><span>(</span><span>environmentRobotsTxt</span><span>))</span>
            <span>{</span>
                <span>output</span> <span>=</span> <span>await</span> <span>File</span><span>.</span><span>ReadAllTextAsync</span><span>(</span><span>environmentRobotsTxt</span><span>);</span>
            <span>}</span>
            <span>// then robots.txt</span>
            <span>else</span> <span>if</span> <span>(</span><span>File</span><span>.</span><span>Exists</span><span>(</span><span>generalRobotsTxt</span><span>))</span>
            <span>{</span>
                <span>output</span> <span>=</span> <span>await</span> <span>File</span><span>.</span><span>ReadAllTextAsync</span><span>(</span><span>generalRobotsTxt</span><span>);</span>
            <span>}</span>
            <span>// then just a general default</span>
            <span>else</span>
            <span>{</span>
                <span>output</span> <span>=</span> <span>Default</span><span>;</span>
            <span>}</span>

            <span>context</span><span>.</span><span>Response</span><span>.</span><span>ContentType</span> <span>=</span> <span>"text/plain"</span><span>;</span>
            <span>await</span> <span>context</span><span>.</span><span>Response</span><span>.</span><span>WriteAsync</span><span>(</span><span>output</span><span>);</span>
        <span>}</span>
        <span>else</span>
        <span>{</span>
            <span>await</span> <span>next</span><span>(</span><span>context</span><span>);</span>
        <span>}</span>
    <span>}</span>
<span>}</span>
</code></pre></div></div>

<h3 id="calling-robotstxt-in-startupcs"><a href="#calling-robotstxt-in-startupcs">Calling Robots.Txt In Startup.cs</a></h3>

<p>Like any other middleware in an ASP.NET Core application, we need to register it in our middleware pipeline.</p>

<div><div><pre><code><span>public</span> <span>void</span> <span>Configure</span><span>(</span><span>IApplicationBuilder</span> <span>app</span><span>,</span> <span>IWebHostEnvironment</span> <span>env</span><span>)</span>
<span>{</span>
    <span>if</span> <span>(</span><span>env</span><span>.</span><span>IsDevelopment</span><span>())</span>
    <span>{</span>
        <span>app</span><span>.</span><span>UseDeveloperExceptionPage</span><span>();</span>
    <span>}</span>
    <span>else</span>
    <span>{</span>
        <span>app</span><span>.</span><span>UseExceptionHandler</span><span>(</span><span>"/Error"</span><span>);</span>
        <span>app</span><span>.</span><span>UseHsts</span><span>();</span>
    <span>}</span>

    <span>app</span><span>.</span><span>UseHttpsRedirection</span><span>();</span>
    <span>// Register Our Middleware</span>
    <span>app</span><span>.</span><span>UseRobotsTxt</span><span>(</span><span>env</span><span>);</span>
    <span>app</span><span>.</span><span>UseStaticFiles</span><span>();</span>

    <span>app</span><span>.</span><span>UseRouting</span><span>();</span>

    <span>app</span><span>.</span><span>UseAuthorization</span><span>();</span>

    <span>app</span><span>.</span><span>UseEndpoints</span><span>(</span><span>endpoints</span> <span>=&gt;</span> <span>{</span> <span>endpoints</span><span>.</span><span>MapRazorPages</span><span>();</span> <span>});</span>
<span>}</span>
</code></pre></div></div>

<p>This particular middleware takes <code>IWebHostEnvironment</code> as a constructor parameter. Passing in the <code>IWebHostEnvironment</code> allows the middleware to read values like the environment name and if a root path is not provided at registration.</p>

<h3 id="serving-robotstxt"><a href="#serving-robotstxt">Serving robots.txt</a></h3>

<p>By navigating to <code>/robots.txt</code> we see the file based on our environment.</p>

<p><img src="https://d33wubrfki0l68.cloudfront.net/e77e5ad1196c03f7792409cfef6e1d804168ba02/eb59f/assets/images/generated/robotstxt/site_robotstxt_visit-556-3e9309.png" alt="serving the https://d33wubrfki0l68.cloudfront.net/05fffa36524262cdffb4f70a41c5bc9fccc2d0b8/fde2c/robots.txt" srcset="https://d33wubrfki0l68.cloudfront.net/4504837e32023cac700ee943cb04d5dcecf58096/667ab/assets/images/generated/robotstxt/site_robotstxt_visit-400-3e9309.png 400w, https://d33wubrfki0l68.cloudfront.net/e77e5ad1196c03f7792409cfef6e1d804168ba02/eb59f/assets/images/generated/robotstxt/site_robotstxt_visit-556-3e9309.png 556w"></p>

<p>Hooray! We did it! By setting a breakpoint, we see we hit the middleware for the environment we expect.</p>

<p><img src="https://d33wubrfki0l68.cloudfront.net/ed2ada531dfae445591a9901bd38a32b2de46aae/e4d04/assets/images/generated/robotstxt/csharp_environment_breakpoint-780-d6f2bb.png" alt="middleware breakpoint" srcset="https://d33wubrfki0l68.cloudfront.net/8e124f349b69dc9d4283cbf0b7bbc7199325231d/f522c/assets/images/generated/robotstxt/csharp_environment_breakpoint-400-d6f2bb.png 400w, https://d33wubrfki0l68.cloudfront.net/f04859023c1c3238fb33e2480cfd0bd60f1efe0d/8bc95/assets/images/generated/robotstxt/csharp_environment_breakpoint-600-d6f2bb.png 600w, https://d33wubrfki0l68.cloudfront.net/ed2ada531dfae445591a9901bd38a32b2de46aae/e4d04/assets/images/generated/robotstxt/csharp_environment_breakpoint-780-d6f2bb.png 780w"></p>

<p>To download the project, go to my <a href="https://github.com/khalidabuhakmeh/RobotsTxt">GitHub repository page</a>. Note, you’ll need the <a href="https://dot.net/">.NET Core 3.0 SDK</a>.</p>

<h2 id="conclusion"><a href="#conclusion">Conclusion</a></h2>

<p>While the code may seem trivial, serving a correct <code>robots.txt</code> can have an enormous impact on the success of your site. I cannot recommend enough you think about who sees your content, including bots. It’s also important to know that most visitors find your site through a search, so in reality, your most important user is a bot.</p>

<p><img src="https://media3.giphy.com/media/8vtm3YCdxtUvjTn0U3/giphy.webp?cid=5a38a5a2981250c94e6088ed6ea239d67c617329e10766ea&amp;rid=giphy.webp" alt="bender bot neat"></p>


            </div></div></div>
    </div>
    <footer>
        <div>created by buildstarted &copy; 2020 <a href="/about">about</a></div>
        <div>Share this page on social media: copy and paste this url https://linksfor.dev/</div>
        <div>If you prefer RSS: <a href="https://linksfor.dev/feed.xml">https://linksfor.dev/feed.xml</a></div>
		<div>Customer satisfaction guaranteed to be optional.</div>
    </footer>
    
    <script async defer>
        _dna = window._dna || {};
        _dna.siteId = "linksfor.devs";
        _dna.outlink = true;

        (function () {
            let dna = document.createElement('script');
            dna.type = 'text/javascript';
            dna.async = true;
            dna.src = '//dna.buildstarted.com/t.js';
            let s = document.getElementsByTagName('script')[0];
            s.parentNode.insertBefore(dna, s);
        })();
    </script>
    <noscript><img src="//dna.buildstarted.com/g?siteId=linksfor.devs"/></noscript>
</body>
</html>