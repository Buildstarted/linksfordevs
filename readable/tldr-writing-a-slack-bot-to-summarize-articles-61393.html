<!DOCTYPE html>
<html lang="en">
<head>
    <title>
TLDR: Writing a Slack bot to Summarize Articles - linksfor.dev(s)    </title>
    <meta charset="utf-8">
    <link rel="alternate" type="application/rss+xml" title="Linksfor.dev(s) feed" href="https://linksfor.dev/feed.rss" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta name="google" value="notranslate">
    <link rel="stylesheet" type="text/css" href="/style.css" />
    <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon">
    <link rel="icon" href="/favicon.ico" type="image/x-icon">
    <meta property="og:title" content="TLDR: Writing a Slack bot to Summarize Articles - linksfor.dev(s)"/>
    <meta property="article:author" content="https://blog.concurlabs.com/@chrispogeek"/>
    <meta property="og:description" content="Using state-of-the-art NLP to read more news, faster"/>
    <meta property="og:type" content="website"/>
    <meta property="og:url" content="https://blog.concurlabs.com/how-to-write-a-tldr-chat-bot-ec02d9e1649c?gi=43f4aa712e27"/>

<meta property="og:site_name" content="linksfor.dev(s)" />
</head>
<body>
	<div class="devring" style="background: #222">
		<div class="grid">
			<div style="display: grid; grid-template-columns: .5fr 1fr 1fr 1fr; text-align: center;">
				<span class="devring-title"><a href="https://devring.club/">devring.club</a></span>
				<a href="https://devring.club/sites/1/prev" class="devring-previous">Previous</a>
				<a href="https://devring.club/random" class="devring-random">Random</a>
				<a href="https://devring.club/sites/1/next" class="devring-next">Next</a>
			</div>
		</div>
	</div>
    <div class="grid">
        <h1>
<a href="/" style="color:inherit">linksfor.dev(s)</a>
        </h1>
        <title>linksfor.dev(s) - TLDR: Writing a Slack bot to Summarize Articles</title>
<div class="readable">
        <h1>TLDR: Writing a Slack bot to Summarize Articles</h1>
            <div>by https://blog.concurlabs.com/@chrispogeek</div>
            <div>Reading time: 12-15 minutes</div>
        <div>Posted here: 06 May 2020</div>
        <p><a href="https://blog.concurlabs.com/how-to-write-a-tldr-chat-bot-ec02d9e1649c?gi=43f4aa712e27">https://blog.concurlabs.com/how-to-write-a-tldr-chat-bot-ec02d9e1649c?gi=43f4aa712e27</a></p>
        <hr/>
<div id="readability-page-1" class="page"><section><div><div><h2 id="adc9">Using state-of-the-art NLP to read more news, faster</h2><div><div><div><p><a href="https://blog.concurlabs.com/@chrispogeek?source=post_page-----ec02d9e1649c----------------------" rel="noopener"><img alt="Chris Ismael" src="https://miro.medium.com/fit/c/96/96/0*TAmpBBJLfb86KXt5.jpg" width="48" height="48"></a></p></div></div></div><figure><div><div><div><div><p><img src="https://miro.medium.com/max/60/1*NovotauEVd4QODQdjDAceQ.jpeg?q=20" width="5318" height="3545" role="presentation"></p><p><img width="5318" height="3545" srcset="https://miro.medium.com/max/552/1*NovotauEVd4QODQdjDAceQ.jpeg 276w, https://miro.medium.com/max/1104/1*NovotauEVd4QODQdjDAceQ.jpeg 552w, https://miro.medium.com/max/1280/1*NovotauEVd4QODQdjDAceQ.jpeg 640w, https://miro.medium.com/max/1400/1*NovotauEVd4QODQdjDAceQ.jpeg 700w" sizes="700px" role="presentation" src="https://miro.medium.com/max/5318/1*NovotauEVd4QODQdjDAceQ.jpeg"></p></div></div></div></div><figcaption data-selectable-paragraph=""><em>Photo by </em><a href="https://unsplash.com/@freestocks?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" target="_blank" rel="noopener"><em>freestocks</em></a><em> on </em><a href="https://unsplash.com/?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" target="_blank" rel="noopener"><em>Unsplash</em></a></figcaption></figure><p id="453e" data-selectable-paragraph="">This post outlines how to integrate a state-of-the-art machine learning model into a <strong>Slack bot to generate a summary of articles shared by their URL</strong> — a.k.a “tldr” (<em>too long, didn’t read</em>). To round up the rest of the tech stack, I’ll cover how to set up<a href="https://www.cortex.dev/" target="_blank" rel="noopener"> <strong>Cortex</strong></a><strong> (Machine learning model deployment platform),</strong><a href="https://serverless.com/" target="_blank" rel="noopener"><strong> Serverless Framework</strong></a><strong> (AWS Lambda), and</strong><a href="https://aws.amazon.com/sqs/" target="_blank" rel="noopener"><strong> AWS Simple Queue Service</strong></a><strong> (SQS)</strong>. The end result looks like this:</p><figure><div><div><div><div><p><img src="https://miro.medium.com/freeze/max/60/1*jxpEQB8N9CxYOvhdBtFJOw.gif?q=20" width="800" height="527" role="presentation"></p><p><img width="800" height="527" srcset="https://miro.medium.com/max/552/1*jxpEQB8N9CxYOvhdBtFJOw.gif 276w, https://miro.medium.com/max/1104/1*jxpEQB8N9CxYOvhdBtFJOw.gif 552w, https://miro.medium.com/max/1280/1*jxpEQB8N9CxYOvhdBtFJOw.gif 640w, https://miro.medium.com/max/1400/1*jxpEQB8N9CxYOvhdBtFJOw.gif 700w" sizes="700px" role="presentation" src="https://miro.medium.com/max/800/1*jxpEQB8N9CxYOvhdBtFJOw.gif"></p></div></div></div></div><figcaption data-selectable-paragraph=""><em>Props to the</em><a href="https://concurlabs.com/" target="_blank" rel="noopener"><em> Concur Labs</em></a><em> team for helping me test this @tldr bot, and for their patience while the bot sent countless debug messages their way. Also a special thanks to </em><a href="https://medium.com/@robert.mc.reed" target="_blank" rel="noopener"><em>Robert Reed</em></a>, <a href="https://twitter.com/DrCatNelson" target="_blank" rel="noopener"><em>Catherine Nelson</em></a><em>, and </em><a href="https://www.buildingmlpipelines.com/" target="_blank" rel="noopener"><em>Hannes Hapke</em></a><em> for helping me write this blog post.</em></figcaption></figure><p id="1341" data-selectable-paragraph="">We’re bombarded with information everyday and it’s getting harder and harder to keep up. News articles in particular are such a tease. They lure you in with click-bait titles only to reward you with content that falls flat. Oftentimes I just want to get to the bottom of a headline and decide whether it’s worth reading the entire thing. Sometimes I get my fix from <a href="https://www.reddit.com/r/savedyouaclick/" target="_blank" rel="noopener">/r/savedyouaclick</a>, but that list is manually curated.</p><p id="d520" data-selectable-paragraph="">Wouldn’t it be cool if you could get real-time summaries on-demand? You may even earn back some precious time to spend elsewhere :)</p><p id="915a" data-selectable-paragraph="">The idea to write this bot came from a<a href="https://sshleifer.github.io/blog_v2/jupyter/2020/03/12/bart.html" target="_blank" rel="noopener"> blog post</a> based on the <a href="https://arxiv.org/abs/1910.13461" target="_blank" rel="noopener">BART model</a> created by a team at Facebook AI Research. The post describes an improved machine learning model for achieving better performance in text generation (summarization) over previous models. What’s great is that this model is ready to be used as a library.</p><p id="7264" data-selectable-paragraph="">The diagram below shows what each component of the tldr bot will do. The flow is initiated when a Slack user @ mentions the bot with a URL, and the data then flows through the remaining components and is ultimately returned to the Slack user as a summary. I will explain how each component works, starting from the summarization component on the right, and finishing with the Slack component on the left.</p><figure><div><div><div><div><p><img src="https://miro.medium.com/max/60/0*ElbL07B-bHUY60_I?q=20" width="1600" height="738" role="presentation"></p><p><img width="1600" height="738" srcset="https://miro.medium.com/max/552/0*ElbL07B-bHUY60_I 276w, https://miro.medium.com/max/1104/0*ElbL07B-bHUY60_I 552w, https://miro.medium.com/max/1280/0*ElbL07B-bHUY60_I 640w, https://miro.medium.com/max/1400/0*ElbL07B-bHUY60_I 700w" sizes="700px" role="presentation"></p></div></div></div></div></figure></div></div></section><section><div><div><p id="510f" data-selectable-paragraph="">This part of the workflow calls the library to summarize the text. <a href="https://sshleifer.github.io/blog_v2/jupyter/2020/03/12/bart.html" target="_blank" rel="noopener">This post</a> from Sam Shleifer describes how the BART model works, as well as providing performance comparisons between different text generation techniques (<a href="https://sshleifer.github.io/blog_v2/jupyter/2020/03/12/bart.html#Demo:-BartForConditionalGeneration" target="_blank" rel="noopener">Seq2seq vs GPT2</a>)<strong>.</strong> Sam is a research engineer at <a href="https://huggingface.co/" target="_blank" rel="noopener">Hugging Face</a>, a company that develops state-of-the-art natural language processing (NLP) technologies. My key takeaway from his post is this piece of Python code that does the summarization:</p><figure><div></div><figcaption>PyTorch implementation by Huggingface</figcaption></figure><p id="90d2" data-selectable-paragraph="">This code shows that there is already a model we can use, and we feed it the text to summarize. The model was pretrained on a huge text dataset and fine-tuned to summarize news articles. As a result, it performs best on news articles, as compared to other types of text.</p><p id="aa3f" data-selectable-paragraph="">It wasn’t immediately obvious to me, but a GPU is required to perform a faster inference. What’s more, the model is quite large (the “<a href="https://huggingface.co/models?search=bart" target="_blank" rel="noopener">bart-large-cnn</a>” model is 1.5GB), so I needed to ensure it was pre-loaded before performing inference. At this point I turned to AWS to get a <em>souped-up</em> cloud machine (EC2) on a cluster configured with a GPU. I ended up using an AWS virtual machine with Nvidia T4 GPU (more on this below).</p><p id="4243" data-selectable-paragraph="">While I could manually set this all up via the AWS Console, I deferred the manual work to an open source platform called <a href="https://www.cortex.dev/" target="_blank" rel="noopener">Cortex</a>. <strong>Cortex simplifies the process of deploying machine learning models to the cloud</strong>. In this case to AWS. Below I have re-written the previous Bart code to use Cortex:</p><figure><div></div><figcaption>Click <a href="https://github.com/ismaelc/bart-summarizer-cortex/blob/master/summarizer/predictor.py" target="_blank" rel="noopener">here</a> for the Github repo</figcaption></figure><p id="438d" data-selectable-paragraph="">After<a href="https://www.cortex.dev/install" target="_blank" rel="noopener"> installing Cortex</a> and setting it up to <a href="https://www.cortex.dev/deployments/gpus" target="_blank" rel="noopener">use a GPU</a>, you can configure the cluster via cortex.yaml like <a href="https://github.com/ismaelc/bart-summarizer-cortex/blob/master/summarizer/cortex.yaml" target="_blank" rel="noopener">this</a>:</p><figure><div></div><figcaption>Added “gpu: 1” here</figcaption></figure><p id="32f3" data-selectable-paragraph="">I chose a single GPU VM, with instance size <a href="https://aws.amazon.com/ec2/instance-types/g4/" target="_blank" rel="noopener">g4dn.xlarge</a> as suggested in the Cortex documentation, but it might be possible to run this with other GPUs (K80 in a P2 instance).</p><p id="60ff" data-selectable-paragraph="">Then run “cortex deploy” to deploy the cluster on AWS. At the end of the deployment process you will get an AWS endpoint URL which serves as the interface to the summarizer model.</p><figure><div><div><div><div><p><img src="https://miro.medium.com/max/60/0*ERE3cnz6yWf7ue12?q=20" width="1600" height="244" role="presentation"></p><p><img width="1600" height="244" srcset="https://miro.medium.com/max/552/0*ERE3cnz6yWf7ue12 276w, https://miro.medium.com/max/1104/0*ERE3cnz6yWf7ue12 552w, https://miro.medium.com/max/1280/0*ERE3cnz6yWf7ue12 640w, https://miro.medium.com/max/1400/0*ERE3cnz6yWf7ue12 700w" sizes="700px" role="presentation"></p></div></div></div></div></figure><p id="8d8f" data-selectable-paragraph="">To get accurate summaries the correct text needs to be provided to the summarization service. This means <strong>excluding certain parts of the html and selecting only the section of the page that needs summarizing</strong>.</p><figure><div><div><div><div><p><img src="https://miro.medium.com/max/60/0*NAxv2t_GRalcv5B2?q=20" width="1600" height="862" role="presentation"></p><p><img width="1600" height="862" srcset="https://miro.medium.com/max/552/0*NAxv2t_GRalcv5B2 276w, https://miro.medium.com/max/1104/0*NAxv2t_GRalcv5B2 552w, https://miro.medium.com/max/1280/0*NAxv2t_GRalcv5B2 640w, https://miro.medium.com/max/1400/0*NAxv2t_GRalcv5B2 700w" sizes="700px" role="presentation"></p></div></div></div></div><figcaption data-selectable-paragraph="">Comparing the incorrect and correct selection in a web page. (<a href="https://www.latimes.com/california/story/2020-04-20/coronavirus-serology-testing-la-county" target="_blank" rel="noopener">Source</a>)</figcaption></figure><p id="ad69" data-selectable-paragraph="">My current approach uses a mix of HTML node counting, elimination, and DOM selection. First I find the parent node with the most children, and then extract the text based on the selector (e.g. div#root section div.article). I’m still tweaking this selection logic, but it works on roughly 80% of the sites I’ve tested. I will cover this in greater detail in another post. For now let’s focus on the serverless platform where the HTML selection logic is hosted.</p><p id="bb45" data-selectable-paragraph="">The selector logic runs as an <strong>AWS Lambda function</strong>, accessible via an API endpoint. AWS Lambda is one of many technologies that utilize a serverless architecture. Others include Google Cloud Functions and Azure Functions, to name a few. <strong>In a serverless architecture, code is written as an event-driven cloud-hosted function that runs without the need to maintain or manage a server.</strong> Here’s an example of a Lambda function written using the<a href="https://serverless.com/" target="_blank" rel="noopener"> <em>Serverless Framework</em></a><em>. </em>We pass a “url” parameter to it:</p><figure><div></div><figcaption>“utils.getSummaryFromUrl(url)” encapsulates all the custom logic that sends the parsed HTML to the summarizer service</figcaption></figure><p id="b404" data-selectable-paragraph="">In comparison, here is <a href="https://nodejs.org/en/docs/guides/getting-started-guide/" target="_blank" rel="noopener">a node.js hello world app</a>.</p><figure><div></div></figure><p id="07fc" data-selectable-paragraph="">One major difference between the two is that serverless functions exit as soon as they finish executing. This is in contrast to the node.js example above that runs indefinitely until you force it to exit (Ctrl-C). This is more of a design choice rather than a note on advantage (or disadvantage). I will discuss this briefly at the end of this post.</p><p id="2b54" data-selectable-paragraph="">In AWS, like many platforms, configurability can introduce complexity to an otherwise straightforward task. One such example is the need to manually set up an API gateway to expose a Lambda endpoint. The public endpoint is an optional step because AWS resources can operate within the confines of the AWS environment. However it’s time-consuming to set this up if you’re going to wire up public endpoints for all of your projects.</p><p id="7136" data-selectable-paragraph="">This is where the Serverless Framework comes in. <strong>Serverless (https://serverless.com) is a set of open source tools and services to help you manage and deploy cloud functions</strong>. After <a href="https://serverless.com/framework/docs/getting-started/" target="_blank" rel="noopener">installation</a>, there are two CLI commands to execute to get a <em>hello world</em> Lambda template up and running in AWS:</p><pre><span id="b9fb" data-selectable-paragraph="">sls create --template aws-nodejs</span><span id="5045" data-selectable-paragraph="">sls deploy</span></pre><p id="9925" data-selectable-paragraph="">To define an AWS endpoint for the summarize function, I simply add this to our serverless.yaml file under “functions”, and run `sls deploy` again:</p><figure><div></div></figure><p id="7cce" data-selectable-paragraph="">After a successful deployment, the AWS endpoint URL will be returned via the serverless CLI.</p><figure><div><div><div><div><p><img src="https://miro.medium.com/max/60/0*kqPhCizIL64mAEJZ?q=20" width="1144" height="744" role="presentation"></p><p><img width="1144" height="744" srcset="https://miro.medium.com/max/552/0*kqPhCizIL64mAEJZ 276w, https://miro.medium.com/max/1104/0*kqPhCizIL64mAEJZ 552w, https://miro.medium.com/max/1280/0*kqPhCizIL64mAEJZ 640w, https://miro.medium.com/max/1400/0*kqPhCizIL64mAEJZ 700w" sizes="700px" role="presentation"></p></div></div></div></div></figure><p id="ef4e" data-selectable-paragraph="">The summarization part of this project (HTML DOM selector and Cortex) can take between 2 and 10 seconds to reply to a summarization request, depending on the amount of text in the webpage. This becomes problematic for clients with timeout requirements outside of this window.</p><figure><div><div><div><div><p><img src="https://miro.medium.com/max/60/0*7XsNoQrTavPbRrl1?q=20" width="1200" height="569" role="presentation"></p><p><img width="1200" height="569" srcset="https://miro.medium.com/max/552/0*7XsNoQrTavPbRrl1 276w, https://miro.medium.com/max/1000/0*7XsNoQrTavPbRrl1 500w" sizes="500px" role="presentation"></p></div></div></div></div><figcaption data-selectable-paragraph="">Echo, echo, echo</figcaption></figure><p id="01d4" data-selectable-paragraph="">Slack, for example, times out after 3 seconds. I’ve observed that after 3 seconds, Slack retries the request to the summarizer service until it gets a response within that time frame.</p><p id="26b3" data-selectable-paragraph="">So if a user types “@tldr https://someurl” and the summary service fails to reply within 3 seconds, Slack retries multiple times on the user’s behalf. Once the summary service catches up, <strong>it sends a reply for each of those requests</strong>, resulting in the echo effect in the image above.</p><p id="fb3f" data-selectable-paragraph="">To fix this, I used a queue service — <a href="https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-getting-started.html#step-receive-delete-message" target="_blank" rel="noopener"><strong>AWS Simple Queue Service (SQS)</strong></a>.</p><p id="8bea" data-selectable-paragraph=""><strong>A queue allows us to hand over the processing to the summary service</strong>. This is done by sending the request to the queue and immediately replying to the Slack user with empty text.</p><p id="da04" data-selectable-paragraph="">Here’s a Serverless function that handles the Slack code:</p><figure><div></div></figure><p id="0394" data-selectable-paragraph="">In order for another Lambda function to receive the notifications, SQS can be configured as an event source for that Lambda function. In this case, the receiver is the <strong>HTML DOM Selector</strong> serverless function.</p><p id="4f4d" data-selectable-paragraph="">Setting this up via the<a href="https://serverless.com/" target="_blank" rel="noopener"> Serverless Framework</a> is straightforward. In the serverless.yaml config file, add the ARN of your queue under the “sqs” directive for the function you want to designate as the receiver of the message payload:</p><figure><div></div><figcaption>sqs here points to your SQS ARN in AWS</figcaption></figure><p id="3d2a" data-selectable-paragraph="">With this, our Lambda function gets triggered when the payload is sent to SQS (actually it is polling behind the scenes, but <a href="https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-short-and-long-polling.html" target="_blank" rel="noopener">you can read about that later</a>):</p><figure><div></div><figcaption>This sqsEvent Lambda function gets called when SQS receives a message from the bot backend.</figcaption></figure><p id="6dbd" data-selectable-paragraph="">The bot allows the user to send a URL to the summary service. While there are a variety of bot platforms and providers we can use, the general setup is the same — the bot client communicates with a backend app that you create. The backend receives the user message and builds a response from it.</p><figure><div><div><div><div><p><img src="https://miro.medium.com/max/60/0*u6LR3SpPx4eUKLPy?q=20" width="786" height="502" role="presentation"></p><p><img width="786" height="502" srcset="https://miro.medium.com/max/552/0*u6LR3SpPx4eUKLPy 276w, https://miro.medium.com/max/1000/0*u6LR3SpPx4eUKLPy 500w" sizes="500px" role="presentation"></p></div></div></div></div></figure><p id="4a58" data-selectable-paragraph="">There are two aspects of bot development that I’ll briefly cover here.</p><ol><li id="5245" data-selectable-paragraph="">The bot platform’s* SDK</li><li id="c3d5" data-selectable-paragraph="">The bot platform’s API</li></ol><p id="7374" data-selectable-paragraph="">*Examples of bot platforms include (but are not limited to) <a href="https://api.slack.com/" target="_blank" rel="noopener">Slack</a>, <a href="https://cai.tools.sap/" target="_blank" rel="noopener">SAP Conversational AI</a>, <a href="https://dev.botframework.com/" target="_blank" rel="noopener">Microsoft Bot Framework</a>, and <a href="https://aws.amazon.com/lex/" target="_blank" rel="noopener">Amazon Lex</a>.</p><p id="bb6e" data-selectable-paragraph="">A good example of the importance of the SDK is in filtering bot event notifications sent to your backend. Instead of manually filtering them yourself with code (e.g. <em>Was @tldr mentioned, or was the message intended for someone else?</em>), you can instead rely on the SDK to do the heavy-lifting for you. Here is an <a href="https://github.com/slackapi/node-slack-sdk" target="_blank" rel="noopener">example</a> from the Slack node.js SDK showing direct messages to the bot handled with a few lines of code (used in combination with event configuration in Slack):</p><figure><div></div></figure><p id="d735" data-selectable-paragraph="">A robust public REST API is equally important for many reasons, one of which is flexibility. In my case, I only needed to send a message back to the user after a long wait from the summary service. Instead of importing the SDK, I can directly call the Slack REST API:</p><figure><div></div></figure><p id="2243" data-selectable-paragraph="">The other benefit of a robust bot API is the availability for 3rd-party libraries to build on top of it. This allowed me to use this<a href="https://github.com/johnagan/serverless-slack-app" target="_blank" rel="noopener"> Slack Serverless Boilerplate</a> code to handle OAuth flows and token management in AWS DynamoDB, among other things. If not for this boilerplate, I’d have to write my own code to handle auth and manually set up a database for different bot instances.</p></div></div></section><section><div><div><p id="c3ab" data-selectable-paragraph=""><strong><em>Do I need all this just to summarize text?</em></strong></p><p id="1dc0" data-selectable-paragraph="">Nope. There are tons of summarization libraries you can use, most of which don’t require a powerful machine to run on. However there’s a tradeoff between ease of use/setup and the quality of the summary. For example, here’s an output from a sentence-ranking algorithm implemented in a <a href="https://github.com/xiaoxu193/PyTeaser" target="_blank" rel="noopener">Python library</a> which requires very minimal setup:</p><figure><div></div></figure><p id="6622" data-selectable-paragraph="">While it does a decent job of selecting entire sentences that it thinks represent the article, the output of the BART model feels more natural:</p><pre><span id="1ac2" data-selectable-paragraph="">Twitter has added an advanced layer of protection for HTTPS known as "forward secrecy" The move is the latest response from U.S. Internet firms following disclosures by former spy agency contractor Edward Snowden.</span></pre><p id="375a" data-selectable-paragraph="">The BART model produces high-quality “abstractive” summaries, where it generates new sentences that aren’t in the original article. This is in contrast to “extractive” summaries which contain whole sentences copied from the original. Abstractive summaries, if they’re done well, are much more natural, but the tradeoff is increased processing time and the need to use a GPU.</p><p id="6c95" data-selectable-paragraph="">I chose Cortex as my ML-deployment tool <strong>not</strong> because I had the luxury of testing all the other options out there, but because it had sample code that was easy to integrate with the BART model code. In fact I only had to replace their existing “Summarize” code sample with the BART code. Having said that, there are other tools/ways to deploy a model/endpoint to the cloud — <a href="https://www.tensorflow.org/tfx/guide/serving" target="_blank" rel="noopener">Tensorflow Serving</a>, <a href="https://pytorch.org/serve/" target="_blank" rel="noopener">TorchServe</a>, and <a href="https://onnx.ai/" target="_blank" rel="noopener">ONNX</a> to name a few.</p><p id="5a92" data-selectable-paragraph="">Choosing a serverless architecture is also a matter of preference. There are <a href="https://www.cloudflare.com/learning/serverless/why-use-serverless/" target="_blank" rel="noopener">pros and cons</a> to using serverless but one major aspect to consider is that a serverless function has to “boot up” every time it’s called. While this introduces increased latency, it’s also more cost-effective than hosting a server that is always running.</p><p id="2337" data-selectable-paragraph="">There are endless choices you can make in order to customize this bot to your needs. Play around with them and have fun! I would love to hear your comments, questions, and feedback on the different approaches used to write a @tldr bot.</p></div></div></section></div></div>
    </div>
    <footer>
        <div>created by buildstarted &copy; 2020 <a href="/about">about</a></div>
        <div>Share this page on social media: copy and paste this url https://linksfor.dev/</div>
        <div>If you prefer RSS: <a href="https://linksfor.dev/feed.xml">https://linksfor.dev/feed.xml</a></div>
		<div>Customer satisfaction guaranteed to be optional.</div>
    </footer>
    
    <script async defer>
        _dna = window._dna || {};
        _dna.siteId = "linksfor.devs";
        _dna.outlink = true;

        (function () {
            let dna = document.createElement('script');
            dna.type = 'text/javascript';
            dna.async = true;
            dna.src = '//dna.buildstarted.com/t.js';
            let s = document.getElementsByTagName('script')[0];
            s.parentNode.insertBefore(dna, s);
        })();
    </script>
    <noscript><img src="//dna.buildstarted.com/g?siteId=linksfor.devs"/></noscript>
</body>
</html>