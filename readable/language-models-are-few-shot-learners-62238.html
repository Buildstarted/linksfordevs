<!DOCTYPE html>
<html lang="en">
<head>
    <title>
Language Models are Few-Shot Learners - linksfor.dev(s)    </title>
    <meta charset="utf-8">
    <link rel="alternate" type="application/rss+xml" title="Linksfor.dev(s) feed" href="https://linksfor.dev/feed.rss" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta name="google" value="notranslate">
    <link rel="stylesheet" type="text/css" href="/style.min.css" />
    <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon">
    <link rel="icon" href="/favicon.ico" type="image/x-icon">
    <meta property="og:title" content="Language Models are Few-Shot Learners - linksfor.dev(s)"/>
    <meta property="article:author" content="[Submitted on 28 May 2020]"/>
    <meta property="og:description" content="Recent work has demonstrated substantial gains on many NLP tasks and&#xA;benchmarks by pre-training on a large corpus of text followed by fine-tuning on&#xA;a specific task. While typically task-agnostic in architecture, this method&#xA;still requires task-specific fine-tuning datasets of thousands or tens of&#xA;thousands of examples. By contrast, humans can generally perform a new language&#xA;task from only a few examples or from simple instructions - something which&#xA;current NLP systems still largely struggle to do. Here we show that scaling up&#xA;language models greatly improves task-agnostic, few-shot performance, sometimes&#xA;even reaching competitiveness with prior state-of-the-art fine-tuning&#xA;approaches. Specifically, we train GPT-3, an autoregressive language model with&#xA;175 billion parameters, 10x more than any previous non-sparse language model,&#xA;and test its performance in the few-shot setting. For all tasks, GPT-3 is&#xA;applied without any gradient updates or fine-tuning, with tasks and few-shot&#xA;demonstrations specified purely via text interaction with the model. GPT-3&#xA;achieves strong performance on many NLP datasets, including translation,&#xA;question-answering, and cloze tasks, as well as several tasks that require&#xA;on-the-fly reasoning or domain adaptation, such as unscrambling words, using a&#xA;novel word in a sentence, or performing 3-digit arithmetic. At the same time,&#xA;we also identify some datasets where GPT-3&#x27;s few-shot learning still struggles,&#xA;as well as some datasets where GPT-3 faces methodological issues related to&#xA;training on large web corpora. Finally, we find that GPT-3 can generate samples&#xA;of news articles which human evaluators have difficulty distinguishing from&#xA;articles written by humans. We discuss broader societal impacts of this finding&#xA;and of GPT-3 in general."/>
    <meta property="og:type" content="website"/>
    <meta property="og:url" content="https://arxiv.org/abs/2005.14165"/>

<meta property="og:site_name" content="linksfor.dev(s)" />
</head>
<body>
    <div class="devring" style="background: #222">
        <div style="text-align:center">Explore other dev related sites in this ring. If you would like to join this ring <a href="https://devring.club">click here</a>.</div>
        <div class="grid">
            <div style="display: grid; grid-template-columns: .5fr 1fr 1fr 1fr; text-align: center;">
                <span class="devring-title"><a href="https://devring.club/">devring.club</a></span>
                <a href="https://devring.club/sites/1/prev" class="devring-previous">Previous</a>
                <a href="https://devring.club/random" class="devring-random">Random</a>
                <a href="https://devring.club/sites/1/next" class="devring-next">Next</a>
            </div>
        </div>
    </div>
    <div class="grid">
        <h1 style="margin: unset">
<a href="/" style="color:inherit">linksfor.dev(s)</a>
        </h1>
        <title>linksfor.dev(s) - Language Models are Few-Shot Learners</title>
<div class="readable">
        <h1>Language Models are Few-Shot Learners</h1>
            <div>by [Submitted on 28 May 2020]</div>
            <div>Reading time: 3 minutes</div>
        <div>Posted here: 29 May 2020</div>
        <p><a href="https://arxiv.org/abs/2005.14165">https://arxiv.org/abs/2005.14165</a></p>
        <hr/>
<div id="readability-page-1" class="page"><div id="content-inner">
  <div id="abs">
    
    
    <div><p><span>Authors:</span><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Brown%2C+T+B">Tom B. Brown</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Mann%2C+B">Benjamin Mann</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Ryder%2C+N">Nick Ryder</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Subbiah%2C+M">Melanie Subbiah</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Kaplan%2C+J">Jared Kaplan</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Dhariwal%2C+P">Prafulla Dhariwal</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Neelakantan%2C+A">Arvind Neelakantan</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Shyam%2C+P">Pranav Shyam</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Sastry%2C+G">Girish Sastry</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Askell%2C+A">Amanda Askell</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Agarwal%2C+S">Sandhini Agarwal</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Herbert-Voss%2C+A">Ariel Herbert-Voss</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Krueger%2C+G">Gretchen Krueger</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Henighan%2C+T">Tom Henighan</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Child%2C+R">Rewon Child</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Ramesh%2C+A">Aditya Ramesh</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Ziegler%2C+D+M">Daniel M. Ziegler</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wu%2C+J">Jeffrey Wu</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Winter%2C+C">Clemens Winter</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Hesse%2C+C">Christopher Hesse</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Chen%2C+M">Mark Chen</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Sigler%2C+E">Eric Sigler</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Litwin%2C+M">Mateusz Litwin</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Gray%2C+S">Scott Gray</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Chess%2C+B">Benjamin Chess</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Clark%2C+J">Jack Clark</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Berner%2C+C">Christopher Berner</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=McCandlish%2C+S">Sam McCandlish</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Radford%2C+A">Alec Radford</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Sutskever%2C+I">Ilya Sutskever</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Amodei%2C+D">Dario Amodei</a></p></div>
      
    
  
    <p><a href="https://arxiv.org/pdf/2005.14165">Download PDF</a></p><blockquote>
      <span>Abstract:</span>  Recent work has demonstrated substantial gains on many NLP tasks and
benchmarks by pre-training on a large corpus of text followed by fine-tuning on
a specific task. While typically task-agnostic in architecture, this method
still requires task-specific fine-tuning datasets of thousands or tens of
thousands of examples. By contrast, humans can generally perform a new language
task from only a few examples or from simple instructions - something which
current NLP systems still largely struggle to do. Here we show that scaling up
language models greatly improves task-agnostic, few-shot performance, sometimes
even reaching competitiveness with prior state-of-the-art fine-tuning
approaches. Specifically, we train GPT-3, an autoregressive language model with
175 billion parameters, 10x more than any previous non-sparse language model,
and test its performance in the few-shot setting. For all tasks, GPT-3 is
applied without any gradient updates or fine-tuning, with tasks and few-shot
demonstrations specified purely via text interaction with the model. GPT-3
achieves strong performance on many NLP datasets, including translation,
question-answering, and cloze tasks, as well as several tasks that require
on-the-fly reasoning or domain adaptation, such as unscrambling words, using a
novel word in a sentence, or performing 3-digit arithmetic. At the same time,
we also identify some datasets where GPT-3's few-shot learning still struggles,
as well as some datasets where GPT-3 faces methodological issues related to
training on large web corpora. Finally, we find that GPT-3 can generate samples
of news articles which human evaluators have difficulty distinguishing from
articles written by humans. We discuss broader societal impacts of this finding
and of GPT-3 in general.

    </blockquote>

    <!--CONTEXT-->
    
  </div>
</div><div>
      <h2>Submission history</h2><p> From: Tom B Brown [<a href="https://arxiv.org/show-email/b5cb66e9/2005.14165">view email</a>]
      <br><strong>[v1]</strong>
Thu, 28 May 2020 17:29:03 UTC (6,995 KB)<br></p></div></div></div>
    </div>
    <footer>
        <div>created by <a href="https://buildstarted.com">buildstarted</a> &copy; 2020 <a href="/about">about</a></div>
        <div>Share this page on social media: copy and paste this url https://linksfor.dev/</div>
        <div>If you prefer RSS: <a href="https://linksfor.dev/feed.xml">https://linksfor.dev/feed.xml</a></div>
        <div>Customer satisfaction guaranteed to be optional.</div>
    </footer>
    
    <script async defer>
        _dna = window._dna || {};
        _dna.siteId = "linksfor.devs";
        _dna.outlink = true;

        (function() {
            let dna = document.createElement('script');
            dna.type = 'text/javascript';
            dna.async = true;
            dna.src = '//dna.buildstarted.com/t.js';
            let s = document.getElementsByTagName('script')[0];
            s.parentNode.insertBefore(dna, s);
        })();
    </script>
    <noscript><img src="//dna.buildstarted.com/g?siteId=linksfor.devs"/></noscript>
</body>
</html>