<!DOCTYPE html>
<html lang="en">
<head>
    <title>
Ask a Techspert: How do machine learning models explain themselves? - linksfor.dev(s)    </title>
    <meta charset="utf-8">
    <link rel="alternate" type="application/rss+xml" title="Linksfor.dev(s) feed" href="https://linksfor.dev/feed.rss" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta name="google" value="notranslate">
    <link rel="stylesheet" type="text/css" href="/style.css" />
    <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon">
    <link rel="icon" href="/favicon.ico" type="image/x-icon">
    <meta property="og:title" content="Ask a Techspert: How do machine learning models explain themselves? - linksfor.dev(s)"/>
    <meta property="article:author" content="Andrea Lewis &#xC5;kerman&#xA;&#xA;    &#xA;      &#xA;        Keyword Contributor"/>
    <meta property="og:description" content="Interview with Been Kim, a Google researcher working on the People &#x2B; AI Research (PAIR) team, who devotes her time to making sure artificial intelligence puts people, not machines, at its center."/>
    <meta property="og:type" content="website"/>
    <meta property="og:url" content="https://blog.google/inside-google/googlers/ask-techspert-how-do-machine-learning-models-explain-themselves/?utm_source=feedburner&amp;utm_medium=feed&amp;utm_campaign=Feed%3A%20blogspot%2FMKuf%20(The%20Keyword%20%7C%20Official%20Google%20Blog)"/>

<meta property="og:site_name" content="linksfor.dev(s)" />
</head>
<body>
    <div class="devring" style="background: #222">
        <div style="text-align:center">Explore other dev related sites in this ring. If you would like to join this ring <a href="https://devring.club">click here</a>.</div>
        <div class="grid">
            <div style="display: grid; grid-template-columns: .5fr 1fr 1fr 1fr; text-align: center;">
                <span class="devring-title"><a href="https://devring.club/">devring.club</a></span>
                <a href="https://devring.club/sites/1/prev" class="devring-previous">Previous</a>
                <a href="https://devring.club/random" class="devring-random">Random</a>
                <a href="https://devring.club/sites/1/next" class="devring-next">Next</a>
            </div>
        </div>
    </div>
    <div class="grid">
        <h1>
<a href="/" style="color:inherit">linksfor.dev(s)</a>
        </h1>
        <title>linksfor.dev(s) - Ask a Techspert: How do machine learning models explain themselves?</title>
<div class="readable">
        <h1>Ask a Techspert: How do machine learning models explain themselves?</h1>
            <div>by Andrea Lewis &#xC5;kerman&#xA;&#xA;    &#xA;      &#xA;        Keyword Contributor</div>
            <div>Reading time: 6-8 minutes</div>
        <div>Posted here: 10 Jul 2020</div>
        <p><a href="https://blog.google/inside-google/googlers/ask-techspert-how-do-machine-learning-models-explain-themselves/?utm_source=feedburner&amp;utm_medium=feed&amp;utm_campaign=Feed%3A%20blogspot%2FMKuf%20(The%20Keyword%20%7C%20Official%20Google%20Blog)">https://blog.google/inside-google/googlers/ask-techspert-how-do-machine-learning-models-explain-themselves/?utm_source=feedburner&amp;utm_medium=feed&amp;utm_campaign=Feed%3A%20blogspot%2FMKuf%20(The%20Keyword%20%7C%20Official%20Google%20Blog)</a></p>
        <hr/>
<div id="readability-page-1" class="page"><div><p><b>Why is this field of work so important?</b></p><p>Machine learning is such a powerful tool, and because of that, you want to make sure you’re using it responsibly. Let’s take an electric machine saw as an example. It’s a super powerful tool, but you need to learn how to use it in order not to cut your fingers. Once you learn, it’s so useful and efficient that you’ll never want to go back to using a hand saw. And the same goes for machine learning. We want to help you understand and use machine learning correctly, fairly and safely.&nbsp;</p><p>Since machine learning is used in our everyday lives, it’s also important for everyone to understand how it impacts us. No matter whether you’re a coffee shop owner using machine learning to optimize the purchase of your beans based on seasonal trends, or your doctor diagnoses you with a disease with the help of this technology, it’s often crucial to understand why a machine learning model has produced the outcome it has. It’s also important for developers and decision-makers to be able to explain or present a machine learning model to people in order to do so. This is what we call “interpretability.”&nbsp;</p><p><b>How do you make machine learning models easier to understand and interpret?&nbsp;</b></p><p>There are many different ways to make an ML model easier to understand. One way is to make the model reflect how humans think from the start, and have the model "trained" to provide explanations along with predictions, meaning when it gives you an outcome, it also has to explain how it got there.&nbsp;</p><p>Another way is to try and explain a model after the training on data is done. This is something you can do when the model has been built to use input to provide an output from its own perspective, optimizing for prediction, without a clear “how” included. This means you're able to plug things into it and see what comes out, and that can give you some insight into how the model generally makes decisions, but you don't necessarily know exactly how specific inputs are interpreted by the model in specific cases.&nbsp;</p><p>One way to try and explain models after they’ve been trained is using low level features or high level concepts. Let me give you an example of what this means. Imagine a system that classifies pictures: you give it a picture and it says, “This is a cat.” A low level feature is when I then ask the machine which pixels mattered for that prediction, it can tell us if it was one pixel or the other, and we might be able to see that the pixels in question show the cat’s whiskers. But we might also see that it is a scattering of pixels that don’t appear meaningful to the human eye, or that it’s made the wrong interpretation. High level concepts are more similar to the way humans communicate with one another. Instead of asking about pixels, I’d ask, “Did the whiskers matter for the prediction? or the paws?” and again, the machine can show me what imagery led it to reach this conclusion. Based on the outcome, I can understand the model better. (Together with researchers from Stanford, we’ve published <a href="https://arxiv.org/abs/1902.03129" target="_blank">papers</a> that go into further detail on <a href="https://arxiv.org/abs/1711.11279" target="_blank">this</a> for those who are interested.)</p><p><b>Can machines understand some things that we humans can’t?&nbsp;</b></p><p>Yes! This is an area that I am very interested in myself. I am currently working on a way to showcase how technology can help humans learn new things. Machine learning technology is better at some things than we are; for example it can analyze and interpret data at a much larger scale than humans can. Leveraging this technology, I believe we can enlighten human scientists with knowledge they haven't previously been aware of.&nbsp;</p><p><b>What do you need to be careful of when you’re making conclusions based on machine learning models?</b></p><p>First of all, we have to be careful that human bias doesn't come into play. Humans carry biases that we simply cannot help and are often unaware of, so if an explanation is up to a human’s interpretation, and often it is, then we have a problem. Humans read what they want to read. Now, this doesn’t mean that you should remove humans from the loop. Humans communicate with machines, and vice versa. Machines need to communicate their outcomes in the form of a clear statement using quantitative data, not one that is vague and completely open for interpretation. If the latter happens, then the machine hasn’t done a very good job and the human isn’t able to provide good feedback to the machine. It could also be that the outcome simply lacks additional context only the human can provide, or that it could benefit from having caveats, in order for them to make an informed judgement about the results of the model.&nbsp;</p><p><b>What are some of the main challenges of this work?&nbsp;</b></p><p>Well, one of the challenges for computer scientists in this field is dealing with non mathematical objectives, which are things you might want to optimize for, but don’t have an equation for. You can’t always define what is good for humans using math. That requires us to test and evaluate methods with rigor, and have a table full of different people to discuss the outcome. Another thing has to do with complexity. Humans are so complex that we have a whole field of work - psychology - to study this. So in my work, we don't just have computational challenges, but also complex humans that we have to consider. Value-based questions such as “what defines fairness?” are even harder. They require interdisciplinary collaboration, and a diverse group of people in the room to discuss each individual matter.</p><p><b>What's the most exciting part?&nbsp;</b></p><p>I think interpretability research and methods are making a huge impact. Machine learning technology is a powerful tool that will transform society as we know it, and helping others to use it safely is very rewarding.&nbsp;</p><p>On a more personal note, I come from South Korea and grew up in circumstances where I feel I didn’t have too many opportunities. I was incredibly lucky to get a scholarship to MIT and come to the U.S. When I think about the people who haven't had these opportunities to be educated in science or machine learning, and knowing that this machine learning technology can really help and be useful to them in their everyday lives if they use it safely, I feel really motivated to be working on democratizing this technology. There's many ways to do it, and interpretability is one of the things that I can contribute with.&nbsp;&nbsp;<span></span></p></div></div></div>
    </div>
    <footer>
        <div>created by buildstarted &copy; 2020 <a href="/about">about</a></div>
        <div>Share this page on social media: copy and paste this url https://linksfor.dev/</div>
        <div>If you prefer RSS: <a href="https://linksfor.dev/feed.xml">https://linksfor.dev/feed.xml</a></div>
        <div>Customer satisfaction guaranteed to be optional.</div>
    </footer>
    
    <script async defer>
        _dna = window._dna || {};
        _dna.siteId = "linksfor.devs";
        _dna.outlink = true;

        (function () {
            let dna = document.createElement('script');
            dna.type = 'text/javascript';
            dna.async = true;
            dna.src = '//dna.buildstarted.com/t.js';
            let s = document.getElementsByTagName('script')[0];
            s.parentNode.insertBefore(dna, s);
        })();
    </script>
    <noscript><img src="//dna.buildstarted.com/g?siteId=linksfor.devs" /></noscript>
</body>
</html>