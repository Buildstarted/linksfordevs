<!DOCTYPE html>
<html lang="en">
<head>
    <title>
Evolution-Guided Policy Gradient in Reinforcement Learning - linksfor.dev(s)    </title>
    <meta charset="utf-8">
    <link rel="alternate" type="application/rss+xml" title="Linksfor.dev(s) feed" href="https://linksfor.dev/feed.rss" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta name="google" value="notranslate">
    <link rel="stylesheet" type="text/css" href="/style.css" />
    <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon">
    <link rel="icon" href="/favicon.ico" type="image/x-icon">
    <meta property="og:title" content="Evolution-Guided Policy Gradient in Reinforcement Learning - linksfor.dev(s)"/>
    <meta property="article:author" content="Authors:Shauharda Khadka, Kagan Tumer"/>
    <meta property="og:description" content="Deep Reinforcement Learning (DRL) algorithms have been successfully applied&#xA;to a range of challenging control tasks. However, these methods typically&#xA;suffer from three core difficulties: temporal credit assignment with sparse&#xA;rewards, lack of effective exploration, and brittle convergence properties that&#xA;are extremely sensitive to hyperparameters. Collectively, these challenges&#xA;severely limit the applicability of these approaches to real-world problems.&#xA;Evolutionary Algorithms (EAs), a class of black box optimization techniques&#xA;inspired by natural evolution, are well suited to address each of these three&#xA;challenges. However, EAs typically suffer from high sample complexity and&#xA;struggle to solve problems that require optimization of a large number of&#xA;parameters. In this paper, we introduce Evolutionary Reinforcement Learning&#xA;(ERL), a hybrid algorithm that leverages the population of an EA to provide&#xA;diversified data to train an RL agent, and reinserts the RL agent into the EA&#xA;population periodically to inject gradient information into the EA. ERL&#xA;inherits EA&#x27;s ability of temporal credit assignment with a fitness metric,&#xA;effective exploration with a diverse set of policies, and stability of a&#xA;population-based approach and complements it with off-policy DRL&#x27;s ability to&#xA;leverage gradients for higher sample efficiency and faster learning.&#xA;Experiments in a range of challenging continuous control benchmarks demonstrate&#xA;that ERL significantly outperforms prior DRL and EA methods."/>
    <meta property="og:type" content="website"/>
    <meta property="og:url" content="https://arxiv.org/abs/1805.07917"/>

<meta property="og:site_name" content="linksfor.dev(s)" />
</head>
<body>
    <div class="devring" style="background: #222">
        <div style="text-align:center">Explore other dev related sites in this ring. If you would like to join this ring <a href="https://devring.club">click here</a>.</div>
        <div class="grid">
            <div style="display: grid; grid-template-columns: .5fr 1fr 1fr 1fr; text-align: center;">
                <span class="devring-title"><a href="https://devring.club/">devring.club</a></span>
                <a href="https://devring.club/sites/1/prev" class="devring-previous">Previous</a>
                <a href="https://devring.club/random" class="devring-random">Random</a>
                <a href="https://devring.club/sites/1/next" class="devring-next">Next</a>
            </div>
        </div>
    </div>
    <div class="grid">
        <h1>
<a href="/" style="color:inherit">linksfor.dev(s)</a>
        </h1>
        <title>linksfor.dev(s) - Evolution-Guided Policy Gradient in Reinforcement Learning</title>
<div class="readable">
        <h1>Evolution-Guided Policy Gradient in Reinforcement Learning</h1>
            <div>by Authors:Shauharda Khadka, Kagan Tumer</div>
            <div>Reading time: 2-3 minutes</div>
        <div>Posted here: 27 Feb 2019</div>
        <p><a href="https://arxiv.org/abs/1805.07917">https://arxiv.org/abs/1805.07917</a></p>
        <hr/>
<div id="readability-page-1" class="page"><div id="content-inner">
  <div id="abs">

    
    
    <p>
  
  
  
    
  
  
    
    
  

  (Submitted on 21 May 2018 (<a href="https://arxiv.org/abs/1805.07917v1">v1</a>), last revised 27 Oct 2018 (this version, v2))</p>
    <blockquote><span>Abstract:</span>  Deep Reinforcement Learning (DRL) algorithms have been successfully applied
to a range of challenging control tasks. However, these methods typically
suffer from three core difficulties: temporal credit assignment with sparse
rewards, lack of effective exploration, and brittle convergence properties that
are extremely sensitive to hyperparameters. Collectively, these challenges
severely limit the applicability of these approaches to real-world problems.
Evolutionary Algorithms (EAs), a class of black box optimization techniques
inspired by natural evolution, are well suited to address each of these three
challenges. However, EAs typically suffer from high sample complexity and
struggle to solve problems that require optimization of a large number of
parameters. In this paper, we introduce Evolutionary Reinforcement Learning
(ERL), a hybrid algorithm that leverages the population of an EA to provide
diversified data to train an RL agent, and reinserts the RL agent into the EA
population periodically to inject gradient information into the EA. ERL
inherits EA's ability of temporal credit assignment with a fitness metric,
effective exploration with a diverse set of policies, and stability of a
population-based approach and complements it with off-policy DRL's ability to
leverage gradients for higher sample efficiency and faster learning.
Experiments in a range of challenging continuous control benchmarks demonstrate
that ERL significantly outperforms prior DRL and EA methods.
</blockquote>
    <!--CONTEXT-->
    
  </div>
</div><div>
      <h2>Submission history</h2><p> From: Shauharda Khadka [<a href="https://arxiv.org/show-email/6954e71b/1805.07917">view email</a>]
      <br>
  <strong><a href="https://arxiv.org/abs/1805.07917v1">[v1]</a></strong>
  Mon, 21 May 2018 06:55:58 UTC (1,124 KB)<br><strong>[v2]</strong>
Sat, 27 Oct 2018 17:23:26 UTC (843 KB)<br></p></div></div></div>
    </div>
    <footer>
        <div>created by <a href="https://buildstarted.com">buildstarted</a> &copy; 2020 <a href="/about">about</a></div>
        <div>Share this page on social media: copy and paste this url https://linksfor.dev/</div>
        <div>If you prefer RSS: <a href="https://linksfor.dev/feed.xml">https://linksfor.dev/feed.xml</a></div>
        <div>Customer satisfaction guaranteed to be optional.</div>
    </footer>
    
    <script async defer>
        _dna = window._dna || {};
        _dna.siteId = "linksfor.devs";
        _dna.outlink = true;

        (function() {
            let dna = document.createElement('script');
            dna.type = 'text/javascript';
            dna.async = true;
            dna.src = '//dna.buildstarted.com/t.js';
            let s = document.getElementsByTagName('script')[0];
            s.parentNode.insertBefore(dna, s);
        })();
    </script>
    <noscript><img src="//dna.buildstarted.com/g?siteId=linksfor.devs"/></noscript>
</body>
</html>