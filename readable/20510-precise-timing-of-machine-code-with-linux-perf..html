<!DOCTYPE html>
<html lang="en">
<head>
    <title>
Precise timing of machine code with Linux perf. -
linksfor.dev(s)
    </title>
    <link rel="alternate" type="application/rss+xml" title="Linksfor.dev(s) feed" href="https://linksfor.dev/feed.rss" />
    <meta charset="utf-8">
    <meta name="Description" content="A curated source of links that devs might find interesting. Updated around the clock." />
    <meta name="google" value="notranslate">
    <link rel="stylesheet" type="text/css" href="/style.css" />
    <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon">
    <link rel="icon" href="/favicon.ico" type="image/x-icon">
</head>
<body>
    <div class="grid">
        
<div class="readable">
    <div id="readOverlay" class="style-ebook"><div id="readInner" class="margin-medium size-medium"><h1>Precise timing of machine code with Linux perf.</h1><div><div class="notebody"><p><strong>Contents:</strong></p><hr><p><strong>Subscribe to my <a href="/blog/2019/04/03/Precise-timing-of-machine-code-with-Linux-perf#mc_embed_signup">mailing list</a> and support me on <a href="https://www.patreon.com/dendibakh">Patreon</a>.</strong></p><hr><p>I feel like writing these days, so powered by this feeling I decided to share another quite useful technique that I sometimes use. Today I will show how you can utilize Intel <a href="https://easyperf.net/blog/2018/06/08/Advanced-profiling-topics-PEBS-and-LBR">LBR</a> (Last Branch Record) feature to do cycle-based timing of the code blocks. Knowing precisely how much cycles it took to execute certain number of assembly instructions, how great that would be? Want to know how? Keep on reading and you will learn. Just to tease you, look at this desired report:</p><div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  0000000000400618   movb  $0x0, (%rbp,%rdx,1) 
  000000000040061d   add $0x1, %rdx 
  0000000000400621   cmp $0xc800000, %rdx 
  0000000000400628   jnz 0x400618 
  
  # 5 cycles
</code></pre></div></div><p><strong>How cool is that!</strong></p><p>It’s just an arbitrary code snippet to give you a taste of what you’ll be able to see in this article. This shows precise number of cycles for a given <a href="https://en.wikipedia.org/wiki/Basic_block">basic block</a>: 4 instructions in a block were executed in 5 cycles.</p><p>I find it very educational to look at those numbers and try to understand why you get them. Again, great for improving the mental model of how CPU works.</p><p>I want to thank Andi Kleen for showing me this technique.</p><h3 id="recap-on-lbr">Recap on LBR</h3><p>The underlying CPU feature that allows this to happen is called LBR(Last Branch Record). I previously wrote an <a href="https://easyperf.net/blog/2018/06/08/Advanced-profiling-topics-PEBS-and-LBR">article about LBR</a>, so I encourage you to visit this blog post if you want to know what it is.</p><p>LBR feature is used to track control flow of the program. This feature uses MSRs (Model Specific Registers) to store history of last executed branches. Why we are so interested in branches? Well, because this is how we are able to determine the control flow of our program. Since we are interested in branches which are always the last instructions in a basic blocks and all instructions in the basic block are guaranteed to be executed once, we can only focus on branches. Using this control flow statistics we can determine which path of our program (chain of basic blocks) is the hottest. This is sometimes called a Hyper Block. And there are other applications of LBR feature, see <a href="https://lwn.net/Articles/680985/">here</a>.</p><p>Traditionally LBR entry<sup id="fnref:1"><a href="#fn:1" class="footnote">1</a></sup> has two important components: <code class="language-plaintext highlighter-rouge">FROM_IP</code> and <code class="language-plaintext highlighter-rouge">TO_IP</code>, which are basically source address of the branch and destination address. If we collect long enough history of source-destination pairs, we will be able to unwind the control flow of our program. Just like a call stack! Sounds nice and simple, right?</p><p>Starting from Haswell we already could get the information if the branch was predicted or not. There was a dedicated bit for it in the LBR entry. But since Skylake additional <code class="language-plaintext highlighter-rouge">LBR_INFO</code><sup id="fnref:2"><a href="#fn:2" class="footnote">2</a></sup> component was added to LBR entry which received additional <code class="language-plaintext highlighter-rouge">Cycle Count</code> field:</p><div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Cycle Count - Elapsed core clocks since last update to the LBR stack.
</code></pre></div></div><p>With this new field we are able not only to get the branch history, but also to get precise timing in cycles between two taken branches. Awesome! But be aware that it only works starting from Skylake. Additionally you need to have not too old version of perf (mine is 4.15.18). Here is the <a href="https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?id=48d02a1d5c137d362defd11a5d57d0af4a75a983">commit</a> which added this functionality in perf.</p><h3 id="getting-cycles-count-with-linux-perf">Getting cycles count with linux perf</h3><p>First of all, to use this functionality with perf, LBR must be enabled:</p><div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>dmesg | <span class="nb">grep</span><span class="nt">-i</span> lbr
<span class="o">[</span>    0.228149] Performance Events: PEBS fmt3+, 32-deep LBR, Skylake events, full-width counters, Intel PMU driver.
</code></pre></div></div><p>To demonstrate usefulness of this technique I took the example from one my previous articles about <a href="https://easyperf.net/blog/2019/02/09/Top-Down-performance-analysis-methodology">Top-Down Analysis methodology</a>. The code has a loop with a random load that typically will miss in L3-cache and go to main memory:</p><div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="cp">#include &lt;random&gt;
</span><span class="k">extern</span><span class="s">"C"</span><span class="p">{</span><span class="kt">void</span><span class="n">foo</span><span class="p">(</span><span class="kt">char</span><span class="o">*</span><span class="n">a</span><span class="p">,</span><span class="kt">int</span><span class="n">n</span><span class="p">);</span><span class="p">}</span><span class="k">const</span><span class="kt">int</span><span class="n">_200MB</span><span class="o">=</span><span class="mi">1024</span><span class="o">*</span><span class="mi">1024</span><span class="o">*</span><span class="mi">200</span><span class="p">;</span><span class="kt">int</span><span class="nf">main</span><span class="p">()</span><span class="p">{</span><span class="kt">char</span><span class="o">*</span><span class="n">a</span><span class="o">=</span><span class="p">(</span><span class="kt">char</span><span class="o">*</span><span class="p">)</span><span class="n">malloc</span><span class="p">(</span><span class="n">_200MB</span><span class="p">);</span><span class="c1">// 200 MB buffer</span><span class="k">for</span><span class="p">(</span><span class="kt">int</span><span class="n">i</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span><span class="n">i</span><span class="o">&lt;</span><span class="n">_200MB</span><span class="p">;</span><span class="n">i</span><span class="o">++</span><span class="p">)</span><span class="p">{</span><span class="n">a</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span><span class="p">}</span><span class="k">const</span><span class="kt">int</span><span class="n">min</span><span class="o">=</span><span class="mi">1</span><span class="p">;</span><span class="k">const</span><span class="kt">int</span><span class="n">max</span><span class="o">=</span><span class="n">_200MB</span><span class="p">;</span><span class="n">std</span><span class="o">::</span><span class="n">default_random_engine</span><span class="n">generator</span><span class="p">;</span><span class="n">std</span><span class="o">::</span><span class="n">uniform_int_distribution</span><span class="o">&lt;</span><span class="kt">int</span><span class="o">&gt;</span><span class="n">distribution</span><span class="p">(</span><span class="n">min</span><span class="p">,</span><span class="n">max</span><span class="p">);</span><span class="k">for</span><span class="p">(</span><span class="kt">int</span><span class="n">i</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span><span class="n">i</span><span class="o">&lt;</span><span class="mi">10000000</span><span class="p">;</span><span class="n">i</span><span class="o">++</span><span class="p">)</span><span class="p">{</span><span class="kt">int</span><span class="n">random_int</span><span class="o">=</span><span class="n">distribution</span><span class="p">(</span><span class="n">generator</span><span class="p">);</span><span class="n">foo</span><span class="p">(</span><span class="n">a</span><span class="p">,</span><span class="n">random_int</span><span class="p">);</span><span class="p">}</span><span class="n">free</span><span class="p">(</span><span class="n">a</span><span class="p">);</span><span class="k">return</span><span class="mi">0</span><span class="p">;</span><span class="p">}</span></code></pre></div></div><p>Function <code class="language-plaintext highlighter-rouge">foo</code> is implemented in assembly like this:</p><pre><code class="language-asm">foo:
# start some irrelevant work
One_KB_of_NOPs
# finish some irrelevant work

# load that goes to DRAM
mov     rax, QWORD [rdi + rsi]
# introduce dependency chain
mov     rax, QWORD [rdi + rax]

xor rax, rax
ret
</code></pre><p>You can find complete code sample on my <a href="https://github.com/dendibakh/dendibakh.github.io/tree/master/_posts/code/TimingBasicBlocks">github</a>.</p><p>Let’s collect LBR data on this application:</p><div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>~/perf record <span class="nt">-b</span><span class="nt">-e</span> cycles ./a.out
<span class="o">[</span> perf record: Woken up 13 <span class="nb">times </span>to write data <span class="o">]</span><span class="o">[</span> perf record: Captured and wrote 3.024 MB perf.data <span class="o">(</span>3864 samples<span class="o">)</span><span class="o">]</span></code></pre></div></div><p>Now let’s decode the data we collected. You need <a href="https://github.com/intelxed/xed">xed</a> (Intel X86 Encoder Decoder) to see the instructions not just the raw bytes.</p><div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>~/perf script <span class="nt">-F</span> +brstackinsn  | ../xed <span class="nt">-F</span> insn: <span class="nt">-A</span><span class="nt">-64</span><span class="o">&gt;</span> dump.txt
</code></pre></div></div><p>Now in <code class="language-plaintext highlighter-rouge">dump.txt</code> we have all the branch history records, but we are only interested in those which end with return from <code class="language-plaintext highlighter-rouge">foo</code> instruction:</p><div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  ... &lt;lots of code&gt;
  400df0:       0f 1f 84 00 00 00 00    nop    DWORD PTR [rax+rax*1+0x0]
  400df7:       00 
  400df8:       0f 1f 84 00 00 00 00    nop    DWORD PTR [rax+rax*1+0x0]
  400dff:       00 
  400e00:       48 8b 04 37             mov    rax,QWORD PTR [rdi+rsi*1]
  400e04:       48 8b 04 07             mov    rax,QWORD PTR [rdi+rax*1]
  400e08:       48 31 c0                xor    rax,rax
  400e0b:       c3                      ret                               &lt;== This is the branch of our interest
</code></pre></div></div><p>Let’s find something interesting in the dump using the address of our <code class="language-plaintext highlighter-rouge">ret</code> instruction:</p><div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  ... &lt;lots of code&gt;
  0000000000400df0    nopl  %eax, (%rax,%rax,1)
  0000000000400df8    nopl  %eax, (%rax,%rax,1)
  0000000000400e00    movq  (%rdi,%rsi,1), %rax
  0000000000400e04    movq  (%rdi,%rax,1), %rax
  0000000000400e08    xor %rax, %rax
  0000000000400e0b    retq                            # PRED 266 cycles 0.49 IPC
</code></pre></div></div><p><strong>Cool!</strong> But this is just one snippet out of many. With every sample we also capture entire LBR stack which might have multiple branch records for the block that we are interested in:</p><div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span><span class="nb">grep</span><span class="s2">"0000000000400e0b"</span> dump.txt | <span class="nb">grep</span><span class="s2">"cycles"</span><span class="nt">-c</span>
20536
</code></pre></div></div><p>Notice we have <code class="language-plaintext highlighter-rouge">3864</code> samples, but <code class="language-plaintext highlighter-rouge">20536</code> LBR entries for our branch. On the average for every sample we had roughly 5 LBR entries that we are interested in.</p><h3 id="application-estimating-prefetch-window">Application: estimating prefetch window</h3><p>Let’s see what we can do with this timing information. Let’s collect all the timings for this <code class="language-plaintext highlighter-rouge">RET</code> instruction. Here is the <a href="https://github.com/dendibakh/dendibakh.github.io/blob/master/_posts/code/TimingBasicBlocks/parse.sh">script</a> that creates csv file from the dump:</p><div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span><span class="nb">grep</span><span class="s2">"0000000000400e0b"</span> dump.txt | <span class="nb">grep</span><span class="s2">"cycles"</span> | <span class="nb">sort</span><span class="o">&gt;</span> cycle_lines.txt
<span class="nv">$ </span><span class="nb">sed</span><span class="s1">'s/.*PRED \(.*\) cycles.*/\1/'</span> cycle_lines.txt <span class="o">&gt;</span> cycles.txt
<span class="nv">$ </span><span class="nb">uniq </span>cycles.txt uniq.txt
<span class="nv">$ </span><span class="nb">cat </span>uniq.txt | <span class="k">while </span><span class="nb">read </span>line <span class="p">;</span><span class="k">do </span><span class="nb">echo</span><span class="nt">-n</span><span class="nv">$line</span><span class="s2">","</span><span class="o">&gt;&gt;</span> cycles.csv <span class="o">&amp;&amp;</span><span class="nb">grep</span><span class="nv">$line</span> cycles.txt <span class="nt">-w</span><span class="nt">-c</span><span class="o">&gt;&gt;</span> cycles.csv <span class="p">;</span><span class="k">done</span></code></pre></div></div><p>Now, let’s plot it:</p><p><img src="/img/posts/TimingBasicBlocks/chart.png" alt="" class="center-image"></p><p><em>How to read this chart</em>:</p><p>This chart shows us the number of times we got the certain latency for the basic block. On one hand we don’t want to have high latency, but on the other hand we want to have as higher amount of samples with low latency. Something like that:</p><p><img src="/img/posts/TimingBasicBlocks/asymptote.png" alt="" class="center-image"></p><p>To estimate prefetch window I removed both loads and collected LBR samples once again. I found that 99% of the time function <code class="language-plaintext highlighter-rouge">foo</code> executes in 32 cycles<sup id="fnref:3"><a href="#fn:3" class="footnote">3</a></sup>. It is easy to prove since execution is bound by Retiring. On Skylake we can retire 4 instructions per cycle. In 1 KB of 8-byte NOPs we have <code class="language-plaintext highlighter-rouge">2^10 / 8 = 2^7</code> instructions. Thus it executes in <code class="language-plaintext highlighter-rouge">2^7 / 4 = 32</code> cycles.</p><p><strong>So, this tells us that we have prefetch window of 32 cycles</strong>. In the presented case it’s constant and doesn’t vary, but in the code that you might be dealing with it likely won’t be so.</p><p>Let’s insert prefetch hint and plot the latencies for this case:</p><div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">for</span><span class="p">(</span><span class="kt">int</span><span class="n">i</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span><span class="n">i</span><span class="o">&lt;</span><span class="mi">100000000</span><span class="p">;</span><span class="n">i</span><span class="o">++</span><span class="p">)</span><span class="p">{</span><span class="kt">int</span><span class="n">random_int</span><span class="o">=</span><span class="n">distribution</span><span class="p">(</span><span class="n">generator</span><span class="p">);</span><span class="o">+</span><span class="n">__builtin_prefetch</span><span class="p">(</span><span class="n">a</span><span class="o">+</span><span class="n">random_int</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">);</span><span class="n">foo</span><span class="p">(</span><span class="n">a</span><span class="p">,</span><span class="n">random_int</span><span class="p">);</span><span class="p">}</span></code></pre></div></div><p>Here is combined plot with original (baseline) and improved (prefetched) cases:</p><p><img src="/img/posts/TimingBasicBlocks/chart_pref.png" alt="" class="center-image"></p><p>You see, we lowered the spike around 300 cycles and shifted both spikes to the left which is good (towards lower latencies). Also notice the orange dot for 32 cycles latency which has frequency around 3000 times. That means we now have much less cycles that are wasted due to demanding load that misses in caches. See more details about cache misses statistics for this exact case in my previous article about <a href="https://easyperf.net/blog/2019/02/09/Top-Down-performance-analysis-methodology">Top-Down Analysis methodology</a>.</p><p><strong>That’s all. Hope you enjoyed and found it useful! Good luck in using this powerful feature!</strong></p><hr></div></div></div></div>
</div>
    </div>
    <footer>
        <div>created by buildstarted &copy; 2020 <a href="/about">about</a></div>
        <div>Share this page on social media: copy and paste this url https://linksfor.dev/</div>
        <div>If you prefer RSS: <a href="https://linksfor.dev/feed.xml">https://linksfor.dev/feed.xml</a></div>
    </footer>
    
    <script>
        (function () {
            var COLLECT_URL = "https://dna.buildstarted.com/t";
            var SITE_ID = "linksfor.devs";
            var GLOBAL_VAR_NAME = "__DNA__";

            window[GLOBAL_VAR_NAME] = {};

            window[GLOBAL_VAR_NAME].sendPageView = function () {
                var path = location.pathname;
                var referrer = document.referrer;

                var url = COLLECT_URL + "?siteid=" + SITE_ID + "&p=" + encodeURIComponent(path) + "&r=" + encodeURIComponent(referrer);

                try { fetch(url, { method: "GET" }); } catch (e) { }
            };

            window[GLOBAL_VAR_NAME].sendPageView();
        })();
    </script>
</body>
</html>