<!DOCTYPE html>
<html lang="en">
<head>
    <title>
Fixing Random, bonus episode 3 - linksfor.dev(s)    </title>
    <meta charset="utf-8">
    <link rel="alternate" type="application/rss+xml" title="Linksfor.dev(s) feed" href="https://linksfor.dev/feed.rss" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta name="google" value="notranslate">
    <link rel="stylesheet" type="text/css" href="/style.css" />
    <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon">
    <link rel="icon" href="/favicon.ico" type="image/x-icon">
    <meta property="og:title" content="Fixing Random, bonus episode 3 - linksfor.dev(s)"/>
    <meta property="og:description" content="You might recall that before my immensely long series on ways we could make C# a probabilistic programming language, I did a short series on how we can automatically computed the exact derivative i&#x2026;"/>
    <meta property="og:type" content="website"/>
    <meta property="og:url" content="https://ericlippert.com/2019/12/11/fixing-random-bonus-episode-3/"/>

<meta property="og:site_name" content="linksfor.dev(s)" />
</head>
<body>
    <div class="grid">
        <h1>
                <span style="cursor: default" title="linksfor.dev(s) has been running for 1 year! :partypopper:">üéâ</span>
<a href="/" style="color:inherit">linksfor.dev(s)</a>
        </h1>
        <title>linksfor.dev(s) - Fixing Random, bonus episode 3</title>
<div class="readable">
        <h1>Fixing Random, bonus episode 3</h1>
            <div>Reading time: 10-12 minutes</div>
        <div>Posted here: 13 Dec 2019</div>
        <p><a href="https://ericlippert.com/2019/12/11/fixing-random-bonus-episode-3/">https://ericlippert.com/2019/12/11/fixing-random-bonus-episode-3/</a></p>
        <hr/>
<div id="readability-page-1" class="page"><div>
		<p data-adtags-visited="true">You might recall that before my immensely long series on ways we could make C# a probabilistic programming language, I did a short series on how we can automatically computed the exact derivative in any direction of a real-valued function of any number of variables for a small cost, by using <a href="https://ericlippert.com/2019/01/07/dual-numbers-part-1/">dual numbers</a>. All we need is for the function we are computing to be computed by addition, subtraction, multiplication, division and exponentiation of functions whose derivatives are known, which is quite a lot of possible functions.</p>
<p data-adtags-visited="true">There is a reason why I did that topic before ‚ÄúFixing Random‚Äù, but sadly I never got to the connection between differential calculus and sampling from an arbitrary distribution. I thought I might spend a couple of episodes sketching out how it is useful to use automatic differentiation when trying to sample from a distribution. I‚Äôm not going to write the code; I‚Äôll just give some of the ‚Äúflavour‚Äù of the algorithms.</p>
<p data-adtags-visited="true">Before I get into it, a quick refresher on the Metropolis algorithm. The algorithm is:</p>
<ul>
<li>We have a probability distribution function that takes a point and returns a double that is the ‚Äúprobability weight‚Äù of that point. The PDF need not be ‚Äúnormalized‚Äù; that is, the area under the curve need not add up to 1.0. We wish to generate a series of samples that conforms to this distribution.</li>
<li>We choose a random ‚Äúinitial point‚Äù as the <em>current</em> sample.</li>
<li>We randomly choose a <em>candidate</em> sample from some distribution based solely on the current sample.</li>
<li>If the candidate is higher weight than current, it becomes the new current and we yield the value as the sample.</li>
<li>If it is lower weight, then we take the ratio of candidate weight to current weight, which will be between 0.0 and 1.0. We flip an unfair coin with that probability of getting heads. Heads, we accept the candidate, tails we reject it and try again.</li>
<li>Repeat; choose a new candidate based on the new current.</li>
</ul>
<p data-adtags-visited="true">The Metropolis algorithm is straightforward and works, but it has a few problems.</p>
<ul>
<li><strong>How do we choose the initial point?</strong></li>
</ul>
<p data-adtags-visited="true">Since Metropolis is typically used to compute a posterior distribution after an observation, and we typically have the prior distribution in hand, we can use the prior distribution as our source of the initial point.</p>
<ul>
<li><strong>What if the initial point is accidentally in a low-probability region?</strong> We might produce a series of unlikely samples before we eventually get to a high-probability current point.</li>
</ul>
<p data-adtags-visited="true">We can solve this by ‚Äúburning‚Äù ‚Äî discarding ‚Äî some number of initial samples; we waste computation cycles so we would like the number of samples it takes to get to<br>
‚Äúconvergence‚Äù to the true distribution to be small. As we‚Äôll see, there are ways we can use automatic differentiation to help solve this problem.</p>
<ul>
<li><strong>What distribution should we use to choose the next candidate given the current sample?</strong></li>
</ul>
<p data-adtags-visited="true">This is a tricky problem. The examples I gave in this series were basically ‚Äúchoose a new point by sampling from a normal distribution where the mean is the current point‚Äù, which seems reasonable, but then you realize that the question has been begged. A normal distribution has two parameters: the mean and the standard deviation. The standard deviation corresponds to ‚Äúhow big a step should we typically try?‚Äù&nbsp; If the deviation is too large then we will step from high-probability regions to low-probability regions frequently, which means that we discard a lot of candidates, which wastes time. If it is too small then we get ‚Äústuck‚Äù in a high-probability region and produce a lot of samples close to each other, which is also bad.</p>
<p data-adtags-visited="true">Basically, we have a ‚Äútuning parameter‚Äù in the standard deviation and it is not obvious how to choose it to get the right combination of good performance and uncorrelated samples.</p>
<p data-adtags-visited="true">These last two problems lead us to ask an important question: <strong>is there information we can obtain from the weight function that helps us choose a consistently better candidate? </strong>That would lower the time to convergence and might also result in fewer rejections when we‚Äôve gotten to a high-probability region.</p>
<p data-adtags-visited="true">I‚Äôm going to sketch out one such technique in this episode, and another in the next.</p>
<p data-adtags-visited="true">As I noted above, Metropolis is frequently used to sample points from a high-dimensional distribution; to make it easier to understand, I‚Äôll stick to one-dimensional cases here, but imagine that instead of a simple curve for our PDF, we have a complex multidimensional surface.</p>
<p data-adtags-visited="true">Let‚Äôs use as our motivating example the mixture model from many episodes ago:</p>
<p data-adtags-visited="true"><img data-attachment-id="6590" data-permalink="https://ericlippert.com/2019/12/11/fixing-random-bonus-episode-3/screen-shot-2019-12-10-at-12-18-08-pm/" data-orig-file="https://ericlippert.files.wordpress.com/2019/12/screen-shot-2019-12-10-at-12.18.08-pm.png" data-orig-size="1101,540" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Screen Shot 2019-12-10 at 12.18.08 PM" data-image-description="" data-medium-file="https://ericlippert.files.wordpress.com/2019/12/screen-shot-2019-12-10-at-12.18.08-pm.png?w=300" data-large-file="https://ericlippert.files.wordpress.com/2019/12/screen-shot-2019-12-10-at-12.18.08-pm.png?w=584" src="https://ericlippert.files.wordpress.com/2019/12/screen-shot-2019-12-10-at-12.18.08-pm.png?w=584" alt="Screen Shot 2019-12-10 at 12.18.08 PM.png" srcset="https://ericlippert.files.wordpress.com/2019/12/screen-shot-2019-12-10-at-12.18.08-pm.png?w=584 584w, https://ericlippert.files.wordpress.com/2019/12/screen-shot-2019-12-10-at-12.18.08-pm.png?w=150 150w, https://ericlippert.files.wordpress.com/2019/12/screen-shot-2019-12-10-at-12.18.08-pm.png?w=300 300w, https://ericlippert.files.wordpress.com/2019/12/screen-shot-2019-12-10-at-12.18.08-pm.png?w=768 768w, https://ericlippert.files.wordpress.com/2019/12/screen-shot-2019-12-10-at-12.18.08-pm.png?w=1024 1024w, https://ericlippert.files.wordpress.com/2019/12/screen-shot-2019-12-10-at-12.18.08-pm.png 1101w" sizes="(max-width: 584px) 100vw, 584px"></p>
<p data-adtags-visited="true">Of course we can sample from this distribution directly if we know that it is the sum of two normal distributions, but let‚Äôs suppose that we don‚Äôt know that. We just have a function which produces this weight.&nbsp; Let me annotate this function to say where we want to go next if the current sample is in a particular region.</p>
<p data-adtags-visited="true"><img data-attachment-id="6592" data-permalink="https://ericlippert.com/2019/12/11/fixing-random-bonus-episode-3/annotated/" data-orig-file="https://ericlippert.files.wordpress.com/2019/12/annotated.jpg" data-orig-size="2202,1080" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Annotated" data-image-description="" data-medium-file="https://ericlippert.files.wordpress.com/2019/12/annotated.jpg?w=300" data-large-file="https://ericlippert.files.wordpress.com/2019/12/annotated.jpg?w=584" src="https://ericlippert.files.wordpress.com/2019/12/annotated.jpg?w=584" alt="Annotated.jpg" srcset="https://ericlippert.files.wordpress.com/2019/12/annotated.jpg?w=584 584w, https://ericlippert.files.wordpress.com/2019/12/annotated.jpg?w=1166 1166w, https://ericlippert.files.wordpress.com/2019/12/annotated.jpg?w=150 150w, https://ericlippert.files.wordpress.com/2019/12/annotated.jpg?w=300 300w, https://ericlippert.files.wordpress.com/2019/12/annotated.jpg?w=768 768w, https://ericlippert.files.wordpress.com/2019/12/annotated.jpg?w=1024 1024w" sizes="(max-width: 584px) 100vw, 584px"></p>
<p data-adtags-visited="true">I said that we could use the derivative to help us, but it is very unclear from this diagram how the derivative helps:</p>
<ul>
<li>The derivative is small and positive in the region marked ‚Äúgo hard right‚Äù and in the immediate vicinity of the two peaks and one valley.</li>
<li>The derivative is large and positive in the ‚Äúslight right‚Äù region and to the left of the tall peak.</li>
<li>The derivative is large and negative in the ‚Äúslight left‚Äù region and on the right of the small peak.</li>
<li>The derivative is small and negative in the ‚Äúhard left‚Äù region and in the immediate vicinity of the peaks and valley.</li>
</ul>
<p data-adtags-visited="true">No particular value for the derivative clearly identifies a region of interest. It seems like we cannot use the derivative to help us out here; what we really want is to move away from small-area regions and towards large-area regions.</p>
<p data-adtags-visited="true">Here‚Äôs the trick.</p>
<p data-adtags-visited="true">Ready?</p>
<p data-adtags-visited="true">I‚Äôm going to graph the <em>log</em> of the weight function below the weight function:</p>
<p data-adtags-visited="true"><img data-attachment-id="6593" data-permalink="https://ericlippert.com/2019/12/11/fixing-random-bonus-episode-3/screen-shot-2019-12-10-at-12-44-39-pm/" data-orig-file="https://ericlippert.files.wordpress.com/2019/12/screen-shot-2019-12-10-at-12.44.39-pm.png" data-orig-size="546,580" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Screen Shot 2019-12-10 at 12.44.39 PM" data-image-description="" data-medium-file="https://ericlippert.files.wordpress.com/2019/12/screen-shot-2019-12-10-at-12.44.39-pm.png?w=282" data-large-file="https://ericlippert.files.wordpress.com/2019/12/screen-shot-2019-12-10-at-12.44.39-pm.png?w=546" src="https://ericlippert.files.wordpress.com/2019/12/screen-shot-2019-12-10-at-12.44.39-pm.png?w=584" alt="Screen Shot 2019-12-10 at 12.44.39 PM.png" srcset="https://ericlippert.files.wordpress.com/2019/12/screen-shot-2019-12-10-at-12.44.39-pm.png 546w, https://ericlippert.files.wordpress.com/2019/12/screen-shot-2019-12-10-at-12.44.39-pm.png?w=141 141w, https://ericlippert.files.wordpress.com/2019/12/screen-shot-2019-12-10-at-12.44.39-pm.png?w=282 282w" sizes="(max-width: 546px) 100vw, 546px"></p>
<p data-adtags-visited="true"><em>Now look at the slope of the log-weight</em>. It is very positive in the ‚Äúmove hard right‚Äù region, and becomes more and more positive the farther left we go! Similarly in the ‚Äúmove hard left‚Äù region; the slope of the log-weight is very negative, and becomes more negative to the right.</p>
<p data-adtags-visited="true">In the ‚Äúslight right‚Äù and ‚Äúslight left‚Äù regions, the slope becomes more moderate, and when we are in the ‚Äústay around here‚Äù region, the slope of the log-weight is close to zero.<em> This is what we want.</em></p>
<p data-adtags-visited="true">(ASIDE: Moreover, this is even more what we want because in realistic applications we often <em>already</em> have the log-weight function in hand, not the weight function. Log weights are convenient because you can represent arbitrarily small probabilities with ‚Äúnormal sized‚Äù numbers.)</p>
<p data-adtags-visited="true">We can then use this to modify our candidate proposal distribution as follows: rather than using a normal distribution centered on the <em>current</em> point to propose a candidate, <strong>we compute the derivative of the log of the weight function using dual numbers</strong>, and <em>we use the size and sign of the slope to tweak the center of the proposal distribution. </em></p>
<p data-adtags-visited="true">That is, if our current point is far to the left, we see that the slope of the log-weight is very positive, so we move our proposal distribution some amount to the right, and then we are more likely to get a candidate value that is in a higher-probability region. But if our current point is in the middle, the slope of the log-weight is close to zero so we make only a small adjustment to the proposal distribution.</p>
<p data-adtags-visited="true">(And again, I want to emphasize: realistically we would be doing this in a high-dimensional space, not a one-dimensional space. We would compute the <em>gradient</em> ‚Äî the direction in which the slope increases the most ‚Äî and head that direction.)</p>
<p data-adtags-visited="true">If you work out the math, which I will not do here, the difference is as follows. Suppose our non-normalized weight function is <em>p</em>.</p>
<ul>
<li>In the plain-vanilla proposal algorithm we would use as our candidate distribution a normal centered on <em>current</em> with standard deviation <em>s.</em></li>
<li>In our modified version we would use as our candidate distribution a normal centered on <em>current + (s / 2) * ‚àálog(p(current))</em>, and standard deviation <em>s</em>.</li>
</ul>
<p data-adtags-visited="true">Even without the math to justify it, this should seem reasonable. The typical step in the vanilla algorithm is on the order of the standard deviation; we‚Äôre making an adjustment towards the higher-probability region of about half a step if the slope is moderate, and a small number of steps if the slope is severe; the areas where the slope is severe are the most unlikely areas so we need to get out of them quickly.</p>
<p data-adtags-visited="true">If we do this, we end up doing more math on each step (to compute the log if we do not have it already, and the gradient) but <strong>we converge to the high-probability region much faster.</strong></p>
<p data-adtags-visited="true">If you‚Äôve been following along closely you might have noticed two issues that I seem to be ignoring.</p>
<p data-adtags-visited="true">First, we have not eliminated the need for the user to choose the tuning parameter <em>s</em>. Indeed, this only addresses one of the problems I identified earlier.</p>
<p data-adtags-visited="true">Second, the Metropolis algorithm requires for its correctness that the proposal distribution <em>not ever be biased in one particular direction! </em>But the whole <em>point</em> of this improvement is to bias the proposal towards the high-probability regions. Have I pulled a fast one here?</p>
<p data-adtags-visited="true">I have, but we can fix it. I mentioned in the original series that I would be discussing the Metropolis algorithm, which is the oldest and simplest version of this algorithm. In practice we use a variation on it called Metropolis-Hastings which adds a correction factor to allow non-symmetric proposal distributions.</p>
<p data-adtags-visited="true">The mechanism I‚Äôve sketched out today is called the <a href="https://en.wikipedia.org/wiki/Metropolis-adjusted_Langevin_algorithm">Metropolis Adjusted Langevin Algorithm</a> and it is quite interesting. It turns out that this technique of ‚Äúwalk in the direction of the gradient plus a random offset‚Äù is also how physicists model movements of particles in a viscous fluid where the particle is being jostled by random molecule-scale motions in the fluid. (That is, by Brownian motion.) It‚Äôs nice to see that there is a physical interpretation in what would otherwise be a very abstract algorithm to produce samples.</p>
<hr>
<p data-adtags-visited="true"><strong>Next time on FAIC:</strong> The fact that we have a connection to a real-world physical process here is somewhat inspiring. In the next episode I‚Äôll give a sketch of another technique that uses ideas from physics to improve the accuracy of a Metropolis process.</p>

			
			
						</div></div></div>
    </div>
    <footer>
        <div>created by buildstarted &copy; 2020 <a href="/about">about</a></div>
        <div>Share this page on social media: copy and paste this url https://linksfor.dev/</div>
        <div>If you prefer RSS: <a href="https://linksfor.dev/feed.xml">https://linksfor.dev/feed.xml</a></div>
    </footer>
    
</body>
</html>