<!DOCTYPE html>
<html lang="en">
<head>
    <title>
linksfor.dev(s)
    </title>
    <link rel="alternate" type="application/rss+xml" title="Linksfor.dev(s) feed" href="https://linksfor.dev/feed.rss" />
    <meta charset="utf-8">
    <meta name="Description" content="A curated source of links that devs might find interesting. Updated around the clock." />
    <meta name="google" value="notranslate">
    <link rel="stylesheet" type="text/css" href="/style.css" />
    <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon">
    <link rel="icon" href="/favicon.ico" type="image/x-icon">
</head>
<body>
    <div class="grid">
        
<div class="readable">
    <div id="readOverlay" class="style-ebook"><div id="readInner" class="margin-medium size-medium"><div><div class="_8ky- _1xcn"><div class="_3zn8"><img class="img" src="https://scontent-lax3-1.xx.fbcdn.net/v/t39.2365-6/39247522_360707717801366_1386415567702851584_n.png?_nc_cat=104&amp;_nc_oc=AQnzhLL4Ko1PJT1R97LDocmCwb2olMGniJUU59JOhA8C2H5sP4snMTQSBgDlNmU0s2k&amp;_nc_ht=scontent-lax3-1.xx&amp;oh=d15a66363695b0674a04ab7d4fc47625&amp;oe=5EBE1ED1" width="100%" alt=""><p class="readability-styled" style="display: inline;"> A few examples of the minimal ALU method left to right (Best viewed at 100%):Plus6Int (6 shades), Dither17 (better quality than 16), Dither32 (more shades), Dither64 (more shades, artifacts) </p></div><p class="_p-7 _p-9 _p-b"><i><b>Note:</b> As for all images in this post - they have been created for this work. Images might be scaled and show a repeating extra pattern (Moiré pattern) depending on your chosen browser and browser settings. We suggest the sample application for an undistorted and animated view. Consider your monitor might affect the visuals - especially animated content.</i></p><hr class="_3fwr _4tae"><p class="_p-7 _p-9 _p-b">In this article, we present some ready to use HLSL snippets (small fragments of code in Direct X shading language) for dithering. We also provide details on when to apply them and how they have been crafted. We include some existing and new solutions to the problem. You can find interesting performance numbers at the end.</p><p class="_p-7 _p-9 _p-b"> Efficient rendering algorithms are paramount for VR developers. They allow for more fidelity, higher frame rate, and less latency. That leads to better immersion and more comfort which lead to a longer, more pleasant VR experience.</p><p class="_p-7 _p-9 _p-b"> During development of <a href="https://www.oculus.com/experiences/rift/1174445049267874/">”Dear Angelica”</a> (later in <a href="https://www.oculus.com/experiences/rift/1118609381580656/">”Quill”</a>) and the <a href="https://www.oculus.com/blog/welcome-to-rift-core-beta-now-available/">new Oculus “Home”</a> we needed high-quality <a href="https://l.facebook.com/l.php?u=https%3A%2F%2Fen.wikipedia.org%2Fwiki%2FDither&amp;h=AT3bzdplGGyn5omOeHQCPU7tAv24mfMwxujN_FX6Oyd44aYvecTJXABMyl7FltfN0TbxIFwXYePFuj5ritSTo1rVxuj2PUtkj9Ot5_Ad6t9Aqwy8Y71CBXejedViRjI4N8pV1JqfXz60MhjU" target="_blank" rel="nofollow" data-lynx-mode="hover">dithering</a>. Findings from those productions led to the creation of this blog post.</p><hr class="_3fwr _4tae"><p class="_p-7 _p-9 _p-b"> Dithering is about transforming a value of higher precision (e.g. 0..1) into a value of less precision (e.g. binary) that when averaged with nearby pixels (spatial) or over time (temporal e.g. multiple frames) approximates the input value better than a simple quantization could do. There are many applications to dithering. Code for those applications differs but they all can be unified with a function returning a value in the 0..1 range and getting location (e.g. 2D screen position) and time (e.g. FrameIndex) a input. That value can be used in many ways:</p><ul class="_3e2_ _1ewa _1g4e"><li>Clip/cull pixels when compared with a threshold</li><li>Added to a value that is going to be quantized (color banding or MSAA banding when culling MSAA samples)</li><li>Added to sampling start position that would suffer from banding due to under sampling (e.g. 3d volume tracing start location)</li><li>Rotate a 2D sample set to create more sample position (e.g. Screen Space Ambient Occlusion)</li></ul><p class="_p-7 _p-9 _p-b">Some dithering algorithms (e.g. Floyd Steinberg) require the signal from nearby sample points making a parallel implementation harder or less efficient. In this work we assume a rather smooth signal where employing nearby samples would not add significant value.</p><h2 class="_2e90 _2e92 _2e95">Example applications of dithering:</h2><ul class="_3e2_ _1ewa _1g4e"><li>Some <b>printers </b>use dithering to approximate grey scale values with black dots.</li><li><b> Opacity Mask / Stipple Pattern / Dithered transparency / <a href="https://l.facebook.com/l.php?u=https%3A%2F%2Fdigitalrune.github.io%2FDigitalRune-Documentation%2Fhtml%2Ffa431d48-b457-4c70-a590-d44b0840ab1e.htm&amp;h=AT22LIVPTay_V2QF1divMHsQ2IydGeZY5jNitPM1PesPUc8xx5R8DS6ht7SkB0i6bUyaI99mFr9u8AAU_kg3R8OcUdCbfsyhPEwBqN8yWpkM5gxuU43yhlgh85QViej8H_l_tBH6s0NnyiZf" target="_blank" rel="nofollow" data-lynx-mode="hover">Screen-Door Transparency</a>: </b>Single layer transparency can be approximated by dithering opaque pixels (with clip()) based on the transparency/alpha value. Multiple layers often look acceptable but wrong (no proper <a href="https://l.facebook.com/l.php?u=https%3A%2F%2Fen.wikipedia.org%2Fwiki%2FOrder-independent_transparency&amp;h=AT159j6hphm72lI3U19Z3KUgQCqyXFtldNhsm07DCl0qlGoLo-umf5jo2r2_rpfQlO7LtOAT9tpI0nNNhZQy-nBBHqF5q9VfXKlKFze1aNwufEWdp0X6bu7C86oBTH4JcFasWwFpaJxVi_PR" target="_blank" rel="nofollow" data-lynx-mode="hover">OIT </a>solution). <a href="https://l.facebook.com/l.php?u=https%3A%2F%2Fde45xmedrsdbp.cloudfront.net%2FResources%2Ffiles%2FTemporalAA_small-59732822.pdf&amp;h=AT1qHXZr7SEMDiCQ-73y8VvRCrPbbrZlfFzeFw6a3pYPimYM2nGrivhcHqwV1AfQ5NwbcV1A3IXNeOW2wbrYMVfdSDJTK9qSibckJafXuZGzSbYOpl87lAYRnS30amu-XngDVd4vJzljBNSv" target="_blank" rel="nofollow" data-lynx-mode="hover">TemporalAA </a>can further improve the quality.</li><li> With <b>MSAA </b>the error can be reduced further (with SV_CoverageMask, see <a href="http://l.facebook.com/l.php?u=http%3A%2F%2Ftwvideo01.ubm-us.net%2Fo1%2Fvault%2Fgdc2017%2FPresentations%2FHorne_Oculus%2520Story%2520Studio.pdf&amp;h=AT14Hb8chEM5jhK-xlVeeyfbJBuyaM5N-7FLu2GAUeLN2-Dz6qtvKm2DpXlFMFWiZ0DJ67GnAGk5r2bNF1CZWV8jq5WpbIZBZnJSMhnvDCI8MvW9THkhgIeX17GVCb34XUsGY4v5JCqrB1TK" target="_blank" rel="nofollow" data-lynx-mode="hover">”Dear Angelica”</a>). <br>e.g. 4xMSAA: add 1/4 * dither - 0.5 to OpacityMask in UE4 material<br>Note: Unreal Engine 4 used a <a href="https://l.facebook.com/l.php?u=https%3A%2F%2Fgraphicrants.blogspot.com%2F2013%2F12%2Ftone-mapping.html&amp;h=AT12_CqenBosYHeSM4tI_k6AakEuuuq1SB8NoM5KbLzvnqDfNTX1VYPMjXI8LwonrGJYpy9IgzQnt6Z18KlNdeT9JBj2kBGJY-zc_XbR7PnVICoTskt1W96s5NoFfXYxpkeDdI0N6mXnHbGL" target="_blank" rel="nofollow" data-lynx-mode="hover">weighted average</a> for HDR resolves which can cause minor artifacts</li><li><b>Prevent banding in low-precision buffers <b>(e.g. 8 bit)</b>:</b> e.g. after tone mapper, in G Buffers or from many alpha transparent layers<br>e.g. add 1/256 * (dither - 0.5) to PS output<br>RGB dither would result in more luminance shades at the cost of more dis-colorization<br>see UnrealEngine 4 r.Tonemapper.GrainQuantization</li><li><b>Blending materials </b>for a deferred renderer</li><li>“Softly” transitioning <b>Level Of Details</b> or fading out an object in distance (for better performance)</li><li>Rotate <b>sample set </b>(e.g. <a href="https://l.facebook.com/l.php?u=https%3A%2F%2Fde45xmedrsdbp.cloudfront.net%2FResources%2Ffiles%2FThe_Technology_Behind_the_Elemental_Demo_16x9-1248544805.pdf&amp;h=AT3EP3VHmegx8AbCM9vf_pnbtqk4nTSDEqHl_63hwUm2L08TSfcwBXGhXxHZnKpjJ7pEOjNwvK78isD_y3qSiMIWyH2XD1EJ4jyfbWCt3NEErQbAicmepsYmg3JsAiHXUEX1CU_d4rscmScE" target="_blank" rel="nofollow" data-lynx-mode="hover">UnrealEngine 4 SSAO</a>) to improve sampling efficiency</li><li>Add start distance for ray marching to blur out <b>sampling artifacts</b><a href="http://l.facebook.com/l.php?u=http%3A%2F%2Fshaderbits.com%2Fblog%2Fue4-volumetric-fog-techniques&amp;h=AT1Lwh-IzhHh5udwOBAEPwHPMel98SMcet2qpbwyWFXjUKh6T2uW9kFlTPLKAywjAQ7d1fsIMf41dybjuiPzAbtWpwcUKlWQQQdzLGmwAbrg1S52pIHKUAp442gUp4U-nIgd99JOao6P9VsN" target="_blank" rel="nofollow" data-lynx-mode="hover">e.g. http://shaderbits.com/blog/ue4-volumetric-fog-techniques</a></li></ul><p class="_p-7 _p-9 _p-b"><b>Source code / demo (Win32) link: </b><a href="http://l.facebook.com/l.php?u=http%3A%2F%2Fdeargpu.com%2F&amp;h=AT2rIVpg_xi3_iG-l7PA7qJoa2EdkQCFIixk2bspq0zVBcwB8_643Pf0K3sdhpHcBW2jlys_x0OW20wa4ierk9e9oYfaTGDQemRZcL91SwCuaDNscfvj8i3DIbzPYjFdauX8jEZxEShE4nA9" target="_blank" rel="nofollow" data-lynx-mode="hover">DearGpu.com</a></p><ul class="_3e2_ _1ewa _1g4e"><li>Liberal license: WTFPL <a href="http://l.facebook.com/l.php?u=http%3A%2F%2Fwww.wtfpl.net%2F&amp;h=AT20s31JRpfpRaqhQIzILh2H8bHE_p-ztnlv3v62ADJ_QMpmkNo9N2EoIQy6VMlonSHqtxLW31mukMSLoyusFnW2vqrgGc1oUBTcXV6Pw4xalCKSUsP9toKmQ92qvRUmVYcs-PumniKaZLnE" target="_blank" rel="nofollow" data-lynx-mode="hover">http://www.wtfpl.net</a> (minor exceptions in “Copyright.txt”)</li><li>Controls: Use keyboard left/right/up/down to navigate the options. F5 to reload shaders.</li><li>“DitherExperiment.hlsl” has the relevant HLSL snippets.</li><li>All exposed C++ constants and textures can be found in AutoCommon.hlsl</li><li>For running the benchmark follow the instructions in the application. The output can be found in in the Stats folder and opened with Excel</li></ul><p class="_p-7 _p-9 _p-b"> Some example code (simplified for better illustration):</p><pre>      // @param Pos screen position from SV_Position which is pixel centered meaning +0.5
      // @param FrameIndexMod4 0..3 can be computed on CPU
      // @return 0 (included) .. 1 (excluded), 17 values/shades
      float Dither17(float2 Pos, float FrameIndexMod4)
      {
          // 3 scalar float ALU (1 mul, 2 mad, 1 frac)
          return frac(dot(float3(Pos.xy, FrameIndexMod4), uint3(2, 7, 23) / 17.0f));
      }
      // one example usage (<a href="https://l.facebook.com/l.php?u=https%3A%2F%2Fdocs.microsoft.com%2Fen-us%2Fwindows%2Fdesktop%2Fdirect3dhlsl%2Fdx-graphics-hlsl-clip&amp;h=AT1WniYWv58nQTH_J_zVdRBP2vHKqey1lQUMSqwse4oFvK_d2mAaaxuMc6pYwHmus_QrFdEy3lH9yKxXQGMcSWL-_wQWE-UMpcXT4iqfTBxXdXNR7OarLj-xyk2sUJxh5DgVtKHNbidFgaG1" target="_blank" rel="nofollow" data-lynx-mode="hover">clip/cull pixel if value is &lt;0</a>):
      clip(Dither17(Pos, FrameIndexMod4) - alpha);
  </pre><h2 class="_2e90 _2e92 _2e95">Goals:</h2><ul class="_3e2_ _1ewa _1g4e"><li><b>Input:</b></li><ul class="_3e2_ _1ewa _1g4e"><li>float 2D screen position as provided by HLSL semantic SV_Position (pixel centered meaning +0.5)</li><li>uint FrameIndex (for animation, more efficient: float FrameIndexMod4)</li></ul><li><b>Output:</b></li><ul class="_3e2_ _1ewa _1g4e"><li>float 0..1 (to clip pixel/subsample or add to values that undergo quantization) or bool</li><li>The values should be well distributed over the 0..1 range to evenly distribute the shades </li></ul><li><b>Outside range behavior</b>:</li><ul class="_3e2_ _1ewa _1g4e"><li>an alpha value &lt;0 should always result in an output value &lt;0 (e.g. clip pixel)</li><li>an alpha value &gt;0 should always result in an output value &gt;1 (e.g. no clip pixel)</li></ul><li> Multiple <b>options</b> to allow different trade-offs and adapt to the application<br>e.g. regular / blue noise, static / animated, high frequency / more shades, ALU / texture</li><li>Simple <b>HLSL </b>shader code (but can be adapted to other shader languages)</li><li>Few <b>simple</b> (no sin or cos) <b>floating point </b>(not int/uint) shader instructions for best <b>performance </b>to make the method nearly free (Could be used on mobile GPU, VR or using it in each pass to fight quantization banding)</li><li><b>Easy integration</b> into any code (no / minimal C++ setup, no external data)</li><li><b>High frequency</b> as human perception and other spatial blur (e.g. from VR distortion pass) can hide the pattern</li></ul><h3 class="_2e90 _2e92 _2e96">Existing solutions:</h3><h2 class="_2e90 _2e92 _2e95">Crafting a minimal ALU only dither function:</h2><p class="_p-7 _p-9 _p-b">We've seen existing snippets using only a few multiplications, adds, frac(), and some magic numbers. Finding an even faster function turned out to be an interesting challenge.</p><p class="_p-7 _p-9 _p-b">Inspired by this article: <a href="https://l.facebook.com/l.php?u=https%3A%2F%2Flemire.me%2Fblog%2F2017%2F09%2F18%2Fvisiting-all-values-in-an-array-exactly-once-in-random-order&amp;h=AT10XZ9I3qCZtDUVRZqjxpUUIxn4thlGXumDmx12DxVzCEFt6SPL62SCWC2dsyriigBvDqX35KPaIkobolTiUSaPLen0U48N9uHbZ1YJZFywhdlnwWPgEqZw9c3EOtSOi9UhSql9USwwLoq1" target="_blank" rel="nofollow" data-lynx-mode="hover">Visiting all values in an array exactly once in “random order”</a></p><p class="_p-7 _p-9 _p-b"> We managed to craft a method based on a single modulo/frac() operation. We can transform “visits all values in an array exactly once” to “generates a well-distributed output value from a given input”</p><p class="_p-7 _p-9 _p-b"> We know that on modern GPUs the most efficient primitive is a float MAD (fused MUL and ADD) operation. To create a value in 0..1 range we can use a simple frac() operation (very fast on GPUs). We have to make use of the Screen x and y location and this led to this minimal function (non-temporal):</p><pre>frac(x*k0 + y*k1)</pre><p class="_p-7 _p-9 _p-b"> When comparing this value with an alpha value we can make the decision to keep or reject a sample.</p><p class="_p-7 _p-9 _p-b"> The challenge is to craft the constants k0..k1 to achieve as much as possible of the defined goals.</p><p class="_p-7 _p-9 _p-b"> The problem is simple enough, allowing for a brute force search of the best constants. However, defining a quality metric is non-trivial as a subjective balance between multiple goals needs to be found. In the end we found the best constants by experimentation. With with some math knowledge we managed to reduce the search space to a manageable range of numbers. </p><h2 class="_2e90 _2e92 _2e95">Converting integer math to floating point math</h2><p class="_p-7 _p-9 _p-b">To derive a repetitive pattern it's actually easier to stick to integer math and replace the frac() with the modulo operation:</p><pre>((x*k0 + y*k1) % Count) / (float)Count    ~=      frac(x*k0/Count + y*k1/Count)</pre><p class="_p-7 _p-9 _p-b">It turns out a good integer version can be simply converted to float and frac() without any issues.</p><p class="_p-7 _p-9 _p-b"> Floating point math usually suffers from a precision loss when doing math with large numbers. We can avoid/limit the problem if we limit the value ranges (screen resolution, constants, output). The SV_Position default 0.5 offset should be considered when doing the math as it can affect the stability of function at different screen locations.</p><p class="_p-7 _p-9 _p-b"> Notes:</p><ul class="_3e2_ _1ewa _1g4e"><li>Some hardware has special faster integer math (e.g. 24bit, 16bit, 8bit) but we did not optimize for that.</li><li>From past experience, we know that on some mobile hardware half (16-bit float) precision can cause problems with large resolutions. The functions here have not been tested with that in mind.</li><li>The code here has not been tested with larger screen resolutions (&gt;2048).</li><li>Comparing to an integer reference shows if we can match the integer quality. This has been done visually but could have been tested more extensively.</li></ul><h2 class="_2e90 _2e92 _2e95">Finding the right constants</h2><p class="_p-7 _p-9 _p-b">Using “Count” provides us with a limited set of values which results in as many patterns.</p><p class="_p-7 _p-9 _p-b"> For the 1D case, there is well-known math (see article above) to compute a sequence that visits all values in a pseudo-random order. We need to choose the constants k2 and k3 to be <a href="https://l.facebook.com/l.php?u=https%3A%2F%2Fen.wikipedia.org%2Fwiki%2FCoprime_integers&amp;h=AT1cZLaB8gQGkbXFaR-2bELEsygA8Z8YekMel7D0q32G0OMMzAN5kjGhTEEbXFQG-YOQOO4s27Pjd-3MMA8EJ05WolavM9iuvJ5Gtz6tyRr4g1XKZKmzcJUhgCA0uBM1WJQ6kSiflKgUkDDl" target="_blank" rel="nofollow" data-lynx-mode="hover">coprime </a>to “Count”. Note that the <a href="https://l.facebook.com/l.php?u=https%3A%2F%2Fen.wikipedia.org%2Fwiki%2FHalton_sequence&amp;h=AT169gS9Q5uVhEtOMl8XfX1fC86wFrEjZ5NJkUxECRJdtv2MxObJCUhKqWjlMdnQ6ome7e20RFiItInbcQBQZEuSVjIVGVIWO6Z4ZCPuQnLNnl9QPhGQX7PUjFqXT2-loHT2khcDrPhoXsMH" target="_blank" rel="nofollow" data-lynx-mode="hover">Halton sequence</a> also makes use of coprime numbers, but additionally alters the order (using more expensive integer math and reverse bits function) of the bits which helps the quality with a larger “Count”.</p><p class="_p-7 _p-9 _p-b">Good properties to optimize for (can conflict with each other):</p><ul class="_3e2_ _1ewa _1g4e"><li> Large numbers of shades directly conflict with avoiding visible patterns or low-frequency features<br>=&gt; a small fixed number of shades can avoid visible patterns </li><li> Near 0 and 1 we prefer solid values over patterns (no dither pattern)<br>e.g. top is good, the bottom is bad (very dark areas still show a pattern) - the dialog is explained later</li></ul><ul class="_3e2_ _1ewa _1g4e"><li>Near solid colors (one-pixel case) could be distributed to be nearly equidistant (<b>“Dither17”</b> is better than <b>“Dither16”</b>)</li><li>No distracting patterns e.g. filling up diagonal lines of the pixel (hard to avoid with larger modulo numbers)</li><li>The ideal 50% pattern is a 2x2 pattern.<br>Picking a coprime number near the half of the modulo value e.g. 7 for (x*7)%16 is ideal to create a high-frequency pattern for this case.</li><li>Good performance (instruction type, count, temporaries)</li><li>Consistent quality at any screen location (using float or half on high-resolution displays can be a challenge)</li><li>Consistent quality at any alpha value and when animating alpha</li></ul><h2 class="_2e90 _2e92 _2e95">Visualize the functions</h2><p class="_p-7 _p-9 _p-b">The following diagrams are a visualization that helps to compare and illustrate the properties of the functions. Run the application on <a href="http://l.facebook.com/l.php?u=http%3A%2F%2FDearGPU.com%2F&amp;h=AT0DYQn89RDd7OrzFYS_c80Asuh5CYQ4hiBO_O_in11RHbKA2mRC1pCjUKuXIervsa3tPi8EyGLGZZQ56GFhgd5dJhL_YmMjGn8nSdvuupbUzSUMEVtDEZ9kMMD1o69K14d-4BAYSe6RWE5T" target="_blank" rel="nofollow" data-lynx-mode="hover">DearGPU.com</a> to get a high quality animated visual.</p><p class="_p-7 _p-9 _p-b"> The right shows a “side view” of the comparison with an alpha ramping up from 0 to 1. It's similar to a 2D diagram where x is the alpha value and y is the brightness of the pixel. Because it was evaluated per pixel you see “overhang” pixels. You can ignore that fact. It's useful too if colors blend linearly (straight lines) and how and how much the solution deviates from the reference.</p><p class="_p-7 _p-9 _p-b"> The 5 sub bars on the left show the </p><ul class="_3e2_ _1ewa _1g4e"><li>Animated pattern (at display refresh rate)</li><li>Animated pattern (slowed down 8 times)</li><li>Temporally blurred over 4 frames (weighted: 4 3 2 1 to not blur out low temporal frequency patterns)</li><li>Spatial blurred with a 4x4 box filter</li><li>Temporally blurred like before and spatially blurred with a 2x2 filter</li></ul><p class="_p-7 _p-9 _p-b"> Notes:</p><ul class="_3e2_ _1ewa _1g4e"><li><b>Temporal </b>means over time (e.g. average multiple samples at a fixed screen location over a few frames)</li><li><b>Spatial</b> means over space/position (e.g. average multiple samples that are nearby to each other)</li><li>The images here are <b>not animated</b> - if you want to see the animated version - you have to run the application from <a href="http://l.facebook.com/l.php?u=http%3A%2F%2FDearGPU.com%2F&amp;h=AT0F8pGYc8ex_ewU93J21I_MPjdOE37BHeW7TgNYCLwlMe70mEx2x7-uWoujCwfvTSCv8sqpj8ZGzaFYwly7gAc7XYpndcvaAEbDH6_CpJGmb0NW_v0G9_GobqOS8DhNvASSByYzXtoSMphK" target="_blank" rel="nofollow" data-lynx-mode="hover">DearGPU.com</a>. The temporally blurred versions give a similar impression.</li></ul><hr class="_3fwr _4tae"><p class="_p-7 _p-9 _p-b">The <b>reference </b>lerp (ColorA, ColorB, alpha) looks like this:</p><p class="_p-7 _p-9 _p-b">This ideal image shows a smooth transition from black to white (left). The 5 bars look the same as spatial blurring (in XY) and temporal blurring (in time) have little effect.</p><hr class="_3fwr _4tae"><p class="_p-7 _p-9 _p-b">When looking at the <b>“PseudoRandom” function </b>you can see a transition from black to white but with large artifacts.</p><p class="_p-7 _p-9 _p-b"> The random noise shows a clear deviation win the ramp which shows the result is not very good.</p><hr class="_3fwr _4tae"><p class="_p-7 _p-9 _p-b">The following shows the “<b>DitherArray4x4” function </b>with temporal variation over 4 frames → 4*4*4 = 64 shades</p><p class="_p-7 _p-9 _p-b"><i> Note: The right side uses a box blur which results in a clean ramp. The quality of the result cannot be judged by that alone.</i></p><hr class="_3fwr _4tae"><p class="_p-7 _p-9 _p-b">The following shows a <b>gradient noise</b> pattern used in “Dear Angelica” and in an early version of “Quill” (variant of “COD: ADVANCED WARFARE"):</p><hr class="_3fwr _4tae"><p class="_p-7 _p-9 _p-b">The following shows a <b>32 shade dither pattern</b>. We see 2 images, the reference integer implementation and the float implementation which should look the same but result in better performance:</p><p class="_p-7 _p-9 _p-b">Integer code (offset is to reflect 0.5f pixel offset in In.Pos.xy):</p><pre>      uint offset = dot(float3(0.5f, 0.5f, 0), k0);
      uint Ret = (offset + dot(int3(In.Pos.xy, In.frameIndexMod4), k0)) &amp; 0x1f;
      return frac((Ret + 0.5f) / 32.0f);</pre><p class="_p-7 _p-9 _p-b">Float code:</p><pre>    float Ret = dot(float3(In.Pos.xy, In.frameIndexMod4 + 0.5f), k0 / 32.0f);
    return frac(Ret);</pre><hr class="_3fwr _4tae"><p class="_p-7 _p-9 _p-b">The last shows a 17 shade dither pattern (16 didn't look as good) trading shades with a higher frequent pattern:</p><p class="_p-7 _p-9 _p-b">Notice there is 17 distinct steps in the ramp. Each pattern seems to be regular and high frequent. On a closer look, we can see the 50% area shows a distinct stripe pattern. This is where we had to make a compromise. A table/texture based solution could solve this but the minimal ALU solution limits our options.</p><h2 class="_2e90 _2e92 _2e95">Adding a temporal component:</h2><p class="_p-7 _p-9 _p-b">We decided to include the option of a temporal component. By varying the function over time we can reduce the perception of <b>standing </b>patterns and produce a better, smoother transition. We repeat the adjustment every 4 frames (<b>time window</b>) which works well for 90 Hz frames (Oculus Rift). Less (2 or 3) can make sense on a lower image rate (e.g. 60 / 30 Hz). More is an option on higher image rates or when using post filter like the TemporalAA in Unreal Engine 4. We prefer a small time window to reduce the perception of a <b>moving </b>pattern.</p><p class="_p-7 _p-9 _p-b"> Options adding a temporal component:</p><ul class="_3e2_ _1ewa _1g4e"><li>Alter input</li><ul class="_3e2_ _1ewa _1g4e"><li> Offset input XY from CPU<br>=&gt; CPU code, flexible</li><li> Offset input XY with 2 MAD: x += FrameIdMod * k2; y += FrameIdMod * k3<br>=&gt; simple</li><li> Offset input X or Y with 1 MAD: x += FrameIdMod * k2<br>=&gt; cheaper than XY but might be less effective</li></ul><li>Alter function</li><ul class="_3e2_ _1ewa _1g4e"><li> vary k0/k1 from CPU<br>=&gt; need multiple nearly uncorrelated good constants (e.g. mirror x/y, flip xy), CPU code</li><li>extra constants: dot(float3(xy, FrameIdMod), k)<br>=&gt; more constraints on finding good constants</li></ul><li>Alter output (generally not a good idea)</li><ul class="_3e2_ _1ewa _1g4e"><li> Offset output: + some_func(FrameIdMod)<br>=&gt; more shades (e.g. 4x4 pattern has 16 shades, with 4 temporal we can get 64 shades) but pulsing and standing patterns</li></ul></ul><p class="_p-7 _p-9 _p-b"> Note: <b>Display overdrive </b>(Display hardware that reduces latency by over/undershooting) can make the temporal adjustment more visible and even result in a minor color shift. Oculus Rift employs a similar technology in the shader. The quality of that implementation is very high and we don't see any distracting overdrive color shifts.</p><h2 class="_2e90 _2e92 _2e95">"Focus on the dark shades"</h2><p class="_p-7 _p-9 _p-b">Visually most noticeable is a bright pixel in a dark area. To better observe the artifact in dark shades it's best to reshape the alpha value e.g. new_alpha = pow(alpha,4):</p><p class="_p-7 _p-9 _p-b">Note how some of the functions shown here don't have a solid black near 0. This can be fixed by adding a small offset. To keep the brights a solid color (&gt;=1) a multiplied might be needed. </p><h2 class="_2e90 _2e92 _2e95">Using dither to blend between colors</h2><p class="_p-7 _p-9 _p-b"> Shaping the function to adapt to the display sRGB curve is tempting but it's actually only possible when both colors (Src and Dst in frame buffer blending) are known and grey scale (or when done per channel). Here we see a blend from pink to light green. The right side now shows the RGB color channel in an additive blend.</p><p class="_p-7 _p-9 _p-b">The colors have been chosen (A=0.9, 0.1, 0.2; B=0.1, 1.0, 0.4) to use most of the 0..1 range and see how different blends behave. The right side shows 3 RGB ramps. This was to prove that any reshaping of the dither function would result in wrong results (e.g. compare with sqrt(alpha) instead of alpha). It might appear like a more pleasing transition on sRGB display for transitions from black to white but we want to have it work with any color. Applying any sRGB corrections to non-color channels is wrong. If you have trouble judging the visuals with the chosen colors you can use different colors when running the application (look for GeneralPurposeTweak).</p><h2 class="_2e90 _2e92 _2e95">Texture Blue noise (non-ALU organic high-frequency pattern)</h2><section class="_4gsw _7of _7p0 _7u9s _7u9p"><div class="_7p2"><section class="_4gsw _7of _7p0 _7u9s _7u9p"><div class="_7p2"><div class="_2h23 _1020 _2h24 _1if-"><div data-filter-keys="[]" data-cols="12" data-prefix="0" data-pull="0" data-push="0" data-suffix="0"><div class="_46a3" id="u_0_0"><div data-filter-keys="[]" data-cols="6" data-prefix="0" data-pull="0" data-push="0" data-suffix="0"><div class="_2h23 _1020 _2h24 _1if-"><img class="img" src="https://scontent-lax3-1.xx.fbcdn.net/v/t39.2365-6/39141077_219043725431156_8958518300111798272_n.png?_nc_cat=100&amp;_nc_oc=AQm4n7MGd1faVZX3g10TkUcd_I32giG6AUJLp14UPK01rZoz-8Cq7z6MlDHiHQ9K2y8&amp;_nc_ht=scontent-lax3-1.xx&amp;oh=ee60b2aaf941294025b8c65b72fcaaa2&amp;oe=5EB9C44A" width="100%" alt=""><p class="readability-styled" style="display: inline;"> 64x64 texture</p><br></div></div><div data-filter-keys="[]" data-cols="6" data-prefix="0" data-pull="0" data-push="0" data-suffix="0"><div class="_2h23 _1020 _2h24 _1if-"><img class="img" src="https://scontent-lax3-2.xx.fbcdn.net/v/t39.2365-6/39219982_276959962903566_4432315316424409088_n.png?_nc_cat=107&amp;_nc_oc=AQk_8iZRRv7RVl1QLC4gM25ZazC3RDar-CDkxDD5ZjgGPI3JxkL81yxW4rLtlQxrcpQ&amp;_nc_ht=scontent-lax3-2.xx&amp;oh=349bcf91c4e0dea28488ed008d9b3044&amp;oe=5EC75499" width="100%" alt=""><p class="readability-styled" style="display: inline;"> 16x16 texture/table (more tiling) </p></div></div></div></div></div></div></section></div></section><p class="_p-7 _p-9 _p-b">Blue noise can be used for a more organic look, still maintaining the high-frequency property. We are not aware of very efficient math function that doesn't employ large tables for blue noise. Luckily GPUs have efficient texture mapping. To avoid repetition you want a large texture but to get good texture cache you want it small in memory (size and format). With a small enough texture you can even avoid texture cache trashing altogether. This is the best performance you can get and a lowering the resolution further would not buy you anything. This might be different when combined with other shader code - it's always best to profile this in your shaders and on your platform (e.g. mobile).</p><p class="_p-7 _p-9 _p-b"> Getting blue noise textures:</p><ul class="_3e2_ _1ewa _1g4e"><li>Generate e.g. <a href="https://l.facebook.com/l.php?u=https%3A%2F%2Fgpuopen.com%2Fvdr-follow-up-grain-and-fine-details%2F&amp;h=AT2ByDQCLN2Gq21Z9WGTOI3afV1mbvrwQlTtI-utd5sXm8JUlZ1yaznqN2OME_ryNWJcVoXDlUoOzRM5B_Uf1Nrs62p_oyrVdbv8UuO4-jTaFATPVMcsIny6Fs1l0XzPxNnPYnI8egzSVySp" target="_blank" rel="nofollow" data-lynx-mode="hover">VDR Follow Up – Grain and Fine Details</a></li><li>This Unreal Engine 4 asset reference (in engine content):<br>Texture2D'/Engine/EngineMaterials/Good64x64TilingNoiseHighFreq.Good64x64TilingNoiseHighFreq'<br>is an option but using a true blue noise texture can improve quality quite a bit.</li></ul><p class="_p-7 _p-9 _p-b">In order to avoid standing patterns, it makes sense to give the impression of a never-repeating pattern. This can be implemented by offsetting the pattern/texture in a new random way. A pure random offset will result is short sections of moving patterns so it's best to have a more controlled randomness. Offsets that repeat every 64 .. 2048 frames should be long enough to break up the pattern and short enough to make it easier to judge the quality. A more recent version of “Quill” is cycling through multiple blue noise textures for a organic dissolve function on some materials.</p><h2 class="_2e90 _2e92 _2e95">Table based Blue Noise</h2><p class="_p-7 _p-9 _p-b">For easier code integration we also investigated the option to use an array in HLSL. We limited the size to 16x16 as larger sizes are more likely to cause problems (constant buffer size limit, shader compile time, code size, etc). We found with full HLSL optimizations enabled the compiler generated <b>constant buffers </b>and accessed those. There are limits on constant buffers (size, index at 16 bytes) which might be optimized further in the driver but this is up for further investigation. The table we want to store is in bytes but that type is not available for tables/constant buffers so we use a 32bit uint. We tried packing bytes in <b>“BlueNoiseA16x16” function </b>and using 4 times as many uint in <b>“BlueNoiseB16x16”</b> (simpler shader).</p><p class="_p-7 _p-9 _p-b"><b>Note:</b> Make sure the table is declared outside the function with “static const” and compiler optimizations (e.g. D3DCOMPILE_OPTIMIZATION_LEVEL3) enabled. Without that, we've seen the resulting code be very inefficient (many instructions and temporaries). When using similar code in other shader languages and platforms you might see even more problems.</p><h2 class="_2e90 _2e92 _2e95">Linearity</h2><p class="_p-7 _p-9 _p-b"> Dithering is replacing a soft transition between two values (A and B) by a pattern that statistically amounts to a similar value. This is assuming linearity. </p><p class="_p-7 _p-9 _p-b"> lerp(A, B, x) ~= (A * a + B * b) / (a+b)</p><p class="_p-7 _p-9 _p-b"> a: number of pixels showing A<br>b: number of pixels showing B</p><p class="_p-7 _p-9 _p-b"><i> Note: lerp(A, B, x) is a linear interpolation = B*x + A*(1-x)</i></p><p class="_p-7 _p-9 _p-b"> This assumption might not be correct because of <b>hardware properties</b> but it can be easily verified when comparing a blend between two values (e.g. extremes like black and white) with the linear interpolated reference. You have to do that on the target display (e.g. in VR, not on the monitor). A calibration could fix this but with the few shades, we don't have much fine control. Fixing larger issues (e.g. <a href="https://l.facebook.com/l.php?u=https%3A%2F%2Fmynameismjp.wordpress.com%2F2012%2F10%2F24%2Fmsaa-overview%2F&amp;h=AT15ZOKJpWuB9iO4FNis_NeJZIprPUbZWGOjX4mK24EUV_aZKeCpSG8lTXeW7C7w7FSJDxvafIRFzHdVm0il2kqvnKZg3Ageykd5H6hL8LfL13lvYj2vlfQhm1GEFMF-BhvqfvYJC3kcVSBB" target="_blank" rel="nofollow" data-lynx-mode="hover">sRGB</a>) are only possible if we know the values we want to dither/blend. This is not always possible (e.g. fixed function frame buffer blending doesn't allow that).</p><p class="_p-7 _p-9 _p-b"> The assumption can also be violated by <b>shader code</b> e.g. Unreal Engine 4 TemporalAA is using a <a href="https://l.facebook.com/l.php?u=https%3A%2F%2Fgraphicrants.blogspot.com%2F2013%2F12%2Ftone-mapping.html&amp;h=AT3RGL-wA7yayRAJUyki6LOBNZ9mcWG1cPXh63kb_nb6djf2OHy1zwzLFdN2k0w9Fq-hEhjhHuZg5NpnysubLWgpuTFnbDoifgEASGgdw4v_kab3yNWpZYXENZ7YYODB5pIQdTpcqG_Ftz4a" target="_blank" rel="nofollow" data-lynx-mode="hover">weighted average</a> for HDR. </p><p class="_p-7 _p-9 _p-b"> Even if incorrect, having some kind of transition is often better than no transition.</p><h2 class="_2e90 _2e92 _2e95">Performance</h2><p class="_p-7 _p-9 _p-b">Profiling these minimal shader snippets turned out to be very hard. <b>Shader byte code </b>count can be misleading as the drivers optimize the code further and some instructions take longer. Non-dither shader code can play a large role in the cost of the dither code. The <b>number of temporaries </b>can affect performance, especially if you have a lot of texture cache misses in the other part of the shader but the snipped code is very low on that we don't see the effect. Measuring timing by amplifying the shader code is possible but still tricky (branching adds extra cost). </p><p class="_p-7 _p-9 _p-b">Reading the chart (4 passes of 1024x1024, dynamic loop of 100 calls to the function, Nvidia TITAN Black, driver 398.11):</p><ul class="_3e2_ _1ewa _1g4e"><li>FrameTime is the most important criteria, a Null shader </li><li>InstructionCount and TempRegisterCount is from HLSL Bytecode</li><li><b>Null </b>is clearly the fastest only showing frame buffer writes (additive blending)</li><li><b>Constant </b>is to see the cost of the dynamic loop with minor code to not get it compiled out - it's the fastest</li><li><b>Plus6Int</b> is one of the slower ones, not optimized (using integer)</li><li><b>Dither16</b>, <b>Dither17</b>, <b>Dither64</b> use the same code, just different constants</li><li><b>Dither32</b> is like the ones before but has an additional +0.5 in the shader</li><li><b>BlueNoiseA16x16 </b>unpacks 4 bytes from uint[16*16] in constant buffer, more ALU</li><li><b>BlueNoiseB16x16</b> reads bytes from uint[16*16*4] in constant buffer, more constants, slowest</li><li><b>DitherArray8x8</b> is storing bytes in uint[8*8] in constant buffer</li><li><b>DitherArray4x4 </b>is storing bytes in uint[4*4] in constant buffer</li><li><b>BlueNoiseTex64x64 </b>using a small texture instead of constant buffer, way better than smaller tabes</li><li><b>PseudoRandom </b>sin() makes this expensive</li><li><b>GradientNoise </b>requires two frac() </li><li><b>Halton16</b> and <b>Halton64 </b>make use of integer math and reverse bits, slow</li><li><b>Overview</b> a shader that branches into all other options (temp count is outside of the chart)</li></ul><h2 class="_2e90 _2e92 _2e95">Excellent presentations / posts that cover the topic in more detail:</h2><h2 class="_2e90 _2e92 _2e95">Summary and observations</h2><p class="_p-7 _p-9 _p-b"> We presented a small set of functions that can be integrated easily. Depending on the application you can trade the number of shades, noticeable pattern, math/texture cost and other properties.</p><p class="_p-7 _p-9 _p-b"> It seems large gradients deserve more shades and in cases where small and large gradients can be seen (this is often the case) an adaptive hybrid method could work best (e.g. choose number of shades depending on filterwidth() which is computed with ddx/ddy). This is similar to mip-mapping where you pick a different content depending on the magnification factor. The synthetic performance numbers show texture mapping has very good performance and table (constant buffer) random access is quite expensive. The presented math only solutions are a good choice when you are texture bound and want to integrate a solution quickly.</p><h2 class="_2e90 _2e92 _2e95">My last words and about the development environment</h2><p class="_p-7 _p-9 _p-b">I used “We” in most of this post to reflect a large part of this is derived from the work for others and discussions I had with industry professionals. For this work I especially want to thank:</p><p class="_p-7 _p-9 _p-b"> Dean Beeler, Volga Aksoy, Inigo Quilez, Zeyar Htet and Gillian Belton</p><p class="_p-7 _p-9 _p-b"> To develop functions like this, a good iteration time is important (quickly recompile shaders on a button or on save). I searched for a <a href="https://l.facebook.com/l.php?u=https%3A%2F%2Fwww.shadertoy.com%2F&amp;h=AT0rUXhZwk2NdnuE4QGc4p3wD8vEOWc-unG4W9ICqAtiCG0vhqd45Rj_TcFBQ5ew0c0xfeikHjeSX9usqSXuyrxIBAg8kQSSn-33LZbGkQRYKFWgRKlRbJP1n6NWq4lbg74OhIqL_KS4kSqI" target="_blank" rel="nofollow" data-lynx-mode="hover">Shadertoy </a>like the tool for HLSL that also supports profiling and the closest I found was <a href="https://l.facebook.com/l.php?u=https%3A%2F%2Fgithub.com%2Fvinjn%2Fshadertoy-dx11&amp;h=AT1zl7YOiU3oQRJlQCDpEX1Ci9f1watp7srD8e5ugZWzTf10dOSozmzj-YjAq6jDm_FCQEdAkLjOg7mKHXbf_K6tTbghXXagNkaj7ioSP4Qlovmzq9CLgF7wK7SQK4xSsxQGXUN655DG5IlA" target="_blank" rel="nofollow" data-lynx-mode="hover">Shadertoy-dx11</a>. It was used to create the initial set of images but it wasn't suited to get performance data. This is why I developed a new tool that is more suited for performance experiments.</p><p class="_p-7 _p-9 _p-b"> It is based on <a href="https://l.facebook.com/l.php?u=https%3A%2F%2Fkosmokleaner.wordpress.com%2F2014%2F10%2F30%2Fshadows-demo-with-source-download%2F&amp;h=AT1nClP6kGNI2pQmNaWqFGCDqHM91FMfgn5lqX-OuGccA-1kQH90ecJp8Rg7jomn8LHZXnLyw9Vw3yHp2Zj1k6aCpjBqrq4yW1A7PayQRIZNBM9em3dVoPvfaAxZq5bXeJBEDoxvEuAbgBur" target="_blank" rel="nofollow" data-lynx-mode="hover">AEngine</a> which is a DirectX11 based mini engine (I released that while I worked at Epic Games, as free open source). The new framework called DearGPU (see source link in the beginning) allows for quick iteration (F5 to reload shaders) and convenient C/C++ / HLSL interaction (C++ exposed parameters which generate an <a href="https://l.facebook.com/l.php?u=https%3A%2F%2Fkosmokleaner.wordpress.com%2F2015%2F06%2F24%2Fsimple-c-reflection-to-generate-hlsl-constant-buffer-declaration%2F&amp;h=AT2JrnTelaA-7VxI-skKqvOBSRAWmrM8dvMxbViHlqlIUX40cFrFpllCvvft3tSJcB1ihomMF5cGWEKq7OumNL0YDfPHOPITJTHAAGm_o79MkroVExzXS14_lYIkr6s28X6MEfyYg6rcXkBR" target="_blank" rel="nofollow" data-lynx-mode="hover">HLSL include file</a>, shaders get compiled on demand). Shader defines can be created in few lines of code which makes performance experiments way easier. There is a simple built-in benchmark functionality (choose benchmark type in the UI and run with F3). It will generate multiple .CSV (Comma Separated Value) files that can be imported into Microsoft Excel to create graphs.</p><p class="_p-7 _p-9 _p-b"> You can use the <a href="http://l.facebook.com/l.php?u=http%3A%2F%2FDearGPU.com%2F&amp;h=AT3tytZQjPh-aaLHgdsqA2MfNMb2cx4rpmJd3AVTEjLJNYWTFnvPm3nPGQP7WTwuvoRn-FUKCi9CB6fIXrhly4L1H4NMrUPyG0Lf_Ly5nDjDDNNjsNmzShx_1z5E7i6TyZENywfUCshMefr3" target="_blank" rel="nofollow" data-lynx-mode="hover"><b>DearGPU</b></a> under the same free license. If you want to help me grow this into a shader performance test tool, contact me. Imagine a website where you can read about other people's interesting experiments - each starting with a question like this “Dear GPU, I was wondering ...”.</p><p class="_p-7 _p-9 _p-b"> I also can see the tool running on multiple graphics cards /driver versions/machines gathering more data.</p><p class="_p-7 _p-9 _p-b"><b>Warning:</b> Drivers and hardware are not bug-free and with my experiment I even experienced computer restarts when running extensive (e.g. a single very slow draw call) shaders. I will try to follow up with hardware vendors to get this resolved. </p><hr class="_3fwr _4tae"><h3 class="_2e90 _2e92 _2e96">Bonus: How to use with Unreal Engine 4 Materials</h3><ul class="_3e2_ _1ewa _1g4e"><li><b>“BlueNoise” function</b> (without temporal component):Note: A blue noise texture from the link above will improve quality</li></ul><ul class="_3e2_ _1ewa _1g4e"><li><b>“Dither64” function</b> (with temporal component):Note: View.FrameNumber is actually View.FrameNumber%4</li></ul></div></div></div></div>
</div>
    </div>
    <footer>
        <div>created by buildstarted &copy; 2020 <a href="/about">about</a></div>
        <div>Share this page on social media: copy and paste this url https://linksfor.dev/</div>
        <div>If you prefer RSS: <a href="https://linksfor.dev/feed.xml">https://linksfor.dev/feed.xml</a></div>
    </footer>
    
    <script>
        (function () {
            var COLLECT_URL = "https://dna.buildstarted.com/t";
            var SITE_ID = "linksfor.devs";
            var GLOBAL_VAR_NAME = "__DNA__";

            window[GLOBAL_VAR_NAME] = {};

            window[GLOBAL_VAR_NAME].sendPageView = function () {
                var path = location.pathname;
                var referrer = document.referrer;

                var url = COLLECT_URL + "?siteid=" + SITE_ID + "&p=" + encodeURIComponent(path) + "&r=" + encodeURIComponent(referrer);

                try { fetch(url, { method: "GET" }); } catch (e) { }
            };

            window[GLOBAL_VAR_NAME].sendPageView();
        })();
    </script>
</body>
</html>