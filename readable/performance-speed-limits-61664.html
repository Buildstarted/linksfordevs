<!DOCTYPE html>
<html lang="en">
<head>
    <title>
Performance speed limits - linksfor.dev(s)    </title>
    <meta charset="utf-8">
    <link rel="alternate" type="application/rss+xml" title="Linksfor.dev(s) feed" href="https://linksfor.dev/feed.rss" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta name="google" value="notranslate">
    <link rel="stylesheet" type="text/css" href="/style.css" />
    <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon">
    <link rel="icon" href="/favicon.ico" type="image/x-icon">
    <meta property="og:title" content="Performance speed limits - linksfor.dev(s)"/>
    <meta property="article:author" content="Travis Downs"/>
    <meta property="og:description" content="How fast can it go?"/>
    <meta property="og:type" content="website"/>
    <meta property="og:url" content="https://travisdowns.github.io/blog/2019/06/11/speed-limits.html"/>

<meta property="og:site_name" content="linksfor.dev(s)" />
</head>
<body>
	<div class="devring" style="background: #222">
		<div class="grid">
			<div style="display: grid; grid-template-columns: .5fr 1fr 1fr 1fr; text-align: center;">
				<span class="devring-title"><a href="https://devring.club/">devring.club</a></span>
				<a href="https://devring.club/sites/1/prev" class="devring-previous">Previous</a>
				<a href="https://devring.club/random" class="devring-random">Random</a>
				<a href="https://devring.club/sites/1/next" class="devring-next">Next</a>
			</div>
		</div>
	</div>
    <div class="grid">
        <h1>
<a href="/" style="color:inherit">linksfor.dev(s)</a>
        </h1>
        <title>linksfor.dev(s) - Performance speed limits</title>
<div class="readable">
        <h1>Performance speed limits</h1>
            <div>by Travis Downs</div>
            <div>Reading time: 58-73 minutes</div>
        <div>Posted here: 14 May 2020</div>
        <p><a href="https://travisdowns.github.io/blog/2019/06/11/speed-limits.html">https://travisdowns.github.io/blog/2019/06/11/speed-limits.html</a></p>
        <hr/>
<div id="readability-page-1" class="page"><div aria-label="Content">
      <div>
        <article itemscope="" itemtype="http://schema.org/BlogPosting">

  

  <div itemprop="articleBody">
    <h2 id="how-fast-can-it-go">How fast can it go?</h2>

<p>Sometimes you just want to know how fast your code <em>can</em> go, without benchmarking it. Sometimes you have benchmarked it and want to know how close you are to the maximum speed. Often you just need to know what the current limiting factor is, to guide your optimization decisions.</p>

<p>Well this post is about that determining that <em>speed limit</em><sup id="fnref:speedlemire"><a href="#fn:speedlemire">1</a></sup>. It’s not a comprehensive performance evaluation methodology, but for many <em>small</em> pieces of code it will work very well.</p>

<p><img src="https://travisdowns.github.io/assets/speed-limits/speed-limit-50-ns.svg" alt="Speed Limit" width="300px"></p>

<h2 id="table-of-contents">Table of Contents</h2>

<p>This post is intended to be read from top to bottom, but if it’s not your first time here or you just want to skip to a part you find interesting, here you go:</p>

<ul id="markdown-toc">
  <li><a href="#how-fast-can-it-go" id="markdown-toc-how-fast-can-it-go">How fast can it go?</a></li>
  <li><a href="#table-of-contents" id="markdown-toc-table-of-contents">Table of Contents</a></li>
  <li><a href="#the-limits" id="markdown-toc-the-limits">The Limits</a>    <ul>
      <li><a href="#big-list-of-caveats" id="markdown-toc-big-list-of-caveats">Big List of Caveats</a></li>
    </ul>
  </li>
  <li><a href="#pipeline-width" id="markdown-toc-pipeline-width">Pipeline Width</a>    <ul>
      <li><a href="#remedies" id="markdown-toc-remedies">Remedies</a></li>
    </ul>
  </li>
  <li><a href="#portexecution-unit-limits" id="markdown-toc-portexecution-unit-limits">Port/Execution Unit Limits</a>    <ul>
      <li><a href="#tools" id="markdown-toc-tools">Tools</a></li>
      <li><a href="#measuring-it" id="markdown-toc-measuring-it">Measuring It</a></li>
      <li><a href="#remedies-1" id="markdown-toc-remedies-1">Remedies</a></li>
    </ul>
  </li>
  <li><a href="#load-throughput-limit" id="markdown-toc-load-throughput-limit">Load Throughput Limit</a>    <ul>
      <li><a href="#split-cache-lines" id="markdown-toc-split-cache-lines">Split Cache Lines</a></li>
      <li><a href="#remedies-2" id="markdown-toc-remedies-2">Remedies</a></li>
    </ul>
  </li>
  <li><a href="#memory-and-cache-bandwidth" id="markdown-toc-memory-and-cache-bandwidth">Memory and Cache Bandwidth</a>    <ul>
      <li><a href="#remedies-3" id="markdown-toc-remedies-3">Remedies</a></li>
    </ul>
  </li>
  <li><a href="#carried-dependency-chains" id="markdown-toc-carried-dependency-chains">Carried Dependency Chains</a>    <ul>
      <li><a href="#tools-1" id="markdown-toc-tools-1">Tools</a></li>
      <li><a href="#remedies-4" id="markdown-toc-remedies-4">Remedies</a></li>
    </ul>
  </li>
  <li><a href="#front-end-effects" id="markdown-toc-front-end-effects">Front End Effects</a></li>
  <li><a href="#store-throughput-limit" id="markdown-toc-store-throughput-limit">Store Throughput Limit</a>    <ul>
      <li><a href="#split-cache-lines-1" id="markdown-toc-split-cache-lines-1">Split Cache Lines</a></li>
      <li><a href="#remedies-5" id="markdown-toc-remedies-5">Remedies</a></li>
    </ul>
  </li>
  <li><a href="#complex-addressing-limit" id="markdown-toc-complex-addressing-limit">Complex Addressing Limit</a>    <ul>
      <li><a href="#remedies-6" id="markdown-toc-remedies-6">Remedies</a></li>
    </ul>
  </li>
  <li><a href="#taken-branches" id="markdown-toc-taken-branches">Taken Branches</a></li>
  <li><a href="#out-of-order-limits" id="markdown-toc-out-of-order-limits">Out of Order Limits</a>    <ul>
      <li><a href="#reorder-buffer-size" id="markdown-toc-reorder-buffer-size">Reorder Buffer Size</a>        <ul>
          <li><a href="#remedies-7" id="markdown-toc-remedies-7">Remedies</a></li>
        </ul>
      </li>
      <li><a href="#load-buffer" id="markdown-toc-load-buffer">Load Buffer</a>        <ul>
          <li><a href="#remedies-8" id="markdown-toc-remedies-8">Remedies</a></li>
        </ul>
      </li>
      <li><a href="#store-buffer" id="markdown-toc-store-buffer">Store Buffer</a>        <ul>
          <li><a href="#remedies-9" id="markdown-toc-remedies-9">Remedies</a></li>
        </ul>
      </li>
      <li><a href="#scheduler" id="markdown-toc-scheduler">Scheduler</a>        <ul>
          <li><a href="#remedies-10" id="markdown-toc-remedies-10">Remedies</a></li>
        </ul>
      </li>
      <li><a href="#register-file-size-limit" id="markdown-toc-register-file-size-limit">Register File Size Limit</a>        <ul>
          <li><a href="#remedies-11" id="markdown-toc-remedies-11">Remedies</a></li>
        </ul>
      </li>
      <li><a href="#branches-in-flight" id="markdown-toc-branches-in-flight">Branches in Flight</a>        <ul>
          <li><a href="#remedies-12" id="markdown-toc-remedies-12">Remedies</a></li>
        </ul>
      </li>
      <li><a href="#calls-in-flight" id="markdown-toc-calls-in-flight">Calls in Flight</a>        <ul>
          <li><a href="#remedies-13" id="markdown-toc-remedies-13">Remedies</a></li>
        </ul>
      </li>
    </ul>
  </li>
  <li><a href="#thank-you" id="markdown-toc-thank-you">Thank You</a></li>
  <li><a href="#comments" id="markdown-toc-comments">Comments</a></li>
</ul>

<h2 id="the-limits">The Limits</h2>

<p>There are many possible limits that apply to code executing on a CPU, and in principle the achieved speed will simply be determined by the lowest of all the limits that apply to the code in question. That is, the code will execute <em>only as fast as its narrowest bottleneck</em>.</p>

<p>So I’ll just list some of the known bottlenecks here, starting with common factors first, down through some fairly obscure and rarely discussed ones. The real-world numbers come mostly from Intel x86 CPUs, because that’s what I know off of top of my head, but the concepts mostly apply in general as well, although often different limit values.</p>

<p>Where possible I’ve included specific figures for <em>modern</em> Intel chips and sometimes AMD CPUs. I’m happy to add numbers for other non-x86 CPUs if anyone out there is interested in providing them.</p>

<h3 id="big-list-of-caveats">Big List of Caveats</h3>

<p>First, lets start with this list of very important caveats.</p>

<ul>
  <li>The limits discussed below generally apply to loops of any size, and also straight line or branchy code with no loops in sight. Unfortunately, it is only really possible to apply the simple analyses described to loops, since a steady state will be reached and the limit values will apply. For most straight line code, however, no steady state is reached and the actual behavior depends on many details of the architecture such as various internal buffer and queue sizes. Analyzing such code sections basically requires a detailed simulation, not a back-of-napkin estimate as we attempt here.</li>
  <li>Similarly, even large loops may not reach a steady state, if the loop is big enough that iterations don’t completely overlap. This is discussed a bit more in the <a href="#out-of-order-limits">Out of Order Limits</a> section.</li>
  <li>The limits below are all <em>upper bounds</em>, i.e., the CPU will never go faster than this (in a steady state) - but it doesn’t mean you can achieve these limits in every case. For each limit, I have found code that you gets you to the limit - but you can’t expect that to be the case every time. There may be inefficiencies in the implementation, or unmodeled effects that make the actual limit lower in practice. Don’t call Intel and complain that you aren’t achieving your two loads per cycle! It’s a speed <em>limit</em>, not a guaranteed maximum<sup id="fnref:thatsaid"><a href="#fn:thatsaid">2</a></sup>.</li>
  <li>There are known limits not discussed below, such as instruction throughput for not-fully-pipelined instructions.</li>
  <li>There are certainly also unknown limits or not well understood limits not discussed here.</li>
  <li>More caveats are mentioned in the individual sections.</li>
  <li>I simply ignore branch prediction for now: this post just got too long (it’s a problem I have). It also deserves a whole post to itself.</li>
  <li>This methodology is unsuitable for analyzing entire applications - it works best for a small hotspot of say 1 to 50 lines of code, which hopefully produce less than about 50 assembly instructions. Trying to apply it to larger stuff may lead to madness. I highly recommend <a href="https://software.intel.com/en-us/vtune-amplifier-cookbook-top-down-microarchitecture-analysis-method">Intel’s Top-Down</a> analysis method for more complex tasks. It always starts with performance counter measurements and tries to identify the problems from there. A free implementation is available in Andi Kleen’s <a href="https://github.com/andikleen/pmu-tools">pmu-tools</a> for Linux. On Windows, free licenses of VTune are available though the 90-day community license for System Studio.</li>
</ul>

<h2 id="pipeline-width">Pipeline Width</h2>

<p><strong>Intel:</strong> Maximum 4 fused-uops<sup id="fnref:ICL"><a href="#fn:ICL">3</a></sup> per cycle<br>
<strong>AMD:</strong> Maximum 5 fused-uops per cycle</p>

<p>At a fundamental level, every CPU can execute only a maximum number of operations per cycle. For many early CPUs, this was always less than one per cycle, but modern pipelined <a href="https://en.wikipedia.org/wiki/Superscalar_processor">superscalar</a> processors can execute more than one per cycle, up to some limit. This underlying limit is not always be imposed in the same place, e.g., some CPUs may be limited by instruction encoding, others by register renaming or retirement - but there is always a limit (sometimes more than one limit depending on what you are counting).</p>

<p>For modern Intel chips this limit is 4 <em>fused-domain</em><sup id="fnref:fused-domain"><a href="#fn:fused-domain">4</a></sup> operations, and for modern AMD it is 5 macro-operations. So if your loop contains N fused-uops, it will never execute at more than 1 iteration per cycle.</p>

<p>Consider the following simple loop, which separately adds up the top and bottom 16-bit halves of every 32-bit integer in an array:</p>

<div><div><pre><code><span>uint32_t</span> <span>top</span> <span>=</span> <span>0</span><span>,</span> <span>bottom</span> <span>=</span> <span>0</span><span>;</span>
<span>for</span> <span>(</span><span>size_t</span> <span>i</span> <span>=</span> <span>0</span><span>;</span> <span>i</span> <span>&lt;</span> <span>len</span><span>;</span> <span>i</span> <span>+=</span> <span>2</span><span>)</span> <span>{</span>
    <span>uint32_t</span> <span>elem</span><span>;</span>

    <span>elem</span> <span>=</span> <span>data</span><span>[</span><span>i</span><span>];</span>
    <span>top</span>    <span>+=</span> <span>elem</span> <span>&gt;&gt;</span> <span>16</span><span>;</span>
    <span>bottom</span> <span>+=</span> <span>elem</span> <span>&amp;</span> <span>0xFFFF</span><span>;</span>

    <span>elem</span> <span>=</span> <span>data</span><span>[</span><span>i</span> <span>+</span> <span>1</span><span>];</span>
    <span>top</span>    <span>+=</span> <span>elem</span> <span>&gt;&gt;</span> <span>16</span><span>;</span>
    <span>bottom</span> <span>+=</span> <span>elem</span> <span>&amp;</span> <span>0xFFFF</span><span>;</span>
<span>}</span>
</code></pre></div></div>

<p>This compiles to the following assembly:</p>

<div><div><pre><code><span>top</span><span>:</span>
    <span>mov</span>    <span>r8d</span><span>,</span><span>DWORD</span> <span>[</span><span>rdi</span><span>+</span><span>rcx</span><span>*</span><span>4</span><span>]</span>          <span>; 1</span>
    <span>mov</span>    <span>edx</span><span>,</span><span>DWORD</span> <span>[</span><span>rdi</span><span>+</span><span>rcx</span><span>*</span><span>4</span><span>+</span><span>0x4</span><span>]</span>      <span>; 2</span>
    <span>add</span>    <span>rcx</span><span>,</span><span>0x2</span>                        <span>; 3</span>
    <span>mov</span>    <span>r11d</span><span>,</span><span>r8d</span>                       <span>; 4</span>
    <span>movzx</span>  <span>r8d</span><span>,</span><span>r8w</span>                        <span>; 5</span>
    <span>mov</span>    <span>r9d</span><span>,</span><span>edx</span>                        <span>; 6</span>
    <span>shr</span>    <span>r11d</span><span>,</span><span>0x10</span>                      <span>; 7</span>
    <span>movzx</span>  <span>edx</span><span>,</span><span>dx</span>                         <span>; 8</span>
    <span>shr</span>    <span>r9d</span><span>,</span><span>0x10</span>                       <span>; 9</span>
    <span>add</span>    <span>edx</span><span>,</span><span>r8d</span>                        <span>; 10</span>
    <span>add</span>    <span>r9d</span><span>,</span><span>r11d</span>                       <span>; 11</span>
    <span>add</span>    <span>eax</span><span>,</span><span>edx</span>                        <span>; 12</span>
    <span>add</span>    <span>r10d</span><span>,</span><span>r9d</span>                       <span>; 13</span>
    <span>cmp</span>    <span>rcx</span><span>,</span><span>rsi</span>                        <span>; (fuses w/ jb)</span>
    <span>jb</span>     <span>top</span>                            <span>; 14</span>
</code></pre></div></div>

<p>I’ve annotated the total <abbr title="Micro-operation: instructions are translated into one or more uops, which are simple operations executed by the CPU's execution units.">uop</abbr> count on each line: there is nothing tricky here as instruction is one fused <abbr title="Micro-operation: instructions are translated into one or more uops, which are simple operations executed by the CPU's execution units.">uop</abbr>, except for the <code>cmp; jb</code> pair which <abbr title="The fusing of an ALU operation and subsequent jump, such as `dec eax; jnz label` into one operation">macro-fuse</abbr> into a single <abbr title="Micro-operation: instructions are translated into one or more uops, which are simple operations executed by the CPU's execution units.">uop</abbr>. The are 14 uops in this loop, so at best, on my Intel laptop I expect this loop to take 14 / 4 = 3.5 cycles per iteration (1.75 cycles per element). Indeed, when I time this<sup id="fnref:sum-halves"><a href="#fn:sum-halves">5</a></sup> I get 3.51 cycles per iteration, so we are executing 3.99 fused uops per cycle, and we have certainly hit the pipeline width speed limit.</p>

<p>For more complicated code where you don’t actually want to calculate the <abbr title="Micro-operation: instructions are translated into one or more uops, which are simple operations executed by the CPU's execution units.">uop</abbr> count by hand, you can use performance counters - the <code>uops_issued.any</code> counter counts fused-domain uops:</p>

<div><div><pre><code>$ ./uarch-bench.sh --timer=perf --test-name=cpp/sum-halves --extra-events=uops_issued.any
...
Resolved and programmed event 'uops_issued.any' to 'cpu/config=0x10e/', caps: R:1 UT:1 ZT:1 index: 0x1
Running benchmarks groups using timer perf

** Running group cpp : Tests written in C++ **
                               Benchmark    Cycles    uops_i
        Sum 16-bit halves of array elems      3.51     14.03
</code></pre></div></div>

<p>The counter reflects the 14 uops/iteration we calculated by looking at the assembly. If you <em>calculate</em> a value very close to 4 uops per cycle using this metric, you know without examining the code that you are bumping up against this speed limit.</p>

<h3 id="remedies">Remedies</h3>

<p>In a way this is the simplest of the limits to understand: you simply can’t execute any more operations per cycle. You code is already maximally efficient in an operations/cycle sense: you don’t have to worry about cache misses, expensive operations, too many jumps, branch mispredictions or anything like that because they aren’t limiting you.</p>

<p>Your only goal is to reduce the number of operations (in the fused domain), which usually means reducing the number of instructions. You can do that by:</p>

<ul>
  <li>Removing instructions, i.e., “classic” instruction-oriented optimization. Way too involved to cover in a bullet point, but briefly you can try to unroll loops (indeed, by unrolling the loop above, I cut execution time by ~15%), use different instructions that are more efficient, remove instructions (e.g., the <code>mov r11d,r8d</code> and <code>mov r9d,edx</code> are not necessary and could be removed with a slight reoganization), etc. If you are writing in a high level language you can’t do this <em>directly</em>, but you can try to understand the assembly the compiler is generating and make changes to the code or compiler flags that get it to do what you want.</li>
  <li>Vectorization. Try to do more work with one instruction. This is an obvious huge win for this method. If you compile the same code with <code>-O3</code> rather than <code>-O2</code>, gcc vectorizes it (and doesn’t even do a great job<sup id="fnref:gcc-notgreat"><a href="#fn:gcc-notgreat">7</a></sup>) and we get a 4.6x speedup, to 0.76 cycles per iteration (0.38 cycles per element). If you vectorized it by hand or massaged the auto-vectorization a bit more I think you could get to an additional 3x speed, down to roughly 0.125 cycles per element.</li>
  <li>Micro-fusion. Somewhat specific to x86, but you can look for opportunities to fold a load and an ALU operation together, since such micro-fused operations only count as one in the fused domain, compared to two for the separate instructions. This generally applies only for values loaded and used once, but <em>rarely</em> it may even be profitable to load the same value <em>twice</em> from memory, in two different instructions, in order to eliminate a standalone <code>mov</code> from memory. This is more complicated than I make it sound because of the <a href="https://stackoverflow.com/q/26046634">complication of de-lamination</a>, which varies by model and is not fully described<sup id="fnref:delamopt"><a href="#fn:delamopt">8</a></sup> in the optimization manual.</li>
</ul>

<h2 id="portexecution-unit-limits">Port/Execution Unit Limits</h2>

<p><strong>Intel, AMD:</strong> One operation per port, per cycle</p>

<p>Let us use our newfound knowledge of the pipeline width limitation, and tackle another example loop:</p>

<div><div><pre><code><span>uint32_t</span> <span>mul_by</span><span>(</span><span>const</span> <span>uint32_t</span> <span>*</span><span>data</span><span>,</span> <span>size_t</span> <span>len</span><span>,</span> <span>uint32_t</span> <span>m</span><span>)</span> <span>{</span>
    <span>uint32_t</span> <span>sum</span> <span>=</span> <span>0</span><span>;</span>
    <span>for</span> <span>(</span><span>size_t</span> <span>i</span> <span>=</span> <span>0</span><span>;</span> <span>i</span> <span>&lt;</span> <span>len</span> <span>-</span> <span>1</span><span>;</span> <span>i</span><span>++</span><span>)</span> <span>{</span>
        <span>uint32_t</span> <span>x</span> <span>=</span> <span>data</span><span>[</span><span>i</span><span>],</span> <span>y</span> <span>=</span> <span>data</span><span>[</span><span>i</span> <span>+</span> <span>1</span><span>];</span>
        <span>sum</span> <span>+=</span> <span>x</span> <span>*</span> <span>y</span> <span>*</span> <span>m</span> <span>*</span> <span>i</span> <span>*</span> <span>i</span><span>;</span>
    <span>}</span>
    <span>return</span> <span>sum</span><span>;</span>
<span>}</span>
</code></pre></div></div>

<p>The loop compiles to the following assembly. I’ve marked <abbr title="Micro-operation: instructions are translated into one or more uops, which are simple operations executed by the CPU's execution units.">uop</abbr> counts as before.</p>

<div><div><pre><code><span>930</span><span>:</span>
    <span>mov</span>    <span>r10d</span><span>,</span><span>DWORD</span> <span>[</span><span>rdi</span><span>+</span><span>rcx</span><span>*</span><span>4</span><span>+</span><span>0x4</span><span>]</span> <span>;  1 y = data[i + 1]</span>
    <span>mov</span>    <span>r8d</span><span>,</span><span>r10d</span>                   <span>;  2 setup up r8d to hold result of multiplies</span>
    <span>imul</span>   <span>r8d</span><span>,</span><span>ecx</span>                    <span>;  3 i * y</span>
    <span>imul</span>   <span>r8d</span><span>,</span><span>edx</span>                    <span>;  4 ↑ * m</span>
    <span>imul</span>   <span>r8d</span><span>,</span><span>ecx</span>                    <span>;  5 ↑ * i</span>
    <span>add</span>    <span>rcx</span><span>,</span><span>0x1</span>                    <span>;  6 i++</span>
    <span>imul</span>   <span>r8d</span><span>,</span><span>r9d</span>                    <span>;  7 ↑ * x</span>
    <span>mov</span>    <span>r9d</span><span>,</span><span>r10d</span>                   <span>;  8 stash y for next iteration</span>
    <span>add</span>    <span>eax</span><span>,</span><span>r8d</span>                    <span>;  9 sum += ...</span>
    <span>cmp</span>    <span>rcx</span><span>,</span><span>rsi</span>                    <span>;    i &lt; len (fuses with jne)</span>
    <span>jne</span>    <span>930</span>                        <span>; 10</span>
</code></pre></div></div>

<p>Despite the source containing two loads per iteration (<code>x = data[i]</code> and <code>y = data[i + 1]</code>), the compiler was clever enough to reduce that to one, since <code>y</code> in iteration <code>n</code> becomes <code>x</code> in iteration <code>n + 1</code>, so it saves the loaded value in a register across iterations.</p>

<p>So we can just apply our pipeline width technique to this loop, right? We count 10 uops (again, the only trick is that <code>cmp; jne</code> are <abbr title="The fusing of an ALU operation and subsequent jump, such as `dec eax; jnz label` into one operation">macro-fused</abbr>). We can confirm it in <abbr title="Microarchitecture: a specific implementation of an ISA, e.g., &quot;Haswell microarchitecture&quot;.">uarch</abbr>-bench:</p>

<div><div><pre><code>$ ./uarch-bench.sh --timer=perf --test-name=cpp/mul-4 --extra-events=uops_issued.any,uops_retired.retire_slots
....
** Running group cpp : Tests written in C++ **
                               Benchmark    Cycles    uops_i    uops_r
                    Four multiplications      ????     10.01     10.00
</code></pre></div></div>

<p>Right, 10 uops. So this should take 10 / 4 = 2.5 cycles per iteration on modern Intel then, right? No. The hidden <code>????</code> value in the benchmark output indicates that it actually takes 4.01 cycles.</p>

<p>What gives? As it turns out, the limitation is the <code>imul</code> instructions. Although up to four <code>imul</code> instructions can be <em>issued<sup id="fnref:issued"><a href="#fn:issued">9</a></sup></em> every cycle, there is only a single scalar multiplication unit on the CPU, and so only one multiplication can begin execution every cycle. Since there are four multiplications in the loop, it takes at least four cycles to execute it, and in fact that’s exactly what we find.</p>

<p>On modern chips all operations execute only through a limited number of ports<sup id="fnref:ports"><a href="#fn:ports">10</a></sup> and for multiplications that is always only <abbr title="port 1 (GP and SIMD ALU, integer mul)">p1</abbr>. You can get this information from Agner’s <a href="https://www.agner.org/optimize/#manual_instr_tab">instruction tables</a>:</p>

<p><img src="https://travisdowns.github.io/assets/speed-limits/agner-imul.png" alt="Agner's port usage info"></p>

<p>… or from <a href="http://uops.info/html-instr/IMUL_R32_R32.html">uops.info</a>:</p>

<p><img src="https://travisdowns.github.io/assets/speed-limits/uops-info-imul.png" alt="uops-info port usage info"></p>

<p>On modern Intel some simple integer arithmetic (<code>add</code>, <code>sub</code>, <code>inc</code>, <code>dec</code>), bitwise operation (<code>or</code>, <code>and</code>, <code>xor</code>) and flag setting tests (<code>test</code>, <code>cmp</code>) run on four ports, so you aren’t very likely to see a port bottleneck for these operations (since the pipeline width bottleneck is more general and is also four), but many operations compete for only a few ports. For example, shift instructions and bit test/set operations like <code>bt</code>, <code>btr</code> and friends use only <abbr title="port 1 (GP and SIMD ALU, integer mul)">p1</abbr> and <abbr title="port 6 (GP ALU, all branches)">p6</abbr>. More advanced bit operations like <code>popcnt</code> and <code>tzcnt</code> execute only <code>p1</code>, and so on. Note that in some cases instructions which can go to wide variety of ports, such as <code>add</code> may execute on a port that is under contention by other instructions rather than on the less loaded ports: a scheduling quirk that can reduce performance. Why that happens is <a href="http://stackoverflow.com/questions/40681331/how-are-x86-uops-scheduled-exactly">not fully understood</a>.</p>

<p>One of the most common cases of port contention is with vector operations. There are only three vector ports, so the best case is three vector operations per cycle, and for AVX-512 there are only two ports so the best case is two per cycle. Furthermore, only a few operations can use all three ports (mostly simple integer arithmetic and bitwise operations and 32 and 64-bit <abbr title="When discussing assembly instructions an immediate is a value embedded in the instruction itself, e.g., the 1 in add eax, 1.">immediate</abbr> blends) - many are restricted to one or two ports. In particular, shuffles run only on <abbr title="port 5 (GP and SIMD ALU, vector shuffles)">p5</abbr> and can be a bottleneck for shuffle heavy algorithm.</p>

<h3 id="tools">Tools</h3>

<p>In the example above it was easy to see the port pressure because the <code>imul</code> instructions go to only a single port, and the remainder of the instructions are mostly simple instructions that can go to any of four ports, so a 4 cycle <em>solution</em> to the port assignment problem is easy to find. In more complex cases, with many instructions that go to many ports, it is less clear what the ideal solution is (and even less clear what the CPU will actually do without testing it), so you can use one of a few tools:</p>

<p><strong>Intel IACA</strong></p>

<p>Tries to solve for port pressure (algorithm unclear) and displays it in a table. Has reached end of life but can still be downloaded <a href="https://software.intel.com/en-us/articles/intel-architecture-code-analyzer">here</a>.</p>

<p><strong>RRZE-HPC OSACA</strong></p>

<p>Essnentially an open-source version if IACA. Displays cumulative port pressure in a similar way to IACA, although it simply divides each instruction evenly among the ports it can use and doesn’t look for a more ideal solution. On <a href="https://github.com/RRZE-HPC/OSACA">github</a>.</p>

<p><strong>LLVM-MCA</strong></p>

<p>Another tool similar to IACA and OSACA, shows port pressure in a similar way and attempts to find an ideal solution (algorithm unclear, but it’s open source so someone could check). Comes with LLVM 7 or higher and documentation is <a href="https://llvm.org/docs/CommandGuide/llvm-mca.html">here</a>.</p>

<h3 id="measuring-it">Measuring It</h3>

<p>You can measure the actual port pressure using the <code>perf</code> and the <code>uops_dispatched_port</code> counters. For example, to measure the full port pressure across all 8 ports, you can do the following in <abbr title="Microarchitecture: a specific implementation of an ISA, e.g., &quot;Haswell microarchitecture&quot;.">uarch</abbr>-bench:</p>

<div><div><pre><code>./uarch-bench.sh --timer=perf --test-name=cpp/mul-4 --extra-events=uops_dispatched_port.port_0,uops_dispatched_port.port_1,uops_dispatched_port.port_2,uops_dispatched_port.port_3,uops_dispatched_port.port_4,uops_dispatched_port.port_5,uops_dispatched_port.port_6,uops_dispatched_port.port_7
...
Running benchmarks groups using timer perf

** Running group cpp : Tests written in C++ **
           Benchmark       Cycles       uops_d       uops_d       uops_d       uops_d       uops_d       uops_d       uops_d       uops_d
Four multiplications         4.00         1.06         4.00         0.50         0.50         0.00         0.97         1.81         0.00
</code></pre></div></div>

<p>While noting that that the column naming scheme is <a href="https://github.com/travisdowns/uarch-bench/issues/50">really bad</a> in this case, we see that the port1 (the 3rd numeric column) has 4 operations dispatched every iteration, and iterations take 4 cycles, so the port is active every cycle, i.e., 100% pressure. None of the other ports have significant pressure at all, they are all active less than 50% of the time.</p>

<h3 id="remedies-1">Remedies</h3>

<ul>
  <li>Of course, any solution that removes instructions causing port pressure can help, so most of the same remedies that apply to the <em>pipeline width</em> limit also apply here.</li>
  <li>Additionally, you might try replacing instructions which contend for a high-pressure port with others that use different ports, even if the replacement results in more total instructions/uops. For example, sometimes <abbr title="port 5 (GP and SIMD ALU, vector shuffles)">p5</abbr> shuffle operations can be replaced with blend operations: you need more total blends but the resulting code can be faster since the blends execute on otherwise underused <abbr title="port 0 (GP and SIMD ALU, not-taken branches)">p0</abbr> and <abbr title="port 1 (GP and SIMD ALU, integer mul)">p1</abbr>. Some 32 and 64-bit register-to-register broadcasts that use <abbr title="port 5 (GP and SIMD ALU, vector shuffles)">p5</abbr> don’t use <abbr title="port 5 (GP and SIMD ALU, vector shuffles)">p5</abbr> at all if you instead use a memory source, a rare case where memory source can be <em>faster</em> than register source for the same operation.</li>
</ul>

<h2 id="load-throughput-limit">Load Throughput Limit</h2>

<p><strong>Intel, AMD:</strong> 2 loads per cycle</p>

<p>Modern Intel and AMD chips (and many others) have a limit of two loads per cycle, which you can achieve if both loads hit in L1. You could just consider this the same as the “port pressure” limit, since there only two load ports - but the limit is interesting enough to call out on its own.</p>

<p>Of course, like all limits this is a best case scenario: you might achieve much less than two loads if you are not hitting in L1 or even for L1-resident data due to things like bank conflicts present on AMD and older Intel chips<sup id="fnref:bankconf"><a href="#fn:bankconf">11</a></sup>. Still, it is interesting to note how <em>high</em> this limit is: given the pipeline width of four, fully <em>half</em> of your instructions can be loads while still running at maximum speed. In a throughput sense, loads that hit in cache are not all that expensive even compared to simple ALU ops.</p>

<p>It’s not all that common to this hit this limit, but you can certainly do it. The loads have to be mostly independent (not part of a carried dependency chain), since otherwise the load latency will limit you more than the throughput.</p>

<p>It’s not all <em>that</em> common to hit this limit, but it can often happen in an indirect load scenario (where part of the load address is itself calculated using a value from memory), or when heavy use of lookup tables is made. Consider the following loop, does an indirect loop in <code>data</code> based on the <code>offsets</code> array and sums the values it finds<sup id="fnref:written-weirdly"><a href="#fn:written-weirdly">12</a></sup>:</p>

<div><div><pre><code><span>do</span> <span>{</span>
    <span>sum1</span> <span>+=</span> <span>data</span><span>[</span><span>offsets</span><span>[</span><span>i</span> <span>-</span> <span>1</span><span>]];</span>
    <span>sum2</span> <span>+=</span> <span>data</span><span>[</span><span>offsets</span><span>[</span><span>i</span> <span>-</span> <span>2</span><span>]];</span>
    <span>i</span> <span>-=</span> <span>2</span><span>;</span>
<span>}</span> <span>while</span> <span>(</span><span>i</span><span>);</span>
</code></pre></div></div>

<p>This compiles to the following assembly:</p>

<div><div><pre><code><span>88</span><span>:</span>                                       <span>; total fused uops</span>
    <span>mov</span>    <span>r8d</span><span>,</span><span>DWORD</span> <span>PTR</span> <span>[</span><span>rsi</span><span>+</span><span>rdx</span><span>*</span><span>4</span><span>-</span><span>0x4</span><span>]</span>  <span>; 1</span>
    <span>add</span>    <span>ecx</span><span>,</span><span>DWORD</span> <span>PTR</span> <span>[</span><span>rdi</span><span>+</span><span>r8</span><span>*</span><span>4</span><span>]</span>       <span>; 2</span>
    <span>mov</span>    <span>r8d</span><span>,</span><span>DWORD</span> <span>PTR</span> <span>[</span><span>rsi</span><span>+</span><span>rdx</span><span>*</span><span>4</span><span>-</span><span>0x8</span><span>]</span>  <span>; 3</span>
    <span>add</span>    <span>eax</span><span>,</span><span>DWORD</span> <span>PTR</span> <span>[</span><span>rdi</span><span>+</span><span>r8</span><span>*</span><span>4</span><span>]</span>       <span>; 4</span>
    <span>sub</span>    <span>rdx</span><span>,</span><span>0x2</span>                        <span>; (fuses w/ jne)</span>
    <span>jne</span>    <span>88</span>                             <span>; 5</span>
</code></pre></div></div>

<p>There are only 5 fused-uops<sup id="fnref:delam2"><a href="#fn:delam2">13</a></sup> here, so maybe this executes in 1.25 cycles? Not so fast - it takes 2 cycles because there are 4 loads and we have a speed limit of 2 loads per cycle<sup id="fnref:add-indirect"><a href="#fn:add-indirect">14</a></sup>.</p>

<p>Note that gather instructions count “one” against this limit for <em>each</em> element they they load. <code>vpgatherdd ymm0, ...</code> for example, counts as 8 against this limit since it loads eight elements.</p>

<h3 id="split-cache-lines">Split Cache Lines</h3>

<p>For the purposes of this speed limit, on Intel, all loads that hit in the L1 cache count as one, except loads that split a cache line, which count as two. A split cache line load is of at least two bytes and crosses a 64-byte boundary. If your loads are <abbr title="Naturally aligned data is data whose location in memory is a multiple of its size, e.g., a 4 byte element whose address is a multiple of 4 bytes.">naturally aligned</abbr>, you will never split a cache line. If your loads have totally random alignment, how often you split a cache line depends on the load size: for a load of N bytes, you’ll split a cache line with probability (N-1)/64. Hence, 32-bit random unaligned loads split less than 5% of the time but 256-bit AVX loads split 48% of the time and AVX-512 loads more than 98% of the time.</p>

<p>On AMD Zen1 loads suffer a penalty when crossing any 32-byte boundary - such loads also count as two against the load limit. 32-byte (AVX) loads also count as two on Zen1 since the implemented vector path is only 128-bit, so two loads are needed. Any 32-byte load that is not 16-byte aligned counts as three, since in that case exactly one of the 16-byte halve will cross a 32-byte boundary.</p>

<h3 id="remedies-2">Remedies</h3>

<p>If you are lucky enough to hit this limit, you just need less loads. Note that the limit is not expressed in terms of the <em>number of bytes loaded</em>, but in the number of separate loads. So sometimes you can combine two or more adjacent loads into a single load. An obvious application of that is vector loads: 32-byte AVX loads <em>still</em> have the same limit of two per cycle as byte loads. It is difficult to use vector loads in concert with scalar code however: although you can do 8x 32-bit loads at once, if you want to feed those loads to scalar code you have trouble, because you can’t efficiently get that data into scalar registers<sup id="fnref:vector-scalar"><a href="#fn:vector-scalar">15</a></sup>. That is, you’ll have to work on vectorizing the code that consumes the loads as well.</p>

<p>You can also sometimes use wider scalar loads in this way. In the example above, we do four 32-bit loads - two of which are scattered (the access to <code>data[]</code>), but two of which are adjacent (the accesses to <code>offsets[i - 1]</code> and <code>offsets[i - 2]</code>). We could combine those two adjacent loads into one 64-bit load, like so<sup id="fnref:portable"><a href="#fn:portable">16</a></sup>:</p>

<div><div><pre><code><span>do</span> <span>{</span>
    <span>uint64_t</span> <span>twooffsets</span><span>;</span>
    <span>std</span><span>::</span><span>memcpy</span><span>(</span><span>&amp;</span><span>twooffsets</span><span>,</span> <span>offsets</span> <span>+</span> <span>i</span> <span>-</span> <span>2</span><span>,</span> <span>sizeof</span><span>(</span><span>uint64_t</span><span>));</span>
    <span>sum1</span> <span>+=</span> <span>data</span><span>[</span><span>twooffsets</span> <span>&gt;&gt;</span> <span>32</span><span>];</span>
    <span>sum2</span> <span>+=</span> <span>data</span><span>[</span><span>twooffsets</span> <span>&amp;</span> <span>0xFFFFFFFF</span><span>];</span>
    <span>i</span> <span>-=</span> <span>2</span><span>;</span>
<span>}</span> <span>while</span> <span>(</span><span>i</span><span>);</span>
</code></pre></div></div>

<p>This compiles to:</p>

<div><div><pre><code><span>98</span><span>:</span>                                        <span>; total fused uops</span>
    <span>mov</span>    <span>rcx</span><span>,</span><span>QWORD</span> <span>PTR</span> <span>[</span><span>rsi</span><span>+</span><span>rdx</span><span>*</span><span>4</span><span>-</span><span>0x8</span><span>]</span>   <span>; 1</span>
    <span>mov</span>    <span>r9</span><span>,</span><span>rcx</span>                          <span>; 2</span>
    <span>mov</span>    <span>ecx</span><span>,</span><span>ecx</span>                         <span>; 3</span>
    <span>shr</span>    <span>r9</span><span>,</span><span>0x20</span>                         <span>; 4</span>
    <span>add</span>    <span>eax</span><span>,</span><span>DWORD</span> <span>PTR</span> <span>[</span><span>rdi</span><span>+</span><span>rcx</span><span>*</span><span>4</span><span>]</span>       <span>; 5</span>
    <span>add</span>    <span>r8d</span><span>,</span><span>DWORD</span> <span>PTR</span> <span>[</span><span>rdi</span><span>+</span><span>r9</span><span>*</span><span>4</span><span>]</span>        <span>; 6</span>
    <span>sub</span>    <span>rdx</span><span>,</span><span>0x2</span>                         <span>; (fuses w/ jne)</span>
    <span>jne</span>    <span>98</span>                              <span>; 7</span>
</code></pre></div></div>

<p>We have 7 fused-domain uops rather than 5, yet this runs in 1.81 cycles, about 10% faster. The theoretical limit based on pipeline width is 7 / 4 = 1.75 cycles, so we are probably getting collisions on <abbr title="port 6 (GP ALU, all branches)">p6</abbr> between the <code>shr</code> and the taken branch (unrolling a bit more would help). Clang 5.0 manages to do better, by one <abbr title="Micro-operation: instructions are translated into one or more uops, which are simple operations executed by the CPU's execution units.">uop</abbr>:</p>

<div><div><pre><code><span>70</span><span>:</span>
    <span>mov</span>    <span>r8</span><span>,</span><span>QWORD</span> <span>PTR</span> <span>[</span><span>rsi</span><span>+</span><span>rdx</span><span>*</span><span>4</span><span>-</span><span>0x8</span><span>]</span>
    <span>mov</span>    <span>r9d</span><span>,</span><span>r8d</span>
    <span>shr</span>    <span>r8</span><span>,</span><span>0x20</span>
    <span>add</span>    <span>ecx</span><span>,</span><span>DWORD</span> <span>PTR</span> <span>[</span><span>rdi</span><span>+</span><span>r8</span><span>*</span><span>4</span><span>]</span>
    <span>add</span>    <span>eax</span><span>,</span><span>DWORD</span> <span>PTR</span> <span>[</span><span>rdi</span><span>+</span><span>r9</span><span>*</span><span>4</span><span>]</span>
    <span>add</span>    <span>rdx</span><span>,</span><span>0xfffffffffffffffe</span>
    <span>jne</span>    <span>70</span>
</code></pre></div></div>

<p>It avoided the <code>mov r9,rcx</code> instruction by combining that and the zero extension (which is effectively the <code>&amp; 0xFFFFFFFF</code>) into a single <code>mov r9d,rd8</code>. It runs at 1.67 cycles per iteration, saving 20% over the 4-load version, but still slower than the 1.5 limit implied by the 4-wide fused-domain limit.</p>

<p>This code is an obvious candidate for vectorization with gather, which could in principle approach 1.25 cycles per iteration (8 gathered loads + 1 256-bit load from <code>offset</code> per 4 iterations) and newer clang versions even manage to do it, if you allow some inlining so they can see the size and alignment of the buffer. However, <a href="https://gist.github.com/travisdowns/b8294098c5082886f4a043ef8b6607bd">the result</a> is not good: it was more than twice as slow as the scalar approach.</p>

<h2 id="memory-and-cache-bandwidth">Memory and Cache Bandwidth</h2>

<p>The load and store limits discuss the ideal scenario where loads and stores hit in L1 (or hit in L1 “on average” enough to not slow things down), but there are throughput limits for other levels of the cache. If your know your loads hit primarily in a particular level of the cache you can use these limits to get a speed limit.</p>

<p>The limits are listed in <em>cache lines per cycle</em> and not in bytes, because that’s how you need to count the accesses: in unique cache lines accessed. The hardware transfers full lines. You can achieve these limits, but you may not be able to consume all the bytes from each cache line, because demand accesses to the L1 cache cannot occur on the same cycle that the L1 cache receives data from the outer cache levels. So, for example, the L2 can provide 64 bytes of data to the L1 cache per cycle, but you cannot <em>also</em> access 64 bytes every cycle since the L1 cannot satisfy those reads from the core <em>and</em> the incoming data from the L2 every cycle. All the gory details are <a href="https://github.com/travisdowns/uarch-bench/wiki/How-much-bandwidth-does-the-L2-have-to-give,-anyway%3F">over here</a>.</p>

<table>
  <thead>
    <tr>
      <th>Vendor</th>
      <th>Microarchitecture</th>
      <th>L2</th>
      <th>L3 (Shared)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Intel</td>
      <td><abbr title="Intel's Cannon Lake (client) architecture, the i3-8121U was the only SKU ever released">CNL</abbr></td>
      <td>0.75</td>
      <td>0.2 - 0.3</td>
    </tr>
    <tr>
      <td>Intel</td>
      <td><abbr title="Intel's Skylake (server) architecture including Skylake-SP, Skylake-X and Skylake-W">SKX</abbr></td>
      <td>1</td>
      <td>~0.1 (?)</td>
    </tr>
    <tr>
      <td>Intel</td>
      <td><abbr title="Intel's Skylake (client) architecture, aka 6th Generation Intel Core i3,i5,i7">SKL</abbr></td>
      <td>1</td>
      <td>0.2 - 0.3</td>
    </tr>
    <tr>
      <td>Intel</td>
      <td><abbr title="Intel's Haswell architecture, aka 4th Generation Intel Core i3,i5,i7">HSW</abbr></td>
      <td>0.5</td>
      <td>0.2 - 0.3</td>
    </tr>
    <tr>
      <td>AMD</td>
      <td>Zen</td>
      <td>0.5</td>
      <td>0.5</td>
    </tr>
    <tr>
      <td>AMD</td>
      <td>Zen2</td>
      <td>0.5</td>
      <td>0.5</td>
    </tr>
  </tbody>
</table>

<p>The very poor figure of 0.1 cache lines per cycle (about 6-7 bytes a cycle) from L3 on <abbr title="Intel's Skylake (server) architecture including Skylake-SP, Skylake-X and Skylake-W">SKX</abbr> is at odds with Intel’s manuals, but it’s what I measured on a W-2104. For architectures earlier than Haswell I think the numbers will be similar back to Sandy Bridge.</p>

<p>If your accesses go to a mix of cache levels: you will probably get slightly worse bandwidth than what you’d get if you calculated the speed limit based on the assumption the cache levels can be accessed independently.</p>

<p>Memory bandwidth is a bit more complicated. You can calculate your theoretical value based on your memory channel count (or look it up on ARK), but this is complicated by the fact that many chips cannot reach the maximum bandwidth from a single core since they cannot generate enough requests to saturate the DRAM bus, due to limited fill buffers. So you are better off just measuring it.</p>

<h3 id="remedies-3">Remedies</h3>

<p>The usual remedies to improve caching performance apply: pack your structures more tightly, try to ensure locality of reference and prefetcher friendly access patterns, use cache blocking, etc.</p>

<h2 id="carried-dependency-chains">Carried Dependency Chains</h2>

<p><strong>Sum of latencies in the longest carried dependency chain</strong></p>

<p>Everything discussed so far is a limited based on <em>throughput</em> - the machine can only do so many things per cycle, and we count the number of things and apply those limits to determine the speed limit. We don’t care about how long each instruction takes to finish (as long as we can <em>start</em> one per cycle), or from where it gets its inputs. In practice, that can matter a lot.</p>

<p>Let’s consider, for example, a modified version of the multiply loop above, one that’s a lot simpler:</p>

<div><div><pre><code><span>for</span> <span>(</span><span>size_t</span> <span>i</span> <span>=</span> <span>0</span><span>;</span> <span>i</span> <span>&lt;</span> <span>len</span><span>;</span> <span>i</span><span>++</span><span>)</span> <span>{</span>
    <span>uint32_t</span> <span>x</span> <span>=</span> <span>data</span><span>[</span><span>i</span><span>];</span>
    <span>product</span> <span>*=</span> <span>x</span><span>;</span>
<span>}</span>
</code></pre></div></div>

<p>This does only a single multiplication per iteration, and compiles to the following tight loop:</p>

<div><div><pre><code><span>50</span><span>:</span>
    <span>imul</span>   <span>eax</span><span>,</span><span>DWORD</span> <span>PTR</span> <span>[</span><span>rdi</span><span>]</span>
    <span>add</span>    <span>rdi</span><span>,</span><span>0x4</span>
    <span>cmp</span>    <span>rdi</span><span>,</span><span>rdx</span>
    <span>jne</span>    <span>50</span>
</code></pre></div></div>

<p>That’s only 3 fused uops, so our pipeline speed limit is 0.75 cycles/iteration. But wait, we know the imul needs <abbr title="port 1 (GP and SIMD ALU, integer mul)">p1</abbr>, and the other two operations can go to other ports, so the <abbr title="port 1 (GP and SIMD ALU, integer mul)">p1</abbr> pressure means a limit of 1 cycle/iteration. What does the real world have to say?</p>

<div><div><pre><code>./uarch-bench.sh --timer=perf --test-name=cpp/mul-chain --extra-events=$PE_PORTS
              Benchmark       Cycles       uops_d       uops_d       uops_d       uops_d       uops_d       uops_d       uops_d       uops_d
Chained multiplications         2.98         0.50         1.00         0.50         0.50         0.00         0.50         1.00         0.00
</code></pre></div></div>

<p>Bleh, 2.98 cycles, or 3x slower than we predicted.</p>

<p>What happened? As it turns out, the <code>imul</code> instruction has a <em>latency</em> of 3 cycles. That means that the result is not available until 3 cycles after the operation starts executing. This contrasts with 1 latency cycle for most simple arithmetic operations. Since on each iteration the multiply instruction depends on the result of the result of the <em>previous</em> iteration’s multiply<sup id="fnref:srcdest"><a href="#fn:srcdest">17</a></sup>, every multiply can only start when the previous one finished, i.e., 3 cycles later. So 3 cycles is the speed limit for this loop.</p>

<p>Note that we mostly care about <em>loop carried</em> dependencies, which are dependency chains that cross loop iterations, i.e., where some output register in one iteration is used as an input register for the same chain in the next iteration. In the example, the carried chain involves only <code>eax</code>, but more complex chains are common in practice. In the earlier example, the four <code>imul</code> instructions <em>did</em> form a chain:</p>

<div><div><pre><code><span>930</span><span>:</span>
    <span>mov</span>    <span>r10d</span><span>,</span><span>DWORD</span> <span>[</span><span>rdi</span><span>+</span><span>rcx</span><span>*</span><span>4</span><span>+</span><span>0x4</span><span>]</span> <span>; load</span>
    <span>mov</span>    <span>r8d</span><span>,</span><span>r10d</span>                   <span>;</span>
    <span>imul</span>   <span>r8d</span><span>,</span><span>ecx</span>                    <span>; imul1</span>
    <span>imul</span>   <span>r8d</span><span>,</span><span>edx</span>                    <span>; imul2</span>
    <span>imul</span>   <span>r8d</span><span>,</span><span>ecx</span>                    <span>; imul3</span>
    <span>add</span>    <span>rcx</span><span>,</span><span>0x1</span>                    <span>;</span>
    <span>imul</span>   <span>r8d</span><span>,</span><span>r9d</span>                    <span>; imul4</span>
    <span>mov</span>    <span>r9d</span><span>,</span><span>r10d</span>                   <span>;</span>
    <span>add</span>    <span>eax</span><span>,</span><span>r8d</span>                    <span>; add</span>
    <span>cmp</span>    <span>rcx</span><span>,</span><span>rsi</span>                    <span>;</span>
    <span>jne</span>    <span>930</span>                        <span>;</span>
</code></pre></div></div>

<p>Note how each <code>imul</code> depends on the previous through the input/output <code>r8d</code>. Finally, the result is added to <code>eax</code> ,and <code>eax</code> is indeed used as input in the next iteration, so do we have a loop-carried dependency chain? Yes - but a very small one involving only <code>eax</code>. The dependency chain looks like this:</p>

<div><div><pre><code>iteration 1       load -&gt; imul1 -&gt; imul -&gt; imul -&gt; imul -&gt; add
                                                            |
                                                            v
iteration 2       load -&gt; imul1 -&gt; imul -&gt; imul -&gt; imul -&gt; add
                                                            |
                                                            v
iteration 3       load -&gt; imul1 -&gt; imul -&gt; imul -&gt; imul -&gt; add
                                                            |
                                                            v
etc ...                                                    ...
</code></pre></div></div>

<p>So yes, there is a dependent chain there, and the <code>imul</code> instructions are <em>connected</em> to that chain, but they don’t participate in the carried part. Only the single-cycle latency <code>add</code> instruction participates in the carried dependency chain, so the implied speed limit is 1 cycle/iteration. In fact, all of our examples so far have had carried dependency chains, but they have all been small enough never to be the dominating factor. You may also have <em>multiple</em> carried dependency chains in a loop: the speed limit is set by the longest.</p>

<p>I’ve only touched on this topic and won’t go much further here: for a deeper look check out Fabian Giesen’s <a href="https://fgiesen.wordpress.com/2018/03/05/a-whirlwind-introduction-to-dataflow-graphs/">A whirlwind introduction to dataflow graphs</a>.</p>

<p>Finally, you may have noticed something interesting about the benchmark result of 2.98 cycles. In every other case, the measured time was equal or slightly <em>more</em> than the speed limit, due to test overhead. How were we able to break the speed limit in this case and come <em>under</em> 3.00 cycles, albeit by less than 1%? Maybe it’s just measurement error - the clocks aren’t precise enough to time this more precisely?</p>

<p>Nope. The effect is real and is due to the structure of the test. We run the multiplication code shown above on a buffer of 4096 elements, so there are 4096 iterations. The benchmark loop that calls that function, <em>itself</em> runs 1000 iterations, each one calling the 4096-iteration inner loop. What happens to get the 2.98 is that in between each call of the inner loop, the multiplication chains <em>can</em> be overlapped. Each chain is 4096-elements long, but the start of each function starts a new chain:</p>

<div><div><pre><code><span>uint32_t</span> <span>mul_chain</span><span>(</span><span>const</span> <span>uint32_t</span> <span>*</span><span>data</span><span>,</span> <span>size_t</span> <span>len</span><span>,</span> <span>uint32_t</span> <span>m</span><span>)</span> <span>{</span>
    <span>uint32_t</span> <span>product</span> <span>=</span> <span>1</span><span>;</span>
    <span>for</span> <span>(</span><span>size_t</span> <span>i</span> <span>=</span> <span>0</span><span>;</span> <span>i</span> <span>&lt;</span> <span>len</span><span>;</span> <span>i</span><span>++</span><span>)</span> <span>{</span>
      <span>// ...</span>
</code></pre></div></div>

<p>Note the <code>product = 1</code> - that’s a new chain. So some small amount of overlap is possible near the end of each loop, which shaves about 80-90 cycles off the loop time (i.e., something like ~30 multiplications get to overlap). The size of the overlap is limited by the <abbr title="Out-of-order execution allows CPUs to execute instructions out of order with respect to the source.">out-of-order</abbr> buffer structures in the CPU, in particular the <a href="https://en.wikipedia.org/wiki/Re-order_buffer">re-order buffer</a> and <a href="https://en.wikipedia.org/wiki/Reservation_station">scheduler</a>.</p>

<h3 id="tools-1">Tools</h3>

<p>As fun as tracing out dependency chains by hand is, you’ll eventually want a tool to do this for you. All of IACA, OSACA and llvm-mca can do this type of latency analysis and identity loop carried dependencies implicitly. For example, llvm-mca <a href="https://godbolt.org/z/tD6dd-">correctly identifies</a> that this loop will take 3 cycles/iteration.</p>

<h3 id="remedies-4">Remedies</h3>

<p>The basic remedy is that you have to shorten or break up the dependency chains.</p>

<p>For example, maybe you can use lower latency instructions like addition or shift instead of multiplication. A more generally applicable trick is to turn one long dependency chain into several parallel ones. In the example above, the associativity property of integer multiplication<sup id="fnref:assoc"><a href="#fn:assoc">18</a></sup> allows us to do the multiplications in any order. In particular, we could accumulate every third element into a separate product and multiply them all at the end, like so:</p>

<div><div><pre><code>    <span>uint32_t</span> <span>p1</span> <span>=</span> <span>1</span><span>,</span> <span>p2</span> <span>=</span> <span>1</span><span>,</span> <span>p3</span> <span>=</span> <span>1</span><span>,</span> <span>p4</span> <span>=</span> <span>1</span><span>;</span>
    <span>for</span> <span>(</span><span>size_t</span> <span>i</span> <span>=</span> <span>0</span><span>;</span> <span>i</span> <span>&lt;</span> <span>len</span><span>;</span> <span>i</span> <span>+=</span> <span>4</span><span>)</span> <span>{</span>
        <span>p1</span> <span>*=</span> <span>data</span><span>[</span><span>i</span> <span>+</span> <span>0</span><span>];</span>
        <span>p2</span> <span>*=</span> <span>data</span><span>[</span><span>i</span> <span>+</span> <span>1</span><span>];</span>
        <span>p3</span> <span>*=</span> <span>data</span><span>[</span><span>i</span> <span>+</span> <span>2</span><span>];</span>
        <span>p4</span> <span>*=</span> <span>data</span><span>[</span><span>i</span> <span>+</span> <span>3</span><span>];</span>
    <span>}</span>
    <span>uint32_t</span> <span>product</span> <span>=</span> <span>p1</span> <span>*</span> <span>p2</span> <span>*</span> <span>p3</span> <span>*</span> <span>p4</span><span>;</span>
</code></pre></div></div>

<p>This test runs at 1.00 cycles per iteration, so the latency chain speed limit has been removed. Well, it’s still there: each iteration above takes at least 3 cycles because of the four carried dependency chains between each iteration, but since we are doing 4x as much work now, the <abbr title="port 1 (GP and SIMD ALU, integer mul)">p1</abbr> port limit becomes the dominant limit.</p>

<p>Compilers can sometimes make this transformation for you, but not always. In particular, gcc is reluctant to unroll loops at any optimization level, and unrolling loops is often a prerequisite for this transformation, so often you are stuck doing it by hand.</p>

<h2 id="front-end-effects">Front End Effects</h2>

<p>I’m going to largely gloss over this one. It really deserves a whole blog post, but in recent Intel and AMD architectures the prevalence of front-end effects being the limiting factor in loops has dropped a lot. The introduction of the <abbr title="Micro-operation: instructions are translated into one or more uops, which are simple operations executed by the CPU's execution units.">uop</abbr> cache and better decoders means that it is not as common as it used to be. For a complete<sup id="fnref:sklfe"><a href="#fn:sklfe">19</a></sup> treatment see Agner’s <a href="https://www.agner.org/optimize/#manual_microarch">microarchitecture guide</a>, starting with section 9.1 through 9.7 for Sandy Bridge (and then the corresponding sections for each later <abbr title="Microarchitecture: a specific implementation of an ISA, e.g., &quot;Haswell microarchitecture&quot;.">uarch</abbr> you are interested in).</p>

<p>If you see an effect that depends on code alignment, especially in a cyclic pattern with a period 16, 32 or 64 bytes, it is very likely to be a front-end effect. There are <a href="https://twitter.com/trav_downs/status/1124152129294409729">hacks you can use to test this</a>.</p>

<p>First are simple absolute front-end limits to delivered uops/cycle depending on where the uops are coming from<sup id="fnref:lsdno"><a href="#fn:lsdno">20</a></sup>:</p>

<p><strong>Table 1: Uops delivered per cycle</strong></p>

<table>
  <thead>
    <tr>
      <th>Architecture</th>
      <th>Microcode (<abbr title="Intel's name for the microcode engine: a component handles complex instructions which require more than 4 uops using microcode which feeds uops directly into the IDQ.">MSROM</abbr>)</th>
      <th>Decoder (<abbr title="Intel's name for the &quot;legacy&quot; decoder, i.e., the decoder that usually decodes instructions when they are not found in the MSROM.">MITE</abbr>)</th>
      <th>Uop cache (DSB)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>&lt;= Broadwell</td>
      <td>4</td>
      <td>4</td>
      <td>4</td>
    </tr>
    <tr>
      <td>&gt;= Skylake</td>
      <td>4</td>
      <td>5</td>
      <td>6</td>
    </tr>
  </tbody>
</table>

<p>These might look like important values. I even made a table, one of only two in this whole post. They aren’t very important though, because they are all equal to or larger than the pipeline limit of 4. In fact it is <a href="https://twitter.com/trav_downs/status/1106403269792788480">hard</a> to even carefully design a micro-benchmark which definitively shows the difference between the 5-wide decode on <abbr title="Intel's Skylake (client) architecture, aka 6th Generation Intel Core i3,i5,i7">SKL</abbr> and the 4-wide on Haswell and earlier. So you can mostly ignore these numbers.</p>

<p>The more important limitations are specific to the individual sources. For example:</p>

<ul>
  <li>The legacy decoder (<abbr title="Intel's name for the &quot;legacy&quot; decoder, i.e., the decoder that usually decodes instructions when they are not found in the MSROM.">MITE</abbr>) can only handle up to 16 instruction bytes per cycle, so any time instruction length averages more than four bytes decode throughput will necessarily be lower than four. Certain patterns will have worse throughput than predicted by this formula, e.g., 7 instructions in a 16 byte block will decode in a 6-1-6-1 pattern.</li>
  <li>Only one of the 4 or 5 legacy decoders can handle instructions which generate more than one <abbr title="Micro-operation: instructions are translated into one or more uops, which are simple operations executed by the CPU's execution units.">uop</abbr>, so a series of instructions which generate 2 uops will only decode at 1 per cycle (2 uops per cycle).</li>
  <li>Only one <abbr title="Micro-operation: instructions are translated into one or more uops, which are simple operations executed by the CPU's execution units.">uop</abbr> cache entry (with up to 6 uops) can be accessed per cycle. For larger loops this is rarely a bottleneck, but it means that any loop that crosses a <abbr title="Micro-operation: instructions are translated into one or more uops, which are simple operations executed by the CPU's execution units.">uop</abbr> cache boundary (32 bytes up to and including Broadwell, 64 bytes in Skylake and beyond) will always take 2 cycles, since two <abbr title="Micro-operation: instructions are translated into one or more uops, which are simple operations executed by the CPU's execution units.">uop</abbr> cache entries are involved. It is not unusual to find small loops which normally take as little as 1 cycle split by such boundaries suddenly taking 2 cycles.</li>
  <li>Instructions which use microcode, such as gather (pre-Skylake) have additional restrictions and throughput limitations.</li>
  <li>The <abbr title="Lysergic acid diethylamide or Loop stream detector, but in the context of this blog probably the latter: The so-called loop buffer that can cache small loops of up to ~64 uops on recent Intel architectures. Not actually a separate structure: the hardware justs locks the loop down in the IDQ.">LSD</abbr> suffers from reduced throughput at the boundary between one iteration and the next, although hardware unrolling reduces the impact of the effect. Full details <a href="https://stackoverflow.com/a/39940932">are on Stack Overflow</a>. Note that the <abbr title="Lysergic acid diethylamide or Loop stream detector, but in the context of this blog probably the latter: The so-called loop buffer that can cache small loops of up to ~64 uops on recent Intel architectures. Not actually a separate structure: the hardware justs locks the loop down in the IDQ.">LSD</abbr> is disabled on most recent CPUs due to a bug. It is re-enabled on some of the most recent chips (<abbr title="Intel's Cannon Lake (client) architecture, the i3-8121U was the only SKU ever released">CNL</abbr> and maybe Cascade Lake).</li>
</ul>

<p>Again, this is only scratching the surface - see Agner for a comprehensive treatment.</p>

<h2 id="store-throughput-limit">Store Throughput Limit</h2>

<p><strong>1 store per cycle</strong></p>

<p>Modern Intel and AMD CPUs can perform at most one store per cycle. No matter what, you won’t exceed that. For many algorithms that make a predictable number of stores, this is a useful upper bound on a performance. For example, a 32-bit radix sort that makes 4 passes and does a store per element for each pass will never operate faster than 4 cycles per element (in radix sort, actual performance usually ends up much worse so this isn’t the dominant factor for most implementations).</p>

<p>This limit applies also to vector scatter instructions, where each element counts as “one” against this limit. Like loads, a store that crosses a cache line counts as two, but other unaligned stores only count as one on Intel. On AMD the situation is more complicated: the penalties for stores that cross a boundary is larger, and it’s not just 64-byte boundaries that matter - more <a href="https://www.realworldtech.com/forum/?threadid=176780&amp;curpostid=176849">details here</a>.</p>

<h3 id="split-cache-lines-1">Split Cache Lines</h3>

<p>On Intel, stores that cross a cache line boundary (64 bytes) count as two, but stores of any other alignment suffer no penalty.</p>

<p>On AMD Zen, any store which crosses a 16 byte boundary suffers a significant penalty: such stores can only execute one per <em>five</em> cycles, so maybe you should count these as five for the purposes of this limit. However, it is possible that this penalty isn’t cumulative with other stores but just represents worst case where many such stores occur back-to-back but the performance when mixed with non-crossing stores is better than this worst case. For example 5 non-crossing store + 1 crossing one might not count as 10 but rather 6 or 7. More testing needed on that one. Suffice it to say you should avoid boundary-crossing stores if you can.</p>

<h3 id="remedies-5">Remedies</h3>

<p>Remove unnecessary stores from your core loops. If you are often storing the same value repeatedly to the same location, it can even be profitable to check that the value is different, which requires a load, and only do the store if different, since this can replace a store with a load. Most of all, you want to take advantage of vectorized stores if possible: you can do 8x 32-bit stores in one cycle with a single vectorized store. Of course, if your stores are not contiguous, this will be difficult or impossible.</p>

<h2 id="complex-addressing-limit">Complex Addressing Limit</h2>

<p><strong>Max of 1 load (any addressing) concurrent with a store with complex addressing per cycle.</strong></p>

<p><em>This limit is Intel specific.</em></p>

<p>The load and store limits above are written as if they are independent. That is, they imply that you can do 2 loads <strong>and</strong> 1 store per cycle. Sometimes that is true, but it depends on the addressing modes used.</p>

<p>Each load and store operation needs an <em>address generation</em> which happens in an <abbr title="Address Generation Unit">AGU</abbr>. There are three AGUs on modern Intel chips: <abbr title="port 2 (load/store AGU)">p2</abbr>, <abbr title="port 3 (load/store AGU)">p3</abbr> and <abbr title="port 7 (limited store AGU)">p7</abbr>. However, <abbr title="port 7 (limited store AGU)">p7</abbr> is restricted: it can <em>only</em> be used by stores, and it can only be used if the store addressing mode is simple. <a href="https://stackoverflow.com/a/51664696">Simple addressing</a> is anything that is of the form <code>[base_reg + offset]</code> where <code>offset</code> is in <code>[0, 2047]</code>. So <code>[rax + 1024]</code> is simple addressing, but all of <code>[rax + 4096]</code>, <code>[rax + rcx * 2]</code> and <code>[rax * 2]</code> are not.</p>

<p>To apply this limit, count <em>all</em> load and any stores with complex addressing: these operations cannot execute at more than 2 per cycle.</p>

<h3 id="remedies-6">Remedies</h3>

<p>At the assembly level, the main remedy is make sure that your stores use simple addressing modes. Usually you do this by incrementing a pointer by the size of the element rather indexed addressing modes.</p>

<p>That is, rather than this:</p>

<div><div><pre><code><span>mov</span> <span>[</span><span>rdi</span> <span>+</span> <span>rax</span><span>*</span><span>4</span><span>]</span><span>,</span> <span>rdx</span>
<span>add</span> <span>rax</span><span>,</span> <span>1</span>
</code></pre></div></div>

<p>You want this:</p>

<div><div><pre><code><span>mov</span> <span>[rdi],</span> <span>rdx</span>
<span>add</span> <span>rdi</span><span>,</span> <span>4</span>
</code></pre></div></div>

<p>Of course, that’s often simpler said than done: indexed addressing modes are very useful for using a single loop counter to access multiple arrays, and also when the value of the loop counter is directly used in the loop (as opposed to simply being used for addressing). For example, consider the following loop which writes the element-wise sum of two arrays to a third array:</p>

<div><div><pre><code><span>void</span> <span>sum</span><span>(</span><span>const</span> <span>int</span> <span>*</span><span>a</span><span>,</span> <span>const</span> <span>int</span> <span>*</span><span>b</span><span>,</span> <span>int</span> <span>*</span><span>d</span><span>,</span> <span>size_t</span> <span>len</span><span>)</span> <span>{</span>
    <span>for</span> <span>(</span><span>size_t</span> <span>i</span> <span>=</span> <span>0</span><span>;</span> <span>i</span> <span>&lt;</span> <span>len</span><span>;</span> <span>i</span><span>++</span><span>)</span> <span>{</span>
        <span>d</span><span>[</span><span>i</span><span>]</span> <span>=</span> <span>a</span><span>[</span><span>i</span><span>]</span> <span>+</span> <span>b</span><span>[</span><span>i</span><span>];</span>
    <span>}</span>
<span>}</span>
</code></pre></div></div>

<p>The loop compiles to the following assembly:</p>

<div><div><pre><code><span>.</span><span>L3</span><span>:</span>
    <span>mov</span>     <span>r8d</span><span>,</span> <span>DWORD</span> <span>PTR</span> <span>[</span><span>rsi</span><span>+</span><span>rax</span><span>*</span><span>4</span><span>]</span>
    <span>add</span>     <span>r8d</span><span>,</span> <span>DWORD</span> <span>PTR</span> <span>[</span><span>rdi</span><span>+</span><span>rax</span><span>*</span><span>4</span><span>]</span>
    <span>mov</span>     <span>DWORD</span> <span>PTR</span> <span>[</span><span>rdx</span><span>+</span><span>rax</span><span>*</span><span>4</span><span>]</span><span>,</span> <span>r8d</span>
    <span>add</span>     <span>rax</span><span>,</span> <span>1</span>
    <span>cmp</span>     <span>rcx</span><span>,</span> <span>rax</span>
    <span>jne</span>     <span>.</span><span>L3</span>
</code></pre></div></div>

<p>This loop will be limited by the complex addressing limitation to 1.5 cycles per iteration, since there are 1 store that uses complex addressing, plus one load.</p>

<p>We could use separate pointers for each array and increment all of them, like:</p>

<div><div><pre><code><span>.</span><span>L3</span><span>:</span>
    <span>mov</span>     <span>r8d</span><span>,</span> <span>DWORD</span> <span>PTR</span> <span>[rsi]</span>
    <span>add</span>     <span>r8d</span><span>,</span> <span>DWORD</span> <span>PTR</span> <span>[rdi]</span>
    <span>mov</span>     <span>DWORD</span> <span>PTR</span> <span>[rdx],</span> <span>r8d</span>
    <span>add</span>     <span>rsi</span><span>,</span> <span>4</span>
    <span>add</span>     <span>rdi</span><span>,</span> <span>4</span>
    <span>add</span>     <span>rdx</span><span>,</span> <span>4</span>
    <span>cmp</span>     <span>rcx</span><span>,</span> <span>rdx</span>
    <span>jne</span>     <span>.</span><span>L3</span>
</code></pre></div></div>

<p>Everything uses simple addressing, great! However, we’ve added two uops and so the speed limit is pipeline width: 7/4 = 1.75, so it will probably be slower than before.</p>

<p>The trick is to only use simple addressing for the store, and calculate the load addresses relative to the store address:</p>

<div><div><pre><code><span>.</span><span>L3</span><span>:</span>
    <span>mov</span>     <span>eax</span><span>,</span> <span>DWORD</span> <span>PTR</span> <span>[</span><span>rdx</span><span>+</span><span>rsi</span><span>]</span> <span>; rsi and rdi have been adjusted so that</span>
    <span>add</span>     <span>eax</span><span>,</span> <span>DWORD</span> <span>PTR</span> <span>[</span><span>rdx</span><span>+</span><span>rdi</span><span>]</span> <span>; rsi+rdx points to a and rdi+rdx to b</span>
    <span>mov</span>     <span>DWORD</span> <span>PTR</span> <span>[rdx],</span> <span>eax</span>
    <span>add</span>     <span>rdx</span><span>,</span> <span>4</span>
    <span>cmp</span>     <span>rcx</span><span>,</span> <span>rdx</span>
    <span>ja</span>      <span>.</span><span>L3</span>
</code></pre></div></div>

<p>When working in a higher level language, you may not always be able to convince the compiler to generate the code we want as it might simply see through our transformations. In this case, however, <a href="https://godbolt.org/z/PPutUu">we can convince</a> gcc to generate the code we want by writing out the transformation ourselves:</p>

<div><div><pre><code><span>void</span> <span>sum2</span><span>(</span><span>const</span> <span>int</span> <span>*</span><span>a</span><span>,</span> <span>const</span> <span>int</span> <span>*</span><span>b</span><span>,</span> <span>int</span> <span>*</span><span>d</span><span>,</span> <span>size_t</span> <span>len</span><span>)</span> <span>{</span>
    <span>int</span> <span>*</span><span>end</span> <span>=</span> <span>d</span> <span>+</span> <span>len</span><span>;</span>
    <span>ptrdiff_t</span> <span>a_offset</span> <span>=</span> <span>(</span><span>a</span> <span>-</span> <span>d</span><span>);</span>
    <span>ptrdiff_t</span> <span>b_offset</span> <span>=</span> <span>(</span><span>b</span> <span>-</span> <span>d</span><span>);</span>
    <span>for</span> <span>(;</span> <span>d</span> <span>&lt;</span> <span>end</span><span>;</span> <span>d</span><span>++</span><span>)</span> <span>{</span>
        <span>*</span><span>d</span> <span>=</span> <span>*</span><span>(</span><span>d</span> <span>+</span> <span>a_offset</span><span>)</span> <span>+</span> <span>*</span><span>(</span><span>d</span> <span>+</span> <span>b_offset</span><span>);</span>
    <span>}</span>
<span>}</span>
</code></pre></div></div>

<p>This is UB all over the place if you pass in arbitrary arrays, because we subtract unrelated pointers (<code>a - d</code>) and use pointer arithmetic which outside of the bounds of the original array (<code>d + a_offset</code>) - but I’m not aware of any compiler that will take advantage of this (as a standalone function it seems unlikely that will ever be the case: because the arrays all <em>could</em> be related, so the function isn’t always UB). Still you should avoid stuff like this unless you have a <em>really</em> good reason to push the boundaries. You could achieve the same effect with <code>uintptr_t</code> which isn’t UB but only unspecified, and that will work on every platform I’m aware of.</p>

<p>Another way to get simple addressing without adding too much overhead for separate loop pointers is to unroll the loop a little bit. The increment only needs to be done once per iteration, so every unroll reduces the cost.</p>

<p>Note that even if stores have non-complex addressing, it may not be possible to sustain 2 loads/1 store, because the store may sometimes choose one of the port 2 or port 3 AGUs instead, starving a load that cycle.</p>

<h2 id="taken-branches">Taken Branches</h2>

<p><strong>Intel: 1 per 2 cycles (see exception below)</strong></p>

<p>If you believe the instruction tables, one taken branch can be executed per cycle, but experiments show that this is true only for very small loops with a single backwards branch. For larger loops or any forward branches, the limit is 1 per 2 cycles.</p>

<p>So avoid many dense taken branches: organize the likely path instead as untaken. This is something you want to do anyways for front-end throughput and code density.</p>

<h2 id="out-of-order-limits">Out of Order Limits</h2>

<p>Here we will cover several limits which all affect the effective window over which the processor can reorder instructions. These limits all have the same pattern: in order to execute instructions out of order, the CPU needs to track in-flight operations in certain structures. If any of these structures becomes full, the effect is the same: no more operations are issued until space in that structure is freed. Already issued instructions can still execute, but no more operations else will enter the pool of waiting ops. In general, we talk about the <em><abbr title="Out-of-order execution allows CPUs to execute instructions out of order with respect to the source.">out-of-order</abbr> window</em> which is roughly the number of instructions/operations that can be in progress, counting from the oldest in-progress instruction to the newest. The limits in this section put an effective limit on this window.</p>

<p>While the effect is the same for each limit, the size of the structures and which operations that are tracked in them vary, so we focus on describing that.</p>

<p>Note that the size of the window is not a hard performance limit in itself: you can’t use it to directly establish an upper bound on cycles per iterations or whatever (i.e., the units for the window aren’t “per cycle”) - but you can use it in concert with other analysis to refine the estimate.</p>

<p>Until now, we have been implicitly assuming an <em>infinite</em> out of order window. That’s why we said, for example, that only loop carried dependencies matter when calculating dependency chains; the implicit assumption is that there is enough <abbr title="Out-of-order execution allows CPUs to execute instructions out of order with respect to the source.">out-of-order</abbr> magic to reorder different loop iterations to hide the effect of all the other chains. Of course, on real CPUs, there is a limit to the magic: if your loops have 1,000 instructions per iteration, and the <abbr title="Out-of-order execution allows CPUs to execute instructions out of order with respect to the source.">out-of-order</abbr> window is only 100 instructions, the CPU will not be able to overlap the much of each iteration at all: the different iterations are too far apart in instruction stream for significant overlap.</p>

<p>All the discussion here refers to the <em>dynamic instruction stream</em> - which is the actual stream of instructions seen by the CPU. This is opposed to the static instruction stream, which is the series of instructions as they appear in the binary. Inside a <abbr title="a straight-line code sequence with no branches in except to the entry and no branches out except at the exit (Wikipedia).">basic block</abbr>, static and dynamic instruction streams are the same: the difference is that the dynamic stream follows all jumps, so it is a trace of actual execution.</p>

<p>For example, take the following nested loops, with inner and outer iteration counts of 2 and 4:</p>

<div><div><pre><code>    <span>xor</span> <span>rdx</span><span>,</span> <span>rdx</span>
    <span>mov</span> <span>rax</span><span>,</span> <span>2</span>

<span>outer</span><span>:</span>
    <span>mov</span> <span>rcx</span><span>,</span> <span>4</span>

<span>inner</span><span>:</span>
    <span>add</span> <span>rdx</span><span>,</span> <span>rcx</span>
    <span>dec</span> <span>rcx</span>
    <span>jnz</span> <span>inner</span>

    <span>dec</span> <span>rax</span>
    <span>jnz</span> <span>outer</span>
</code></pre></div></div>

<p>The static instruction stream is just want you see above, 8 instructions in total. The dynamic instruction stream traces what happens at runtime, so the inner loop appears 8 times, for example:</p>

<div><div><pre><code>    <span>xor</span> <span>rdx</span><span>,</span> <span>rdx</span>
    <span>mov</span> <span>rax</span><span>,</span> <span>2</span>

    <span>; first iteration of outer loop</span>
    <span>mov</span> <span>rcx</span><span>,</span> <span>4</span>

    <span>; inner loop 4x</span>
    <span>add</span> <span>rdx</span><span>,</span> <span>rcx</span>
    <span>dec</span> <span>rcx</span>
    <span>jnz</span> <span>inner</span>

    <span>add</span> <span>rdx</span><span>,</span> <span>rcx</span>
    <span>dec</span> <span>rcx</span>
    <span>jnz</span> <span>inner</span>

    <span>add</span> <span>rdx</span><span>,</span> <span>rcx</span>
    <span>dec</span> <span>rcx</span>
    <span>jnz</span> <span>inner</span>

    <span>add</span> <span>rdx</span><span>,</span> <span>rcx</span>
    <span>dec</span> <span>rcx</span>
    <span>jnz</span> <span>inner</span>

    <span>jnz</span> <span>outer</span>

    <span>; second iteration of outer loop</span>
    <span>mov</span> <span>rcx</span><span>,</span> <span>4</span>

    <span>; inner loop 4x</span>
    <span>add</span> <span>rdx</span><span>,</span> <span>rcx</span>
    <span>dec</span> <span>rcx</span>
    <span>jnz</span> <span>inner</span>

    <span>add</span> <span>rdx</span><span>,</span> <span>rcx</span>
    <span>dec</span> <span>rcx</span>
    <span>jnz</span> <span>inner</span>

    <span>add</span> <span>rdx</span><span>,</span> <span>rcx</span>
    <span>dec</span> <span>rcx</span>
    <span>jnz</span> <span>inner</span>

    <span>add</span> <span>rdx</span><span>,</span> <span>rcx</span>
    <span>dec</span> <span>rcx</span>
    <span>jnz</span> <span>inner</span>

    <span>jnz</span> <span>outer</span>

    <span>; done!</span>
</code></pre></div></div>

<p>All that to say that when you are thinking about out of order window, you have to think about the dynamic instruction/<abbr title="Micro-operation: instructions are translated into one or more uops, which are simple operations executed by the CPU's execution units.">uop</abbr> stream, not the static one. For a loop body with no jumps or calls, you can ignore this distinction. We also talk about <em>older</em>, <em>oldest</em>, <em>youngest</em>, etc instructions - this simply refers to the relative position of instructions or operations in the dynamic stream: the first encountered instructions are the oldest (in the stream above, <code>xor rdx, rdx</code> is the oldest) and the most recently encountered instructions are the youngest.</p>

<p>With that background out of the way, let’s look at the various <abbr title="Out-of-order execution allows CPUs to execute instructions out of order with respect to the source.">OoO</abbr> limits next. Most of these limits have the same <em>effect</em> which is to limit the available <abbr title="Out-of-order execution allows CPUs to execute instructions out of order with respect to the source.">out-of-order</abbr> window, stalling issue until a resource becomes available. They differ mostly in <em>what</em> they count, and how many of that thing can be buffered.</p>

<p><a name="ooo-table"></a>First, here’s a big table of all the resource sizes<sup id="fnref:snote"><a href="#fn:snote">21</a></sup> we’ll talk about the following sections.</p>

<table>
  <thead>
    <tr>
      <th>Vendor</th>
      <th>Uarch</th>
      <th><abbr title="Re-order buffer: n ordered buffer which stores in-progress instructions on an out-of-order processor.">ROB</abbr> Size</th>
      <th>Sched (RS)</th>
      <th>Load Buffer</th>
      <th>Store Buffer</th>
      <th>Integer <abbr title="Physical register file: The hardware registers used for renaming architectural (source visible) registers, usually much larger in number than the architectural register count.">PRF</abbr></th>
      <th>Vector <abbr title="Physical register file: The hardware registers used for renaming architectural (source visible) registers, usually much larger in number than the architectural register count.">PRF</abbr></th>
      <th>Branches (BOB)</th>
      <th>Calls</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Intel</td>
      <td>Sandy Bridge</td>
      <td>168</td>
      <td>54</td>
      <td>64</td>
      <td>36</td>
      <td>160</td>
      <td>144</td>
      <td>48</td>
      <td>15</td>
    </tr>
    <tr>
      <td>Intel</td>
      <td>Ivy Bridge</td>
      <td>168</td>
      <td>54</td>
      <td>64</td>
      <td>36</td>
      <td>160</td>
      <td>144</td>
      <td>48</td>
      <td>15</td>
    </tr>
    <tr>
      <td>Intel</td>
      <td>Haswell</td>
      <td>192</td>
      <td>60</td>
      <td>72</td>
      <td>42</td>
      <td>168</td>
      <td>168</td>
      <td>48</td>
      <td>14</td>
    </tr>
    <tr>
      <td>Intel</td>
      <td>Broadwell</td>
      <td>192</td>
      <td>64</td>
      <td>72</td>
      <td>42</td>
      <td>168</td>
      <td>168</td>
      <td>48</td>
      <td>14</td>
    </tr>
    <tr>
      <td>Intel</td>
      <td>Skylake[-X]</td>
      <td>224</td>
      <td>97</td>
      <td>72</td>
      <td>56</td>
      <td>180</td>
      <td>168</td>
      <td>48</td>
      <td>14?</td>
    </tr>
    <tr>
      <td>Intel</td>
      <td>Sunny Cove</td>
      <td>352</td>
      <td>?</td>
      <td>128</td>
      <td>72</td>
      <td>?</td>
      <td>?</td>
      <td>48</td>
      <td>?</td>
    </tr>
    <tr>
      <td>AMD</td>
      <td>Zen</td>
      <td>192</td>
      <td>180<sup id="fnref:zensched"><a href="#fn:zensched">22</a></sup></td>
      <td>72</td>
      <td>44</td>
      <td>168</td>
      <td>160</td>
      <td>?</td>
      <td>?</td>
    </tr>
    <tr>
      <td>AMD</td>
      <td>Zen2</td>
      <td>224</td>
      <td>188<sup id="fnref:zen2sched"><a href="#fn:zen2sched">23</a></sup></td>
      <td>?</td>
      <td>48</td>
      <td>180</td>
      <td>160</td>
      <td>?</td>
      <td>?</td>
    </tr>
  </tbody>
</table>

<h3 id="reorder-buffer-size">Reorder Buffer Size</h3>

<p>The <abbr title="Re-order buffer: n ordered buffer which stores in-progress instructions on an out-of-order processor.">ROB</abbr> is the largest and most general out of order buffer: all uops, even those that don’t execute such as <code>nop</code> or zeroing idioms, take a slot in the <abbr title="Re-order buffer: n ordered buffer which stores in-progress instructions on an out-of-order processor.">ROB</abbr>. This structure holds instructions from the point at which they are allocated (issued, in Intel speak) until they retire. It puts a hard upper limit on the <abbr title="Out-of-order execution allows CPUs to execute instructions out of order with respect to the source.">OoO</abbr> window as measured from the oldest un-retired instruction to the youngest instruction that can be issued. On Intel, the <abbr title="Re-order buffer: n ordered buffer which stores in-progress instructions on an out-of-order processor.">ROB</abbr> holds micro-fused ops, so the size is measured in the fused-domain.</p>

<p>As an example, a load instruction takes a cache miss which means it cannot retire until the miss is complete. Let’s say the load takes 300 cycles to finish, which is a typical latency. Then, on an Haswell machine with a <abbr title="Re-order buffer: n ordered buffer which stores in-progress instructions on an out-of-order processor.">ROB</abbr> size of 192, <em>at most</em> 191 additional instructions can execute while waiting for the load: at that point the <abbr title="Re-order buffer: n ordered buffer which stores in-progress instructions on an out-of-order processor.">ROB</abbr> window is exhausted and the core stalls. This puts an upper bound on the maximum <abbr title="Instructions per cycle: calculated over an interval by measuring the number of instructions executed and the duration in cycles.">IPC</abbr> of the region of 192 / 300 = 0.64. It also puts a bound on the maximum <abbr title="Memory level parallelism: having multiple misses to memory outstanding from a single core. When used as a metric, it refers to the average number of outstanding requests over some period.">MLP</abbr> achievable, since only loads that appear in the next 191 instructions can (potentially) execute in parallel with the original miss. In fact, this behavior is used by Henry Wong’s <a href="https://github.com/travisdowns/robsize">robsize tool</a> to measure the <abbr title="Re-order buffer: n ordered buffer which stores in-progress instructions on an out-of-order processor.">ROB</abbr> size and other <abbr title="Out-of-order execution allows CPUs to execute instructions out of order with respect to the source.">OoO</abbr> buffer sizes, using a missed load followed by a series of filler instructions and finally another load miss. By varying the number of filler instructions and checking whether the loads executed in parallel or serially, the <abbr title="Re-order buffer: n ordered buffer which stores in-progress instructions on an out-of-order processor.">ROB</abbr> size can be <a href="http://blog.stuffedcow.net/2013/05/measuring-rob-capacity/">determined experimentally</a>.</p>

<h4 id="remedies-7">Remedies</h4>

<p>If you are hitting the <abbr title="Re-order buffer: n ordered buffer which stores in-progress instructions on an out-of-order processor.">ROB</abbr> size limit, you should switch from optimizing the code for the usual metrics and instead try to reduce the number of uops. For example, a slower (longer latency, less throughput) instruction can be used to replace two instructions which would otherwise be faster. Similarly, micro-fusion helps because the <abbr title="Re-order buffer: n ordered buffer which stores in-progress instructions on an out-of-order processor.">ROB</abbr> limit counts in the fused domain.</p>

<p>Reorganizing the instruction stream can help too: if you hit the <abbr title="Re-order buffer: n ordered buffer which stores in-progress instructions on an out-of-order processor.">ROB</abbr> limit after a specific long-latency instruction (usually a load miss) you may want to move expensive instructions into the shadow of that instruction so they can execute while the long latency instruction executes. In this way, there will be less work to do when the instruction completes. Similarly, you may want to “jam” loads that miss together: rather than spreading them out where they would naturally occur, putting them close together allows more of them to fit in the <abbr title="Re-order buffer: n ordered buffer which stores in-progress instructions on an out-of-order processor.">ROB</abbr> window.</p>

<p>In the specific case of load misses, software prefetching can help a lot: it enables you to start a load early, but prefetches can retire before the load completes, so there is no stalling. For example, if you issue the prefetch 200 instructions before the <abbr title="A true load that appears in the source code or assembly, as opposed to loads initiated by software or hardware prefetch.">demand load</abbr> instruction, you have essentially broadened the <abbr title="Re-order buffer: n ordered buffer which stores in-progress instructions on an out-of-order processor.">ROB</abbr> by 200 instructions as it applies to that load.</p>

<h3 id="load-buffer">Load Buffer</h3>

<p>Every load operation, needs a load buffer entry. This means the total <abbr title="Out-of-order execution allows CPUs to execute instructions out of order with respect to the source.">OoO</abbr> window is limited by the number loads appearing in the window. Typical load buffer sizes (72 on <abbr title="Intel's Skylake (client) architecture, aka 6th Generation Intel Core i3,i5,i7">SKL</abbr>) seem to be about one third of the <abbr title="Re-order buffer: n ordered buffer which stores in-progress instructions on an out-of-order processor.">ROB</abbr> size, so if more than about one out of three operations is a load, you are more likely to be limited by the load buffer than the <abbr title="Re-order buffer: n ordered buffer which stores in-progress instructions on an out-of-order processor.">ROB</abbr>.</p>

<p>Gathers need as many entries as there are loaded elements to load in the gather. Sometimes loads are hidden - remember that things like <code>pop</code> involve a load: in general anything that executes an op on <code>p2</code> or <code>p3</code> which is not a store (i.e., does not execute anything on <code>p4</code>) needs an entry in the load buffer.</p>

<h4 id="remedies-8">Remedies</h4>

<p>First, you should evaluate whether getting under this limit will be helpful: it may be that you will almost immediately hit another <abbr title="Out-of-order execution allows CPUs to execute instructions out of order with respect to the source.">OoO</abbr> limit, and it also may be that increasing the <abbr title="Out-of-order execution allows CPUs to execute instructions out of order with respect to the source.">OoO</abbr> window isn’t that useful if the extra included instructions can’t execute or aren’t a bottleneck.</p>

<p>In any case, the remedy is to use fewer loads, or in some cases to reorganize loads relative to other instructions so that the window implied by the full load buffer contains the most useful instructions (in particular, contains long latency instructions like load misses). You can try to combine narrower loads into wider ones. You can ensure you keep values in registers as much as possible, and inline functions that would otherwise pass arguments through memory (e.g., certain structures) to avoid pointless loads. If you need to spill some registers, consider spilling registers to <code>xmm</code> or <code>ymm</code> vector registers rather than the stack.</p>

<h3 id="store-buffer">Store Buffer</h3>

<p>Similarly to the load buffer, the store buffer is required for every operation that involves a store. In fact, filling up the store buffer is pretty much the only way stores can bottleneck performance. Unlike loads, nobody is waiting for a store to complete, except in the case of store-to-load forwarding - but there, by definition, the value is sitting inside the store queue ready to use, so there is no equivalent of the long load miss which blocks dependent operations. You can have long store misses, but they happen after the store has already retired and is sitting in the store buffer (or write-combining buffer). So stores primarily cause a problem if there are enough of them such that the store buffer fill up.</p>

<p>Store buffers are usually smaller than load buffers, about two thirds the size, typically. This reflects the fact that most programs have more loads than stores.</p>

<h4 id="remedies-9">Remedies</h4>

<p>Similar to the load buffer, you want less stores. Ensure you aren’t doing unnecessary spilling to the stack, that you merge stores where possible, that you aren’t doing dead stores (e.g., zeroing a structure before immediately overwriting it anyways) and so on. On some platform giving the compiler more information about array of structure alignment helps it merge stores.</p>

<p>Vectorization of loops with consecutive stores helps a lot since it can turn (for example) 8 32-bit stores into a single 256-bit store, which only takes one entry in the store buffer.</p>

<p>Scatter operations available in AVX-512 don’t really help: they take one store buffer entry per element stored.</p>

<h3 id="scheduler">Scheduler</h3>

<p>After an op is issued, it sits in the reservation station (scheduler) until it is able to execute. This structure is generally much smaller than the <abbr title="Re-order buffer: n ordered buffer which stores in-progress instructions on an out-of-order processor.">ROB</abbr>, about 40-90 entries on modern chips. If this structure fills up, no more operations can issue, even if the other structures have plenty of room. This will occur if there are too many instructions dependent on earlier instructions which haven’t completed yet. A typical example is a load which misses in the cache, followed by many instructions which depend on that load. Those instructions won’t leave the scheduler until the load completes, and if they are enough to fill the structure no further instructions will be evaluated.</p>

<h4 id="remedies-10">Remedies</h4>

<p>Organize your code so that there are some independent instructions to execute following long latency operations, which don’t depend on the result of those operations.</p>

<p>Consider replacing data dependencies (e.g., conditional moves or other arithmetic) with control dependencies, since the latter are predicted and don’t cause a dependency. This also has the advantage of executing many more instructions in parallel, but may lead to branch mispredictions.</p>

<h3 id="register-file-size-limit">Register File Size Limit</h3>

<p>Every instruction with a destination register requires a renamed physical register, which is only reclaimed when the instruction is retired. These registers come from the <em>physical regsiter file</em> (<abbr title="Physical register file: The hardware registers used for renaming architectural (source visible) registers, usually much larger in number than the architectural register count.">PRF</abbr>). So to fill the entire <abbr title="Re-order buffer: n ordered buffer which stores in-progress instructions on an out-of-order processor.">ROB</abbr> with operations that require a destination register, you’ll need a <abbr title="Physical register file: The hardware registers used for renaming architectural (source visible) registers, usually much larger in number than the architectural register count.">PRF</abbr> as large as the <abbr title="Re-order buffer: n ordered buffer which stores in-progress instructions on an out-of-order processor.">ROB</abbr>. In practice, there are two separate register files on Intel and AMD chips: the integer registers file used for scalar registers such as <code>rax</code> and the vector register file used for <abbr title="Single Instruction Multiple Data: an ISA type or ISA extension like Intel's AVX or ARM's NEON that can perform multiple identical operations on elements packed into a SIMD register.">SIMD</abbr> registers such as <code>xmm0</code>, <code>ymm0</code> and <code>zmm0</code>, and the sizes of these register files as shown above are somewhat smaller than the <abbr title="Re-order buffer: n ordered buffer which stores in-progress instructions on an out-of-order processor.">ROB</abbr> size.</p>

<p>Not all of the registers are actually available for renaming: some are used to store the non-speculative values of the architectural registers, or for other purposes, so the available number of register is about 16 to 32 less than the values shown above. Henry Wong has a great description of observed available registers on the <a href="http://blog.stuffedcow.net/2013/05/measuring-rob-capacity/">article</a> I linked earlier, including some non-ideal behaviors that I’ve glossed over here. You can calculate the number of available registers on new architectures using the <a href="https://github.com/travisdowns/robsize">robsize tool</a>.</p>

<p>The upshot is that for given <abbr title="Re-order buffer: n ordered buffer which stores in-progress instructions on an out-of-order processor.">ROB</abbr> sizes, there are only enough registers available in each file for about 75% of the entries.</p>

<p>In practice, some instructions such as branches, zeroing idioms<sup id="fnref:rmwnote"><a href="#fn:rmwnote">24</a></sup> don’t consume <abbr title="Physical register file: The hardware registers used for renaming architectural (source visible) registers, usually much larger in number than the architectural register count.">PRF</abbr> entries, which limit you hit depends on that ratio. Since integer and FP PRFs are distinct on recent Intel, you can consume from each <abbr title="Physical register file: The hardware registers used for renaming architectural (source visible) registers, usually much larger in number than the architectural register count.">PRF</abbr> independently: meaning that vectorized code mixed with at least some <abbr title="General purpose: as opposed to SIMD or FP. On x86 often refers to instructions such as integer addition, or registers such as eax.">GP</abbr> code is unlikely to hit the <abbr title="Physical register file: The hardware registers used for renaming architectural (source visible) registers, usually much larger in number than the architectural register count.">PRF</abbr> limit before it hits the <abbr title="Re-order buffer: n ordered buffer which stores in-progress instructions on an out-of-order processor.">ROB</abbr> limit.</p>

<p>The effect of hitting the <abbr title="Physical register file: The hardware registers used for renaming architectural (source visible) registers, usually much larger in number than the architectural register count.">PRF</abbr> limit is the same as the <abbr title="Re-order buffer: n ordered buffer which stores in-progress instructions on an out-of-order processor.">ROB</abbr> size limit.</p>

<h4 id="remedies-11">Remedies</h4>

<p>There’s not all much you can do for this one beyond the stuff discussed in the <abbr title="Re-order buffer: n ordered buffer which stores in-progress instructions on an out-of-order processor.">ROB</abbr> limit entry section. Maybe try to mix integer and vector code so you consume from each register file. Make sure you are using zeroing idioms like <code>xor eax,eax</code> rather than <code>mov eax, 0</code> but you should already be doing that.</p>

<h3 id="branches-in-flight">Branches in Flight</h3>

<p><strong>Intel: Maximum of 48 branches in flight</strong></p>

<p>Modern Intel chips seem to have a limit of branches <em>in flight</em>, where <em>in flight</em> refers to branches that have not yet retired, usually because some older operation hasn’t yet completed. I first saw this limit described and measured <a href="http://blog.stuffedcow.net/2018/04/ras-microbenchmarks/#inflight">here</a>, although it seems like <a href="https://www.realworldtech.com/haswell-cpu/3/">David Kanter had the scoop</a> way back in 2012:</p>

<blockquote>
  <p>The branch order buffer, which is used to rollback to known good architectural state in the case of a misprediction is still 48 entries, as with Sandy Bridge.</p>
</blockquote>

<p>The effects of exceeding the branch order buffer limit are the same as for the <abbr title="Re-order buffer: n ordered buffer which stores in-progress instructions on an out-of-order processor.">ROB</abbr> limit.</p>

<p><em>Branches</em> here refers to both conditional jumps (<code>jcc</code> where <code>cc</code> is some conditional code) and indirect jumps (things like <code>jmp [rax]</code>).</p>

<h4 id="remedies-12">Remedies</h4>

<p>Although you will rarely hit this limit, the solution is fewer branches. Try to move unnecessary checks out of the hot path, or combine several checks into one. Try to organize multi-predicate conditions such that you can short-circuit the evaluation after the first check (so the subsequent checks don’t appear in the dynamic instruction stream). Consider replacing N 2-way (true/false) conditional jumps with one indirect jump with N^2 targets as this counts as only “one” instead of N against the branch limit. Consider conditional moves or other branch-free techniques.</p>

<p>Ensure that branches can retire as soon as possible, although in practice there often isn’t much opportunity to do this when dealing with already well-compiled code.</p>

<p>Note that many of these are the same things you might consider to reduce branch mispredictions, although they apply here even if there are no mispredictions.</p>

<h3 id="calls-in-flight">Calls in Flight</h3>

<p><strong>Intel: 14-15</strong></p>

<p>Only 14-15 calls can be in-flight at once, exactly analogous to the limitation on in-flight branches described above, except it applies to the <code>call</code> instruction rather than branches. As with the branches in-flight restriction, this comes from <a href="http://blog.stuffedcow.net/2018/04/ras-microbenchmarks/#inflight">testing</a> by Henry Wong, and in this case I am not aware of an earlier source.</p>

<h4 id="remedies-13">Remedies</h4>

<p>Reduce the number of call instructions you make. Consider ensuring the calls can be inlined, or partial inlining (a fast path that can be inlined combined with a slow path that isn’t). In extreme cases you might want to replace <code>call</code> + <code>ret</code> pairs with unconditional <code>jmp</code>, saving the return address in a register, plus indirect branch to return to the saved address. I.e. replace the following:</p>

<div><div><pre><code><span>callee</span><span>:</span>
    <span>; function code goes here</span>
    <span>ret</span>

<span>; caller code</span>
    <span>call</span> <span>callee</span>
</code></pre></div></div>

<p>With the following (which is essentially emulating the <a href="https://en.wikibooks.org/wiki/MIPS_Assembly/Control_Flow_Instructions#Jump_and_Link">JAL instruction</a>:</p>

<div><div><pre><code><span>callee</span><span>:</span>
  <span>; function code goes here</span>
  <span>jmp</span> <span>[r15]</span> <span>; return to address stashed in r15</span>

<span>; caller code</span>
    <span>movabs</span> <span>r15</span><span>,</span> <span>next</span>
    <span>jmp</span> <span>callee</span>
<span>next</span><span>:</span>
</code></pre></div></div>

<p>This pattern is hard to achieve in practice in a high level language, although you might have luck emulating it with gcc’s <a href="https://gcc.gnu.org/onlinedocs/gcc/Labels-as-Values.html">labels as values</a> functionality.</p>

<h2 id="thank-you">Thank You</h2>

<p>That’s it for now, if you made it this far I hope you found it useful.</p>

<p>Thanks to Paul A. Clayton, Adrian, Peter E. Fry, anon, nkurz, maztheman, hyperpape, Arseny Kapoulkine, Thomas Applencourt, haberman, caf, Nick Craver, pczarn, Bruce Dawson, Fabian Giesen, glaebhoerl and Matthew Fernandez for pointing out errors and other feedback.</p>

<p>Thanks to Daniel Lemire for providing access to hardware on which I was able to test and verify some of these limits.</p>



<p>I don’t have a comments system yet, so I’m basically just outsourcing discussion to HackerNews right now: <a href="https://news.ycombinator.com/item?id=20157196">here is the thread</a> for this post.</p>

<p>If you liked this post, check out the <a href="https://travisdowns.github.io/">homepage</a> for others you might enjoy<sup id="fnref:noenjoy"><a href="#fn:noenjoy">26</a></sup>.</p>

<hr>
<hr>




  </div><!-- travis override -->
  
    





  

  
</article>

      </div>
    </div></div></div>
    </div>
    <footer>
        <div>created by buildstarted &copy; 2020 <a href="/about">about</a></div>
        <div>Share this page on social media: copy and paste this url https://linksfor.dev/</div>
        <div>If you prefer RSS: <a href="https://linksfor.dev/feed.xml">https://linksfor.dev/feed.xml</a></div>
		<div>Customer satisfaction guaranteed to be optional.</div>
    </footer>
    
    <script async defer>
        _dna = window._dna || {};
        _dna.siteId = "linksfor.devs";
        _dna.outlink = true;

        (function () {
            let dna = document.createElement('script');
            dna.type = 'text/javascript';
            dna.async = true;
            dna.src = '//dna.buildstarted.com/t.js';
            let s = document.getElementsByTagName('script')[0];
            s.parentNode.insertBefore(dna, s);
        })();
    </script>
    <noscript><img src="//dna.buildstarted.com/g?siteId=linksfor.devs"/></noscript>
</body>
</html>