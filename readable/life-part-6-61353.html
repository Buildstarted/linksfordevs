<!DOCTYPE html>
<html lang="en">
<head>
    <title>
Life, part 6 - linksfor.dev(s)    </title>
    <meta charset="utf-8">
    <link rel="alternate" type="application/rss+xml" title="Linksfor.dev(s) feed" href="https://linksfor.dev/feed.rss" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta name="google" value="notranslate">
    <link rel="stylesheet" type="text/css" href="/style.css" />
    <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon">
    <link rel="icon" href="/favicon.ico" type="image/x-icon">
    <meta property="og:title" content="Life, part 6 - linksfor.dev(s)"/>
    <meta property="og:description" content="Code for this episode can be found here. The only interesting change I&#x2019;ve made to the client is that if you press &#x201C;P&#x201D;, it pauses the simulation and runs 5000 steps of the &#x201C;a&#x2026;"/>
    <meta property="og:type" content="website"/>
    <meta property="og:url" content="https://ericlippert.com/2020/05/04/life-part-6/"/>

<meta property="og:site_name" content="linksfor.dev(s)" />
</head>
<body>
    <div class="devring" style="background: #222">
        <div style="text-align:center">Explore other dev related sites in this ring. If you would like to join this ring <a href="https://devring.club">click here</a>.</div>
        <div class="grid">
            <div style="display: grid; grid-template-columns: .5fr 1fr 1fr 1fr; text-align: center;">
                <span class="devring-title"><a href="https://devring.club/">devring.club</a></span>
                <a href="https://devring.club/sites/1/prev" class="devring-previous">Previous</a>
                <a href="https://devring.club/random" class="devring-random">Random</a>
                <a href="https://devring.club/sites/1/next" class="devring-next">Next</a>
            </div>
        </div>
    </div>
    <div class="grid">
        <h1>
<a href="/" style="color:inherit">linksfor.dev(s)</a>
        </h1>
        <title>linksfor.dev(s) - Life, part 6</title>
<div class="readable">
        <h1>Life, part 6</h1>
            <div>Reading time: 11-14 minutes</div>
        <div>Posted here: 04 May 2020</div>
        <p><a href="https://ericlippert.com/2020/05/04/life-part-6/">https://ericlippert.com/2020/05/04/life-part-6/</a></p>
        <hr/>
<div id="readability-page-1" class="page"><div>
		<p data-adtags-visited="true">Code for this episode can be found <a href="https://github.com/ericlippert/ConwaysLife/tree/episode06">here</a>. The only interesting change I’ve made to the client is that if you press “P”, it pauses the simulation and runs 5000 steps of the “acorn” pattern, and then dumps the number of milliseconds that took to a text file. This is a quick and dirty performance test, but as we’ll see, we can get great results even with this primitive tool.</p>
<hr>
<p data-adtags-visited="true"><a href="https://ericlippert.com/2020/04/30/life-part-5/">Last time</a> we demonstrated that a single tick in the naïve “array of bools” algorithm is O(n) in the number of cells, which is not surprising — after all, we do a neighbor count of every cell on every tick. This gives us the insight that if we quadrupled the number of cells, we’d probably quadruple the time to compute a step, which is good to know, but we still don’t know how bad it is, aside from noting that we’re having no problem keeping up with a cadence of two ticks per second of course!</p>
<p data-adtags-visited="true">As I mentioned in the preamble, I wrote the quickest little perf tester you could imagine and ran it on the release version of the app. There is no point in running performance testing on a debug app, or on a release app while it is running in the debugger; remember, the runtime knows it is being debugged and changes the performance characteristics of the program in order to make it easier to debug.</p>
<p data-adtags-visited="true">On my home machine — a decidedly mediocre mid-end PC game box — I got 5000 steps of “acorn” in 17 seconds, which is around 300 steps per second. By 1992 standards, that would be insanely great, but let me tell you, machines are a lot faster today than they were in 1992.</p>
<p data-adtags-visited="true">I did not deliberately “sandbag” this implementation — I did not write it to have a deliberately slow bit — so there’s no trick to figuring out where we’re spending time. I wrote it with an emphasis on clarity and correctness, knowing that we could come back later and find the hot spots; it’s a lot easier to optimize code if it is already clear and correct.</p>
<p data-adtags-visited="true">The question now is: <strong>where would you guess most of the time is being spent in the calculation?</strong> (Remember, we are omitting the time to draw to the bitmap.)</p>
<p data-adtags-visited="true">If you didn’t have a profiler on your machine — and I don’t! I have the free version of VS on my home machine — what would your gut tell you? Because my gut is almost always at least partially wrong when it comes to performance.</p>
<p data-adtags-visited="true">Let’s quickly review the naïve algorithm. The major parts are:</p>
<p data-adtags-visited="true">Test to see if a point is inside our 8-quad of valid cells:</p>
<pre>private bool IsValidPoint(long x, long y) =&gt; 
  0 &lt;= x &amp;&amp; x &lt; width &amp;&amp; 0 &lt;= y &amp;&amp; y &lt; height;
</pre>
<p data-adtags-visited="true">Fetch the value from the array if the point is valid; dead otherwise:</p>
<pre>public bool this[long x, long y]
{
  get
&nbsp;&nbsp;{
&nbsp;&nbsp;  if (IsValidPoint(x, y))
&nbsp;&nbsp;&nbsp;&nbsp;  return cells[x, y];
&nbsp;&nbsp;&nbsp;&nbsp;return false;
&nbsp;&nbsp;}
</pre>
<p data-adtags-visited="true">Count the neighbors:</p>
<pre>private int LivingNeighbors(int x, int y)
{
  int sum = this[x - 1, y - 1] ? 1 : 0;
&nbsp;&nbsp;sum += this[x - 1, y] ? 1 : 0;
&nbsp;&nbsp;sum += this[x - 1, y + 1] ? 1 : 0;
&nbsp;&nbsp;sum += this[x, y - 1] ? 1 : 0;
&nbsp;&nbsp;sum += this[x, y + 1] ? 1 : 0;
&nbsp;&nbsp;sum += this[x + 1, y - 1] ? 1 : 0;
&nbsp;&nbsp;sum += this[x + 1, y] ? 1 : 0;
&nbsp;&nbsp;sum += this[x + 1, y + 1] ? 1 : 0;
&nbsp;&nbsp;return sum;
}
</pre>
<p data-adtags-visited="true">And the main loop:</p>
<pre>bool[,] newCells = new bool[width, height];
for (int y = 0; y &lt; height; y += 1)
  for (int x = 0; x &lt; width; x += 1)
&nbsp;&nbsp;{
  &nbsp; int count = LivingNeighbors(x, y);
&nbsp;&nbsp;&nbsp; newCells[x, y] = count == 3 || (cells[x, y] &amp;&amp; count == 2);
&nbsp;&nbsp;}
cells = newCells;</pre>
<p data-adtags-visited="true">So, no peeking below: without a profiler to guide you, what would you assume was the low-hanging fruit that could be attacked here to make the whole thing a little faster without changing the basic algorithm or data structures?</p>
<hr>
<p data-adtags-visited="true">I find that I’m dead wrong about a third of the time when I look at a piece of code and guess where the time is being spent, and fortunately this was one of the times I guessed correctly. The biggest cost by far in this loop is that it calls IsValidPoint eight times on every loop iteration. <strong>Since there are 65536 loop iterations per tick, that’s over two billion validity checks in our performance run.</strong></p>
<p data-adtags-visited="true">Moreover, the vast majority of the time, the point is valid; the only “invalid” points we generate are when looking for neighbors when on edges.</p>
<p data-adtags-visited="true">How can we improve this? The easiest way is just to note that we have two cases, the interior points where all neighbors are valid, and the edges where not all neighbors are valid. If we are in the former case, which we can check for once, then we can skip all the other validity checks:</p>
<pre>if (1 &lt;= x &amp;&amp; x &lt; width - 1 &amp;&amp; 1 &lt;= y &amp;&amp; y &lt; height - 1)
{
  int sum = cells[x - 1, y - 1] ? 1 : 0;
&nbsp;&nbsp;sum += cells[x - 1, y] ? 1 : 0;
  ...
&nbsp; return sum;
}
else
{
  // as before:
  int sum = this[x - 1, y - 1] ? 1 : 0;
  ...</pre>
<p data-adtags-visited="true">Making this change takes us from 5000 ticks in 17 seconds to <strong>4.4 seconds</strong>.That’s a 4x improvement right there!</p>
<hr>
<p data-adtags-visited="true">That was no surprise at all, but my experiments with making further tweaks to the code were <em>quite surprising</em>. Before I get into the details, let me hasten to say: I have not actually looked at the generated assembly to figure out why the jitted code has non-obvious performance characteristics, but I have some educated guesses. Take everything below with a grain of salt, and if anyone cares to look at the generated machine code to confirm or deny my guesses, I’d be happy to make an update.</p>
<p data-adtags-visited="true">Here are some things I tried to see if they made an improvement or a regression; I’ll list the winners first:</p>
<hr>
<p data-adtags-visited="true">I wondered if we were paying any penalty for all these conditionals:</p>
<pre>int sum = cells[x - 1, y - 1] ? 1 : 0;</pre>
<p data-adtags-visited="true">On the one hand, the Boolean should be represented as a byte that is either zero or one, so the jitter could make the conditional a no-op, right?</p>
<p data-adtags-visited="true">On the other hand, there are sneaky ways to make a Boolean contain a non-zero, non-one value, and those should all be treated as true, but that would make the “turn this conditional into a no-op” optimization invalid.</p>
<p data-adtags-visited="true">Like I said above, I haven’t checked the codegen to see what exactly it does. But when I turned the bool array into a byte array and removed the conditionals, that trimmed an additional 40 microseconds off of each step for a total of 200 milliseconds over 5000 steps.</p>
<hr>
<p data-adtags-visited="true">Allocating a lot of new arrays has two costs: it has to initialize the new array to zero, and it causes collection pressure which increases the number of GCs. (Fortunately we are sneaking in under the 85K limit for arrays to avoid going on the Large Object Heap, but we’re very close, and that would change things as well.)</p>
<p data-adtags-visited="true">This algorithm fills in every element in the new array, so there’s no need for it to be initialized at all, or allocated and then immediately freed.</p>
<p data-adtags-visited="true">If we go to a solution where we cache the temporary array and re-use it, that shaves off another 200 milliseconds over 5000 steps.</p>
<hr>
<p data-adtags-visited="true">When I was working on the nullable lowering code in the compiler I learned that if we were generating, say, an addition of two nullable integer values x and y, that you always generate the code as if the user wrote:</p>
<pre>result = x.HasValue() &amp; y.HasValue() ? ...</pre>
<p data-adtags-visited="true">and not</p>
<pre>result = x.HasValue() &amp;&amp; y.HasValue() ? ...</pre>
<p data-adtags-visited="true">Why’s that? Because checking whether x has a value and branching if it does not is more expensive than simply calling y.HasValue()! Avoiding the work is more expensive than just doing it. And moreover, every branch is a new basic block, and basic blocks make the jitter work harder and therefore potentially skip more optimizations; the jitter has to be fast.</p>
<p data-adtags-visited="true">I therefore wondered if I could rewrite</p>
<pre>&nbsp; if (1 &lt;= x &amp;&amp; x &lt; width - 1 &amp;&amp; 1 &lt;= y &amp;&amp; y &lt; height - 1)</pre>
<p data-adtags-visited="true">which after all is just</p>
<pre>&nbsp; if (1 &lt;= x)
    if (x &lt; width - 1)
      if (1 &lt;= y)
        if (y &lt; height - 1)</pre>
<p data-adtags-visited="true">as instead</p>
<pre>&nbsp; if (1 &lt;= x &amp; x &lt; width - 1 &amp; 1 &lt;= y &amp; y &lt; height - 1)</pre>
<p data-adtags-visited="true">Turns out, that’s actually a small regression. This was suprising, particularly since the conditions are almost always true and therefore you don’t get much savings by avoiding work on the edges.</p>
<p data-adtags-visited="true">I’m not sure what’s going on here, but I have a guess. Let’s look at the other losers and then I’ll theorize further at the end.</p>
<hr>
<p data-adtags-visited="true">As we’ve seen, the special-purpose code for handling the edges is a source of pain because we do extra work to not dereference outside the array. A standard technique for Life on finite grids is to make a “rectangle of death” along the edge. That is, allocate memory for a 256×256 grid, but only actually do the computations on the 254×254 grid on the interior, and keep the 1000 or so cells on the edges permanently dead. We waste a couple KB of memory, but we save time on not having to constantly check whether we’re computing neighbors of an edge cell because we just skip them entirely.</p>
<p data-adtags-visited="true">When I only restricted the computations to the interior by changing the loop conditions, of course we were now doing 1000 fewer cells than before, so there was a commensurate speedup. But here’s the weird bit. When I removed the now unnecessary:</p>
<pre>if (1 &lt;= x &amp;&amp; x &lt; width - 1 &amp;&amp; 1 &lt;= y &amp;&amp; y &lt; height - 1)</pre>
<p data-adtags-visited="true">check, <strong>the program got slightly but consistently slower</strong>.</p>
<p data-adtags-visited="true">What the heck? It’s doing less work, so how did it get slower?</p>
<hr>
<p data-adtags-visited="true">This code seems to be doing some recomputation:</p>
<pre>int sum = this[x - 1, y - 1] ? 1 : 0; &nbsp;&nbsp;
sum += this[x - 1, y] ? 1 : 0;
...</pre>
<p data-adtags-visited="true">We recompute x-1, x+1, y-1 and y+1 several times.</p>
<p data-adtags-visited="true">When I put those into local variables to avoid recomputing them, again I got a small but consistent regression!</p>
<hr>
<p data-adtags-visited="true">Finally, the one that was most surprising to me. It is fairly well-known that in .NET, the jitter does a better job optimizing operations on single-dimensional arrays than multi-dimensional arrays. I tried changing my cell array to a single-dimensional array, and then accessed it as</p>
<pre>cells[x + y * width]</pre>
<p data-adtags-visited="true">throughout. <strong>This also caused a small regression!</strong></p>
<hr>
<p data-adtags-visited="true">I have a hypothesis that explains these unexpected regressions.</p>
<p data-adtags-visited="true">Arrays are memory-safe in C#. Accessing a single-dimensional array outside its bounds throws an exception, and accessing a multi-dimensional array outside of any of its bounds is an exception even if the offset computed into the underlying memory block would be in range.</p>
<p data-adtags-visited="true">The jitter has a lot of smarts in it to avoid doing unnecessary bounds checks. In particular, if you have:</p>
<pre>for(i = 0; i &lt; a.Length; a += 1)
  sum += a[i]</pre>
<p data-adtags-visited="true">Then the runtime knows that the array access inside the loop will always be in bounds, so it skips emitting a bounds check.</p>
<p data-adtags-visited="true">I suspect that what is going on here with all of these unexpected regressions is that the edit causes the jitter to fall out of a “known to be good” path and it starts generating more array bounds checks.</p>
<p data-adtags-visited="true">We are smart enough to know that the array indices will be in bounds, but the jitter only has a very small time budget to analyze a method body; as I noted above, it makes no sense to do a work-avoiding optimization if determining whether the optimization applies costs more than the work saved!</p>
<p data-adtags-visited="true">This is just a conjecture though; if anyone knows for sure what is going on here, I’d love to see a deeper analysis.</p>
<p data-adtags-visited="true">The obvious comment here is “can we avoid the checks entirely by going to unsafe code?” I’ll address that point in a later episode, so hold off there for now!</p>
<hr>
<p data-adtags-visited="true">Summing up:</p>
<ul>
<li>Our tweaked algorithm, which you can find in BoolArrayOpt.cs, goes from 17 seconds to 4 seconds for 5000 ticks, or 1250 ticks per second. That seems decent for an early attempt.</li>
<li>If we get 1250 TPS with an 8-quad, then we should get about 80 TPS with a 10-quad and about 20 with an 11-quad, so a 1024×1024 grid is about as good as we can get with this implementation if we want to meet our performance goal of animating smoothly. I have not actually tried it though; it would be interesting to see what the scaling factor really is.</li>
<li>The moral of the story here is<strong> performance of managed code can be surprising, so always measure!&nbsp;</strong></li>
</ul>
<p data-adtags-visited="true"><strong>Next time on FAIC:</strong> there’s a (seemingly) completely different flavor of array-based solution to this problem that I’d like to spend an episode or two digging into.</p>
			
			
						</div></div></div>
    </div>
    <footer>
        <div>created by buildstarted &copy; 2020 <a href="/about">about</a></div>
        <div>Share this page on social media: copy and paste this url https://linksfor.dev/</div>
        <div>If you prefer RSS: <a href="https://linksfor.dev/feed.xml">https://linksfor.dev/feed.xml</a></div>
        <div>Customer satisfaction guaranteed to be optional.</div>
    </footer>
    
    <script async defer>
        _dna = window._dna || {};
        _dna.siteId = "linksfor.devs";
        _dna.outlink = true;

        (function () {
            let dna = document.createElement('script');
            dna.type = 'text/javascript';
            dna.async = true;
            dna.src = '//dna.buildstarted.com/t.js';
            let s = document.getElementsByTagName('script')[0];
            s.parentNode.insertBefore(dna, s);
        })();
    </script>
    <noscript><img src="//dna.buildstarted.com/g?siteId=linksfor.devs" /></noscript>
</body>
</html>