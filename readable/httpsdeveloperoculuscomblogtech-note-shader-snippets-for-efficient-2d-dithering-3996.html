<!DOCTYPE html>
<html lang="en">
<head>
    <title>
Tech Note: Shader Snippets for Efficient 2D Dithering | Oculus - linksfor.dev(s)    </title>
    <meta charset="utf-8">
    <link rel="alternate" type="application/rss+xml" title="Linksfor.dev(s) feed" href="https://linksfor.dev/feed.rss" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta name="google" value="notranslate">
    <link rel="stylesheet" type="text/css" href="/style.css" />
    <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon">
    <link rel="icon" href="/favicon.ico" type="image/x-icon">
    <meta property="og:title" content="Tech Note: Shader Snippets for Efficient 2D Dithering | Oculus - linksfor.dev(s)"/>
    <meta property="og:description" content="A few examples of the minimal ALU method left to right (Best viewed at 100%):Plus6Int (6 shades), Dither17 (better quality than 16), Dither32 (more shades), Dither64 (more shades, artifacts)"/>
    <meta property="og:type" content="website"/>
    <meta property="og:url" content="https://developer.oculus.com/blog/tech-note-shader-snippets-for-efficient-2d-dithering/"/>

<meta property="og:site_name" content="linksfor.dev(s)" />
</head>
<body>
	<div class="devring" style="background: #222">
		<div class="grid">
			<div style="display: grid; grid-template-columns: .5fr 1fr 1fr 1fr; text-align: center;">
				<span class="devring-title">devring.club</span>
				<a href="https://devring.club/site/1/previous" class="devring-previous">Previous</a>
				<a href="https://devring.club/random" class="devring-random">Random</a>
				<a href="https://devring.club/sites/1/next" class="devring-next">Next</a>
			</div>
		</div>
	</div>
    <div class="grid">
        <h1>
<a href="/" style="color:inherit">linksfor.dev(s)</a>
        </h1>
        <title>linksfor.dev(s) - Tech Note: Shader Snippets for Efficient 2D Dithering | Oculus</title>
<div class="readable">
        <h1>Tech Note: Shader Snippets for Efficient 2D Dithering | Oculus</h1>
            <div>Reading time: 25-32 minutes</div>
        <div>Posted here: 27 Feb 2019</div>
        <p><a href="https://developer.oculus.com/blog/tech-note-shader-snippets-for-efficient-2d-dithering/">https://developer.oculus.com/blog/tech-note-shader-snippets-for-efficient-2d-dithering/</a></p>
        <hr/>
<div id="readability-page-1" class="page"><div><div><div><p><img src="https://scontent-lax3-1.xx.fbcdn.net/v/t39.2365-6/39247522_360707717801366_1386415567702851584_n.png?_nc_cat=104&amp;_nc_oc=AQlWEikOYXBKCZur-fvmhRFsiJNRP3EdnnDGRNLg9tbo8Lx0Np8bsk3OGLMydIAHnho&amp;_nc_ht=scontent-lax3-1.xx&amp;oh=28a9ee421d2f270c24c2038f0d3f96bf&amp;oe=5EBE1ED1" width="100%" alt=""><br> A few examples of the minimal ALU method left to right (Best viewed at 100%):Plus6Int (6 shades), Dither17 (better quality than 16), Dither32 (more shades), Dither64 (more shades, artifacts) </p><p><i><b>Note:</b> As for all images in this post - they have been created for this work. Images might be scaled and show a repeating extra pattern (Moiré pattern) depending on your chosen browser and browser settings. We suggest the sample application for an undistorted and animated view. Consider your monitor might affect the visuals - especially animated content.</i></p><hr><p>In this article, we present some ready to use HLSL snippets (small fragments of code in Direct X shading language) for dithering. We also provide details on when to apply them and how they have been crafted. We include some existing and new solutions to the problem. You can find interesting performance numbers at the end.</p><p> Efficient rendering algorithms are paramount for VR developers. They allow for more fidelity, higher frame rate, and less latency. That leads to better immersion and more comfort which lead to a longer, more pleasant VR experience.</p><p> During development of <a href="https://www.oculus.com/experiences/rift/1174445049267874/">”Dear Angelica”</a> (later in <a href="https://www.oculus.com/experiences/rift/1118609381580656/">”Quill”</a>) and the <a href="https://www.oculus.com/blog/welcome-to-rift-core-beta-now-available/">new Oculus “Home”</a> we needed high-quality <a href="https://l.facebook.com/l.php?u=https%3A%2F%2Fen.wikipedia.org%2Fwiki%2FDither&amp;h=AT0FmafR5qIE4F3JdOHRSqqhCauJxKan0lJl8pwVDmw6jhGQIQm9G8opt_XtK3AX-6hzWh4IWFzIvxEDIAZi_hZUWEreZCM0Zvgg0NuR12afa-RFL0SrkBM_6Xb2ud2FxWPOASbWJ7Me3KbeGEsKvw" target="_blank" rel="nofollow" data-lynx-mode="asynclazy">dithering</a>. Findings from those productions led to the creation of this blog post.</p><hr><p> Dithering is about transforming a value of higher precision (e.g. 0..1) into a value of less precision (e.g. binary) that when averaged with nearby pixels (spatial) or over time (temporal e.g. multiple frames) approximates the input value better than a simple quantization could do. There are many applications to dithering. Code for those applications differs but they all can be unified with a function returning a value in the 0..1 range and getting location (e.g. 2D screen position) and time (e.g. FrameIndex) a input. That value can be used in many ways:</p><ul><li>Clip/cull pixels when compared with a threshold</li><li>Added to a value that is going to be quantized (color banding or MSAA banding when culling MSAA samples)</li><li>Added to sampling start position that would suffer from banding due to under sampling (e.g. 3d volume tracing start location)</li><li>Rotate a 2D sample set to create more sample position (e.g. Screen Space Ambient Occlusion)</li></ul><p>Some dithering algorithms (e.g. Floyd Steinberg) require the signal from nearby sample points making a parallel implementation harder or less efficient. In this work we assume a rather smooth signal where employing nearby samples would not add significant value.</p><h2>Example applications of dithering:</h2><ul><li>Some <b>printers </b>use dithering to approximate grey scale values with black dots.</li><li><b> Opacity Mask / Stipple Pattern / Dithered transparency / <a href="https://l.facebook.com/l.php?u=https%3A%2F%2Fdigitalrune.github.io%2FDigitalRune-Documentation%2Fhtml%2Ffa431d48-b457-4c70-a590-d44b0840ab1e.htm&amp;h=AT3am_gU86k7_MqgSLlDeTXzJFgcvMAHghVJSuQvtUqyQnZrUD2KMCAChHi4WiFfuKsPFsswz6ID4qainTEHCPtAXQPvXJwKH4_ULhw_E7HAN_eXAMAyDNRoAgZglWYyrg2YI2_2vuSx7yTHPLukPA" target="_blank" rel="nofollow" data-lynx-mode="asynclazy">Screen-Door Transparency</a>: </b>Single layer transparency can be approximated by dithering opaque pixels (with clip()) based on the transparency/alpha value. Multiple layers often look acceptable but wrong (no proper <a href="https://l.facebook.com/l.php?u=https%3A%2F%2Fen.wikipedia.org%2Fwiki%2FOrder-independent_transparency&amp;h=AT3yW8_ZPGmZCM1UGvytsrrk1uE79vTWuT434iAYXpJjQ3AZSPqSSh29BkKzEHVZmBwoxmRmsKszgz2JC01vQfG6-ewgWWfdpCHTRAA4KU57s7ErG-9v9Azgq_f3c_iIBxa5x2UusYcPTWxraykr3w" target="_blank" rel="nofollow" data-lynx-mode="asynclazy">OIT </a>solution). <a href="https://l.facebook.com/l.php?u=https%3A%2F%2Fde45xmedrsdbp.cloudfront.net%2FResources%2Ffiles%2FTemporalAA_small-59732822.pdf&amp;h=AT2ZvWcdZcZCbBb4U-rTVeFcc9aD25Suy_WOSZk_VYyV_0F1dRUZnKf1UDaC64ruhdn7C-yb5_fmXYngHo1OtU6g1M11qaDpO-S-tYkeBbRjSTx42_BGch1qt5Hi8a1rqu5FkHt2QwirIgNr2tX0gw" target="_blank" rel="nofollow" data-lynx-mode="asynclazy">TemporalAA </a>can further improve the quality.</li><li> With <b>MSAA </b>the error can be reduced further (with SV_CoverageMask, see <a href="https://l.facebook.com/l.php?u=http%3A%2F%2Ftwvideo01.ubm-us.net%2Fo1%2Fvault%2Fgdc2017%2FPresentations%2FHorne_Oculus%2520Story%2520Studio.pdf&amp;h=AT3qU4KP3jlNC5SkyYgpnJoyy91mlMp4aZonnX46jkR1JO2bN7TCXjnV3z8oeM4i7EJ8OKfxg7ius-3QLPQJtyLW6oGHls77DxosxcDZS4tMN4_LVWldO_JXWfFZV8rWyTShxTCp0s9itJal07x08g" target="_blank" rel="nofollow" data-lynx-mode="asynclazy">”Dear Angelica”</a>). <br> e.g. 4xMSAA: add 1/4 * dither - 0.5 to OpacityMask in UE4 material<br> Note: Unreal Engine 4 used a <a href="https://l.facebook.com/l.php?u=https%3A%2F%2Fgraphicrants.blogspot.com%2F2013%2F12%2Ftone-mapping.html&amp;h=AT1y9amgfgbKTHDklW-Piiem1fnHRljJDHSNj0fmEmBrGnXFpIKJV87BH9SAWlkUnt4kxIyCJAYMYZWwGSZ3v-VuTE9dZ5-M2s4wp3eenGW7-keRbtOIUIifictqUL6-TcIXJbDYtf4tT4mGpBcejA" target="_blank" rel="nofollow" data-lynx-mode="asynclazy">weighted average</a> for HDR resolves which can cause minor artifacts</li><li><b>Prevent banding in low-precision buffers <b>(e.g. 8 bit)</b>:</b> e.g. after tone mapper, in G Buffers or from many alpha transparent layers<br> e.g. add 1/256 * (dither - 0.5) to PS output<br> RGB dither would result in more luminance shades at the cost of more dis-colorization<br>see UnrealEngine 4 r.Tonemapper.GrainQuantization</li><li><b>Blending materials </b>for a deferred renderer</li><li>“Softly” transitioning <b>Level Of Details</b> or fading out an object in distance (for better performance)</li><li>Rotate <b>sample set </b>(e.g. <a href="https://l.facebook.com/l.php?u=https%3A%2F%2Fde45xmedrsdbp.cloudfront.net%2FResources%2Ffiles%2FThe_Technology_Behind_the_Elemental_Demo_16x9-1248544805.pdf&amp;h=AT0VU-d1iQLJxVKFpjykq6q5AsLAyZh3J9klpH1lPSZPALus-wC1J2_oH-UKPk9CvvpPCvOfRDjMG327R0Cj_dlN_9iiOdIMZpYtf-3STt9Ka7yH-03Ma0SEXMennBzxYX9uNp564vVNdImQloQDbG2SQMVyH7l5Ri8" target="_blank" rel="nofollow" data-lynx-mode="async">UnrealEngine 4 SSAO</a>) to improve sampling efficiency</li><li>Add start distance for ray marching to blur out <b>sampling artifacts</b><a href="https://l.facebook.com/l.php?u=http%3A%2F%2Fshaderbits.com%2Fblog%2Fue4-volumetric-fog-techniques&amp;h=AT0fBp-bzf5SQT0d-CwNviKg6cl2fbEvxCEhKwr_amcVgIFb9NVWmoNujqze14i6wWHOv5Q5RSxUhDwjqmRoyTOrug2KdMJWgfBucUHNkwfcXq0iMA2KDDp2axPoGSYwdoXICniAoAKQb2RX3Y7Cxg" target="_blank" rel="nofollow" data-lynx-mode="asynclazy">e.g. http://shaderbits.com/blog/ue4-volumetric-fog-techniques</a></li></ul><p><b>Source code / demo (Win32) link: </b><a href="https://l.facebook.com/l.php?u=http%3A%2F%2Fdeargpu.com%2F&amp;h=AT0zShVeaN0vlalpToQK1KGPFDH2T9hnN8F0xzE97TwKPWrV8Y_PIrTaz1QEXUnY9z3ePiU6MrqRa7-vAwAJfRlQN3jbtjlfyj59_VTq-vKGNjGw17h9TfghJ_XYmtkPoc9fHkfvZ0T-zuWRoRLksQ" target="_blank" rel="nofollow" data-lynx-mode="asynclazy">DearGpu.com</a></p><ul><li>Liberal license: WTFPL <a href="https://l.facebook.com/l.php?u=http%3A%2F%2Fwww.wtfpl.net%2F&amp;h=AT2ieJvb0jt3ykHIqb8CqZJpA1UDJAevoMcSjaXGwcfjNGxa0L8MNdMGPallyIC1NbifDbNoafv-AqVvwfyjgQinuvBhOMootmXKON71-6MWbKuyzOK5UUdWiCaDsilXcHtVDyF1lHayJbYnUF3WOw" target="_blank" rel="nofollow" data-lynx-mode="asynclazy">http://www.wtfpl.net</a> (minor exceptions in “Copyright.txt”)</li><li>Controls: Use keyboard left/right/up/down to navigate the options. F5 to reload shaders.</li><li>“DitherExperiment.hlsl” has the relevant HLSL snippets.</li><li>All exposed C++ constants and textures can be found in AutoCommon.hlsl</li><li>For running the benchmark follow the instructions in the application. The output can be found in in the Stats folder and opened with Excel</li></ul><p> Some example code (simplified for better illustration):</p><pre>      // @param Pos screen position from SV_Position which is pixel centered meaning +0.5
      // @param FrameIndexMod4 0..3 can be computed on CPU
      // @return 0 (included) .. 1 (excluded), 17 values/shades
      float Dither17(float2 Pos, float FrameIndexMod4)
      {
          // 3 scalar float ALU (1 mul, 2 mad, 1 frac)
          return frac(dot(float3(Pos.xy, FrameIndexMod4), uint3(2, 7, 23) / 17.0f));
      }
      // one example usage (<a href="https://l.facebook.com/l.php?u=https%3A%2F%2Fdocs.microsoft.com%2Fen-us%2Fwindows%2Fdesktop%2Fdirect3dhlsl%2Fdx-graphics-hlsl-clip&amp;h=AT18aqRZZwVhOfwQXQQvcOYXodsNTOPs3APeUy1q5f4XOSsdihGGi6nDnZrpNNzYHYvbDTonAyWMBARAPXeNpVHTDC5VXSSbyiviJ1BGuVWWxVl7-PER5Ac1T_oYjl9lwl1Rd4bQ_q4iK59ODgTV2w" target="_blank" rel="nofollow" data-lynx-mode="asynclazy">clip/cull pixel if value is &lt;0</a>):
      clip(Dither17(Pos, FrameIndexMod4) - alpha);
  </pre><h2>Goals:</h2><ul><li><b>Input:</b></li><ul><li>float 2D screen position as provided by HLSL semantic SV_Position (pixel centered meaning +0.5)</li><li>uint FrameIndex (for animation, more efficient: float FrameIndexMod4)</li></ul><li><b>Output:</b></li><ul><li>float 0..1 (to clip pixel/subsample or add to values that undergo quantization) or bool</li><li>The values should be well distributed over the 0..1 range to evenly distribute the shades </li></ul><li><b>Outside range behavior</b>:</li><ul><li>an alpha value &lt;0 should always result in an output value &lt;0 (e.g. clip pixel)</li><li>an alpha value &gt;0 should always result in an output value &gt;1 (e.g. no clip pixel)</li></ul><li> Multiple <b>options</b> to allow different trade-offs and adapt to the application<br> e.g. regular / blue noise, static / animated, high frequency / more shades, ALU / texture</li><li>Simple <b>HLSL </b>shader code (but can be adapted to other shader languages)</li><li>Few <b>simple</b> (no sin or cos) <b>floating point </b>(not int/uint) shader instructions for best <b>performance </b>to make the method nearly free (Could be used on mobile GPU, VR or using it in each pass to fight quantization banding)</li><li><b>Easy integration</b> into any code (no / minimal C++ setup, no external data)</li><li><b>High frequency</b> as human perception and other spatial blur (e.g. from VR distortion pass) can hide the pattern</li></ul><h3>Existing solutions:</h3><ul><li><b> “DitherArray4x4” and <b>“DitherArray8x8” </b>function </b>A classic table based dither that results in 16 / 64 shades.<br>=&gt; axis aligned features making dither pattern very obvious</li><li><b>“GradientNoise” function</b>:<br> shipped in “Quill” and “Dear Angelica”, from<br> "NEXT GENERATION POST PROCESSING IN CALL OF DUTY: ADVANCED WARFARE"<br><a href="https://l.facebook.com/l.php?u=http%3A%2F%2Fadvances.realtimerendering.com%2Fs2014%2Findex.html&amp;h=AT0fLZy-D3m38U59IzZRUOhTkRs5OQue7qM7LvnHySRRvygI7Fc1q641-cNpOGB-iK7AfQORz60V_-ixfsZeVPsSvI5TqszebJczsIe6Hve5gk7kSZR8h1Zc7db-FWzkN362QK-F-SGm0HZKJd5gEA" target="_blank" rel="nofollow" data-lynx-mode="asynclazy">http://advances.realtimerendering.com/s2014/index.html</a><br>see Custom Alpha2Coverage slide in here:<a href="https://l.facebook.com/l.php?u=http%3A%2F%2Ftwvideo01.ubm-us.net%2Fo1%2Fvault%2Fgdc2017%2FPresentations%2FHorne_Oculus%2520Story%2520Studio.pdf&amp;h=AT3QKg00LRfX-wQITdnL9LyaGuG40Twiw-EHk9C4s_3HX8reI8PwliP11j2WxGixr0sKzX3HSl_L9G035gVRK26FV2Hd6HnToGmmnRI9j5EZKAUTVdqBm086zGw9Ogv6lZ3ApSCsWhe8DED04PB30w" target="_blank" rel="nofollow" data-lynx-mode="asynclazy">http://twvideo01.ubm-us.net/o1/vault/gdc2017/Presentations/Horne_Oculus%20Story%20Studio.pdf</a></li><li><b> “PseudoRandom” function: </b>Not truly random as output only depends on input, sin() is expensive, clumping as random noise is not high frequent <a href="https://l.facebook.com/l.php?u=https%3A%2F%2Fstackoverflow.com%2Fquestions%2F12964279%2Fwhats-the-origin-of-this-glsl-rand-one-liner&amp;h=AT1M4V1vzziSl5mQyoY9uttOO-N7vZRVxH_CBoWiCNEmx_NbXZFVKv_mi-YVLPC3T8m7ntp0D8V4hPvv3Dv5wuMwxh8uvOBdoMWUV4vfLqcwJftiSuc_WizCS71yGw6MqXt4MqNMREE4q8a5vDnHYA" target="_blank" rel="nofollow" data-lynx-mode="asynclazy">https://stackoverflow.com/questions/12964279/whats-the-origin-of-this-glsl-rand-one-liner</a></li><li><b><b>“</b>Plus5Int” function: </b>5 values (aligns with 5 pixel blur) from <a href="https://l.facebook.com/l.php?u=http%3A%2F%2Fgdcvault.com%2Fplay%2F1025467%2FReal-Time-Reflections-in-Mafia&amp;h=AT1IE3Ay10tYhQNSMdKYH1WAdBZb4SNy8MQSyOp5kJNLloxYtyeorJ9Bkf7Up-56MESgjeizXvOQJZn_6BJkcklTjF1waQWjz3cPYLNsB2eVWatBIwwDLwlkAeJdRXLmJY6I-OiuV23nvlB3zLpWJQ" target="_blank" rel="nofollow" data-lynx-mode="asynclazy">Mafia</a></li></ul><p><img src="https://scontent-lax3-1.xx.fbcdn.net/v/t39.2365-6/38824825_689276211449295_7719231376913858560_n.png?_nc_cat=100&amp;_nc_oc=AQmq-7pw0abH-tTOcl2kxrIbd06oENnhzKsG0A5AKaJKqu7G-aRUAK3yZyHDdFqV_hs&amp;_nc_ht=scontent-lax3-1.xx&amp;oh=9db28843cae6b0eb5cc95e510cbebb9b&amp;oe=5EFDD9BE" width="100%" alt=""></p><h2>Crafting a minimal ALU only dither function:</h2><p>We've seen existing snippets using only a few multiplications, adds, frac(), and some magic numbers. Finding an even faster function turned out to be an interesting challenge.</p><p>Inspired by this article: <a href="https://l.facebook.com/l.php?u=https%3A%2F%2Flemire.me%2Fblog%2F2017%2F09%2F18%2Fvisiting-all-values-in-an-array-exactly-once-in-random-order&amp;h=AT0-A31QAva7d4LVT1FTQZskwcb6dKIBWf6NSlK-DzAVNGx1krsUEpz1bbsvdODHFEl4jYd4nSslTgpTndDZXKImK7DIVsisOSe9qveAfwEAy8QN7B_656WG6Vuj-i0al99oCCG_avo3MyRaAb64KQ" target="_blank" rel="nofollow" data-lynx-mode="asynclazy">Visiting all values in an array exactly once in “random order”</a></p><p> We managed to craft a method based on a single modulo/frac() operation. We can transform “visits all values in an array exactly once” to “generates a well-distributed output value from a given input”</p><p> We know that on modern GPUs the most efficient primitive is a float MAD (fused MUL and ADD) operation. To create a value in 0..1 range we can use a simple frac() operation (very fast on GPUs). We have to make use of the Screen x and y location and this led to this minimal function (non-temporal):</p><pre>frac(x*k0 + y*k1)</pre><p> When comparing this value with an alpha value we can make the decision to keep or reject a sample.</p><p> The challenge is to craft the constants k0..k1 to achieve as much as possible of the defined goals.</p><p> The problem is simple enough, allowing for a brute force search of the best constants. However, defining a quality metric is non-trivial as a subjective balance between multiple goals needs to be found. In the end we found the best constants by experimentation. With with some math knowledge we managed to reduce the search space to a manageable range of numbers. </p><h2>Converting integer math to floating point math</h2><p>To derive a repetitive pattern it's actually easier to stick to integer math and replace the frac() with the modulo operation:</p><pre>((x*k0 + y*k1) % Count) / (float)Count    ~=      frac(x*k0/Count + y*k1/Count)</pre><p>It turns out a good integer version can be simply converted to float and frac() without any issues.</p><p> Floating point math usually suffers from a precision loss when doing math with large numbers. We can avoid/limit the problem if we limit the value ranges (screen resolution, constants, output). The SV_Position default 0.5 offset should be considered when doing the math as it can affect the stability of function at different screen locations.</p><p> Notes:</p><ul><li>Some hardware has special faster integer math (e.g. 24bit, 16bit, 8bit) but we did not optimize for that.</li><li>From past experience, we know that on some mobile hardware half (16-bit float) precision can cause problems with large resolutions. The functions here have not been tested with that in mind.</li><li>The code here has not been tested with larger screen resolutions (&gt;2048).</li><li>Comparing to an integer reference shows if we can match the integer quality. This has been done visually but could have been tested more extensively.</li></ul><h2>Finding the right constants</h2><p>Using “Count” provides us with a limited set of values which results in as many patterns.</p><p> For the 1D case, there is well-known math (see article above) to compute a sequence that visits all values in a pseudo-random order. We need to choose the constants k2 and k3 to be <a href="https://l.facebook.com/l.php?u=https%3A%2F%2Fen.wikipedia.org%2Fwiki%2FCoprime_integers&amp;h=AT3LheSxX1E4iBayJZPjFNxo4xpA65vZ9aXV8r5s3czA5cG_DMe8-NsSlTdl-mbZLr4lLC1P2pW7kJwFACh2Mx4gxWNwM0DonWLydvqhqHGy2LM-FgADC6KDM4V79Xh2Igd8bu02PBvOR4psskbWRw" target="_blank" rel="nofollow" data-lynx-mode="asynclazy">coprime </a>to “Count”. Note that the <a href="https://l.facebook.com/l.php?u=https%3A%2F%2Fen.wikipedia.org%2Fwiki%2FHalton_sequence&amp;h=AT0GyjoO91hWdLDCwxI4Dfog9mKL6lSPAat1KX_EwJyr3cQxllp1WCSPHnVsLiwvHpCx3TXl2Rz791GbMpwsnOw2vN6iFax_YrZcNZNGKXfP26wzWx52ZImnJnhELqPETtKKanmBzMpvjZhddzHk-A" target="_blank" rel="nofollow" data-lynx-mode="asynclazy">Halton sequence</a> also makes use of coprime numbers, but additionally alters the order (using more expensive integer math and reverse bits function) of the bits which helps the quality with a larger “Count”.</p><p>Good properties to optimize for (can conflict with each other):</p><ul><li> Large numbers of shades directly conflict with avoiding visible patterns or low-frequency features<br>=&gt; a small fixed number of shades can avoid visible patterns </li><li> Near 0 and 1 we prefer solid values over patterns (no dither pattern)<br>e.g. top is good, the bottom is bad (very dark areas still show a pattern) - the dialog is explained later</li></ul><p><img src="https://scontent-lax3-1.xx.fbcdn.net/v/t39.2365-6/39237013_1008552962648181_6775727581397254144_n.png?_nc_cat=101&amp;_nc_oc=AQnTAw8R42-FaKwZ0YP-jX8WHusIrUO0cNYV0Dn97iK6RQAFFNaCKPqvYY_XjikVAxA&amp;_nc_ht=scontent-lax3-1.xx&amp;oh=b1f12a7ed79046ea4ae6fa105f1df761&amp;oe=5EF81467" width="100%" alt=""></p><ul><li>Near solid colors (one-pixel case) could be distributed to be nearly equidistant (<b>“Dither17”</b> is better than <b>“Dither16”</b>)</li><li>No distracting patterns e.g. filling up diagonal lines of the pixel (hard to avoid with larger modulo numbers)</li><li>The ideal 50% pattern is a 2x2 pattern.<br>Picking a coprime number near the half of the modulo value e.g. 7 for (x*7)%16 is ideal to create a high-frequency pattern for this case.</li><li>Good performance (instruction type, count, temporaries)</li><li>Consistent quality at any screen location (using float or half on high-resolution displays can be a challenge)</li><li>Consistent quality at any alpha value and when animating alpha</li></ul><h2>Visualize the functions</h2><p>The following diagrams are a visualization that helps to compare and illustrate the properties of the functions. Run the application on <a href="https://l.facebook.com/l.php?u=http%3A%2F%2FDearGPU.com%2F&amp;h=AT2lghmsSlUmvCD0BSUlF2uN1jiMBlx5JpKs60m0NrobrBImGdSeeww98TOMgr1yzqw-o9N1f8yu6PS7FkfGcXWgjfIBPXIv13P7Fc7NBg1QE4J75duJ0L3vNdlAA0Vjem8jZWMJAGFLui7j3UVEuA" target="_blank" rel="nofollow" data-lynx-mode="asynclazy">DearGPU.com</a> to get a high quality animated visual.</p><p> The right shows a “side view” of the comparison with an alpha ramping up from 0 to 1. It's similar to a 2D diagram where x is the alpha value and y is the brightness of the pixel. Because it was evaluated per pixel you see “overhang” pixels. You can ignore that fact. It's useful too if colors blend linearly (straight lines) and how and how much the solution deviates from the reference.</p><p> The 5 sub bars on the left show the </p><ul><li>Animated pattern (at display refresh rate)</li><li>Animated pattern (slowed down 8 times)</li><li>Temporally blurred over 4 frames (weighted: 4 3 2 1 to not blur out low temporal frequency patterns)</li><li>Spatial blurred with a 4x4 box filter</li><li>Temporally blurred like before and spatially blurred with a 2x2 filter</li></ul><p> Notes:</p><ul><li><b>Temporal </b>means over time (e.g. average multiple samples at a fixed screen location over a few frames)</li><li><b>Spatial</b> means over space/position (e.g. average multiple samples that are nearby to each other)</li><li>The images here are <b>not animated</b> - if you want to see the animated version - you have to run the application from <a href="https://l.facebook.com/l.php?u=http%3A%2F%2FDearGPU.com%2F&amp;h=AT3UIvU_KBTMnCIXx6roD2TeGgMSFFEYWXfBFYAPY4VqH-SbgpO1Kxm5B1X1ilJ6D-6zewAoIdE2B7jihAXsdF9oYDhpF7lGPo7p3LxRyWpka8MQGSfCe4hHDhhi2JrDaEJvDGZ7FQDt5b_o7iixFQ" target="_blank" rel="nofollow" data-lynx-mode="asynclazy">DearGPU.com</a>. The temporally blurred versions give a similar impression.</li></ul><hr><p>The <b>reference </b>lerp (ColorA, ColorB, alpha) looks like this:</p><p><img src="https://scontent-lax3-1.xx.fbcdn.net/v/t39.2365-6/39280047_282398229216951_8829662632622424064_n.png?_nc_cat=101&amp;_nc_oc=AQlXMYbd-MBTXaTOyrOW-DuuOiVow-vlQ1NRPL0xrXfh3xDEHbxrDwKuakJiRUVl258&amp;_nc_ht=scontent-lax3-1.xx&amp;oh=02866967b8a32f25b546789b8fad0d77&amp;oe=5EFEBF61" width="100%" alt=""></p><p>This ideal image shows a smooth transition from black to white (left). The 5 bars look the same as spatial blurring (in XY) and temporal blurring (in time) have little effect.</p><hr><p>When looking at the <b>“PseudoRandom” function </b>you can see a transition from black to white but with large artifacts.</p><p> The random noise shows a clear deviation win the ramp which shows the result is not very good.</p><p><img src="https://scontent-lax3-1.xx.fbcdn.net/v/t39.2365-6/39167814_2106602376267850_8446520510090575872_n.png?_nc_cat=103&amp;_nc_oc=AQnXtdV0yqrM0qob3vhjgsridUZlAD3ygfbkEOTz12zbCOLvdoA3mkX8zBI-AGTUNec&amp;_nc_ht=scontent-lax3-1.xx&amp;oh=72d141284ce8f111ab4349a954c1e62d&amp;oe=5EC90F96" width="100%" alt=""></p><hr><p>The following shows the “<b>DitherArray4x4” function </b>with temporal variation over 4 frames → 4*4*4 = 64 shades</p><p><i> Note: The right side uses a box blur which results in a clean ramp. The quality of the result cannot be judged by that alone.</i></p><p><img src="https://scontent-lax3-1.xx.fbcdn.net/v/t39.2365-6/39188393_2311396615554179_6043904255541116928_n.png?_nc_cat=105&amp;_nc_oc=AQkWOcM3pslSu-SqTl-hbRjvCRWF6_ZPP4Yx1-MJlfbfwipSU3yP-nHkfdwUzIjlC9c&amp;_nc_ht=scontent-lax3-1.xx&amp;oh=6f8b278f2db7ab1268049c287f70d819&amp;oe=5EF883DD" width="100%" alt=""></p><hr><p>The following shows a <b>gradient noise</b> pattern used in “Dear Angelica” and in an early version of “Quill” (variant of “COD: ADVANCED WARFARE"):</p><p><img src="https://scontent-lax3-1.xx.fbcdn.net/v/t39.2365-6/39241153_2097480033836551_1450118002946080768_n.png?_nc_cat=109&amp;_nc_oc=AQm_Xm9qTgjDhlEi3mF29QbqjHLL1qXtSw_1bXHz1ChHFlbrJSaVJ1I-KAvGNspah1s&amp;_nc_ht=scontent-lax3-1.xx&amp;oh=327f3accf11c0f0dbee588607ecd2826&amp;oe=5EC8C9A1" width="100%" alt=""></p><hr><p>The following shows a <b>32 shade dither pattern</b>. We see 2 images, the reference integer implementation and the float implementation which should look the same but result in better performance:</p><p><img src="https://scontent-lax3-2.xx.fbcdn.net/v/t39.2365-6/39240723_1806488072764458_6208669392973070336_n.png?_nc_cat=111&amp;_nc_oc=AQlGZYqOisxKLpsSvx1-4JVZFD1JGOsMgKLaj3V6e5ANauC6VO8mRUTIA89sGCvg4dM&amp;_nc_ht=scontent-lax3-2.xx&amp;oh=1a1ebc85b8c5f01d44fbcacc3fc7e9d0&amp;oe=5EB6CBE1" width="100%" alt=""></p><p>Integer code (offset is to reflect 0.5f pixel offset in In.Pos.xy):</p><pre>      uint offset = dot(float3(0.5f, 0.5f, 0), k0);
      uint Ret = (offset + dot(int3(In.Pos.xy, In.frameIndexMod4), k0)) &amp; 0x1f;
      return frac((Ret + 0.5f) / 32.0f);</pre><p>Float code:</p><pre>    float Ret = dot(float3(In.Pos.xy, In.frameIndexMod4 + 0.5f), k0 / 32.0f);
    return frac(Ret);</pre><hr><p>The last shows a 17 shade dither pattern (16 didn't look as good) trading shades with a higher frequent pattern:</p><p><img src="https://scontent-lax3-1.xx.fbcdn.net/v/t39.2365-6/39220312_295700767680952_5450075608975409152_n.png?_nc_cat=108&amp;_nc_oc=AQnjl06JJBGrLlJMZZFrz8BqHBZHTB9EhFaEvErY9zVbTtiQ9ZZE6m_e71TKsMC4_RI&amp;_nc_ht=scontent-lax3-1.xx&amp;oh=ac2e6ac0f11b8b19ec96ce774472d7bd&amp;oe=5EC47E59" width="100%" alt=""></p><p>Notice there is 17 distinct steps in the ramp. Each pattern seems to be regular and high frequent. On a closer look, we can see the 50% area shows a distinct stripe pattern. This is where we had to make a compromise. A table/texture based solution could solve this but the minimal ALU solution limits our options.</p><h2>Adding a temporal component:</h2><p>We decided to include the option of a temporal component. By varying the function over time we can reduce the perception of <b>standing </b>patterns and produce a better, smoother transition. We repeat the adjustment every 4 frames (<b>time window</b>) which works well for 90 Hz frames (Oculus Rift). Less (2 or 3) can make sense on a lower image rate (e.g. 60 / 30 Hz). More is an option on higher image rates or when using post filter like the TemporalAA in Unreal Engine 4. We prefer a small time window to reduce the perception of a <b>moving </b>pattern.</p><p> Options adding a temporal component:</p><ul><li>Alter input</li><ul><li> Offset input XY from CPU<br>=&gt; CPU code, flexible</li><li> Offset input XY with 2 MAD: x += FrameIdMod * k2; y += FrameIdMod * k3<br>=&gt; simple</li><li> Offset input X or Y with 1 MAD: x += FrameIdMod * k2<br>=&gt; cheaper than XY but might be less effective</li></ul><li>Alter function</li><ul><li> vary k0/k1 from CPU<br> =&gt; need multiple nearly uncorrelated good constants (e.g. mirror x/y, flip xy), CPU code</li><li>extra constants: dot(float3(xy, FrameIdMod), k)<br>=&gt; more constraints on finding good constants</li></ul><li>Alter output (generally not a good idea)</li><ul><li> Offset output: + some_func(FrameIdMod)<br>=&gt; more shades (e.g. 4x4 pattern has 16 shades, with 4 temporal we can get 64 shades) but pulsing and standing patterns</li></ul></ul><p> Note: <b>Display overdrive </b>(Display hardware that reduces latency by over/undershooting) can make the temporal adjustment more visible and even result in a minor color shift. Oculus Rift employs a similar technology in the shader. The quality of that implementation is very high and we don't see any distracting overdrive color shifts.</p><h2>"Focus on the dark shades"</h2><p>Visually most noticeable is a bright pixel in a dark area. To better observe the artifact in dark shades it's best to reshape the alpha value e.g. new_alpha = pow(alpha,4):</p><p><img src="https://scontent-lax3-1.xx.fbcdn.net/v/t39.2365-6/39276059_231372354241025_4787139246399946752_n.png?_nc_cat=102&amp;_nc_oc=AQkKKSZ-gYBnrpZIUqsZ5oDkTPH8831WS_0nTOJPhgiDsb5ZwZ4vQt0UfNJFjTQrrh0&amp;_nc_ht=scontent-lax3-1.xx&amp;oh=b02cf0c721ad0413a9bc891347873397&amp;oe=5EBED570" width="100%" alt=""></p><p>Note how some of the functions shown here don't have a solid black near 0. This can be fixed by adding a small offset. To keep the brights a solid color (&gt;=1) a multiplied might be needed. </p><h2>Using dither to blend between colors</h2><p> Shaping the function to adapt to the display sRGB curve is tempting but it's actually only possible when both colors (Src and Dst in frame buffer blending) are known and grey scale (or when done per channel). Here we see a blend from pink to light green. The right side now shows the RGB color channel in an additive blend.</p><p><img src="https://scontent-lax3-1.xx.fbcdn.net/v/t39.2365-6/39199786_268530520419649_7674045250058321920_n.png?_nc_cat=105&amp;_nc_oc=AQkaEFkqXh4yvVx2tMI34by0TjFp1r5typspRHXSrpy9JGO4VkPQrUL5X8m9kHjUmwg&amp;_nc_ht=scontent-lax3-1.xx&amp;oh=4818595f282820ed0b4c3d852fbbd6ea&amp;oe=5EBF7212" width="100%" alt=""></p><p>The colors have been chosen (A=0.9, 0.1, 0.2; B=0.1, 1.0, 0.4) to use most of the 0..1 range and see how different blends behave. The right side shows 3 RGB ramps. This was to prove that any reshaping of the dither function would result in wrong results (e.g. compare with sqrt(alpha) instead of alpha). It might appear like a more pleasing transition on sRGB display for transitions from black to white but we want to have it work with any color. Applying any sRGB corrections to non-color channels is wrong. If you have trouble judging the visuals with the chosen colors you can use different colors when running the application (look for GeneralPurposeTweak).</p><h2>Texture Blue noise (non-ALU organic high-frequency pattern)</h2><section><div><section><div><div><div data-filter-keys="[]" data-cols="12" data-prefix="0" data-pull="0" data-push="0" data-suffix="0"><div id="u_0_0"><div><div><p><span><div data-filter-keys="[]" data-cols="6" data-prefix="0" data-pull="0" data-push="0" data-suffix="0"><p><img src="https://scontent-lax3-1.xx.fbcdn.net/v/t39.2365-6/39141077_219043725431156_8958518300111798272_n.png?_nc_cat=100&amp;_nc_oc=AQkl6RG3SA9lKUMdIUIZ9ME4267u7D02RAlfszN5nwUax5COM7J6EttnZ6MSIvh7TBY&amp;_nc_ht=scontent-lax3-1.xx&amp;oh=e4ef1aa6c4f161df8c2772df38394211&amp;oe=5EB9C44A" width="100%" alt=""><br> 64x64 texture<br></p></div></span></p><p><span><div data-filter-keys="[]" data-cols="6" data-prefix="0" data-pull="0" data-push="0" data-suffix="0"><p><img src="https://scontent-lax3-2.xx.fbcdn.net/v/t39.2365-6/39219982_276959962903566_4432315316424409088_n.png?_nc_cat=107&amp;_nc_oc=AQlkAbxG01RgMB4EevJ1MQ7quNY38cSGJkpYBCBL1oZ5KRak52FQOKtSstEX2th7nOI&amp;_nc_ht=scontent-lax3-2.xx&amp;oh=da9cb8589e5c81d6553b6ac8c1048529&amp;oe=5EC75499" width="100%" alt=""><br> 16x16 texture/table (more tiling) </p></div></span></p></div></div></div></div></div></div></section></div></section><p>Blue noise can be used for a more organic look, still maintaining the high-frequency property. We are not aware of very efficient math function that doesn't employ large tables for blue noise. Luckily GPUs have efficient texture mapping. To avoid repetition you want a large texture but to get good texture cache you want it small in memory (size and format). With a small enough texture you can even avoid texture cache trashing altogether. This is the best performance you can get and a lowering the resolution further would not buy you anything. This might be different when combined with other shader code - it's always best to profile this in your shaders and on your platform (e.g. mobile).</p><p> Getting blue noise textures:</p><ul><li>Download from: <a href="https://l.facebook.com/l.php?u=http%3A%2F%2Fmomentsingraphics.de%2F%3Fp%3D127&amp;h=AT2CwdlODif4dDVKdPNPNF2ZT-C3AvDVFzT-UooJf7KhT4CWgIOzDjeE1mi-XMCL4ia18rjwexe8suHU8cfQ-4OxfOugTKrer_IStEge7l0zZbMV9LcvxD1Ru_n_pT4mxZQWDowfYFCiw8v2S0nhzA" target="_blank" rel="nofollow" data-lynx-mode="asynclazy">Free blue noise texture</a></li></ul><ul><li>Generate e.g. <a href="https://l.facebook.com/l.php?u=https%3A%2F%2Fgpuopen.com%2Fvdr-follow-up-grain-and-fine-details%2F&amp;h=AT3xS7612kjd5hdADYJ1-0yclNtnIwiP2RrAyys_2Lqr5R8HQd4gYDLN_2fb6gd_Ph6ZhCpgSdZsCC5HHSjLEAKbljAO5snXKARZ1mm0K6I8AKiFa28vaDsQri3MLPtdwhKd3lkE_z6MooISxE41yg" target="_blank" rel="nofollow" data-lynx-mode="asynclazy">VDR Follow Up – Grain and Fine Details</a></li><li>This Unreal Engine 4 asset reference (in engine content):<br> Texture2D'/Engine/EngineMaterials/Good64x64TilingNoiseHighFreq.Good64x64TilingNoiseHighFreq'<br> is an option but using a true blue noise texture can improve quality quite a bit.</li></ul><p>In order to avoid standing patterns, it makes sense to give the impression of a never-repeating pattern. This can be implemented by offsetting the pattern/texture in a new random way. A pure random offset will result is short sections of moving patterns so it's best to have a more controlled randomness. Offsets that repeat every 64 .. 2048 frames should be long enough to break up the pattern and short enough to make it easier to judge the quality. A more recent version of “Quill” is cycling through multiple blue noise textures for a organic dissolve function on some materials.</p><h2>Table based Blue Noise</h2><p>For easier code integration we also investigated the option to use an array in HLSL. We limited the size to 16x16 as larger sizes are more likely to cause problems (constant buffer size limit, shader compile time, code size, etc). We found with full HLSL optimizations enabled the compiler generated <b>constant buffers </b>and accessed those. There are limits on constant buffers (size, index at 16 bytes) which might be optimized further in the driver but this is up for further investigation. The table we want to store is in bytes but that type is not available for tables/constant buffers so we use a 32bit uint. We tried packing bytes in <b>“BlueNoiseA16x16” function </b>and using 4 times as many uint in <b>“BlueNoiseB16x16”</b> (simpler shader).</p><p><b>Note:</b> Make sure the table is declared outside the function with “static const” and compiler optimizations (e.g. D3DCOMPILE_OPTIMIZATION_LEVEL3) enabled. Without that, we've seen the resulting code be very inefficient (many instructions and temporaries). When using similar code in other shader languages and platforms you might see even more problems.</p><h2>Linearity</h2><p> Dithering is replacing a soft transition between two values (A and B) by a pattern that statistically amounts to a similar value. This is assuming linearity. </p><p> lerp(A, B, x) ~= (A * a + B * b) / (a+b)</p><p> a: number of pixels showing A<br> b: number of pixels showing B</p><p><i> Note: lerp(A, B, x) is a linear interpolation = B*x + A*(1-x)</i></p><p> This assumption might not be correct because of <b>hardware properties</b> but it can be easily verified when comparing a blend between two values (e.g. extremes like black and white) with the linear interpolated reference. You have to do that on the target display (e.g. in VR, not on the monitor). A calibration could fix this but with the few shades, we don't have much fine control. Fixing larger issues (e.g. <a href="https://l.facebook.com/l.php?u=https%3A%2F%2Fmynameismjp.wordpress.com%2F2012%2F10%2F24%2Fmsaa-overview%2F&amp;h=AT1Q3K9qeRvuTDEnpOw4LTlRDEEHItQ_qRLbwTgEZzsaQXBCg1Cs4ZrE9G72WftM4QhkTmE809Eg0qKHUSTwqo10qYObdruQkKv-0OIq-c1ww9JUoTN7eEKqCt8AqfMK53ZjH7CpwPDEsiVkDvFldg" target="_blank" rel="nofollow" data-lynx-mode="asynclazy">sRGB</a>) are only possible if we know the values we want to dither/blend. This is not always possible (e.g. fixed function frame buffer blending doesn't allow that).</p><p> The assumption can also be violated by <b>shader code</b> e.g. Unreal Engine 4 TemporalAA is using a <a href="https://l.facebook.com/l.php?u=https%3A%2F%2Fgraphicrants.blogspot.com%2F2013%2F12%2Ftone-mapping.html&amp;h=AT1HKBIh3XTb2Coy_3T_3F5_no2kYE4SCgaUPZI5FsIoJh8yTaD3dQqs1Xt3n_xKrkZDIyKFaSGrYgg8t6YB7cAv_tTQigUmFv32apqt45QMNoOoZY5Jq79xcY95DseLqtd1Xys7TgKvKkjJWV1ZAA" target="_blank" rel="nofollow" data-lynx-mode="asynclazy">weighted average</a> for HDR. </p><p> Even if incorrect, having some kind of transition is often better than no transition.</p><h2>Performance</h2><p>Profiling these minimal shader snippets turned out to be very hard. <b>Shader byte code </b>count can be misleading as the drivers optimize the code further and some instructions take longer. Non-dither shader code can play a large role in the cost of the dither code. The <b>number of temporaries </b>can affect performance, especially if you have a lot of texture cache misses in the other part of the shader but the snipped code is very low on that we don't see the effect. Measuring timing by amplifying the shader code is possible but still tricky (branching adds extra cost). </p><p><img src="https://scontent-lax3-1.xx.fbcdn.net/v/t39.2365-6/38770476_2134026166813128_3286188458669768704_n.png?_nc_cat=101&amp;_nc_oc=AQkk0kC9awwR4aMuy1_Fw-vFgG9QrEnIa2yFtlzA6vzvCR-u3pgbNF4NuUx6MkdBiyE&amp;_nc_ht=scontent-lax3-1.xx&amp;oh=9269c7440d71aa0d06b973b76e82a33e&amp;oe=5EBD8E53" width="100%" alt=""></p><p><img src="https://scontent-lax3-2.xx.fbcdn.net/v/t39.2365-6/38958204_1909347935819731_3908669188549378048_n.png?_nc_cat=111&amp;_nc_oc=AQl777-VIGih3X6jW8tsRWuATDuOpTs9FJnMxfhuGnofARijowMJYpWT20D-jgQcCpo&amp;_nc_ht=scontent-lax3-2.xx&amp;oh=d6ff1ddc40c30199ec7927759d112b1e&amp;oe=5EC2A612" width="100%" alt=""></p><p>Reading the chart (4 passes of 1024x1024, dynamic loop of 100 calls to the function, Nvidia TITAN Black, driver 398.11):</p><ul><li>FrameTime is the most important criteria, a Null shader </li><li>InstructionCount and TempRegisterCount is from HLSL Bytecode</li><li><b>Null </b>is clearly the fastest only showing frame buffer writes (additive blending)</li><li><b>Constant </b>is to see the cost of the dynamic loop with minor code to not get it compiled out - it's the fastest</li><li><b>Plus6Int</b> is one of the slower ones, not optimized (using integer)</li><li><b>Dither16</b>, <b>Dither17</b>, <b>Dither64</b> use the same code, just different constants</li><li><b>Dither32</b> is like the ones before but has an additional +0.5 in the shader</li><li><b>BlueNoiseA16x16 </b>unpacks 4 bytes from uint[16*16] in constant buffer, more ALU</li><li><b>BlueNoiseB16x16</b> reads bytes from uint[16*16*4] in constant buffer, more constants, slowest</li><li><b>DitherArray8x8</b> is storing bytes in uint[8*8] in constant buffer</li><li><b>DitherArray4x4 </b>is storing bytes in uint[4*4] in constant buffer</li><li><b>BlueNoiseTex64x64 </b>using a small texture instead of constant buffer, way better than smaller tabes</li><li><b>PseudoRandom </b>sin() makes this expensive</li><li><b>GradientNoise </b>requires two frac() </li><li><b>Halton16</b> and <b>Halton64 </b>make use of integer math and reverse bits, slow</li><li><b>Overview</b> a shader that branches into all other options (temp count is outside of the chart)</li></ul><h2>Excellent presentations / posts that cover the topic in more detail:</h2><ul><li> “Low Complexity, High Fidelity - INSIDE Rendering”<br> by Mikkel Gjoel and Mikkel Svendsen<br> video: <a href="https://l.facebook.com/l.php?u=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DRdN06E6Xn9E&amp;h=AT0OEXk6TMUqzJoRH13es_laBS1MhdXQKbTMu35VZdYgmcQs_PKqBld_63arIKMm0wgjaLsoQMcxfcgO1F8Y-Io1b2GNYWZzQwx7aNryHIVXxMmM0pVNVZid3LUvMjvz2zWKDG2FHhPMGq-wWU5Ytg" target="_blank" rel="nofollow" data-lynx-mode="asynclazy">https://www.youtube.com/watch?v=RdN06E6Xn9E</a><br>slides: <a href="https://l.facebook.com/l.php?u=https%3A%2F%2Fwww.gdcvault.com%2Fplay%2F1023002%2FLow-Complexity-High-Fidelity-INSIDE&amp;h=AT0J2bE5HVeq9_W_8nXI7tFcIQDWx4Bb3X5q0XnLDIqn5zuhLf2A092EZ9JczkG1ss_IypdMPCpV4nKurDzobsDS40oKTMHmgOZ3F0AibzC_SEyi8DPkz8XH8lT_ovwCJey5Z32oO1vCO_lq9qUojQ" target="_blank" rel="nofollow" data-lynx-mode="asynclazy">https://www.gdcvault.com/play/1023002/Low-Complexity-High-Fidelity-INSIDE</a></li><li> Dithering part three – real world 2D quantization dithering<br><a href="https://l.facebook.com/l.php?u=https%3A%2F%2Fbartwronski.com%2Ftag%2Finterleaved-gradient-noise&amp;h=AT35tEHpe3Rg2XPWh_Wj0gPArbvi7KPA-9hpfGt25ntyaPQkHFnth794rG0Y5nBR9Rcuk6HztkLvcNgInUi82-HTQXZC0-d8FDOwzFg70ruyGNFzQ46jVNny_Qs-_CVzXYTSQbc6nPwJdtuBbivkfw" target="_blank" rel="nofollow" data-lynx-mode="asynclazy">https://bartwronski.com/tag/interleaved-gradient-noise</a></li><li>Anti-aliased Alpha Test: The Esoteric Alpha To Coverage<a href="https://l.facebook.com/l.php?u=https%3A%2F%2Fmedium.com%2F%40bgolus%2Fanti-aliased-alpha-test-the-esoteric-alpha-to-coverage-8b177335ae4f&amp;h=AT1JOvP-i0FZ7LshMNZW9uQqleSpFGzk_3wb2WFAAaI0loku-tszYEvV_i-qrbMPsoEZX1q8_B-jSQVAcWeajLEEmn1yCDNAqg2yrfACBFKphgqm_Ldr6dNNbdMv2JSl3TG99SyJViofTgWeblh2ag" target="_blank" rel="nofollow" data-lynx-mode="asynclazy">https://medium.com/@bgolus/anti-aliased-alpha-test-the-esoteric-alpha-to-coverage-8b177335ae4f</a></li><li> When Random Numbers Are Too Random: Low Discrepancy Sequences<br><a href="https://l.facebook.com/l.php?u=https%3A%2F%2Fblog.demofox.org%2F2017%2F05%2F29%2Fwhen-random-numbers-are-too-random-low-discrepancy-sequences%2F&amp;h=AT1XKXGkZXw_3_vOhaZ_r4D-EqVjZN5Pqqwzt4RkKBmFerEVLVj_Mw6xDc75ZupmYrSwp6oGXOD5ctNIyu3CQ2Fm3N0-uLIYj0Sv_M0YBZKjfRtcrkM8g_vp_2q9Xh8kV_OUiFsLNLu2IYeP2VQJ_g" target="_blank" rel="nofollow" data-lynx-mode="asynclazy">https://blog.demofox.org/2017/05/29/when-random-numbers-are-too-random-low-discrepancy-sequences/</a></li><li> Animating Noise For Integration Over Time<br><a href="https://l.facebook.com/l.php?u=https%3A%2F%2Fblog.demofox.org%2F2017%2F10%2F31%2Fanimating-noise-for-integration-over-time&amp;h=AT1HSJrSETQ48tInCVWoUvBMGb_6XPdGo31QIJ95FQTnDaMS8rMoFIzdaZWh77uSBvo24IbVmX6NYljdqDH_UAXAMqAPa0xuKYdUFwEkSovi7NmfdwvePDiPy6HS-OLUYj-ZSqoBrOhqqqo8tkBF8w" target="_blank" rel="nofollow" data-lynx-mode="asynclazy">https://blog.demofox.org/2017/10/31/animating-noise-for-integration-over-time</a></li></ul><h2>Summary and observations</h2><p> We presented a small set of functions that can be integrated easily. Depending on the application you can trade the number of shades, noticeable pattern, math/texture cost and other properties.</p><p> It seems large gradients deserve more shades and in cases where small and large gradients can be seen (this is often the case) an adaptive hybrid method could work best (e.g. choose number of shades depending on filterwidth() which is computed with ddx/ddy). This is similar to mip-mapping where you pick a different content depending on the magnification factor. The synthetic performance numbers show texture mapping has very good performance and table (constant buffer) random access is quite expensive. The presented math only solutions are a good choice when you are texture bound and want to integrate a solution quickly.</p><h2>My last words and about the development environment</h2><p>I used “We” in most of this post to reflect a large part of this is derived from the work for others and discussions I had with industry professionals. For this work I especially want to thank:</p><p> Dean Beeler, Volga Aksoy, Inigo Quilez, Zeyar Htet and Gillian Belton</p><p> To develop functions like this, a good iteration time is important (quickly recompile shaders on a button or on save). I searched for a <a href="https://l.facebook.com/l.php?u=https%3A%2F%2Fwww.shadertoy.com%2F&amp;h=AT0ggNeOttI7hUZBLMzqlTMteWQIISAPnzc-AJRwFHDqYgXbk5-KQWjalrRWC38rZGn7zDVJS77wZ54_eJw6yWubAAkRjXCv7eCAFmvw9hjV4y200ri4RTu7FbUqup2uACNe67IbGCMIzePINkVL5g" target="_blank" rel="nofollow" data-lynx-mode="asynclazy">Shadertoy </a>like the tool for HLSL that also supports profiling and the closest I found was <a href="https://l.facebook.com/l.php?u=https%3A%2F%2Fgithub.com%2Fvinjn%2Fshadertoy-dx11&amp;h=AT2UHTPVkhIFvALpqiNPpa0QuHAU2Qc4L6E5EbdykplRmy_BsGcIv3pLb8epxVUYEIkuYFgT005Jh0ERZ5L8QIZBcnz6eRTk2L7DquOkSBI9dwVZqe0OxNnzTgbZ41mXFbpLoGCH8fvr2OZQ5uNAjw" target="_blank" rel="nofollow" data-lynx-mode="asynclazy">Shadertoy-dx11</a>. It was used to create the initial set of images but it wasn't suited to get performance data. This is why I developed a new tool that is more suited for performance experiments.</p><p> It is based on <a href="https://l.facebook.com/l.php?u=https%3A%2F%2Fkosmokleaner.wordpress.com%2F2014%2F10%2F30%2Fshadows-demo-with-source-download%2F&amp;h=AT0agt_5TXr6PK3vWba1y4BVqSsMKG7P2RDJpUAO6C5F8K12Lauj8GmQ4HSMBccRotQ6O_9thHs4Vu-vhTa2Oj6RbJomfiigryjt7Aw7m9yLxuvhoKjMIzKbFsv1Ns_xjLaegrJpfbHgZbqka55mag" target="_blank" rel="nofollow" data-lynx-mode="asynclazy">AEngine</a> which is a DirectX11 based mini engine (I released that while I worked at Epic Games, as free open source). The new framework called DearGPU (see source link in the beginning) allows for quick iteration (F5 to reload shaders) and convenient C/C++ / HLSL interaction (C++ exposed parameters which generate an <a href="https://l.facebook.com/l.php?u=https%3A%2F%2Fkosmokleaner.wordpress.com%2F2015%2F06%2F24%2Fsimple-c-reflection-to-generate-hlsl-constant-buffer-declaration%2F&amp;h=AT1bpB6SvK2hZv6odSa2hb2LIfaelqwBbki7diFijR82iMdZJn4Kc7c8CsT2rTbDqOcRNnjXh6gIE1WCH9B9b8HO-4T2QBJA-GeDfx5JX0HFJgFbN_OGnRWwbCTrP5yyTlDNTpseapiG_wCJAEAugw" target="_blank" rel="nofollow" data-lynx-mode="asynclazy">HLSL include file</a>, shaders get compiled on demand). Shader defines can be created in few lines of code which makes performance experiments way easier. There is a simple built-in benchmark functionality (choose benchmark type in the UI and run with F3). It will generate multiple .CSV (Comma Separated Value) files that can be imported into Microsoft Excel to create graphs.</p><p> You can use the <a href="https://l.facebook.com/l.php?u=http%3A%2F%2FDearGPU.com%2F&amp;h=AT1a7ChCBKBijIuiRomEvbd8Ojin3k26D1fUoOykpyDjw8kT3V7BBMaGpRX2a0GQ5Cq3XkBvdFW8Xl9tQB1T0OxNLDRYur7UF7p6jFe_cjDp1I3k5TFsylxMV1UCtxVaKs9Id1b3ToNnWfR705Woaw" target="_blank" rel="nofollow" data-lynx-mode="asynclazy"><b>DearGPU</b></a> under the same free license. If you want to help me grow this into a shader performance test tool, contact me. Imagine a website where you can read about other people's interesting experiments - each starting with a question like this “Dear GPU, I was wondering ...”.</p><p> I also can see the tool running on multiple graphics cards /driver versions/machines gathering more data.</p><p><b>Warning:</b> Drivers and hardware are not bug-free and with my experiment I even experienced computer restarts when running extensive (e.g. a single very slow draw call) shaders. I will try to follow up with hardware vendors to get this resolved. </p><hr><h3>Bonus: How to use with Unreal Engine 4 Materials</h3><ul><li><b>“BlueNoise” function</b> (without temporal component):Note: A blue noise texture from the link above will improve quality</li></ul><p><img src="https://scontent-lax3-2.xx.fbcdn.net/v/t39.2365-6/38806503_523347354790082_5619586831298330624_n.png?_nc_cat=106&amp;_nc_oc=AQnHI2b5L4iJsFRpAQUuXKFMZbdoH0b-6-sprvVNRwLBNOAUhd9MMc7ba-EDi2OpkMY&amp;_nc_ht=scontent-lax3-2.xx&amp;oh=444cc42995f5003af2b3e7d62be71e56&amp;oe=5EF8FEE5" width="100%" alt=""></p><ul><li><b>“Dither64” function</b> (with temporal component):Note: View.FrameNumber is actually View.FrameNumber%4</li></ul><p><img src="https://scontent-lax3-1.xx.fbcdn.net/v/t39.2365-6/38857292_282144755710530_3110393501624303616_n.png?_nc_cat=105&amp;_nc_oc=AQmq-O-0OlAiaXTq0Tznka8K-CdXMJhFArg0QHeL2KLTmYh9livrp7b9u4qjW692Dk4&amp;_nc_ht=scontent-lax3-1.xx&amp;oh=6a7015a4576781aa39caba0516cc62ba&amp;oe=5EBAB6B6" width="100%" alt=""></p></div></div></div></div></div>
    </div>
    <footer>
        <div>created by buildstarted &copy; 2020 <a href="/about">about</a></div>
        <div>Share this page on social media: copy and paste this url https://linksfor.dev/</div>
        <div>If you prefer RSS: <a href="https://linksfor.dev/feed.xml">https://linksfor.dev/feed.xml</a></div>
		<div>Customer satisfaction guaranteed to be optional.</div>
    </footer>
    
    <script async defer>
        _dna = window._dna || {};
        _dna.siteId = "linksfor.devs";
        _dna.outlink = true;

        (function () {
            let dna = document.createElement('script');
            dna.type = 'text/javascript';
            dna.async = true;
            dna.src = '//dna.buildstarted.com/t.js';
            let s = document.getElementsByTagName('script')[0];
            s.parentNode.insertBefore(dna, s);
        })();
    </script>
    <noscript><img src="//dna.buildstarted.com/g?siteId=linksfor.devs"/></noscript>
</body>
</html>