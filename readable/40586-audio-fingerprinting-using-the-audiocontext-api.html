<!DOCTYPE html>
<html lang="en">
<head>
    <title>
Audio Fingerprinting using the AudioContext API -
linksfor.dev(s)
    </title>
	<link rel="alternate" type="application/rss+xml" title="Linksfor.dev(s) feed" href="https://linksfor.dev/feed.rss" />
    <meta charset="utf-8">
    <meta name="Description" content="A curated source of links that devs might find interesting. Updated around the clock." />
    <meta name="google" value="notranslate">
    <link rel="stylesheet" type="text/css" href="/style.css" />
</head>
<body>
    <div class="grid">
        
<div class="readable">
    <h1>Audio Fingerprinting using the AudioContext API</h1>
    <div class="kg-card-markdown">
<p id="time">Reading time: 30 minutes | Coding time: 10 minutes</p>
<p>Fingerprinting as introduced in <a href="https://iq.opengenus.org/canvas-fingerprinting/"><strong>this article</strong></a> is a way of identifying users based on one or more set of unique device characteristics. Along with Canvas fingerprinting, <strong>Audio fingerprinting</strong> takes advantage of device performance specs to build up an identifying fingerprint of a user. The problem is it does not need to take any <strong>permission</strong> from the users and works on all browsers and can be used to <mark><strong>track users</strong></mark> across browsers.</p>
<p>It has the same basic process of doing this as canvas fingerprinting. Assign a task to the browser, record how it is executed, and use such data to build the fingerprint.</p>
<p>Recently, several sites have been found to be using such techniques to track activity across browsers on a device.</p>
<h3 id="moredetails">More details</h3>
<p>In the case of audio fingerprinting, the fingerprinting is based on the <strong>device&apos;s audio stack</strong>. Just as canvas fingerprinting takes advantage of the Canvas API, the technology that makes audio fingerprinting possible is an API called the <strong>AudioContext API</strong>. It is an <strong>interface of the Web Audio API</strong> that is a part of most modern browsers.</p>
<p>The browser is assigned the task of <strong>generating an audio signal</strong> and it&apos;s processed based on the device&apos;s audio setting and audio hardware installed on it.</p>
<p>A website uses the <strong>AudioContext API</strong> to send a low frequency audio through the browser to the computer. It then measures how the computer processes this sent data. Based on how this signal is processed, the results from the AudioContext API can help identify the same user across different browsers.</p>
<p>This process <strong>doesn&apos;t require access to the device permissions</strong> like microphone or speakers. No audio is recorded, collected or played by any means. It gathers the audio signature of a user&apos;s device and uses it to create an identifier to track that user. It simply relies on the difference in the way these generated signals are processed on each device.</p>
<p>Since this technique utilizes the <strong>device&apos;s (hardware) capability differences</strong> and not just the browser&apos;s &#x2014; it can be used to <strong>track users across different browsers</strong> as long as they&apos;re on the same device. Browser compartmentalization (a method where a person uses two or more dedicated browsers for different Internet activities) can be used to escape cookies set by different trackers.</p>
<p><strong>Firefox&apos;s multi-account containers</strong> add-on offers the same type of functionality and can be used to separate various web tasks into containers which are kept apart from each other. Cookies have become a less effective technique to these and various other prevention methods. However, methods like audio fingerprinting make it possible to identify users despite the fact that they are using different browsers.</p>
<p>Consider this code snippet:</p>
<pre><code class="language-javascript">    console.log(new AudioContext());
    
    // Logs an AudioContext object 
    // to the console:
    {
        &quot;baseLatency&quot;: 0,
        &quot;outputLatency&quot;: 0,
        &quot;sampleRate&quot;: 48000,
        &quot;state&quot;: &quot;suspended&quot;,
        &quot;maxChannelCount&quot;: 2,
        &quot;numberOfInputs&quot;: 1,
        &quot;numberOfOutputs&quot;: 1,
        &quot;channelCount&quot;: 1,
        &quot;channelCountMode&quot;: &quot;max&quot;,
        &quot;channelInterpretation&quot;: &quot;speakers&quot;,
        &quot;fftSize&quot;: 2048,
        &quot;frequencyBinCount&quot;: 1024,
        &quot;minDecibels&quot;: -100,
        &quot;maxDecibels&quot;: -30,
        &quot;smoothingTimeConstant&quot;: 0.8
    }
</code></pre>
<p>Using methods such as <code>createAnalyser()</code>, <code>createDynamicsCompressor()</code> and <code>createOscillator()</code> can be used to further develop unique device information. Other fingerprinting methods can be used in conjunction with this one to get an even more accurate identifier. All the information collected is passed onto a hash function to make up the fingerprint.</p>
<ul>
<li><code>createAnalyser()</code>: can be used to reveal time and frequency data of the audio and create data visualizations through a node created called <code>AnalyserNode</code>.</li>
<li><code>createDynamicsCompressor()</code>: can be used to compress an audio signal through the <code>DynamicsCompressorNode</code>.</li>
<li><code>createOscillator()</code>: results in the creation of a given frequency of a given periodic wave to be created through <code>OscillatorNode</code>.</li>
<li><code>createGain()</code>: creates a <code>GainNode</code> which is responsible for detecting the change in volume.</li>
</ul>
<p>This piece of code performs fingerprint as found in <a href="https://www.cdn-net.com/cc.js">cc.js of CDN-NET</a> and <a href="https://audiofingerprint.openwpm.com">OpenWPM</a></p>
<ol>
<li>First, we need to create an array to store frequency values.</li>
</ol>
<pre><code class="language-javascript">    let freq_data = [];
</code></pre>
<ol>
<li>We create an AudioContext object and create the various nodes needed to generate signal and collect the information using the built-in methods of the AudioContext object.</li>
</ol>
<pre><code class="language-javascript">    // Create nodes
   const ctx = new AudioContext(); // AudioContext Object
   const oscillator = ctx.createOscillator(); // OscillatorNode
   const analyser = ctx.createAnalyser(); // AnalyserNode
   const gain = ctx.createGain(); // GainNode
   const scriptProcessor = ctx.createScriptProcessor(4096, 1, 1); // ScriptProcessorNode
</code></pre>
<ol>
<li>We disable the volume and connect the nodes with each other.</li>
</ol>
<pre><code class="language-javascript">    // Disable volume
   gain.gain.value = 0;
   
   // Connect oscillator output (OscillatorNode) to analyser input
   oscillator.connect(analyser);
   // Connect analyser output (AnalyserNode) to scriptProcessor input
   analyser.connect(scriptProcessor);
   // Connect scriptProcessor output (ScriptProcessorNode) to gain input
   scriptProcessor.connect(gain);
   // Connect gain output (GainNode) to AudioContext destination
   gain.connect(ctx.destination);
</code></pre>
<ol>
<li>Using the <code>ScriptProcessorNode</code>, we create a function that collects frequency data while the audio is being processed.</li>
</ol>
<ul>
<li>The function creates a <code>Float32Array</code> typed array with a length that equals the number of (frequency) data values in the <code>AnalyserNode</code> and then populates it with the values.</li>
<li>These values are then copied to the array we created earlier (<code>freq_data</code>) so we can log them easily to an output.</li>
<li>We disconnect the nodes and log the output.</li>
</ul>
<pre><code class="language-javascript">   scriptProcessor.onaudioprocess = function(bins) {
      // The number of (frequency) data values
      bins = new Float32Array(analyser.frequencyBinCount);
      // Fill the Float32Array array of these based on values
      analyser.getFloatFrequencyData(bins);
      
      // Copy frequency data to &apos;freq_data&apos; array
      for (var i = 0; i &lt; bins.length; i = i + 1) {
          freq_data.push(bins[i]);
      }
      
      // Disconnect the nodes from each other
      analyser.disconnect();
      scriptProcessor.disconnect();
      gain.disconnect();
      
      // Log output of frequency data
      console.log(freq_data);
   }; 
</code></pre>
<ol>
<li>We start playing the tone so the audio is generated and processed in accordance with the function.</li>
</ol>
<pre><code class="language-javascript">   // Start playing tone
   oscillator.start(0);
</code></pre>
<p>We get an output like the one displayed below. The values are a lot longer than 10 values. This is for the sake of simplification.</p>
<pre><code class="language-javascript">   /*
     Output:
     [ 
         -119.79788967947266, -119.29875891113281, -118.90072674835938,
         -118.08164726269531, -117.02244567871094, -115.73435120521094,
         -114.24555969238281, -112.56678771972656, -110.70404089034375,
         -108.64968109130886, ...
     ]
   */
</code></pre>
<p>A combination of all these audio data values can be processed through a hash function to create a unique fingerprint. Values passed from other methods of fingerprinting can also be aggregated to be hashed and produce a fingerprint of a device.</p>
<p>Some possible defenses against this tracking method work by adding a small noise to the actual fingerprint to generate a random fake value and reporting it as such. This is seen in add-ons like <a href="https://addons.mozilla.org/en-US/firefox/addon/audioctx-fingerprint-defender/">AudioContext Fingerprint Defender</a>. The TOR browser is by far the only browser that blocks these types of tracking methods by default.</p>
<h3 id="references">References</h3>
<p><a href="https://iq.opengenus.org/canvas-fingerprinting/"><strong>Canvas fingerprinting</strong></a><br>
<a href="https://iq.opengenus.org/methods-to-track-user-on-web/"><strong>Methods to track users on the Web</strong></a><br>
<a href="https://audiofingerprint.openwpm.com">OpenWPM</a><br>
<a href="https://developer.mozilla.org/en-US/docs/Web/API/AudioContext">AudioContext API</a><br>
<a href="https://yinzhicao.org/TrackingFree/crossbrowsertracking_NDSS17.pdf">(Cross-)Browser Fingerprinting via OS and Hardware Level Features</a></p>
</div>
</div>
    </div>
    <footer>
        <div>created by buildstarted &copy; 2019 <a href="/about">about</a></div>
        <div>Share this page on social media: copy and paste this url https://linksfor.dev/</div>
    </footer>
    
    <script>
        (function() {
            var COLLECT_URL = "https://dna.buildstarted.com/t";
            var SITE_ID = "linksfor.devs";
            var GLOBAL_VAR_NAME = "__DNA__";

            window[GLOBAL_VAR_NAME] = {};

            window[GLOBAL_VAR_NAME].sendPageView = function() {
                var path = location.pathname;
                var referrer = document.referrer;

                var url = COLLECT_URL + "?siteid=" + SITE_ID + "&p=" + encodeURIComponent(path) + "&r=" + encodeURIComponent(referrer);

                try { fetch(url, { method: "GET" }); } catch (e) {}
            };

            window[GLOBAL_VAR_NAME].sendPageView();
        })();
    </script>
</body>
</html>