<!DOCTYPE html>
<html lang="en">
<head>
    <title>
Announcing accelerated training with ONNX Runtime&#x2014;train models up to 45% faster - Open Source Blog - linksfor.dev(s)    </title>
    <meta charset="utf-8">
    <link rel="alternate" type="application/rss+xml" title="Linksfor.dev(s) feed" href="https://linksfor.dev/feed.rss" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta name="google" value="notranslate">
    <link rel="stylesheet" type="text/css" href="/style.css" />
    <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon">
    <link rel="icon" href="/favicon.ico" type="image/x-icon">
    <meta property="og:title" content="Announcing accelerated training with ONNX Runtime&#x2014;train models up to 45% faster - Open Source Blog - linksfor.dev(s)"/>
    <meta property="article:author" content="Manash Goswami&#x9;&#x9;&#x9;&#x9;&#xA;&#x9;&#x9;&#x9;&#x9;&#x9;&#x9;&#x9;&#x9;&#x9;Principal Program Manager, Machine Learning Platform"/>
    <meta property="og:description" content="Today we are introducing significant updates to ONNX Runtime, including the ability to train models up to 45% faster."/>
    <meta property="og:type" content="website"/>
    <meta property="og:url" content="https://cloudblogs.microsoft.com/opensource/2020/05/19/announcing-support-for-accelerated-training-with-onnx-runtime/"/>

<meta property="og:site_name" content="linksfor.dev(s)" />
</head>
<body>
	<div class="devring" style="background: #222">
		<div class="grid">
			<div style="display: grid; grid-template-columns: .5fr 1fr 1fr 1fr; text-align: center;">
				<span class="devring-title">devring.club</span>
				<a href="https://devring.club/site/1/previous" class="devring-previous">Previous</a>
				<a href="https://devring.club/random" class="devring-random">Random</a>
				<a href="https://devring.club/sites/1/next" class="devring-next">Next</a>
			</div>
		</div>
	</div>
    <div class="grid">
        <h1>
<a href="/" style="color:inherit">linksfor.dev(s)</a>
        </h1>
        <title>linksfor.dev(s) - Announcing accelerated training with ONNX Runtime&#x2014;train models up to 45% faster - Open Source Blog</title>
<div class="readable">
        <h1>Announcing accelerated training with ONNX Runtime&#x2014;train models up to 45% faster - Open Source Blog</h1>
            <div>by Manash Goswami&#x9;&#x9;&#x9;&#x9;&#xA;&#x9;&#x9;&#x9;&#x9;&#x9;&#x9;&#x9;&#x9;&#x9;Principal Program Manager, Machine Learning Platform</div>
            <div>Reading time: 5-6 minutes</div>
        <div>Posted here: 19 May 2020</div>
        <p><a href="https://cloudblogs.microsoft.com/opensource/2020/05/19/announcing-support-for-accelerated-training-with-onnx-runtime/">https://cloudblogs.microsoft.com/opensource/2020/05/19/announcing-support-for-accelerated-training-with-onnx-runtime/</a></p>
        <hr/>
<div id="readability-page-1" class="page"><div role="document">
	<div>

					<main id="main">
					<article>
		
		<hr>
		
		<div>
			<p><a href="https://microsoft.github.io/onnxruntime/" target="_blank" rel="noopener noreferrer">ONNX Runtime</a> is an open source project that is designed to accelerate machine learning across a wide range of frameworks, operating systems, and hardware platforms. It is used extensively in Microsoft products, like Office 365 and Bing, delivering over 20 billion inferences every day and up to 17 times faster inferencing.</p>
<p>Today we are introducing significant updates to ONNX Runtime. In addition to improvements for model inferencing, we’re announcing the preview of training acceleration.</p>
<h2>ONNX Runtime for training</h2>
<p>ONNX Runtime now supports accelerated training of transformer models. Transformer models have become the building blocks for advanced language processing and generation. These models contain hundreds of millions of parameters and training them can consume many clusters of GPUs over days. Reducing the total training time can help enable rapid improvements in, and thus faster deployment of, these models.</p>
<div>
<div><p>Today’s preview release of training acceleration incorporates innovations from the </p><u><a title="https://aka.ms/aa87dvg" href="https://aka.ms/AA87dvg" target="_blank" rel="noreferrer noopener">AI at Scale</a></u><p> initiative, such as </p><u><a title="https://www.microsoft.com/en-us/research/blog/zero-deepspeed-new-system-optimizations-enable-training-models-with-over-100-billion-parameters/" href="https://www.microsoft.com/en-us/research/blog/zero-deepspeed-new-system-optimizations-enable-training-models-with-over-100-billion-parameters/" target="_blank" rel="noreferrer noopener">&nbsp;ZeRO optimization</a></u><p> and </p><u><a title="https://www.microsoft.com/en-us/research/project/parasail/" href="https://www.microsoft.com/en-us/research/project/parasail/" target="_blank" rel="noreferrer noopener">Project Parasail</a></u><p>, that improve memory utilization and parallelism on GPUs. <span>ONNX Runtime also features mixed precision implementation to fit more training data in a single NVIDIA GPU’s available memory, helping training jobs converge faster, thereby saving time. It is integrated into the existing trainer code for PyTorch and TensorFlow.</span></p></div>
</div>
<p>ONNX Runtime is already being used for training models at Microsoft. For example:</p>
<ul>
<li>Office 365 uses ONNX Runtime to accelerate pre-training of the Turing Natural Language Representation (T-NLR) model, a transformer model with more than 400 million parameters, powering rich end-user features like <a href="https://aka.ms/SuggestedRepliesMay2020" target="_blank" rel="noopener noreferrer"><em>Suggested Replies</em></a><em>, Smart Find,</em> and <a href="https://aka.ms/InsideLookMay2020" target="_blank" rel="noopener noreferrer"><em>Inside Look</em></a>. Using ONNX Runtime has reduced training time by 45% on a cluster of 64 NVIDIA V100 Tensor Core GPUs in Azure Machine Learning.</li>
<li>Bing uses large transformer models with more than 500 million parameters to train and service task-specific models. These models use ONNX Runtime to accelerate pre-training and fine-tuning throughput, cutting training time by 44%.</li>
<li>Visual Studio uses ONNX Runtime to accelerate pre-training a model, similar to GPT-2 Medium, <span>with more than 300 million parameters to power code autocompletion in the </span><a href="https://visualstudio.microsoft.com/services/intellicode/" target="_blank" rel="noopener noreferrer">IntelliCode</a><span> feature.</span></li>
</ul>
<p><img alt="a screenshot of a cell phone" width="1024" height="485" data-sizes="" data-src="https://cloudblogs.microsoft.com/uploads/prod/sites/37/2020/05/ONNX-Runtime-chart-of-accelerated-training-stats_v2-1024x485.webp" data-srcset="" src="https://cloudblogs.microsoft.com/uploads/prod/sites/37/2020/05/ONNX-Runtime-chart-of-accelerated-training-stats_v2-1024x485.webp"></p>
<p>To further accelerate training, we built custom kernels and graph optimizations to eliminate redundant operations. Additionally, ONNX Runtime enables larger batch sizes on the same 32GB memory of NVIDIA V100 Tensor Core GPUs. We tested ONNX Runtime by pretraining BERT-Large, reusing the training scripts and datasets from <a href="https://github.com/NVIDIA/DeepLearningExamples/tree/master/PyTorch/LanguageModeling/BERT#pre-training-nvidia-dgx-1-with-32g" target="_blank" rel="noopener noreferrer">benchmarking tests by NVIDIA</a>.</p>
<p>In the table below, you’ll see the relative training time improvements for pre-training the BERT-Large model on a 4 node NVIDIA DGX-2 cluster. The batch sizes reflect the Phase-1 and Phase-2 stages for the training experiment, using the datasets as detailed in NVIDIA <a href="https://github.com/NVIDIA/DeepLearningExamples" target="_blank" rel="noopener noreferrer">repo</a>. The detailed test report is <a href="https://aka.ms/onnxruntime-technical-blog" target="_blank" rel="noopener noreferrer">here</a>.</p>
<table>
<tbody>
<tr>
<td><strong>4x DGX2 </strong><br>
<strong>(64x V100 32GB)</strong></td>
<td><strong>PyTorch 1.5 </strong><br>
<strong>with <a href="https://ngc.nvidia.com/registry/nvidia-pytorch" target="_blank" rel="noopener noreferrer">NGC 20.03-py3</a></strong></td>
<td><strong>PyTorch 1.5 </strong><br>
<strong>with ONNX Runtime</strong></td>
<td><strong>% Gain </strong><br>
<strong>with ONNX Runtime</strong></td>
</tr>
<tr>
<td>Phase 1 time (hours)</td>
<td>11.12</td>
<td>9.99</td>
<td>10.16%</td>
</tr>
<tr>
<td>Phase 2 time (hours)</td>
<td>6.62</td>
<td>5.77</td>
<td>12.84%</td>
</tr>
<tr>
<td>Total time (hours)</td>
<td>17.74</td>
<td>15.76</td>
<td>11.16%</td>
</tr>
</tbody>
</table>

<p>Developers can use the sample for pretraining BERT-Large with ONNX Runtime and fine-tune to their datasets as needed. We have also published a ready-to-use sample to start experiments in Azure Machine Learning. To use in custom environments, developers can build from the source code using the instructions published <a href="https://github.com/microsoft/onnxruntime-training-examples" target="_blank" rel="noopener noreferrer">here</a>.</p>
<h2>ONNX Runtime for inferencing</h2>
<p>We continue to improve inference acceleration with ONNX Runtime and are now <a href="https://aka.ms/hf-ort-blog1" target="_blank" rel="noopener noreferrer">partnering with Hugging Face</a> to make it easy to accelerate popular transformer models.</p>
<blockquote><p><em>We have seen gains from using ONNX Runtime with transformer models and are excited to release functionality that makes it easy to inference Hugging Face Transformer models with ONNX Runtime.</em></p>
<p>Clément Delangue, CEO of Hugging Face.</p></blockquote>
<p>Today, we are also releasing multiple updates to ONNX Runtime for inferencing. The new ONNX Runtime inference version 1.3 includes:</p>
<ul>
<li>Compatibility with the new <a href="https://github.com/onnx/onnx/issues/2614" target="_blank" rel="noopener noreferrer">ONNX v1.7</a> spec</li>
<li><a href="https://aka.ms/dml4ort" target="_blank" rel="noopener noreferrer">DirectML execution provider</a> on Windows 10 platform generally available (GA)</li>
<li>Javascript APIs preview, and Java APIs GA</li>
<li>Python package for ARM64 CPU for Ubuntu, CentOS, and variants</li>
<li>Preview release for Execution Provider 2.0 with support for the latest Intel® Distribution of OpenVINO™ tool kit</li>
<li>Preview release for integration with the VitisAI for acceleration on the Xilinx U250 FPGA platform</li>
<li>Preview release for integration with Rockchip RK1808 AIoT chipset</li>
</ul>
<h2>Get Started</h2>
<ul>
<li><a href="https://github.com/microsoft/onnxruntime" target="_blank" rel="noopener noreferrer">ONNX Runtime</a></li>
<li><a href="https://github.com/microsoft/onnxruntime-training-examples" target="_blank" rel="noopener noreferrer">ONNX Runtime training samples</a></li>
<li><a href="https://github.com/microsoft/onnxruntime/tree/master/samples" target="_blank" rel="noopener noreferrer">ONNX Runtime inferencing samples</a></li>
</ul>
<p>Questions or feedback? Please let us know in the comments below.</p>
		</div>
		
	</article>
			</main><!-- /.main -->

							<!-- /.sidebar -->
					
	</div><!-- /.content -->
</div></div></div>
    </div>
    <footer>
        <div>created by buildstarted &copy; 2020 <a href="/about">about</a></div>
        <div>Share this page on social media: copy and paste this url https://linksfor.dev/</div>
        <div>If you prefer RSS: <a href="https://linksfor.dev/feed.xml">https://linksfor.dev/feed.xml</a></div>
		<div>Customer satisfaction guaranteed to be optional.</div>
    </footer>
    
    <script async defer>
        _dna = window._dna || {};
        _dna.siteId = "linksfor.devs";
        _dna.outlink = true;

        (function () {
            let dna = document.createElement('script');
            dna.type = 'text/javascript';
            dna.async = true;
            dna.src = '//dna.buildstarted.com/t.js';
            let s = document.getElementsByTagName('script')[0];
            s.parentNode.insertBefore(dna, s);
        })();
    </script>
    <noscript><img src="//dna.buildstarted.com/g?siteId=linksfor.devs"/></noscript>
</body>
</html>