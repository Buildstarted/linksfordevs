<!DOCTYPE html>
<html lang="en">
<head>
    <title>
Peddle the Pedal to the Metal -
linksfor.dev(s)
    </title>
	<link rel="alternate" type="application/rss+xml" title="Linksfor.dev(s) feed" href="https://linksfor.dev/feed.rss" />
    <meta charset="utf-8">
    <meta name="Description" content="A curated source of links that devs might find interesting. Updated around the clock." />
    <meta name="google" value="notranslate">
    <style type="text/css">
        html {
            font-family: sans-serif;
            line-height: 1.15;
            -webkit-text-size-adjust: 100%;
            -webkit-tap-highlight-color: transparent;
            height: 100%;
        }

        *, ::after, ::before {
            box-sizing: border-box;
        }

        body {
            margin: 0;
            font-family: -apple-system,BlinkMacSystemFont,"Segoe UI",Roboto,"Helvetica Neue",Arial,"Noto Sans",sans-serif,"Apple Color Emoji","Segoe UI Emoji","Segoe UI Symbol","Noto Color Emoji";
            font-size: 1rem;
            font-weight: 400;
            line-height: 1.5;
            color: #60656a;
            text-align: left;
            background-color: #323b44;
        }

        h1 {
            font-size: 6rem;
            font-weight: 300;
            line-height: 1.2;
            margin-top: 0;
            margin-bottom: 0.5rem;
            margin-bottom: 0.5rem
        }

        a {
            color: #007bff;
            color: #ccc;
            text-decoration: none;
            background-color: transparent;
            word-break: break-all;
        }

        .unseen a {
            font-weight: bold;
        }

        h3 {
            margin-top: 0;
            padding-top: 0;
            font-weight: normal;
        }

        .grid {
            -ms-flex-direction: column;
            flex-direction: column;
            width: 1024px;
            margin: 0 auto;
            flex: 1 0 auto;
        }

        .row {
            -ms-flex-direction: row;
            flex-direction: row;
            width: 100%;
            -ms-flex-wrap: wrap;
            flex-wrap: wrap;
            display: -ms-flexbox;
            display: flex;
        }

        .col {
            margin: 0 10px 0 10px;
            box-sizing: border-box;
            vertical-align: top;
        }

        .col-3-of-4, .col-6-of-8, .col-9-of-12 {
            width: calc(75% - 20px);
        }

        .col-1-of-4, .col-2-of-8, .col-3-of-12 {
            width: calc(25% - 20px);
        }

        @media (max-width:1023px) {
            /* big landscape tablets, laptops, and desktops */
            body {
                overflow-x: hidden;
            }

            main {
                width: 99%;
            }

            h1 {
                font-size: 50px;
            }
        }

        .text-right {
            text-align: right;
        }

        footer {
            left: 0;
            width: 100%;
            margin-top: 2em;
            padding: 50px 0;
            text-align: center;
            -moz-box-sizing: border-box;
            -webkit-box-sizing: border-box;
            box-sizing: border-box;
        }

        .readable {
            color: #949ba2;
        }

        svg:not(:root).svg-inline--fa {
            color: #60656a;
            overflow: visible;
        }

        .svg-inline--fa.fa-w-12 {
            width: 0.75em;
        }

        svg:not(:root) {
            overflow: hidden;
        }

        .svg-inline--fa {
            display: inline-block;
            font-size: inherit;
            height: 1em;
            overflow: visible;
            vertical-align: -0.125em;
        }

        img {
            max-width: 100%;
        }

        .text-center {
            text-align: center;
        }

        .readable h1 {
            font-size: 2em;
        }
    </style>
</head>
<body>
    <div class="grid">
        
<div class="readable">
    <h1>Peddle the Pedal to the Metal</h1>
    <div id="presentationNotes"> <p>Chu: I&apos;ll give you somewhat of an idea of what we&apos;re going to cover. First, just to set some context of what we&apos;re trying to do, why we&apos;re trying to do it, some tools to help you in your quest for performance, my own experience working on OpenLDAP, the problems that we ran into or discovered along the way, and the solutions we used to fix them. One of the things you find as you improve a code base, is once you fix one problem, several more become apparent. So you see more problems, you need more tools. And while you&apos;re refining and improving, sometimes you&apos;ll also find that incremental changes aren&apos;t good enough to get you where you want to be. Sometimes you need more drastic measures.</p> <h2 class="expanded">Context, Philosophy &amp; Impact</h2> <p>This is experience working on the OpenLDAP project for last almost 20 years. From the very first version of the code to the version that we&apos;re running today, it has accelerated by over a factor of 100. Now, you know, we&apos;re not quite programming right on the metal, I&apos;m not using assembly language anywhere, this is all Portable C language stuff. This chart shows you where search performance was on the very left at the very first version of OpenLDAP, and on the very right, the current version. As I mentioned up here, from 60 milliseconds per single operation to 0.6 milliseconds, 0.6 milliseconds also happens to be the ping time on that network. So we&apos;re running at network speed.</p> <p>It&apos;s interesting, if you look at what other famous computer scientists have written about performance and their perspectives over time. So Donald Knuth wrote this in 1967, which happens to be the year I was born. The idea then was you needed to understand something about the machines you&apos;re working with. And then a few years later, it&apos;s almost like he changed his perspective. He said, &quot;We&apos;re focusing too much on optimization. Maybe we should back off of that a little bit.&quot; Now, this quote has probably circulated through all of your consciousnesses. Optimization is the root of all evil. But that&apos;s the misquote that gets handed out a lot of the times. You have to take it really in its full context, of premature optimization is the root of all evil, and take those numbers with a huge grain of salt. His perspective there is, 97% of the time you don&apos;t need to worry about optimization.</p> <p>And I think, in the decades from 1974 into the 1990s, everybody took that a little too much to heart because you shouldn&apos;t be able to get a 100 to 1 performance boost on a well-written piece of code. You shouldn&apos;t be able to do that. But I believe people have shied a little too far away from thinking about optimization. So we need to bring this back into focus and think about what&apos;s good, what makes code run efficiently on a machine and be a little more aware.</p> <p>Of course, you know, the choices you make in writing your code can differ quite a bit, depending on if you&apos;re starting a new project from scratch or if you&apos;re refactoring existing code. How many of you have been faced with optimizing an existing project? Yes, pretty much, everybody. And how many of you have started a brand new project from scratch and thought about optimization in your design process? Very healthy. So, one of the things about the phrase premature optimization is kind of tricky too. There&apos;s context to that. Some things are so well-established in the literature already, that to think about them at the beginning of the project still isn&apos;t premature. If you&apos;re writing a project today, you don&apos;t use BubbleSort. That&apos;s just an easy one. So there are some things that you can think about, even at the very beginning of a project that would not be premature.</p> <p>Of course, as you go along, if you&apos;re doing very well, eventually you hit a limit. You get to diminishing returns, and every decision you make, where you gain something, you also lose something else, the tradeoffs start to add up. But I think if you look across the industry, most of the code is nowhere near that limit. Most of the code that you see can easily slash a huge amount of what it&apos;s doing without affecting any losses. Also, sometimes it&apos;s really clear that the solution in front of you is the best it&apos;s going to get, and there is no need to do any more work on it.</p> <p>For example, if you just want to add up the values of all these numbers in an array, the simplest and most straightforward solution is probably the best one. You&#x2019;ve got array of A elements, let&apos;s just do a simple for loop through it, and you&apos;re done. That&apos;s great. You don&apos;t need fancy algorithms, you don&apos;t need divide and conquer, or anything else like that. In fact, if you try to do a fancy algorithm here, all you&apos;re doing is making your code more complicated. While the basic number of add operations involved here is the same, you&apos;ve added a whole bunch of extraneous overhead in your more complex algorithm. So, one of the guiding principles should be, simplicity is better.</p> <p>A couple of the speakers mentioned this earlier today, your code has to be correct first, always. When you have correct code, it&apos;s fairly easy to make it faster. It&apos;s a lot faster than the opposite, where you have fast&#xA0;broken code and trying to fix it, make it do the right thing. So, yes, you always have to get correct code first; optimization is kind of a secondary consideration. But the other thing you really do want to try as much as you can, is to get everything right the first time around, because if you look at this and say, &quot;Well, I don&apos;t have time to think about this that hard. I can&apos;t get it right the first time around.&quot; If you don&apos;t have time, when are you going to have time to come back and fix it? Yes, it&apos;s not going to happen.</p> <p>Then a final consideration, which I always inject in here, computers are supposed to be fast. These things processors are running 3 gigahertz or whatever. So, if your code is correct, even if it gives you the right answer, if it gives it to you too late, that code is broken, at least in my perspective.</p> <h2 class="expanded">Profiling Tools</h2> <p>How do you even know that you have a problem? I think if you&apos;ve been following the performance track today, this answer is fairly obvious. You&#x2019;ve got to profile your code. You have to measure what it&apos;s doing to see if there&apos;s a problem and where the problems are. Now, there are a bunch of different approaches to profiling. They each have different strengths and weaknesses. Today, we can use Linux perf, which is a pretty wonderful tool. It can tell you all kinds of things very easily, very non-invasively. Used to be called OProfile, that&apos;s when I was using it first. It uses statistical sampling, like was mentioned in the previous talk, Richard&apos;s talk. And because it uses very low overhead sampling, you can run it on a live system with fairly little impact. But one of the things that I noticed about it is, it can still miss details. The call graphs that you get out of it can be missing entire hierarchies of the call tree. So you can&apos;t solely rely on that.</p> <p>Other tools that I&apos;ve also used, one of these is called FunctionCheck, and it uses the GCC compilers instrument functions feature. So if you compile a project with the flag F instrument functions, then GCC will emit custom profiling calls at the beginning and exit of every function. Then if you provide your own shared library that fills in the stubs that GCC expects, you can do whatever you want in the Prologs and epilogs of every function in your code. And so, FunctionCheck takes advantage of this and lets you measure the time, and also, as a side benefit, it tracks memory allocations.</p> <p>Now, it&apos;s not the easiest thing to use this, because you need a specially compiled binary, and also, anything that you link to, that hasn&apos;t also been compiled with instrumentation, isn&apos;t going to give you any information. So you get a fairly coarse&#xA0;profile. It only tells you how much time was spent in a function, but it doesn&apos;t tell you where in that function. But the advantage of it is, the call graphs are always accurate. There are no elements missing in the call tree.</p> <p>Then the other option that there is, is one of the sub tools of Valgrind, called callgrind, actually. This is really pretty complete. It can give you instruction level profiling, complete call graphs. The only problem is it&apos;s running it through an emulated machine, so it&apos;s at least 100 times slower than running in real time. How many of you have used Valgrind? If you haven&apos;t used, you should definitely check it out, because it&apos;s got a lot of cool features. Again, the only big problem with it is it runs so much more slowly, so you don&apos;t always get an accurate view of the overall allocation of time.</p> <p>One of the other tricks I&apos;ve learned with this is, if you&apos;re using a sampling profiler like Perf, run it on the slowest CPU you can because it makes all the time samples a lot easier to capture. I used to run it on a Pentium M laptop that it would set down to the battery save mode. So it would run at 600 megahertz, and I would get the best profiles that way, so. Perf is so easy to use, this should always be your first go-to tool. The results can be fairly obvious. In the very first version of OpenLDAP, which was inherited from the University of Michigan, we found 95% of our execution time was in the C library. Basically only 5% of the time was executing LDAP protocol. So this code was pretty horrible to begin with. I would say it was representative of code written in the mid-1990s. This was people who did not think about memory allocation because they weren&apos;t taught to think about it. I think they were taught not to think about it. And they used the standard C string library, because that&apos;s what every C programmer does, even though it&apos;s really not that great.</p> <h2 class="expanded">Problems and Effective Solutions</h2> <p>First of all, you never actually know how bad things are until you look. So you always have to take at least that first look. Of course, you can miss some details with Perf, with a sampler. So it&apos;s always a good idea to use more than one tool. How many of you have seen DRY, don&apos;t repeat yourself? Normally it&apos;s applied to writing a source code. You don&apos;t want to copy and paste functions or whatever. You try and minimize repetition in your source base. I would say, also don&apos;t repeat yourself at execution time. You don&apos;t want to compute the same information over and over again, and throw it away each time if you&apos;re going to use it a lot.</p> <p>On the opposite, you don&apos;t want to cache information if you don&apos;t use it that often, or if it&apos;s very easy to retrieve it again. So, looking into our problem, 40% of execution time in the string library, we find that 25% of our time is just on strlen(), counting the length of strings. Length of strings that we received over the network, which were transmitted in a binary protocol that always sends us the length. This was completely unnecessary work and we found a way to fix this, which is, well, already the code base uses structured strings, which is a string pointer with an explicit length variable. This is already in the code base. Why aren&apos;t they taking advantage of it? Well, now we are. After addressing this, we could get string length completely removed from the runtime profile. So right away, we gained 25% runtime performance.</p> <p>The next thing it&apos;s doing is dissecting these strings that are coming in, tearing them apart, and then putting them back together again, using strcat(), which is what C string library gives you. How many of you have seen, Shlemiel the painter problem? The idea here is, you&apos;ve got Shlemiel, his job is to paint the lines down a roadway. Every day, he paints some lines, and the following day, he gets less done, and every day, he gets less and less work done. And someone asked him, &quot;Well, what&apos;s going on Shlemiel? Why are you slowing down so much?&quot; He says, &quot;Well, every day I have to walk back further to the paint bucket.&quot; The Shlemiel the painter problem with strcat() is, it always starts at the beginning of the string. So as you keep concatenating strings to it, it gets longer and longer and longer, and slower and slower and slower. So strcat() is a horrible function.</p> <p>We fix this in OpenLDAP just by instead of having a copy function that always returns the pointer of the destination, we always return a pointer to the tail of the copy. Since you have the tail of the copy, you can just keep adding to it immediately from where you are. Now, this was 2001. This was a custom function of ours. Now you can find exactly this feature in a modern C library. It&apos;s called stpcpy(). But here&apos;s something that still isn&apos;t in modern C libraries that really needs to be. This is a string copy function that will never overflow its buffer. It&apos;s a lot easier to use than strcpy or strcpy_s or whatever, because you don&apos;t have to re-compute the remaining space on the buffer whenever you use it. All you do is you is, say, here&apos;s the end of the buffer, I don&apos;t care how much I put into it. I don&apos;t have to compute any lengths at all. Just keep stuffing into it until you can&apos;t stuff any else in. It also has the same feature where it returns a pointer to the end of what it copied. Again, you can just easily concatenate stuff together very quickly and completely safely.</p> <p>The standard C string library is kind of horrible. It&apos;s so easy to do a better job of what it does. When you have the opportunity to fix problems like this, you should do it every chance you get. If you&apos;ve got an application that does a lot of string handling and you&apos;ve been using the standard C string library, you can probably do a much better job. If you need to manage strings around, you probably need to use something like the struct berval or you keep an explicit length. It will save you a lot of work.</p> <p>Malloc was 50-some percent of our profile. Again, it was horrible. These days, we have super high performance Malloc libraries that we can use, JEmalloc, or Google&apos;s TC Malloc. This should not be your first reaction. You should not just say, &quot;Oh, let me put a better Malloc library in there.&quot; If you don&apos;t fix anything else, the most you&apos;re going to get out of them is maybe 10%. So, you really want to examine where your memory allocations are occurring and see if they really are doing something that&apos;s necessary.</p> <p>The most common occurrence that we found was in functions that return an object to the caller where the object is filled with certain bits of data. I think this is a horrible pattern, but it&apos;s one that C++ uses all the time. You get constructors&#xA0;who will say, &quot;Here&apos;s an object for you.&quot; And it&apos;s so easy to fix this. You say, &quot;Hey, pass&#xA0;in the container.&quot; Most of the time the container can be a local variable and the caller can be on the stack. So you go from stupid Mallocs that are going to be immediately freed as soon as you&apos;ve consumed the data, to no Mallocs. So, C++ is horrible. Don&apos;t do things the way C++ does them.</p> <p>These were very easy wins. That again, got us another 25% boost in performance, at least. So the next few bits will be a little bit more involved. Another frequent issue is we&apos;ve parsed some data, we&apos;re going to build a structure out of it, and we&apos;re going to start with a known size and just keep reallocing as we add elements to it. Again, this is horrible. It gets slower, the more you use it. So if you do things like this, the smart thing to do is iterate through all of the elements you think you&apos;re going to add in, count up their sizes, do a single Malloc for the right size, and then set the fields in. So instead of doing a series of Malloc, realloc, realloc, realloc, realloc, you&apos;re just doing one alloc. That&apos;ll save quite a bit of time.</p> <p>Then, we&apos;re talking about a network service that takes incoming requests over the wire, parses them up, does something to process them, and then spits a reply back. Now, again, we&apos;re talking about a binary protocol where every message has its length already encoded in it. We know the size of what we&apos;re dealing with, we&apos;ve got a buffer that&apos;s big enough to hold it, and then we parse it into individual fields. The old code would Malloc a new copy for every element to the field that it was parsing out. So again, this is a whole bunch of Mallocs that are happening, really for no good reason because the original network buffer is still there. So we change this to wherever we parse the value, we just set a pointer into the original network buffer. Instead of having lots of Mallocs and memcpys, we now have zero memcpys.</p> <p>This should be a simpler one. If everything you&apos;re doing is request-based, if you have individual requests that don&apos;t affect anything else that don&apos;t leave any global state, then it&apos;s really effective to have an arena that&apos;s associated with individual request. Then you can just allocate a whole bunch of stuff as you need to process that request. Once the request is done, you can throw the entire thing away. You can just reset it back to its virgin state without having to go through the trouble of freeing one element at a time. The funny thing about this is this is the dynamic memory model that Pascal used back in the 1980s. It&apos;s really simple and it&apos;s really fast.</p> <p>Some of the other more obvious techniques- if you know you&apos;re going to need a bunch of request structures, pre-allocate a whole bunch of them at startup time. It&apos;s also a good idea to keep a small number of them reserved. For example, if your server gets overloaded, you always have a couple of request structures available that you can use to send replies back and say, &quot;Hey, too busy,&quot; or something. So you never get an unresponsive server.</p> <p>With these techniques, Malloc doesn&apos;t even show up in our runtime profiles anymore. It doesn&apos;t show up at least in the top 100. Just another note, if you make some mistakes along the way, it happens. Now you might discover you&apos;ve got some memory leaks in your code. I encourage you to check out this project on GitHub, which I wrote, which is at least eight times faster than the Malloc tracer in TC Malloc. It&apos;s six times faster than the one in glibc. So it&apos;ll save you a lot of time. It won&apos;t heavily impact your runtime performance.</p> <p>After you&apos;ve tackled all the big hot spots on your call profile, you might see that everything&apos;s kind of flat and there&apos;s no longer any big obstacles that are apparent. If the performance is where you want it to be, then that&apos;s cool, you&apos;re done. If you still need more performance out of it, you might have some problems, you&apos;ve got some harder thinking to do. Also, there are a lot of interesting overheads that won&apos;t show up in a source level call graph. For example, if you spawn a lot of threads, which are supposed to be cheap, supposed to be easy to spawn them and throw them away as much as you want, the reality is there is a cost to starting up a thread and shutting down a thread. If you&apos;re going to do a lot of thread work over and over again, you really don&apos;t want to incur that cost repeatedly. So again, don&apos;t repeat yourself; don&apos;t do the same processing steps more than necessary. I would say, if you&apos;re using threads and you&apos;re just spawning them on demand, you really need to switch to a thread pool, where you just keep reusing the same threads over and over again.</p> <p>I don&apos;t know if this is something you think about very often, but do you typically have debug or log functions scattered throughout your code? Yes, no? In OpenLDAP, this is littered all over the code base. The basic idea is you say, &quot;Here&apos;s a message, output it if the log level is selected to be this value.&quot; So the debug function always has this check, is the provided value matched with the current system debug level, and if it is, then we&apos;ll do the message processing. If it&apos;s not, we just return and do nothing. It turns out if you have this scattered through your code quite a bit, that just the overhead of calling into a function and returning again immediately, is a significant impact on your throughput. On a modern CPU, it kills your Branch Target Buffer, it kills your return cache. There are a few things that it&apos;s just not good.</p> <p>The simple thing to do here is, you use a macro to invoke your DEBUG function. In the macro, you put the &quot;If&quot; statements that checks your debug level, and then you&apos;re golden. In situations like this, it&apos;s cheaper just to do a single If, than it is to do a function call to a no up function, you know, try to avoid that. Here&apos;s another that is probably more common in languages like C++. The LDAP search operation has something like 12 parameters, base, scope, filter, attributes only, values only. We pass&#xA0;this thing in from the network and then we pass&#xA0;this from the front-end into the back-end, and we do syntax tracking and all these things. At every function that we pass&#xA0;this off to, we got to send all 12 parameters across. It was kind of horrible.</p> <p>And as Alan Perlis says, having a large number of parameters to a function, tends to mean you&apos;ve done something wrong. But even if you&apos;ve got it correct every single time, you&apos;re wasting time, because every one of those parameters has to get pushed onto the stack. And then they get popped off the stack by the callee, and the callee does what it wants to do, and they call something else and has to push them back onto the stack. It&apos;s kind of horrible.</p> <p>In our code, we replaced all of that with a single structure. A structure still has 12 fields or whatever it is, but when you&apos;re passing it around, it&apos;s only costing us one pointer in the parameter list. Surprisingly, this can be up to 8% of your execution time. And this doesn&apos;t show up in the source level&#xA0;profile. You can&apos;t see that.</p> <p>If you were in Nitsan&apos;s talk earlier today, you might have caught his mention of false sharing. Basically, if you have shared data that&apos;s accessed from multiple CPUs at the same time and if the fields of your data straddle the CPU cache line, then every CPU that accesses it is causing cache invalidation on every other CPU that accesses it. It&apos;s kind of horrible. It&apos;s not a problem that occurs on a single processor machine. But it&apos;s something that you run into more and more, especially these days, we&#x2019;ve got 16, 32 core, whatever boxes. So it&apos;s something you have to be more aware of now.</p> <p>The advice here about ordering elements within a structure, this is actually something that I was taught in the 1980s when I was learning FORTRAN. But it still applies today. If you can avoid it, you don&apos;t really want hidden padding in your structures. If you&apos;ve got data structures that are going to be shared between threads, you do want them to be CPU cache line aligned. If you can&apos;t just declare them that way, the portable methods would be to use posix_memalign, or mmap, or V Malloc, whatever is available to you. If you want to see the impact of what&apos;s going on, again, the Linux Perf command can show this to you, because it can show you a cache hit and miss ratios and cache&#xA0;timing delays.</p> <p>Another thing that tends not to show up in the source level call graph. Now, by default, these profilers show you CPU time used. And when you&apos;re sitting in a lock, you&apos;re using zero CPU time. So, mutexes typically don&apos;t show up in a normal call trace. There is this cool little tool called mutrace, whose only job is to measure mutex times. It shows mutexes and condition variables. We found, for example, in OpenLDAP, we had high contention on a single mutex for our thread pool work queue. Kind of makes sense if there&apos;s only one of them. The interesting thing is testing on a quad-core box, if we split this up into 4 queues, we actually get a better than 4X performance boost. It&apos;s nonlinear. So, this is again something to be aware of. It won&apos;t show up in a normal call trace, you have to go explicitly looking for it.</p> <p>You discover over the course of time - I&apos;m talking about a development process where we discovered these things, over intervals of months - you eliminate one problem and something else pops up. Sometimes it&apos;s amazing to use, like how did this big problem hide from us so long? But it&apos;s just a matter of changing the dynamic of your program. As you fix certain problems, other timing relationships change. And so certain things that weren&apos;t a problem before, become more of one. The way to survive this from a morale standpoint is to keep good notes and just to show that you have been making progress over time. Those notes have to be test parameters, profile results, all of these things that you can use to give yourself documentation that says, &quot;Oh, yes, we changed this function, we&apos;ve eliminated this bottleneck, so this is something new.&quot;</p> <p>Then at the end of all of this, refinement, iteration, whatever, you may still discover it&apos;s not where it needs to be. Then the only solution is to start over. How many of you have used Berkeley DB in your projects? What&apos;s your experience with it? Positive? OpenLDAP, we&apos;ve used it for quite a long time. It always was a massive source of complexity for us because Berkeley DB itself is too slow for the request rates we want to process. So we have our own way of caching on top of it. And this turns out to be massive overhead because that means data can appear in three places at once. It can be in the file system cache. It&apos;s almost always on the file system cache anyway. It can be in Berkeley DB&apos;s own&#xA0;cache, and then it&apos;s in our entry cache, data cache. So the techniques we used here to make Berkeley&apos;s performance acceptable were hugely wasteful, just in terms of memories.</p> <p>The other thing too is Berkeley has its own transaction management system and its own locking system. If you use it properly, you still get deadlocks in all of the operations you&apos;re doing. Even when it&apos;s being used correctly, you have to detect deadlocks and abort and back off and retry. So it&apos;s pretty poor for throughput. Then the other thing is you have to coordinate its locking system with the locks we had to use to protect our own cache. We&apos;re now talking about two levels of locks and three levels of caches. It was pretty much horrible over the course of years. It was always the cause of new bug reports.</p> <p>Sometimes you have to think and say, &quot;Gosh, we keep on having problems with this thing. Is this particular thing what we should be doing?&quot; It&apos;s like, &#x201C;If Berkeley DB is this slow, that we have to build all these band aids around it, why are we using it?&quot; Well, it took us nine years to realize maybe that&apos;s not the thing to do. With LMDB, we looked at all the things that we hated about Berkeley, lock management, cache management. And we said, &quot;Get rid of all those things.&quot; Now, in order to have the flexibility to make such a large change, is this making, the entire local data store, toss it out and put in a new one. We were kind of lucky that we already had a modular back-end interface. So actually plugging in new data stores isn&apos;t too traumatic for us. But it could be a much larger problem and you could get lost in the muck along the way to it. So you have to make sure that you have the original design goals well in mind, and make sure that you&apos;re actually solving them with a new solution that you bring in.</p> <p>So it turns up with LMDB, since we&apos;re using MVCC, we can actually do reads without any locks at all. Because we use a read-only memory map, we can actually return pointers to database data without doing any memcpys or Mallocs. In fact, when you run OpenLDAP today with back-MDB, and do a pure read search load on it, the read threads actually do no blocking calls at all. The only system calls that they invoke are the ones to write packets back to the client. Also, because of the way we implemented MVCC and the read-only memory map, the database structure is actually completely immune to system crash corruption. You can kick the disk drives, you can pull out the power plug, whatever, the database will come up instantly, perfectly every time.</p> <p>Just a recap. You do always have to aim for correctness first. Yes, you can tolerate slow code at the very beginning of the project. But, at least in my perspective, getting the right answer too late is still wrong. Fixing these kinds of problems is an iterative process. You&apos;re always going to discover a new bottleneck after you fix the last one. There are lots of tools out there that can help you with this process. You probably need to use several of them, because the more perspectives you get into the problem, the more clear the solution becomes. And sometimes after all this iteration, you might just have to throw things out and start over. The ultimate goal is to just do what&apos;s necessary. An efficient program just does what it needs to do to get your work done, and then gets out of the way, and that&apos;s really what you want to do.</p> <p><big><strong>See more <a href="https://www.infoq.com/transcripts/presentations/">presentations with transcripts</a></strong></big></p> </div>
</div>
    </div>
    <footer>
        <div>created by buildstarted &copy; 2019 <a href="/about">about</a></div>
        <div>Share this page on social media: copy and paste this url https://linksfor.dev/</div>
    </footer>
    
</body>
</html>