<!DOCTYPE html>
<html lang="en">
<head>
    <title>
Fun with the Spiral of Death - linksfor.dev(s)    </title>
    <meta charset="utf-8">
    <link rel="alternate" type="application/rss+xml" title="Linksfor.dev(s) feed" href="https://linksfor.dev/feed.rss" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta name="google" value="notranslate">
    <link rel="stylesheet" type="text/css" href="/style.css" />
    <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon">
    <link rel="icon" href="/favicon.ico" type="image/x-icon">
    <meta property="og:title" content="Fun with the Spiral of Death - linksfor.dev(s)"/>
    <meta property="article:author" content="Marc Gravell"/>
    <meta property="og:description" content="Subtitled: &quot;a cautionary tale of SemaphoreSlim &quot;, an adventure in two parts:   In part 1 I want to discuss a very fun series of problems we ..."/>
    <meta property="og:type" content="website"/>
    <meta property="og:url" content="https://blog.marcgravell.com/2019/02/fun-with-spiral-of-death.html"/>

<meta property="og:site_name" content="linksfor.dev(s)" />
</head>
<body>
    <div class="grid">
        <h1>
                <span style="cursor: default" title="linksfor.dev(s) has been running for 1 year! :partypopper:">ðŸŽ‰</span>
<a href="/" style="color:inherit">linksfor.dev(s)</a>
        </h1>
        <title>linksfor.dev(s) - Fun with the Spiral of Death</title>
<div class="readable">
        <h1>Fun with the Spiral of Death</h1>
            <div>by Marc Gravell</div>
            <div>Watching time: 15-19 minutes</div>
        <div>Posted here: 27 Feb 2019</div>
        <p><a href="https://blog.marcgravell.com/2019/02/fun-with-spiral-of-death.html">https://blog.marcgravell.com/2019/02/fun-with-spiral-of-death.html</a></p>
        <hr/>
<div id="readability-page-1" class="page"><div id="post-body-8908076159438940521" itemprop="description articleBody">
<p>Subtitled: "a cautionary tale of <code>SemaphoreSlim</code>", an adventure in two parts:</p>
<ul>
<li>In part 1 I want to discuss a very fun series of problems we had in some asynchronous code - where "fun" here means "I took Stack Overflow offline, again". Partly because it is a fun story, but mostly because I think there's some really useful learning points in there for general adventures in asynchronicity</li>
<li>In part 2 I want to look at some of the <em>implementation details</em> of our eventual fix, which covers some slightly more advanced themes around how to implement awaitable code in non-trivial scenarios</li>
</ul>
<h2>I took Stack Overflow offline, again</h2>
<p>As a side note: many of the themes here run hand-in-hand with David and Damian's recent presentation "Why your ASP.NET Core application won't scale" at NDC; if you haven't seen it yet: <a href="https://www.youtube.com/watch?v=-cJjnVTJ5og" rel="nofollow"><em>go watch it</em></a> - in particular everything around "the application works fine until it suddenly doesn't" and "don't sync-over-async or async-over-sync".</p>
<p>A lot of this journey relates to our migration of <a href="https://github.com/StackExchange/StackExchange.Redis" rel="nofollow">StackExchange.Redis</a> to use "pipelines", the new IO layer in .NET (previously discussed <a href="https://blog.marcgravell.com/2018/07/pipe-dreams-part-1.html" rel="nofollow">here</a>, <a href="https://blog.marcgravell.com/2018/07/pipe-dreams-part-2.html" rel="nofollow">here</a>, <a href="https://blog.marcgravell.com/2018/07/pipe-dreams-part-3.html" rel="nofollow">here</a>, and <a href="https://blog.marcgravell.com/2018/07/pipe-dreams-part-31.html" rel="nofollow">here</a> - I love me some pipelines). One of the key design choices in StackExchange.Redis is for the library to implement <a href="https://stackexchange.github.io/StackExchange.Redis/PipelinesMultiplexers" rel="nofollow">multiplexing</a> to allow multiple concurrent calling threads to communicate over the same underlying socket to the server; this keeps the socket count low while also helping to reduce packet fragmentation, but it means that we need to do some synchronization around how the many caller threads access the underlying socket.</p>
<p>Before the pipeline migration, this code was <em>basically</em> synchronous (it was a bit more complex, butâ€¦ that's close enough), and the "write an actual command" code could be expressed (if we take some liberties for readability) as below:</p>
<div><pre><span>readonly</span> <span>object</span> <span>syncLock</span> <span>=</span> <span>new</span> <span>object</span>(); <span><span>//</span> single writer</span>

<span>void</span> <span>WriteMessage</span>(<span>Message</span> <span>message</span>)
{
    <span>bool</span> <span>haveLock</span> <span>=</span> <span>false</span>;
    <span>try</span>
    {
        <span>Monitor</span>.<span>TryEnter</span>(<span>syncLock</span>, <span>timeout</span>, <span>ref</span> <span>haveLock</span>);
        <span>if</span> (<span>!</span><span>haveLock</span>) <span>ThrowTimeout</span>();

        <span>ActuallyWriteTheThing</span>(<span>message</span>);
        <span>Flush</span>();
    }
    <span>finally</span>
    {
        <span>if</span> (<span>haveLock</span>) <span>Monitor</span>.<span>Exit</span>(<span>syncLock</span>);
    }
}</pre></div>
<p>This is a fairly normal style of coding - the <code>try</code>/<code>finally</code>/<code>Monitor</code>/<code>haveLock</code> code here is just a standard implementation of "<code>lock</code> with a timeout", so all this really does is:</p>
<ul>
<li>try to acquire exclusive access to the socket, guarded by <code>syncLock</code></li>
<li>if successful, write and flush</li>
</ul>
<p>All reasonable. But then we moved to pipelines, and one of the defining features of the pipelines implementation is that key steps in it are <code>async</code>. You might assume that it is the <em>write</em> that is <code>async</code> - but since you write to a buffer pool, this isn't actually the case - it's the <em>flush</em> that is <code>async</code>. The <em>flush</em> in pipelines achieves a few different things:</p>
<ul>
<li>if necessary, it activates the <em>consumer</em> that is pulling work from the pipe and sending it to the next step (a socket in our case)</li>
<li>it provides back-pressure to the <em>provider</em> (<code>WriteMessage</code> in this case), so that if the consumer is falling behind and there's too much backlog, we can slow down the provider (in an asynchronous way) so we don't get unbounded buffer growth</li>
</ul>
<p>All very neat.</p>
<p>But switching from synchronous code to an API that uses <code>async</code> is not always trivial - <code>async</code> begets <code>async</code>, and once you start going <code>async</code>, it <em>all</em> goes <code>async</code>. Soâ€¦ I did a bad thing; I was lazy, and figured "hey, flush will almost always complete synchronously anyway; we can probably get away with a sync-over-async here" (<em>narrator: they didn't get away with it</em>).</p>
<p>So; what I did was something like:</p>
<div><pre><span>readonly</span> <span>object</span> <span>syncLock</span> <span>=</span> <span>new</span> <span>object</span>(); <span><span>//</span> single writer</span>

<span>void</span> <span>WriteMessage</span>(<span>Message</span> <span>message</span>)
{
    <span>bool</span> <span>haveLock</span> <span>=</span> <span>false</span>;
    <span>try</span>
    {
        <span>Monitor</span>.<span>TryEnter</span>(<span>syncLock</span>, <span>timeout</span>, <span>ref</span> <span>haveLock</span>);
        <span>if</span> (<span>!</span><span>haveLock</span>) <span>ThrowTimeout</span>();

        <span>ActuallyWriteTheThing</span>(<span>message</span>);
        <span>FlushSync</span>();
    }
    <span>finally</span>
    {
        <span>if</span> (<span>haveLock</span>) <span>Monitor</span>.<span>Exit</span>(<span>syncLock</span>);
    }
}

<span>void</span> <span>FlushSync</span>() <span><span>//</span> evil hack, DO NOT USE</span>
{
    <span>var</span> <span>flush</span> <span>=</span> <span>FlushAsync</span>();
    <span>if</span> (<span>!</span><span>flush</span>.<span>IsCompletedSuccessfully</span>)
    {
        <span>flush</span>.<span>Wait</span>();
    }
}</pre></div>
<p>The <code>IsCompletedSuccessfully</code> here is a check you can use on many task-like (awaitable) results to see if it completed synchronously and without faulting; if it did, you're safe to access the <code>.Result</code> (etc.) and it will all be available already - a good trick for avoiding the <code>async</code> state-machine complications in high-throughput code (typically library code, not application code). The bad bit is the <code>.Wait(â€¦)</code> when it <em>isn't</em> already completed - this is a sync-over-async.</p>
<h2>What happened next?</h2>
<p>A key thing to keep in mind is that StackExchange.Redis exposes both synchronous and asynchronous APIs - i.e. there are twin methods, for example:</p>
<ul>
<li><code>RedisValue StringGet(RedisKey key)</code></li>
<li><code>Task&lt;RedisValue&gt; StringGetAsync(RedisKey key)</code></li>
</ul>
<p>Internally they are implemented very differently so that they both get the job done with the minimum of fuss and overhead, but they were both calling into the same <code>WriteMessage</code> at some point. Actually, never afraid to double-down on the anti-patterns, this means that for the <em>async</em> callers, they were effectively doing async-over-sync-over-async; ouch.</p>
<p>The <code>WriteMessage</code> code above is used from both the <em>synchronous</em> and <em>asynchronous</em> call paths. As it happens, much of our internal existing <em>application</em> codebase mostly uses the synchronous paths (we're gradually adding more <code>async</code>, but we need to complete our in-progress transition from .NET Framework to .NET Core to be able to do it more extensively), and on the synchronous paths you were always going to be blocked <em>anyway</em>, so from the perspective of synchronous callers, there's not really that much wrong with the above. It does what it promises: execute synchronously.</p>
<p>The <em>problem</em> here comes from <em>asynchronous</em> callers, who thought they were calling <code>StringGetAsync</code>, and their thread got blocked. The golden rule of <code>async</code> is: don't block an <code>async</code> caller unless you <em>really, really</em> have to. We broke this rule, and we had reports from users about big thread-jams with <code>async</code> call paths all stuck at <code>WriteMessage</code>, because <em>one thread</em> had paused for the flush, and all the other threads were trying to obtain the lock.</p>
<p>Note: the problem here isn't that "a backlog happened, and we had to delay" - that's just business as normal. That happens, especially when you need mutex-like semantics. The problem is that we <em>blocked the worker threads</em> (although we did at least have the good grace to include a timeout), which <em>under heavy load</em> caused thread-pool starvation and a cascading failure (again: watch the video above).</p>
<h2>So what <em>should</em> we have done <strong>in theory</strong>?</h2>
<p>Given that we have both synchronous and asynchronous call-paths, what we <em>should</em> do is have two versions of the write code:</p>
<ul>
<li><code>void WriteMessage(Message message)</code></li>
<li><code>ValueTask WriteMessageAsync(Message message)</code></li>
</ul>
<p>but we get into immediate problems when we talk about our locking mechanism. We can see this more clearly if we use a simple <code>lock</code> rather than the more complex <code>Monitor</code> usage above - the following <strong>does not compile</strong>:</p>
<div><pre><span>async</span> <span>ValueTask</span> <span>Foo</span>()
{
    <span>lock</span> (<span>syncLock</span>)
    {
        <span><span>//</span> CS1996 - Cannot await in the body of a lock statement</span>
        <span>await</span> <span>Task</span>.<span>Delay</span>(<span>SomeAmount</span>);
    }
}</pre></div>
<p>The reason this doesn't work is that <code>lock</code> (aka <code>Monitor</code>) is <strong>thread-oriented</strong>. You need to <code>[Try]Enter</code> (take the lock) and <code>Exit</code> (release the lock) the constrained region from the same thread. But the moment you <code>await</code>, you're saying "this might continue synchronously, or it might resume later <em>on a different thread</em>". This actually has two consequences:</p>
<ul>
<li>it would mean that when we try to release the lock, it will fail because the resuming thread probably won't actually have it</li>
<li>when we <code>await</code>, we're releasing the <em>current</em> thread back to do <em>whatever else needs doing</em>â€¦ which <em>could</em> actually end up calling back into <code>Foo</code>â€¦ and <code>Monitor</code> is "re-entrant", meaning: if you have the lock <em>once</em>, you can actually <code>lock</code> <strong>again</strong> successfully (it maintains a counter internally), which means that code in a completely unrelated execution context could incorrectly end up <strong>inside</strong> the <code>lock</code>, <em>before</em> we've resumed from the <code>await</code> and logically released it</li>
</ul>
<p>As a side note, it is worth knowing that the compiler <em>only</em> spots this (CS1996) if you use <code>lock</code>; if you use manual <code>Monitor</code> code (because of timeouts), it won't warn you - you just need to know not to do this (which perhaps by itself is good motivation for "<code>lock</code> with timeout" as a language feature). Fortunately, I <em>did</em> know not to do this - and I moved to the next most obvious locking primitive: <a href="https://docs.microsoft.com/en-us/dotnet/api/system.threading.semaphoreslim" rel="nofollow"><code>SemaphoreSlim</code></a>. A semaphore is like <code>Monitor</code>, but instead of being thread-based, it is purely counter-based. Theoretically you can use a semaphore to say "no more than 5 in here", but in reality it is often used as a mutex by saying "no more than 1". <code>SemaphoreSlim</code> is particularly enticing because it has both synchronous and asynchronous APIs, allowing us to split our code in two fairly neatly:</p>
<div><pre><span>readonly</span> <span>SemaphoreSlim</span> <span>singleWriter</span>
    <span>=</span> <span>new</span> <span>SemaphoreSlim</span>(<span>1</span>); <span><span>//</span> single writer</span>

<span>void</span> <span>WriteMessage</span>(<span>Message</span> <span>message</span>)
{
    <span>if</span> (<span>!</span><span>singleWriter</span>.<span>Wait</span>(<span>timeout</span>))
        <span>ThrowTimeout</span>();
    <span>try</span>
    {
        <span>ActuallyWriteTheThing</span>(<span>message</span>);
        <span>FlushSync</span>(); <span><span>//</span> our hack from before</span>
    }
    <span>finally</span>
    {
        <span>singleWriter</span>.<span>Release</span>();
    }
}

<span>async</span> <span>ValueTask</span> <span>WriteMessageAsync</span>(<span>Message</span> <span>message</span>)
{
    <span>if</span> (<span>!</span><span>await</span> <span>singleWriter</span>.<span>WaitAsync</span>(<span>timeout</span>))
        <span>ThrowTimeout</span>();
    <span>try</span>
    {
        <span>ActuallyWriteTheThing</span>(<span>message</span>);
        <span>await</span> <span>FlushAsync</span>();
    }
    <span>finally</span>
    {
        <span>singleWriter</span>.<span>Release</span>();
    }
}</pre></div>
<p>This looks broadly similar to what we had before; the <code>new SemaphoreSlim(1)</code> initializes the semaphore with a limit of 1, i.e. a mutex. In the synchronous path, it works mostly like it always did, but the asynchronous path (now used by the asynchronous callers) now <em>correctly</em> releases worker threads back to wherever worker threads go - when either they can't get the lock yet, <em>or</em> when they are waiting (or rather: awaiting) on the flush. We still have the sync-over-async in the sync path, but that's not really a problem in this case - but we've completely fixed the async path. Short of <em>removing</em> or <a href="https://github.com/aspnet/Announcements/issues/342" rel="nofollow"><em>optionally disabling</em> the sync path</a> (which is an idea I'm putting serious thought into doing, as an opt-in thing), that's probably about as good as we can get.</p>
<p>This looks like it should work, and the chances are that this would have completely solved the problems being seen by our consumers with heavily asynchronous workloads. But one of the nice things about working at Stack Overflow is that I have an opportunity to dogfood library releases under Stack Overflow load (which isn't "big big" by any stretch, but it is comfortably big enough to give me confidence that the library isn't pathologically broken). So, we dropped the above changes into production (after testing etc.), and: BOOM!</p>
<p>We went down.</p>
<h2>What happened there?</h2>
<p>Fortunately, we were lucky enough to manage to grab some process dumps from the production servers in their death-throes before we stood them back up (with the older version of the library), and the stack-traces in the doomed processes were very interesting; they are pretty verbose, but something that kept recurring (note: I've inverted and summarised this trace for readability):</p>
<pre><code>WriteMessage
â€¦
System.Threading.SemaphoreSlim.Wait
â€¦
System.Threading.SemaphoreSlim.WaitAsync
â€¦
KERNELBASE!WaitForMultipleObjectsEx
</code></pre>
<p>This was the case for 650+ threads - almost all of them; and critically, <strong>no-one</strong> actually had the lock - nobody was doing anything useful. The semaphore had, in an edge case, failed to activate the lucky winner of the <a href="https://en.wikipedia.org/wiki/Lord_of_the_Flies" rel="nofollow">conch</a>.</p>
<h3>What actually went wrong?</h3>
<p>Looking at it, our <em>synchronous</em> <code>WriteMessage</code> implementation, when calling <code>Wait</code> on the semaphore, was calling into <code>WaitAsync</code>, and then blocking at the kernel for the object. Despite looking odd, this by itself <em>isn't actually</em> a terrible idea. It turns out that <code>SemaphoreSlim</code> has different strategies that it uses internally:</p>
<ul>
<li>if you're just using synchronous <code>Wait</code> calls, it can handle everything using regular synchronous code and syncronous blocks</li>
<li>if you're just using <code>WaitAsync</code>, because it wants to release the caller promptly, it needs to maintain a queue (actually a linked-list) of waiting callers as <code>Task&lt;bool&gt;</code>; when you release something, it takes the next item from one end of the list, and reactivates (<code>TrySetResult</code>) that caller</li>
<li>if you're using a mixture of <code>Wait</code> and <code>WaitAsync</code>, if it can't get access immediately, then it uses the <code>WaitAsync</code> approach so that the <code>Wait</code> and <code>WaitAsync</code> consumers are in the same queue - otherwise you'd have two separate queues and it gets very confusing and unpredictable</li>
</ul>
<p>Now this <em>seems</em> fine, but it turns out that the way it was using <code>TrySetResult</code> wasâ€¦ problematic. It wasn't using <code>TrySetResult</code> <em>directly</em>, but instead was <em>enqueuing</em> a work item to do the <code>TrySetResult</code>. There's actually a good - albeit now legacy - reason for this: thread stealing, <em>another</em> problem I've had to contend with many times.</p>
<p>When you call <code>TrySetResult</code> etc. on a <code>Task&lt;T&gt;</code> (usually via <code>TaskCompletionSource&lt;T&gt;</code>), it is possible (likely, even) that the <em>async continuation</em> is going to run immediately and inline <strong>on the thread that called <code>TrySetResult</code>.</strong> This is something you need to be really careful about - it can lead to dedicated IO threads somehow ending up serving web requests; or more generally: just â€¦ not doing what you expected. But in the scenario presented we got into a "spiral of death": due to a very brief blip from the <code>FlushAsync</code>, our workers had got stuck in the <code>Wait</code>-&gt;<code>WaitAsync</code> path, and the <strong>very thing</strong> that was meant to unblock everything: needed a worker. To release (resource) you need more of (resource), and (resource) is currently exhausted. It is <strong>almost impossible</strong> to recover from that situation due to the growth limits on workers, and the servers became increasingly unstable until they stopped working completely.</p>
<p>This is clearly a dangerous scenario, so we reported it as an issue, and amazingly within a day Stephen Toub had a <a href="https://github.com/dotnet/corefx/commit/ecba811b1438517ac90a957e2cfe4cef64a13861" rel="nofollow">surprisingly minimal and elegant fix for <code>SemaphoreSlim</code></a>. The commit message (and code changes themselves) explain it in more depth, but by configuring the queued tasks with the <code>TaskCreationOptions.RunContinuationsAsynchronously</code> flag, it means the "release" code can call <code>TrySetResult</code> <strong>directly</strong>, without needing an extra worker as an intermediary. In the specific case where the only thing waiting on the task is a synchronous <code>Wait</code>, the task code already has specific detection to unblock that scenario directly without needing a worker <em>at all</em>, and in the genuine <code>async</code>/<code>await</code> case, we just end up with the <em>actual work</em> going to the queue, rather than the "call <code>TrySetResult</code>" going to the queue. Tidy!</p>
<h2>But that isn't the end of the story</h2>
<p>It would be nice to say "all's well that ends well; bug in <code>SemaphoreSlim</code> fixed", but it isn't as easy as that. The fix for <code>SemaphoreSlim</code> <em>has</em> been merged, but a) that won't help "us" until the next .NET Framework service release, and b) as library authors, we can't rely on which service releases are on our <em>consumers'</em> machines. We need a fix that works reliably everywhere. So whilst it is great to know that our pain has improved things for future users of <code>SemaphoreSlim</code>, we needed something more immediate and framework-independent. So that's when I went away and created a bespoke synchronous/asynchronous <code>MutexSlim</code> that we are now using in StackExchange.Redis.</p>
<p>It is <em>amazing</em> how much simpler things become if you limit yourself to "0 or 1 people in the pool", so it wasn't <em>actually</em> that much work; but: I thought I knew a lot about <code>async</code>/<code>await</code>, yet in writing <code>MutexSlim</code> I dove deeper into that topic than I have usually had to; and in the second part I'll talk about some of what I learned.</p>

</div></div></div>
    </div>
    <footer>
        <div>created by buildstarted &copy; 2020 <a href="/about">about</a></div>
        <div>Share this page on social media: copy and paste this url https://linksfor.dev/</div>
        <div>If you prefer RSS: <a href="https://linksfor.dev/feed.xml">https://linksfor.dev/feed.xml</a></div>
    </footer>
    
    <script async defer>
        _dna = window._dna || {};
        _dna.siteId = "linksfor.devs";
        _dna.outlink = true;

        (function () {
            let dna = document.createElement('script');
            dna.type = 'text/javascript';
            dna.async = true;
            dna.src = '//dna.buildstarted.com/t.js';
            let s = document.getElementsByTagName('script')[0];
            s.parentNode.insertBefore(dna, s);
        })();
    </script>
    <noscript><img src="//dna.buildstarted.com/g?siteId=linksfor.devs"/></noscript>
</body>
</html>