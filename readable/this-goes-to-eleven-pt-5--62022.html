<!DOCTYPE html>
<html lang="en">
<head>
    <title>
This Goes to Eleven (Pt. 5/&#x221E;) - linksfor.dev(s)    </title>
    <meta charset="utf-8">
    <link rel="alternate" type="application/rss+xml" title="Linksfor.dev(s) feed" href="https://linksfor.dev/feed.rss" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta name="google" value="notranslate">
    <link rel="stylesheet" type="text/css" href="/style.min.css" />
    <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon">
    <link rel="icon" href="/favicon.ico" type="image/x-icon">
    <meta property="og:title" content="This Goes to Eleven (Pt. 5/&#x221E;) - linksfor.dev(s)"/>
    <meta property="article:author" content="damageboy"/>
    <meta property="og:description" content="Decimating Array.Sort with AVX2. I ended up going down the rabbit hole re-implementing array sorting with AVX2 intrinsics. There&#x2019;s no reason I should go down alone."/>
    <meta property="og:type" content="website"/>
    <meta property="og:url" content="https://bits.houmus.org/2020-02-02/this-goes-to-eleven-pt5"/>

<meta property="og:site_name" content="linksfor.dev(s)" />
</head>
<body>
    <div class="devring" style="background: #222">
        <div style="text-align:center">Explore other dev related sites in this ring. If you would like to join this ring <a href="https://devring.club">click here</a>.</div>
        <div class="grid">
            <div style="display: grid; grid-template-columns: .5fr 1fr 1fr 1fr; text-align: center;">
                <span class="devring-title"><a href="https://devring.club/">devring.club</a></span>
                <a href="https://devring.club/sites/1/prev" class="devring-previous">Previous</a>
                <a href="https://devring.club/random" class="devring-random">Random</a>
                <a href="https://devring.club/sites/1/next" class="devring-next">Next</a>
            </div>
        </div>
    </div>
    <div class="grid">
        <h1 style="margin: unset">
<a href="/" style="color:inherit">linksfor.dev(s)</a>
        </h1>
        <title>linksfor.dev(s) - This Goes to Eleven (Pt. 5/&#x221E;)</title>
<div class="readable">
        <h1>This Goes to Eleven (Pt. 5/&#x221E;)</h1>
            <div>by damageboy</div>
            <div>Reading time: 43-54 minutes</div>
        <div>Posted here: 22 May 2020</div>
        <p><a href="https://bits.houmus.org/2020-02-02/this-goes-to-eleven-pt5">https://bits.houmus.org/2020-02-02/this-goes-to-eleven-pt5</a></p>
        <hr/>
<div id="readability-page-1" class="page"><div>
      

      <section itemprop="text">
        
        <p>I ended up going down the rabbit hole re-implementing array sorting with AVX2 intrinsics, and there’s no reason I should go down alone.</p>

<p>Since there’s a lot to go over here, I’ll split it up into a few parts:</p>

<ol>
  <li>In <a href="https://bits.houmus.org/2020-01-28/this-goes-to-eleven-pt1">part 1</a>, we start with a refresher on <code>QuickSort</code> and how it compares to <code>Array.Sort()</code>.</li>
  <li>In <a href="https://bits.houmus.org/2020-01-29/this-goes-to-eleven-pt2">part 2</a>, we go over the basics of vectorized hardware intrinsics, vector types, and go over a handful of vectorized instructions we’ll use in part 3. We still won’t be sorting anything.</li>
  <li>In <a href="https://bits.houmus.org/2020-01-30/this-goes-to-eleven-pt3">part 3</a>, we go through the initial code for the vectorized sorting, and start seeing some payoff. We finish agonizing courtesy of the CPU’s branch predictor, throwing a wrench into our attempts.</li>
  <li>In <a href="https://bits.houmus.org/2020-02-01/this-goes-to-eleven-pt4">part 4</a>, we go over a handful of optimization approaches that I attempted trying to get the vectorized partition to run faster, seeing what worked and what didn’t.</li>
  <li>In this part, we’ll take a deep dive into how to deal with memory alignment issues.</li>
  <li>In part 6, we’ll take a pause from the vectorized partitioning, to get rid of almost 100% of the remaining scalar code, by implementing small, constant size array sorting with yet more AVX2 vectorization.</li>
  <li>In part 7, We’ll circle back and try to deal with a nasty slowdown left in our vectorized partitioning code</li>
  <li>In part 8, I’ll tell you the sad story of a very twisted optimization I managed to pull off while failing miserably at the same time.</li>
  <li>In part 9, I’ll try some algorithmic improvements to milk those last drops of perf, or at least those that I can think of, from this code.</li>
</ol>

<h2 id="trying-to-squeeze-some-more-vectorized-juice">(Trying) to squeeze some more vectorized juice</h2>

<p>I thought it would be nice to show a bunch of things I ended up trying to improve performance.
I tried to keep most of these experiments in separate implementations, both the ones that yielded positive results and the failures. These can be seen in the original repo under the <a href="https://github.com/damageboy/VxSort/tree/research/VxSortResearch/Unstable/AVX2/Happy">Happy</a> and <a href="https://github.com/damageboy/VxSort/tree/research/VxSortResearch/Unstable/AVX2/Sad">Sad</a> folders.</p>

<p>While some worked, and some didn’t, I think a bunch of these were worth mentioning, so here goes:</p>

<h3 id="aligning-our-expectations">Aligning our expectations</h3>

<center>

</center>

<p>This quote, taken from Hennessy and Patterson’s <a href="https://www.elsevier.com/books/computer-architecture/hennessy/978-0-12-811905-1">“Computer Architecture: A Quantitative Approach, 6th Edition”</a>, which is traced to all the way back to the fathers of modern-day computing in 1946 can be taken as a foreboding warning for the pains that are related to anything that deals with the complexity of memory hierarchies.</p>

<p>With modern computer hardware, CPUs <em>might</em> access memory more efficiently when it is naturally aligned: in other words, when the <em>address</em> we use is a multiple of some magical constant. The constant is classically the machine word size, 4/8 bytes on 32/64 bit machines. These constants are related to how the CPU is physically wired and constructed internally. Historically, older processors used to be very limited, either disallowing or severely limiting performance, with non-aligned memory access. To this day, very simple micro-controllers (like the ones you might find in IoT devices, for example) will exhibit such limitations around memory alignment, essentially forcing memory access to conform to multiples of 4/8 bytes. With more modern (read: more expensive) CPUs, these requirements have become increasingly relaxed. Most programmers can simply afford to <em>ignore</em> this issue. The last decade or so worth of modern processors are oblivious to this problem per-se, as long as we access memory within a <strong>single cache-line</strong>, or 64-bytes on almost any modern-day processors.</p>

<p>What is this cache-line? I’m actively fighting my internal inclination, so I <strong>won’t  turn</strong> this post into a detour about computer micro-architecture. Caches have been covered elsewhere ad-nauseam by far more talented writers, that I’ll never do it justice anyway. Instead, I’ll just do the obligatory one-paragraph reminder where we recall that CPUs don’t directly communicate with RAM, as it is dead slow; instead, they read and write from internal, on-die, special/fast memory called caches. Caches contain partial copies of RAM. Caches are faster, smaller, and organized in multiple levels (L1/L2/L3 caches, to name them), where each level is usually larger in size and slightly slower in terms of latency. When the CPU is instructed to access memory, it instead communicates with the cache units, but it never does so in small units. Even when our code is reading a <em>single byte</em>, the CPU will communicate with it’s cache subsystem in a unit-of-work known as a cache-line. In theory, every CPU model may have its own definition of a cache-line, but in practice, the last 15 years of processors seem to have converged on 64-bytes as that golden number.</p>

<p>Now, what happens when, lets say, our read operations end up <strong>crossing</strong> cache-lines?</p>

<center>

</center>

<p>As mentioned, the unit-of-work, as far as the CPU is concerned, is a 64-byte cache-line. Therefore, such reads literally cause the CPU to issue <em>two</em> read operations downstream, ultimately directed at the cache units<sup id="fnref:0" role="doc-noteref"><a href="#fn:0">1</a></sup>. These cache-line crossing reads <em>do</em> have a sustained effect on perfromance<sup id="fnref:1" role="doc-noteref"><a href="#fn:1">2</a></sup>. But how often do they occur? Let’s consider this by way of example:<br>
Imagine we are processing a single array sequentially, reading 32-bit integers at a time, or 4-bytes; if for some reason, our starting address is <em>not</em> divisible by 4, cross cache-line reads would occur at a rate of <code>4/64</code> or <code>6.25%</code> of reads. Even this paltry rate of cross cache-line reads usually remains in the <em>realm of theory</em> since we have the memory allocator and compiler working in tandem, behind the scenes, to make this go away:</p>

<ul>
  <li>The default allocator <em>always</em> returns memory aligned at least to machine word size on the one hand.</li>
  <li>The compiler/JIT use padding bytes within our classes/structs in-between members, as needed, to ensure that individual members are aligned to 4/8 bytes.</li>
</ul>

<p>So far, I’ve told you why/when you <em>shouldn’t</em> care about alignment. This was my way of both easing you into the topic and helping you feel OK if this is news to you. You really can afford <em>not to think</em> about this without paying any penalty, for the most part. Unfortunately, this <strong>stops</strong> being true for <code>Vector256&lt;T&gt;</code> sized reads, which are 32 bytes wide (256 bits / 8). And this is <em>doubly not true</em> for our partitioning problem:</p>

<ul>
  <li>The memory handed to us for partitioning/sorting is rarely aligned to 32-bytes, except for dumb luck.<br>
The allocator, allocating an array of 32-bit integers, simply doesn’t care about 32-<strong>byte</strong> alignment.</li>
  <li>Even if it were magically aligned to 32-bytes, it would do us little good; Once a <em>single</em> partition operation is complete, further sub-divisions, inherent with QuickSort, are determined by the (random) new placement of the last pivot we used.<br>
There is no way we will get lucky enough that <em>every partition</em> will be 32-byte aligned.</li>
</ul>

<p>Now that it is clear that we won’t be 32-byte aligned, we finally realize that as we go over the array sequentially (left to right and right to left as we do) issuing <strong>unaligned</strong> 32-byte reads on top of a 64-byte cache-line, we end up reading across cache-lines every <strong>other</strong> read! Or at a rate of 50%! This just escalated from being “…generally not a problem” into a “Houston, we have a problem” very quickly.</p>

<p>You’ve endured through a lot of hand waving so far, let’s try to see if we can get some damning evidence for all of this, by launching <code>perf</code>, this time tracking the oddly specific <code>mem_inst_retired.split_loads</code> HW counter:</p>

<div><div><pre><code><table><tbody><tr>
<td><pre>1
2
3
4
5
6
7
8
9
10
11
</pre></td>
<td><pre><span>$ COMPlus_PerfMapEnabled</span><span>=</span>1 perf record <span>-Fmax</span> <span>-e</span> mem_inst_retired.split_loads <span></span>
    ./Example <span>--type-list</span> DoublePumpJedi <span>--size-list</span> 100000 <span></span>
        <span>--max-loops</span> 1000 <span>--no-check</span>
<span>$ </span>perf report <span>--stdio</span> <span>-F</span> overhead,sym | <span>head</span> <span>-20</span>

<span># To display the perf.data header info, please use --header/--header-only options.</span>
<span># Event count (approx.): 87102613</span>
<span># Overhead  Symbol</span>
    86.68%  <span>[</span>.] ...DoublePumpJedi::VectorizedPartitionInPlace<span>(</span>int32<span>*</span>,int32<span>*</span><span>)</span>
     5.74%  <span>[</span>.] ...DoublePumpJedi::Sort<span>(</span>int32<span>*</span>,int32<span>*</span>,int32<span>)</span>
     2.99%  <span>[</span>.] __memmove_avx_unaligned_erms
</pre></td>
</tr></tbody></table></code></pre></div></div>

<p>We ran the same sort operation <code>1,000</code> times and got <code>87,102,613</code> split-loads, with <code>86.68%</code> attributed to our partitioning function. This means <code>(87102613 * 0.8668) / 1000</code> or <code>75,500</code> split-loads <em>per sort</em> of <code>100,000</code> elements. To seal the deal, we need to figure out how many vector loads per sort we are performing in the first place; Luckily I can generate an answer quickly: I have statistics collection code embedded in my code, so I can issue this command:</p>

<div><div><pre><code><table><tbody><tr>
<td><pre>1
2
3
</pre></td>
<td><pre><span>$ </span>./Example <span>--type-list</span> DoublePumpJedi <span></span>
      <span>--size-list</span> 100000 <span>--max-loops</span> 10000 <span></span>
      <span>--no-check</span> <span>--stats-file</span> jedi-100k-stats.json
</pre></td>
</tr></tbody></table></code></pre></div></div>

<p>And in return I get this beutiful thing back:</p>

<table>
<tbody><tr>
<td><span>Note</span></td>
<td>
<div>
        <p>These numbers are vastly different than the ones we last saw in the end of the 3<sup>rd</sup> post, for example. There is a good reason for this: We’ve spent the previous post tweaking the code in a few considerable ways:</p>
        <ul>
          <li>Changing the cut-off point for vectorized sorting from 16 ⮞ 40, there-by reducing the amount of vectorized partitions we’re performing in the first place.</li>
          <li>Changing the permutation entry loading code to read 8-byte values from memroy, rather than full 32-byte <code>Vector256&lt;int&gt;</code> entries,
cutting the number of <code>Vector256&lt;int&gt;</code> loads by half.</li>
        </ul>
      </div>
</td>
</tr>
</tbody></table>

<div>
<!-- <button class="helpbutton" data-toggle="chardinjs" onclick="$('body').chardinJs('start')"><object style="pointer-events: none;" type="image/svg+xml" data="/assets/images/help.svg"></object></button> -->

<div>
      
      
      <div>
      
      <div>
      <p><span>
      <span>Loading, please wait</span>
      <span><span></span></span>
      </span>
      </p>
      <table data-json="../_posts/jedi-stats.json" data-id-field="name" data-pagination="false" data-intro="Each row in this table contains statistics collected &amp; averaged out of thousands of runs with random data" data-position="left" data-show-pagination-switch="false" data-page-list="[5,10,20,50,all]" data-show-header="true" data-striped="true">
  <thead data-intro="The header can be used to sort/filter by clicking" data-position="right"><tr><th data-field="MethodName"><p><span data-intro="The name of the benchmarked method" data-position="top">Method<br>Name</span>
        </p></th><th data-field="ProblemSize"><div>
            
        </div></th><th data-field="MaxDepthScaledDataTable"><div>
            
        </div></th><th data-field="NumPartitionOperationsScaledDataTable"><div>
            
        </div></th><th data-field="NumVectorizedLoadsScaledDataTable"><div>
            
        </div></th><th data-field="NumVectorizedStoresScaledDataTable"><div>
            
        </div></th><th data-field="NumPermutationsScaledDataTable"><div>
            
        </div></th><th data-field="AverageSmallSortSizeScaledDataTable"><div>
            
        </div></th><th data-field="NumScalarComparesScaledDataTable"><div>
            
        </div></th><th data-field="PercentSmallSortCompares"><div>
            
        </div></th></tr></thead><tbody><tr data-index="0"><td>DoublePumpJedi</td><td>100,000</td><td><div><p><span>23</span></p></div></td><td><div><p><span>4,194</span></p></div></td><td><div><p><span>173,597</span></p></div></td><td><div><p><span>347,194</span></p></div></td><td><div><p><span>173,597</span></p></div></td><td><div><p><span>23.04</span></p></div></td><td><div><p><span>1,532,140</span></p></div></td><td>87.44%</td></tr></tbody>
</table></div>
      
      </div>
      
      </div>
</div>

<p>In total, we perform <code>173,597</code> vector loads per sort operation of <code>100,000</code> elements in <code>4,194</code> partitioning calls. Assuming our array is aligned to 4-bytes to begin with (which C#’s allocator does very reliably), every partitioning call has a <code>4/32</code> or <code>12.5%</code> of ending up being 32-byte aligned: In other words <code>21,700</code> of the total vector reads should be aligned by sheer chance, which leaves <code>173597-21700</code> or <code>151,898</code> that should be <em>unaligned</em>, of which, I claim that that ½ would cause split-loads: <code>50%</code> of <code>151,898</code> is <code>75,949</code> while we measured <code>75,500</code> with <code>perf</code>! I don’t know how your normal day goes about, but in mine, reality and my hallucinations rarely go hand-in-hand like this.</p>

<p>Fine, we now <strong>know</strong> we have a problem. The first step was acknowledging/accepting reality: Our code does indeed generate a lot of split memory operations. Let’s consider our memory access patterns when reading/writing with respect to alignment, and see if we can do something about it:</p>

<ul>
  <li>For writing, we’re all over the place: we always advance the write pointers according to how the data was partitioned, e.g. it is completely data-dependent, and there is little we can say about our write addresses. In addition, as it happens, Intel CPUs, as almost all other modern CPUs, employ another common trick in the form of <a href="https://en.wikipedia.org/wiki/Write_combining">store buffers, or write-combining buffers (WCBs)</a>. I’ll refrain from describing them here, but the bottom line is we both can’t/don’t need to care about the writing side of our algorithm.</li>
  <li>For reading, the situation is entirely different: We <em>always</em> advance the read pointers by 8 elements (32-bytes) on the one hand, and we even have a special intrinsic: <code>Avx.LoadAlignedVector256() / VMOVDQA</code><sup id="fnref:2" role="doc-noteref"><a href="#fn:2">3</a></sup> that helps us ensure that our reading is properly aligned to 32-bytes.</li>
</ul>

<h4 id="aligning-to-cpu-cache-lines-1">Aligning to CPU Cache-lines: <img title=":+1:" alt=":+1:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f44d.png" height="20" width="20">
</h4>

<p>With this lengthy introduction out of the way, it’s time we do something about these cross-cache line reads. Initially, I got “something” working quickly: remember that we needed to deal with the <em>remainder</em> of the array, when we had less than 8-elements, anyway. In the original code at the end of the 3<sup>rd</sup> post, we did so right after our vectorized loop. If we move that scalar code from the end of the function to its beginning while also modifying it to perform scalar partitioning until both <code>readLeft</code>/<code>readRight</code> pointers are aligned to 32 bytes, our work is complete. There is a slight wrinkle in this otherwise simple approach:</p>

<ul>
  <li>Previously, we had anywhere between <code>0-7</code> elements left as a remainder for scalar partitioning per partition call.
    <ul>
      <li>
<code>3.5</code> elements on average.</li>
    </ul>
  </li>
  <li>Aligning from the edges of our partition with scalar code means we will now have <code>0-7</code> elements per-side…
    <ul>
      <li>So <code>3.5 x 2 == 7</code> elements on average.</li>
    </ul>
  </li>
</ul>

<p>In other words, doing this sort of inwards pre-alignment optimization is not a clean win: We end up with more scalar work than before on the one hand (which is unfortunate), but on the other hand, we can change the vector loading code to use <code>Avx.LoadAlignedVector256()</code> and <em>know for sure</em> that we will no longer be causing the CPU to issue a single cross cache-line read (The latter being the performance boost).<br>
It’s understandable if while reading this, your gut reaction is thinking that adding 3.5 scalar operations doesn’t sound like much of a trade-off, but we have to consider that:</p>

<ul>
  <li>Each scalar comparison comes with a likely branch misprediction, as discussed before, so it has a higher cost than what you might be initially pricing in.</li>
  <li>More importantly: we can’t forget that this is a recursive function, with ever <em>decreasing</em> partition sizes. If you go back to the initial stats we collected in previous posts, you’ll be quickly reminded that we partition upwards of 340k times for 1 million element arrays, so this scalar work both piles up, and represents a larger portion of our workload as the partition sizes decrease…</li>
</ul>

<p>I won’t bother showing the entire code listing for <a href="https://github.com/damageboy/VxSort/blob/research/VxSortResearch/Unstable/AVX2/Happy/B5_1_DoublePumpAligned.cs"><code>B5_1_DoublePumpAligned.cs</code></a>, but I will show the rewritten scalar partition block, which is now tasked with aligning our pointers before we go full vectorized partitioning. Originally it was right after the double-pumped loop and looked like this:</p>

<div><div><pre><code><table><tbody><tr>
<td><pre>1
2
3
4
5
6
7
8
9
10
</pre></td>
<td><pre>    <span>// ...</span>
    <span>while</span> <span>(</span><span>readLeft</span> <span>&lt;</span> <span>readRight</span><span>)</span> <span>{</span>
        <span>var</span> <span>v</span> <span>=</span> <span>*</span><span>readLeft</span><span>++;</span>

        <span>if</span> <span>(</span><span>v</span> <span>&lt;=</span> <span>pivot</span><span>)</span> <span>{</span>
            <span>*</span><span>tmpLeft</span><span>++</span> <span>=</span> <span>v</span><span>;</span>
        <span>}</span> <span>else</span> <span>{</span>
            <span>*--</span><span>tmpRight</span> <span>=</span> <span>v</span><span>;</span>
        <span>}</span>
    <span>}</span>
</pre></td>
</tr></tbody></table></code></pre></div></div>

<p>The aligned variant, with the alignment code now at the top of the function, looks like this:</p>

<div><div><pre><code><table><tbody><tr>
<td><pre>1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
</pre></td>
<td><pre>    <span>const</span> <span>ulong</span> <span>ALIGN</span> <span>=</span> <span>32</span><span>;</span>
    <span>const</span> <span>ulong</span> <span>ALIGN_MASK</span> <span>=</span> <span>ALIGN</span> <span>-</span> <span>1</span><span>;</span>

    <span>if</span> <span>(((</span><span>ulong</span><span>)</span> <span>readLeft</span> <span>&amp;</span> <span>ALIGN_MASK</span><span>)</span> <span>!=</span> <span>0</span><span>)</span> <span>{</span>
        <span>var</span> <span>nextAlign</span> <span>=</span> <span>(</span><span>int</span> <span>*)</span> <span>(((</span><span>ulong</span><span>)</span> <span>readLeft</span> <span>+</span> <span>ALIGN</span><span>)</span> <span>&amp;</span> <span>~</span><span>ALIGN_MASK</span><span>);</span>
        <span>while</span> <span>(</span><span>readLeft</span> <span>&lt;</span> <span>nextAlign</span><span>)</span> <span>{</span>
            <span>var</span> <span>v</span> <span>=</span> <span>*</span><span>readLeft</span><span>++;</span>
            <span>if</span> <span>(</span><span>v</span> <span>&lt;=</span> <span>pivot</span><span>)</span> <span>{</span>
                <span>*</span><span>tmpLeft</span><span>++</span> <span>=</span> <span>v</span><span>;</span>
            <span>}</span> <span>else</span> <span>{</span>
                <span>*--</span><span>tmpRight</span> <span>=</span> <span>v</span><span>;</span>
            <span>}</span>
        <span>}</span>
    <span>}</span>
    <span>Debug</span><span>.</span><span>Assert</span><span>(((</span><span>ulong</span><span>)</span> <span>readLeft</span> <span>&amp;</span> <span>ALIGN_MASK</span><span>)</span> <span>==</span> <span>0</span><span>);</span>

    <span>if</span> <span>(((</span><span>ulong</span><span>)</span> <span>readRight</span> <span>&amp;</span> <span>ALIGN_MASK</span><span>)</span> <span>!=</span> <span>0</span><span>)</span> <span>{</span>
        <span>var</span> <span>nextAlign</span> <span>=</span> <span>(</span><span>int</span> <span>*)</span> <span>((</span><span>ulong</span><span>)</span> <span>readRight</span> <span>&amp;</span> <span>~</span><span>ALIGN_MASK</span><span>);</span>
        <span>while</span> <span>(</span><span>readRight</span> <span>&gt;</span> <span>nextAlign</span><span>)</span> <span>{</span>
            <span>var</span> <span>v</span> <span>=</span> <span>*--</span><span>readRight</span><span>;</span>
            <span>if</span> <span>(</span><span>v</span> <span>&lt;=</span> <span>pivot</span><span>)</span> <span>{</span>
                <span>*</span><span>tmpLeft</span><span>++</span> <span>=</span> <span>v</span><span>;</span>
            <span>}</span> <span>else</span> <span>{</span>
                <span>*--</span><span>tmpRight</span> <span>=</span> <span>v</span><span>;</span>
            <span>}</span>
        <span>}</span>
    <span>}</span>
    <span>Debug</span><span>.</span><span>Assert</span><span>(((</span><span>ulong</span><span>)</span> <span>readRight</span> <span>&amp;</span> <span>ALIGN_MASK</span><span>)</span> <span>==</span> <span>0</span><span>);</span>
</pre></td>
</tr></tbody></table></code></pre></div></div>

<p>What it does now is check when alignment is necessary, then proceeds to align while also partitioning each side into the temporary memory.</p>

<p>Where do we end up performance-wise with this optimization?</p>

<div>
  <div>

<ul data-uk-switcher="{connect:'#4dafdfad-bd57-4836-bed6-2a8e5e6743f5'}">

	<li><a href="#" aria-expanded="true"><i></i> Scaling</a></li>

	<li><a href="#" aria-expanded="false"><i></i> Time/N</a></li>

	<li><a href="#" aria-expanded="false"><i></i> Benchmarks</a></li>

	<li><a href="#" aria-expanded="false"><i></i> Setup</a></li>

</ul>

<ul id="4dafdfad-bd57-4836-bed6-2a8e5e6743f5">

	<li>
<div>
<div data-intro="Size of the sorting problem, 10..10,000,000 in powers of 10" data-position="bottom">
<div data-intro="Performance scale: Array.Sort (solid gray) is always 100%, and the other methods are scaled relative to it" data-position="left">
<div data-intro="Click legend items to show/hide series" data-position="right">
<div>
<canvas data-chart="line" height="500" width="1020">
N,100,1K,10K,100K,1M,10M
Jedi,         1   , 1   , 1  , 1   , 1    , 1
Aligned, 1.082653616,    1.091733385,    0.958578753,    0.959159569,    0.964604818,    0.980102965
<!-- 
{ 
 "data" : {
  "datasets" : [
  { 
    "backgroundColor": "rgba(66,66,66,0.35)",
    "rough": { "fillStyle": "hachure", "hachureAngle": -30, "hachureGap": 9, "fillWeight": 0.3}
  },
  { 
    "backgroundColor": "rgba(33,220,33,.9)",
    "rough": { "fillStyle": "hachure", "hachureAngle": 60, "hachureGap": 3}
  }  
  ]
 },
 "options": {
    "title": { "text": "AVX2 Aligned Sorting - Scaled to Jedi", "display": true },
    "scales": { 
      "yAxes": [{
       "ticks": {
         "fontFamily": "Indie Flower",
         "min": 0.90, 
         "callback": "ticksPercent"
        },
        "scaleLabel": {
          "labelString": "Scaling (%)",
          "display": true
        }
      }]
    }
 },
 "defaultOptions": {"scales":{"xAxes":[{"scaleLabel":{"display":"true,","labelString":"N (elements)","fontFamily":"Indie Flower"},"ticks":{"fontFamily":"Indie Flower"}}]},"legend":{"display":true,"position":"bottom","labels":{"fontFamily":"Indie Flower","fontSize":14}},"title":{"position":"top","fontFamily":"Indie Flower","fontSize":16}}
}
--> </canvas>

</div>
</div>
</div>
</div>
</div>

</li>

	<li>
<div>
<div data-intro="Size of the sorting problem, 10..10,000,000 in powers of 10" data-position="bottom">
<div data-intro="Time in nanoseconds spent sorting per element. Array.Sort (solid gray) is the baseline, again" data-position="left">
<div data-intro="Click legend items to show/hide series" data-position="right">
<p>
<canvas data-chart="line" height="0" width="0">
N,100,1K,10K,100K,1M,10M
Jedi, 18.3938  ,20.7342  ,24.6347  ,26.9067  ,23.9922  ,25.5122
Aligned, 19.9128, 22.6363, 23.6143, 25.8078, 23.143, 25.0046
<!-- 
{ 
 "data" : {
  "datasets" : [
  { 
    "backgroundColor": "rgba(66,66,66,0.35)",
    "rough": { "fillStyle": "hachure", "hachureAngle": -30, "hachureGap": 9, "fillWeight": 0.3}
  },
  { 
    "backgroundColor": "rgba(33,220,33,.9)",
    "rough": { "fillStyle": "hachure", "hachureAngle": 60, "hachureGap": 3}
  }
  ]
 },
 "options": {
    "title": { "text": "AVX2 Jedi Sorting + Aligned - log(Time/N)", "display": true },
    "scales": { 
      "yAxes": [{ 
        "type": "logarithmic",
        "ticks": {
          "min": 15,
          "max": 28,
          "callback": "ticksNumStandaard",
          "fontFamily": "Indie Flower"          
        },
        "scaleLabel": {
          "labelString": "Time/N (ns)",
          "fontFamily": "Indie Flower",
          "display": true
        }
      }]
    }
 },
 "defaultOptions": {"scales":{"xAxes":[{"scaleLabel":{"display":"true,","labelString":"N (elements)","fontFamily":"Indie Flower"},"ticks":{"fontFamily":"Indie Flower"}}]},"legend":{"display":true,"position":"bottom","labels":{"fontFamily":"Indie Flower","fontSize":14}},"title":{"position":"top","fontFamily":"Indie Flower","fontSize":16}}
}
--> </canvas>

</p>
</div>
</div>
</div>
</div>
</li>

	<li>
<div>
<div>
      
      
      <div>
      
      <div>
      <p><span>
      <span>Loading, please wait</span>
      <span><span></span></span>
      </span>
      </p>
      <table data-json="../_posts/Bench.BlogPt5_1_Int32_-report.datatable.json" data-id-field="name" data-pagination="false" data-page-list="[9, 18]" data-intro="Each row in this table represents a benchmark result" data-position="left" data-show-pagination-switch="false" data-show-header="true" data-striped="true">
  <thead data-intro="The header can be used to sort/filter by clicking" data-position="right"><tr><th data-field="TargetMethodColumn.Method"><p><span data-intro="The name of the benchmarked method" data-position="top">
            Method<br>Name
          </span>
        </p></th><th data-field="N"><p><span data-intro="The size of the sorting problem being benchmarked (# of integers)" data-position="top">
            Problem<br>Size
            </span>
        </p></th><th data-field="TimePerNDataTable"><p><span data-intro="Time in nanoseconds spent sorting each element in the array (with confidence intervals in parenthesis)" data-position="top">
              Time /<br>Element (ns)
            </span>
        </p></th><th data-field="RatioDataTable"><p><span data-intro="Each result is scaled to its baseline (Array.Sort in this case)" data-position="top">
                  Scaling
            </span>
        </p></th><th data-field="Measurements"><p><span data-intro="Raw benchmark results visualize how stable the result it. Longest/Shortest runs marked with <span style='color: red'>Red</span>/<span style='color: green'>Green</span>" data-position="top">Measurements</span>
        </p></th></tr></thead><tbody><tr data-index="0"><td>Jedi</td><td>100</td><td>18.39 <small>(17.23 - 19.56)</small></td><td><div><p><span>100%</span></p></div></td><td><canvas data-value="6376,5865,5270,5423,5135,5211,5236,5924,6231,6306,6385,5390,5230,5361,5552,6442.5;#AA0000,6394.5,5288.5,5277.5,4815.5;#00AA00,4887.5,5035.5,5072.5,5421.5,5179.5,5320.5,5191.5,5285.5" width="252" height="25"></canvas></td></tr><tr data-index="1"><td>Aligned</td><td>100</td><td>19.91 <small>(18.51 - 21.31)</small></td><td><div><p><span>109%</span></p></div></td><td><canvas data-value="7510.5;#AA0000,6816.5,5932.5,5489.5,5812.5,7233.5,5646.5,5762.5,7020.5,6311.5,6586.5,6089.5,5870.5,6257.5,6102.5,5529.5,5460.5,5559.5,5697.5,5517.5,5470.5,5539.5,5930.5,5741.5,5401.5,5683.5,5319.5;#00AA00" width="243" height="25"></canvas></td></tr><tr data-index="2"><td>Jedi</td><td>1,000</td><td>20.73 <small>(20.09 - 21.38)</small></td><td><div><p><span>100%</span></p></div></td><td><canvas data-value="62464.5,62520.5,62379.5,60179.5,65453.5,65069.5,62933.5,65467.5,62500.5,64189.5,69697.5;#AA0000,63607.5,59535,61785,57060;#00AA00,60760,60712,61729,59671,61487,58968,60865,60389,61662,63977" width="225" height="25"></canvas></td></tr><tr data-index="3"><td>Aligned</td><td>1,000</td><td>22.64 <small>(20.84 - 24.43)</small></td><td><div><p><span>110%</span></p></div></td><td><canvas data-value="66690,65615,64624,63959,65798,64286,65141,67177,66566,64169,65093,63166,64176,67285,66397,64300,64150,72723,67568,93114;#AA0000,68309,84439,85604,62571;#00AA00,63467,63337,63819" width="243" height="25"></canvas></td></tr><tr data-index="4"><td>Jedi</td><td>10,000</td><td>24.63 <small>(23.86 - 25.40)</small></td><td><div><p><span>100%</span></p></div></td><td><canvas data-value="713505,753569,705020,707779,723896,719208,733040,722799,724490,741569,710202,788505,755752,713804,782890,714477,769510,766206,668918;#00AA00,764347,770945,782291,766457,798708;#AA0000,735989,722322,697875" width="243" height="25"></canvas></td></tr><tr data-index="5"><td>Aligned</td><td>10,000</td><td>23.61 <small>(22.99 - 24.24)</small></td><td><div><p><span>96%</span></p></div></td><td><canvas data-value="721359,690978,697484,696171,709132,756206;#AA0000,717908,656814,741541,689753,685139,665248,647276;#00AA00,684313,686876,716102,735029,696513,722389,684323,714372,736497,753886,722908,722056,736093,715526,741408,701140" width="261" height="25"></canvas></td></tr><tr data-index="6"><td>Jedi</td><td>100,000</td><td>26.91 <small>(26.53 - 27.28)</small></td><td><div><p><span>100%</span></p></div></td><td><canvas data-value="8192936,8173158,8094167,7952126,8176988,8203279,7989033,7964697,8000075,7985012,8038300,7991138,8020601,7949732,8330182,7530129.5;#00AA00,8029174.5,8087497.5,8052067.5,8360826.5;#AA0000,7840865.5,8270024.5,7908200.5,8322839.5,8086234.5,8237178.5,8096874.5,8180071.5,8152214.5,7944395.5" width="270" height="25"></canvas></td></tr><tr data-index="7"><td>Aligned</td><td>100,000</td><td>25.81 <small>(25.50 - 26.11)</small></td><td><div><p><span>96%</span></p></div></td><td><canvas data-value="7506665.5,7925810.5,7709396.5,7786190.5,7765455.5,7816658.5,7560944.5,7739391.5,7588607.5,7639000.5,7692467.5,7755197.5,7767372.5,7538481.5,7379596.5;#00AA00,7790310,7756156,7908631,7776460,7711659,7711562,7815458,7852866,7872380,7910544,7991292;#AA0000,7720076,7631925,7911114,7738404" width="270" height="25"></canvas></td></tr><tr data-index="8"><td>Jedi</td><td>1,000,000</td><td>23.99 <small>(23.82 - 24.17)</small></td><td><div><p><span>100%</span></p></div></td><td><canvas data-value="72640443.5,71569135.5,71233737.5,70788340.5,71005166.5,71629603.5,70811633.5,72970935.5,72055927.5,72360967.5,71035051.5,71276335.5,70591828.5;#00AA00,72026078.5,70956933.5,73263559.5;#AA0000,71623411.5,71969304.5,72178087.5,72499935.5,72748372.5,72296666.5,73027307.5,72822222.5,72462011.5,72619711.5,72809408.5,72267598.5,71783444.5" width="261" height="25"></canvas></td></tr><tr data-index="9"><td>Aligned</td><td>1,000,000</td><td>23.14 <small>(23.01 - 23.27)</small></td><td><div><p><span>96%</span></p></div></td><td><canvas data-value="70717210.5;#AA0000,69267985.5,69212086.5,68577151.5,69241775.5,68908889.5,69082086.5,70189442.5,69262740.5,68719815.5,68834994.5,70233976.5,69354460.5,69965943.5,69562748.5,70568502.5,69851945.5,69822860.5,68517584.5;#00AA00,69723013.5,70069638.5,69044970.5,69332978.5,69165280.5,69666402.5,68840536.5,69595204.5,69265561.5,68602649.5,69672557.5" width="270" height="25"></canvas></td></tr><tr data-index="10"><td>Jedi</td><td>10,000,000</td><td>25.51 <small>(25.47 - 25.55)</small></td><td><div><p><span>100%</span></p></div></td><td><canvas data-value="766937067.5,765747405.5,763759088.5,764915747.5,765447471.5,764689733.5,768831826.5,765429686.5,764947491.5,767808246.5,764683948.5,764677718.5,769933449.5;#AA0000,764989613.5,766499890,763139999;#00AA00,764103986,764332479,763852574,764177711,767609580,765396517,764378646,767078352,764392544,763242792,763885337" width="243" height="25"></canvas></td></tr><tr data-index="11"><td>Aligned</td><td>10,000,000</td><td>25.00 <small>(24.98 - 25.03)</small></td><td><div><p><span>98%</span></p></div></td><td><canvas data-value="750332044.5,751746243.5,751329964.5,750819120.5,749770705.5,749648113.5,748626318.5;#00AA00,749978822.5,751188272.5,752995947.5;#AA0000,750341674.5,749833568.5,750690786.5,749663789.5,749219676.5,750262536.5,748787723.5,749938295.5,749550612.5,750448513.5,749872046.5,749619483.5,749461297.5,750293623.5,750157717.5,749004247.5" width="234" height="25"></canvas></td></tr></tbody>
</table></div>
      
      </div>
      
      </div>
</div>

</li>

	<li>
<div><div><pre><code><table><tbody><tr>
<td><pre>1
2
3
4
5
6
7
8
9
10
11
12
13
14
</pre></td>
<td><pre><span>BenchmarkDotNet</span><span>=</span>v0.12.0, <span>OS</span><span>=</span>clear-linux-os 32120
Intel Core i7-7700HQ CPU 2.80GHz <span>(</span>Kaby Lake<span>)</span>, 1 CPU, 4 logical and 4 physical cores
.NET Core <span>SDK</span><span>=</span>3.1.100
  <span>[</span>Host]     : .NET Core 3.1.0 <span>(</span>CoreCLR 4.700.19.56402, CoreFX 4.700.19.56404<span>)</span>, X64 RyuJIT
  Job-DEARTS : .NET Core 3.1.0 <span>(</span>CoreCLR 4.700.19.56402, CoreFX 4.700.19.56404<span>)</span>, X64 RyuJIT

<span>InvocationCount</span><span>=</span>3  <span>IterationCount</span><span>=</span>15  <span>LaunchCount</span><span>=</span>2
<span>UnrollFactor</span><span>=</span>1  <span>WarmupCount</span><span>=</span>10

<span>$ </span><span>grep</span> <span>'stepping|model|microcode'</span> /proc/cpuinfo | <span>head</span> <span>-4</span>
model           : 158
model name      : Intel<span>(</span>R<span>)</span> Core<span>(</span>TM<span>)</span> i7-7700HQ CPU @ 2.80GHz
stepping        : 9
microcode       : 0xb4
</pre></td>
</tr></tbody></table></code></pre></div></div>

</li>

</ul>

</div>

  <p>The whole attempt ends up as a mediocre improvement, so it would seem:</p>

  <ul>
    <li>We’re are seeing a speedup/improvement, in the high counts.</li>
    <li>We seem to be slowing down due to the higher scalar operation count, in the low problem sizes.</li>
  </ul>

  <p>It’s kind of a mixed bad, and perhaps slightly unimpressive at first glance. However, when we stop to remember that we somehow managed both to speed up the function while doubling the amount of scalar work done, the interpretation of the results becomes more nuanced: The pure benefit from alignment itself is larger than what the results are showing right now since it’s being masked, to some extent, by the extra scalar work we tacked on. If only there was a way we could skip that scalar work all together… If only there was a way… If only…</p>
</div>

<h3 id="re-partitioning-overlapping-regions-1-1">(Re-)Partitioning overlapping regions: <img title=":+1:" alt=":+1:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f44d.png" height="20" width="20"> <img title=":+1:" alt=":+1:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f44d.png" height="20" width="20">
</h3>

<p>Next up is a different optimization approach to the same problem, and a natural progression from the last one. At the risk of sounding pompous, I think I <em>might</em> have found something here that no-one has done before in the context of partitioning<sup id="fnref:3" role="doc-noteref"><a href="#fn:3">4</a></sup>: The basic idea here is we get rid of all (ok, ok, <em>almost all</em>) scalar partitioning in our vectorized code path. If we can partition and align the edges of the segment we are about to process with vectorized code, we would be reducing the total number instructions executed. At the same time, we would be retaining more of the speed-up that was lost with the alignment optimization above. This would have a double-whammy compounded effect. But how?</p>



<p>We could go about it the other way around! Instead of aligning <em>inwards</em> in each respective direction, we could align <strong><em>outwards</em></strong> and enlarge the partitioned segment to include a few more (up to 7) elements on the outer rims of each partition and <u>re-partition</u> them using the new pivot we’ve just selected. If this works, we end up doing both 100% aligned reads and eliminating all scalar work in one optimization! This might <em>sound simple</em> and <strong>safe</strong>, but this is the sort of humbling experience that QuickSort is quick at dispensing (sorry, I had to…) at people trying to nudge it in the wrong way. At some point, I was finally able to screw my own head on properly with respect to this re-partitioning attempt and figure out what precisely are the critical constraints we must respect for this to work.</p>

<table>
<tbody><tr>
<td><span>Note</span></td>
<td>
<p>This is a slightly awkward optimization when you consider that I’m suggesting we should <strong>partition more data</strong> in order to <em>speed up</em> our code. This sounds bonkers, unless we dig deep within for some mechanical empathy: not all work is equal in the eyes of the CPU. When we are executing scalar partitioning on <em>n</em> elements, we are really telling the CPU to execute <em>n</em> branches, comparisons, and memory accesses, which are completely data-dependent. The CPU “hates” this sort of work. It has to guess what happens next, and will do so no better than flipping a coin, or 50%, for truly random data. What’s worse, as mentioned before, whenever the CPU mispredicts, there’s a price to pay in the form of a full pipeline flush which roughly costs us 14-15 cycles on a modern CPU. Paying this <strong>once</strong>, is roughly equivalent to partitioning 2 x 8 element vectors with our vectorized partition block! This is the reason that doing “more” might be faster.</p>
</td>
</tr>
</tbody></table>

<p>Back to the constraints. There’s one thing we can <strong>never</strong> do: move a pivot that was previously partitioned. I (now) call them “buried pivots” (since they’re in their final resting place, get it?); Everyone knows, you don’t move around dead bodies, that’s always the first bad thing that happens in a horror movie. There’s our motivation: not being the stupid person who dies first. That’s about it. It sounds simple, but it requires some more serious explanation: When a previous partition operation is complete, the pivot used during that operation is moved to its final resting place. It’s new position is used to subdivide the array, and effectively stored throughout numerous call stacks of our recursive function. There’s a baked-in assumption here that all data left/right of that buried pivot is smaller/larger than it. And that assumption must <strong>never</strong> be broken. If we intend to <strong>re-partition</strong> data to the left and right of a given partition, as part of this overlapping alignment effort, we need to consider that this extra data might already contain buried pivots, and we can not, under any circumstances ever move them again.<br>
In short: Buried pivots stay buried where we left them, or bad things happen.</p>

<p>When we call our partitioning operation, we have to consider what initially looks like an asymmetry of the left and right edges of our to-be-partitioned segment:</p>

<ul>
  <li>For the left side:
    <ul>
      <li>There might not be additional room on the left with extra data to read from.
        <ul>
          <li>We are too close to the edge of the array on the left side!<br>
This happens for all partitions starting at the left-edge of the entire array.</li>
        </ul>
      </li>
      <li>We always partition first left, then right of any buried pivot, we know for a fact that all elements left of “our” partition at any given moment are sorted. e.g. they are all buried pivots, and we can’t re-order them.</li>
      <li>
<em>Important:</em> We also know that each of those values is smaller than or equal to whatever pivot value we <em>will select</em> for the current partitioning operation.</li>
    </ul>
  </li>
  <li>For the right side, it is almost the same set of constraints:
    <ul>
      <li>There might not be additional room on the right with extra data to read from.
        <ul>
          <li>We are too close to the edge of the array on the right side!<br>
This happens for all partitions ending on the right-edge of the entire array.</li>
        </ul>
      </li>
      <li>The immediate value to our right side is a buried pivot, and all other values to its right are larger-than-or-equal to it.</li>
      <li>There might be additional pivots immediately to our right as well.</li>
      <li>
<em>Important:</em> We also know that each of those values is larger-then-or-equal to whatever pivot value we <em>will select</em> for the current partitioning operation.</li>
    </ul>
  </li>
</ul>

<p>All this information is hard to integrate at first, but what it boils down to is that whenever we load up the left overlapping vector, there are anywhere between 1-7 elements we are <strong>not</strong> allowed to reorder on the <em>left side</em>, and when we load the right overlapping vector, there are, again, anywhere between 1-7 elements we are <strong>not</strong> allowed to re-order on <em>that right side</em>. That’s the challenge; the good news is that all those overlapping elements are also guaranteed to also be smaller/larger than whatever pivot we end up selecting from out original (sans overlap) partition. This knowledge gives us the edge we need: We know in advance that the extra elements will generate predictable comparison results compared to <em>any</em> pivot <em>within</em> our partition.</p>

<p>What we need are permutation entries that are <strong><em>stable</em></strong>. I’m coining this phrase freely as I’m going along:<br>
Stable partitioning means that the partitioning operation <strong>must not</strong> <em>reorder</em> values that need to go on the left amongst themselves (we keep their internal ordering amongst themselves). Likewise, it <strong>must not</strong> reorder the values that go on the right amongst themselves. If we manage to do this, we’re in the clear: The combination of stable permutation and predictable comparison results means that the overlapping elements will stay put while other elements will be partitioned properly on both edges of our overlapping partition. After this weird permutation, we just need to forget we ever read those extra elements, and the whole thing just… works? … yes!</p>

<p>Let’s start with cementing this idea of what stable partitioning is: Up to this point, there was no such requirement, and the initial partition tables I generated failed to satisfy this requirement.
Here’s a simple example for stable/unstable permutation entries, let’s imagine we partition the following values around a pivot value of 500:</p>

<table>
  <thead>
    <tr>
      <th>Bit</th>
      <th>0</th>
      <th>1</th>
      <th>2</th>
      <th>3</th>
      <th>4</th>
      <th>5</th>
      <th>6</th>
      <th>7</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>
<code>Vector256&lt;T&gt;</code> Value</td>
      <td>99</td>
      <td>100</td>
      <td>666</td>
      <td>101</td>
      <td>102</td>
      <td>777</td>
      <td>888</td>
      <td>999</td>
    </tr>
    <tr>
      <td>Mask</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <td>Unstable Permutation</td>
      <td>0</td>
      <td>1</td>
      <td><strong>7</strong></td>
      <td>2</td>
      <td>3</td>
      <td><strong>6</strong></td>
      <td><strong>5</strong></td>
      <td><strong>4</strong></td>
    </tr>
    <tr>
      <td>Unstable Result</td>
      <td>99</td>
      <td>100</td>
      <td>101</td>
      <td>102</td>
      <td><strong>999</strong></td>
      <td><strong>888</strong></td>
      <td><strong>777</strong></td>
      <td><strong>666</strong></td>
    </tr>
    <tr>
      <td>Stable Permutation</td>
      <td>0</td>
      <td>1</td>
      <td>4</td>
      <td>2</td>
      <td>3</td>
      <td>5</td>
      <td>6</td>
      <td>7</td>
    </tr>
    <tr>
      <td>Stable Result</td>
      <td>99</td>
      <td>100</td>
      <td>101</td>
      <td>102</td>
      <td>666</td>
      <td>777</td>
      <td>888</td>
      <td>999</td>
    </tr>
  </tbody>
</table>

<p>In the above example, the unstable permutation is a perfectly <em><u>valid</u></em> permutation for general case partitioning. It successfully partitions the sample vector around the pivot value of 500, but the 4 elements marked in bold are re-ordered with respect to each other when compared to the original array. In the stable permutation entry, the internal ordering amongst the partitioned groups is <em>preserved</em>.</p>

<p>Armed with new, stable permutation entries, We can proceed with this overlapping re-partitioning hack: The idea is to find the optimal alignment point on the left and on the right (assuming one is available, e.g. there is enough room on that side), read that data with the <code>LoadVectorAligned256</code> intrinsic, and partition it into the temporary area. The final twist: We need to keep tabs on how many elements <em>do not belong</em> to this partition (e.g. originate from our overlap gymnastics), and remember not to copy them back into our partition at the end of the function, relying on our stable partitioning to keep them grouped at the edges of the temporary buffer we’re copying from… To my amazement, that was kind of it. It just works! (I’ve conveniently ignored a small edge-case here in words, but not in the code :).</p>

<p>The end result is super delicate. If you feel you’ve got it, skip this paragraph, but if you need an alternative view on how this works, here it is: I’ve just described how to partition the initial 2x8 elements (8 on each side); out of those initial 8, We <em>always</em> have a subset that must <strong>never</strong> be reordered (the overlap), and a subset we need to re-order, as is normal, with respect to some pivot. We know that whatever <em>possible</em> pivot value <em>might</em> be selected from our internal partition, it will always be larger/smaller than the elements in the overlapping areas. Knowing that, we can rely on having stable permutation entries that <strong>do not</strong> reorder those extra elements. In the end, we read extra elements, feed them through our partitioning machine, but ignore the extra overlapping elements and avoid <em>all</em> scalar partitioning thanks to this scheme.</p>

<p>In the end, we literally get to eat our cake and keep it whole: For the 99% case we <strong>kill</strong> scalar partitioning all-together, doing <em>zero</em> scalar work, at the same time aligning everything to <code>Vector256&lt;T&gt;</code> size and being nice to our processor. Just to make this victory a tiny touch sweeter, even the <em>initial</em> 2x8 partially overlapping vectors are read using aligned reads!
I named this approach “overligned” (overlap + align) in my code-base; it is available in full in <a href="https://github.com/damageboy/VxSort/blob/research/VxSortResearch/Unstable/AVX2/Happy/B5_2_DoublePumpOverlined.cs"><code>B5_2_DoublePumpOverlined.cs</code></a>. It implements this overlapping alignment approach, with some extra small points for consideration:</p>

<ul>
  <li>When it is <strong>impossible</strong> to align outwards, we fall back to the alignment mechanic introduced in the previous section.<br>
This is uncommon: Going back to the statistical data we collected about random-data sorting in the 3<sup>rd</sup> post, we anticipate a recursion depth of around 40 when sorting 1M elements and ~340K partitioning calls. We will have <em>at least</em> 40x2 (for both sides) such cases where we align inwards for that 1M case, as an example. This is small change compared to the <code>340K - 80</code> calls we can optimize with outward alignment, but it does mean we have to keep that old code lying around.</li>
  <li>Once we calculate for a given partition how much alignment is required on each side, we can cache that calculation recursively for the entire depth of the recursive call stack: This again reduces the overhead we are paying for this alignment strategy.
In the code you’ll see I’m squishing two 32-bit integers into a 64-bit value I call <code>alignHint</code> and I keep reusing one half of 64-bit value without recalculating the alignment <em>amount</em>; If we’ve made it this far, let’s shave a few more cycles off while we’re here.</li>
</ul>

<p>There’s another small optimization I tacked on to this version, which I’ll discuss immediately after providing the results:</p>

<div>
  <div>

<ul data-uk-switcher="{connect:'#ebf23837-fce4-4ad1-95ce-66f5bf844327'}">

	<li><a href="#" aria-expanded="true"><i></i> Scaling</a></li>

	<li><a href="#" aria-expanded="false"><i></i> Time/N</a></li>

	<li><a href="#" aria-expanded="false"><i></i> Benchmarks</a></li>

	<li><a href="#" aria-expanded="false"><i></i> Setup</a></li>

</ul>

<ul id="ebf23837-fce4-4ad1-95ce-66f5bf844327">

	<li>
<div>
<div data-intro="Size of the sorting problem, 10..10,000,000 in powers of 10" data-position="bottom">
<div data-intro="Performance scale: Array.Sort (solid gray) is always 100%, and the other methods are scaled relative to it" data-position="left">
<div data-intro="Click legend items to show/hide series" data-position="right">
<div>
<canvas data-chart="line" height="500" width="1020">
N,100,1K,10K,64K,100K,1M,1.5M,10M
Jedi,         1   , 1  , 1 , 1  , 1   , 1  , 1  , 1
Overlined, 1.012312,    0.995069647, 0.904921232, 0.905092554, 0.915092554, 0.9212314, 0.929801383, 0.960170878

<!-- 
{ 
 "data" : {
  "datasets" : [
  { 
    "backgroundColor": "rgba(66,66,66,0.35)",
    "rough": { "fillStyle": "hachure", "hachureAngle": -30, "hachureGap": 9, "fillWeight": 0.3}
  },
  { 
    "backgroundColor": "rgba(33,220,33,.9)",
    "rough": { "fillStyle": "hachure", "hachureAngle": 60, "hachureGap": 3}
  }  
  ]
 },
 "options": {
    "title": { "text": "AVX2 Overlined Sorting - Scaled to Jedi", "display": true },
    "scales": { 
      "yAxes": [{
       "ticks": {
         "fontFamily": "Indie Flower",
         "min": 0.88, 
         "callback": "ticksPercent"
        },
        "scaleLabel": {
          "labelString": "Scaling (%)",
          "display": true
        }
      }]
    },
    "annotation": {
      "annotations": [{
        "drawTime": "afterDatasetsDraw",
        "type": "line",
        "mode": "vertical",
        "scaleID": "x-axis-0",
        "value": "1.5M",

        "borderColor": "#666666",
        "borderWidth": 2,
      "borderDash": [5, 5],
       "borderDashOffset": 5,
        "label": {
          "yAdjust": 5,
          "backgroundColor": "rgba(255, 0, 0, 0.75)",
          "fontFamily": "Indie Flower",
          "fontSize": 14,
          "content": "L3 Cache Size",
          "enabled": true
        }
      },
      {
        "drawTime": "afterDatasetsDraw",
        "type": "line",
        "mode": "vertical",
        "scaleID": "x-axis-0",
        "value": "64K",
        "borderColor": "#666666",
        "borderWidth": 2,
      "borderDash": [5, 5],
       "borderDashOffset": 5,
        "label": {
          "yAdjust": 65,
          "backgroundColor": "rgba(255, 0, 0, 0.75)",
          "fontFamily": "Indie Flower",
          "fontSize": 14,
          "content": "L2 Cache Size",
          "enabled": true
        }
      }]
    }
 },
 "defaultOptions": {"scales":{"xAxes":[{"scaleLabel":{"display":"true,","labelString":"N (elements)","fontFamily":"Indie Flower"},"ticks":{"fontFamily":"Indie Flower"}}]},"legend":{"display":true,"position":"bottom","labels":{"fontFamily":"Indie Flower","fontSize":14}},"title":{"position":"top","fontFamily":"Indie Flower","fontSize":16}}
}
--> </canvas>

</div>
</div>
</div>
</div>
</div>

</li>

	<li>
<div>
<div data-intro="Size of the sorting problem, 10..10,000,000 in powers of 10" data-position="bottom">
<div data-intro="Time in nanoseconds spent sorting per element. Array.Sort (solid gray) is the baseline, again" data-position="left">
<div data-intro="Click legend items to show/hide series" data-position="right">
<p>
<canvas data-chart="line" height="0" width="0">
N,100,1K,10K,100K,1M,10M
Jedi, 19.4547,  20.8907,  23.8802, 24.7229, 22.8053, 25.7011
Overlined, 20.092,  20.7878,  21.6097, 22.6238, 21.2044, 24.6774
<!-- 
{ 
 "data" : {
  "datasets" : [
  { 
    "backgroundColor": "rgba(66,66,66,0.35)",
    "rough": { "fillStyle": "hachure", "hachureAngle": -30, "hachureGap": 9, "fillWeight": 0.3}
  },
  { 
    "backgroundColor": "rgba(33,220,33,.9)",
    "rough": { "fillStyle": "hachure", "hachureAngle": 60, "hachureGap": 3}
  }
  ]
 },
 "options": {
    "title": { "text": "AVX2 Jedi Sorting + Overlined - log(Time/N)", "display": true },
    "scales": { 
      "yAxes": [{ 
        "type": "logarithmic",
        "ticks": {
          "min": 15,
          "max": 28,
          "callback": "ticksNumStandaard",
          "fontFamily": "Indie Flower"          
        },
        "scaleLabel": {
          "labelString": "Time/N (ns)",
          "fontFamily": "Indie Flower",
          "display": true
        }
      }]
    }
 },
 "defaultOptions": {"scales":{"xAxes":[{"scaleLabel":{"display":"true,","labelString":"N (elements)","fontFamily":"Indie Flower"},"ticks":{"fontFamily":"Indie Flower"}}]},"legend":{"display":true,"position":"bottom","labels":{"fontFamily":"Indie Flower","fontSize":14}},"title":{"position":"top","fontFamily":"Indie Flower","fontSize":16}}
}
--> </canvas>

</p>
</div>
</div>
</div>
</div>
</li>

	<li>
<div>
<div>
      
      
      <div>
      
      <div>
      <p><span>
      <span>Loading, please wait</span>
      <span><span></span></span>
      </span>
      </p>
      <table data-json="../_posts/Bench.BlogPt5_2_Int32_-report.datatable.json" data-id-field="name" data-pagination="false" data-page-list="[9, 18]" data-intro="Each row in this table represents a benchmark result" data-position="left" data-show-pagination-switch="false" data-show-header="true" data-striped="true">
  <thead data-intro="The header can be used to sort/filter by clicking" data-position="right"><tr><th data-field="TargetMethodColumn.Method"><p><span data-intro="The name of the benchmarked method" data-position="top">
            Method<br>Name
          </span>
        </p></th><th data-field="N"><p><span data-intro="The size of the sorting problem being benchmarked (# of integers)" data-position="top">
            Problem<br>Size
            </span>
        </p></th><th data-field="TimePerNDataTable"><p><span data-intro="Time in nanoseconds spent sorting each element in the array (with confidence intervals in parenthesis)" data-position="top">
              Time /<br>Element (ns)
            </span>
        </p></th><th data-field="RatioDataTable"><p><span data-intro="Each result is scaled to its baseline (Array.Sort in this case)" data-position="top">
                  Scaling
            </span>
        </p></th><th data-field="Measurements"><p><span data-intro="Raw benchmark results visualize how stable the result it. Longest/Shortest runs marked with <span style='color: red'>Red</span>/<span style='color: green'>Green</span>" data-position="top">Measurements</span>
        </p></th></tr></thead><tbody><tr data-index="0"><td>Jedi</td><td>100</td><td>30.45 <small>(12.54 - 48.37)</small></td><td><div><p><span>100%</span></p></div></td><td><canvas data-value="6373,6451,6376,5989,5830,5481,5509,5636,6301,6570,6257,6426,6552,6220,27994.5,6863.5,27044.5,5771.5,26706.5,6687.5,5986.5,5772.5,5671.5,31763.5;#AA0000,5956.5,5458.5;#00AA00,5774.5,5930.5,5602.5" width="261" height="25"></canvas></td></tr><tr data-index="1"><td>Overlined</td><td>100</td><td>20.09 <small>(19.15 - 21.03)</small></td><td><div><p><span>87%</span></p></div></td><td><canvas data-value="6541.5,5669.5,6191.5,6485.5,6281.5,5429.5;#00AA00,5765.5,5642.5,5636.5,5767.5,6147.5,5908.5,6676.5,6886.5,6917.5;#AA0000,5867.5,5990.5,5911.5,6312.5,5929.5,5952.5,5839.5,6075.5,5876.5,5756.5,5740.5,5545.5" width="243" height="25"></canvas></td></tr><tr data-index="2"><td>Jedi</td><td>1,000</td><td>20.89 <small>(20.41 - 21.37)</small></td><td><div><p><span>100%</span></p></div></td><td><canvas data-value="62117.5,61184.5,59647.5,60430.5,59524.5;#00AA00,66146.5;#AA0000,65247.5,65182.5,64380.5,64247.5,64751.5,62972.5,63255,62116,62951,64998,60727,60724,60938,63953,61931,61927,64490,60873,62090" width="225" height="25"></canvas></td></tr><tr data-index="3"><td>Overlined</td><td>1,000</td><td>20.79 <small>(18.99 - 22.59)</small></td><td><div><p><span>98%</span></p></div></td><td><canvas data-value="63523,59596,58096,57290,59161,58256,60027,57159,59314,58187,56507;#00AA00,57022,82305.5;#AA0000,63317.5,63042.5,59898.5,60506.5,79460.5,68380.5,59507.5,59115.5,60080.5,57920.5,59228.5,62835.5,81708.5" width="234" height="25"></canvas></td></tr><tr data-index="4"><td>Jedi</td><td>10,000</td><td>23.88 <small>(23.26 - 24.50)</small></td><td><div><p><span>100%</span></p></div></td><td><canvas data-value="666594.5;#00AA00,744011.5,716834.5,739904.5,739847.5,744695.5,788935.5;#AA0000,725918.5,695263.5,696163.5,693651.5,710527.5,721587.5,682399.5,730396.5,705843.5,706058.5,738685.5,698997.5,701941.5,730001.5,683432.5,685085.5,768767.5,729984.5,726207.5,705412.5,706828.5,691793.5" width="261" height="25"></canvas></td></tr><tr data-index="5"><td>Overlined</td><td>10,000</td><td>21.61 <small>(21.02 - 22.19)</small></td><td><div><p><span>91%</span></p></div></td><td><canvas data-value="611235.5;#00AA00,674326.5,631066.5,707198.5,638225.5,665807.5,627738.5,630511.5,666396.5,662346.5,627967.5,629807.5,639865.5,630053.5,706702.5,621366,660476,625253,661470,667555,662932,663843,622832,626685,639997,708076;#AA0000,636611,632581,629918,639877" width="270" height="25"></canvas></td></tr><tr data-index="6"><td>Jedi</td><td>100,000</td><td>24.72 <small>(24.47 - 24.97)</small></td><td><div><p><span>100%</span></p></div></td><td><canvas data-value="7441766,7482429,7452769,7443300,7423823,7568880,7572200,7532974,7330802,7436623,7489731,7461549,7409056,7270691,7223628;#00AA00,7309329,7234650,7256400,7428901,7291910,7432147,7515080,7576125;#AA0000,7569852,7295099,7447060,7333003,7442769" width="252" height="25"></canvas></td></tr><tr data-index="7"><td>Overlined</td><td>100,000</td><td>22.62 <small>(22.40 - 22.84)</small></td><td><div><p><span>92%</span></p></div></td><td><canvas data-value="6798164,6930207,6740782,6849440,6793821,6774846,6870216,6945964,6874004,6800903,6793163,6835879,6823248,6724539,6604875.5;#00AA00,6865488.5,6652329.5,6730168.5,6684975.5,6765935.5,6672930.5,6773871.5,6945909.5,6962501.5;#AA0000,6854287.5,6756934.5,6649880.5,6689979.5,6661447.5" width="261" height="25"></canvas></td></tr><tr data-index="8"><td>Jedi</td><td>1,000,000</td><td>23.62 <small>(23.49 - 23.76)</small></td><td><div><p><span>100%</span></p></div></td><td><canvas data-value="71603970,70700057,71621046,70761818,70746630,70345205,70235914,70332529,70565253,70498998,72473325;#AA0000,72001661,71576289,71190706,70988107,70646894,70731153,70380670,70657810,71536843,70564691,70995802,70224308,71620764,70242411,70705898,70827025,70181253;#00AA00,70408679" width="261" height="25"></canvas></td></tr><tr data-index="9"><td>Overlined</td><td>1,000,000</td><td>21.14 <small>(21.01 - 21.27)</small></td><td><div><p><span>96%</span></p></div></td><td><canvas data-value="70717210.5;#AA0000,69267985.5,69212086.5,68577151.5,69241775.5,68908889.5,69082086.5,70189442.5,69262740.5,68719815.5,68834994.5,70233976.5,69354460.5,69965943.5,69562748.5,70568502.5,69851945.5,69822860.5,68517584.5;#00AA00,69723013.5,70069638.5,69044970.5,69332978.5,69165280.5,69666402.5,68840536.5,69595204.5,69265561.5,68602649.5,69672557.5" width="270" height="25"></canvas></td></tr><tr data-index="10"><td>Jedi</td><td>10,000,000</td><td>25.70 <small>(25.40 - 26.01)</small></td><td><div><p><span>100%</span></p></div></td><td><canvas data-value="758223751.5,758126368.5,755716365.5;#00AA00,757916917.5,757712720.5,757833855.5,757897234.5,757164630.5,759353051.5,758412488.5,757041270.5,757942081.5,781836050.5,782018803.5,782449996.5,783184162.5,782546425.5,781701949.5,781676572.5,782316342.5,783238302.5,782506392.5,782198264.5,783251632.5;#AA0000,782320671.5,782259015.5" width="234" height="25"></canvas></td></tr><tr data-index="11"><td>Overlined</td><td>10,000,000</td><td>24.68 <small>(24.61 - 24.74)</small></td><td><div><p><span>96%</span></p></div></td><td><canvas data-value="743022126.5,743585245.5,742525085.5,742795066.5,742539344.5,742321064.5,742738734.5,743294898.5,743838486.5;#AA0000,743018190.5,742706643.5,742670602.5,741970148.5,742481048.5,743646322.5,737203173,737251152,736388194,736902900,736709307,738064813,738026146,737561976,738163415,738515948,737845025,737375865,735882023;#00AA00" width="252" height="25"></canvas></td></tr></tbody>
</table></div>
      
      </div>
      
      </div>
</div>

</li>

	<li>
<div><div><pre><code><table><tbody><tr>
<td><pre>1
2
3
4
5
6
7
8
9
10
11
12
13
14
</pre></td>
<td><pre><span>BenchmarkDotNet</span><span>=</span>v0.12.0, <span>OS</span><span>=</span>clear-linux-os 32120
Intel Core i7-7700HQ CPU 2.80GHz <span>(</span>Kaby Lake<span>)</span>, 1 CPU, 4 logical and 4 physical cores
.NET Core <span>SDK</span><span>=</span>3.1.100
  <span>[</span>Host]     : .NET Core 3.1.0 <span>(</span>CoreCLR 4.700.19.56402, CoreFX 4.700.19.56404<span>)</span>, X64 RyuJIT
  Job-DEARTS : .NET Core 3.1.0 <span>(</span>CoreCLR 4.700.19.56402, CoreFX 4.700.19.56404<span>)</span>, X64 RyuJIT

<span>InvocationCount</span><span>=</span>3  <span>IterationCount</span><span>=</span>15  <span>LaunchCount</span><span>=</span>2
<span>UnrollFactor</span><span>=</span>1  <span>WarmupCount</span><span>=</span>10

<span>$ </span><span>grep</span> <span>'stepping|model|microcode'</span> /proc/cpuinfo | <span>head</span> <span>-4</span>
model           : 158
model name      : Intel<span>(</span>R<span>)</span> Core<span>(</span>TM<span>)</span> i7-7700HQ CPU @ 2.80GHz
stepping        : 9
microcode       : 0xb4
</pre></td>
</tr></tbody></table></code></pre></div></div>

</li>

</ul>

</div>

  <p>This is much better! The improvement is much more pronounced here, and we have a lot to consider:</p>

  <ul>
    <li>The performance improvements are not spread evenly through-out the size of the sorting problem.</li>
    <li>I’ve conveniently included two vertical markers, per my specific machine model, they show the size of the L2/L3 caches translated to <code>#</code> of 32-bit elements in our array.</li>
    <li>It can be clearly seen that as long as we’re sorting roughly within the size of our L2-L3 cache size range, this optimization pays in spades: we’re seeing ~10% speedup in runtime in many cases!</li>
    <li>It is also clear that as we progress outside the size of the L2 into the L3 cache size, and ultimately exhaust the size of our caches entirely, the returns on this optimization diminish gradually.</li>
    <li>While not shown here, since I’ve lost access to that machine, on older Intel/AMD machines, where only one load operation can be executed by the processor at any given time (Example: Intel Broadwell processors), this can lead to an improvement of 20% in total runtime; This should make sense: the less load ports the CPU has, the better this split-load reducing technique performs.</li>
    <li>Another thing to consider is that in future variations of this code when I finally get access and ability to use AVX-512, with 64-byte wide registers, the effects of this optimization will be much more pronounced again for a different reason: With vector registers spanning 64-bytes each, split-loading becomes a bigger problem (every single un-aligned read becomes a split-load). Therefore, removing it is even more important.</li>
  </ul>

</div>

<p>As the problem size goes beyond the size of the L2 cache, we are hit with the realities of CPU cache latency numbers. As service to the reader here is a visual representation for the <a href="https://www.7-cpu.com/cpu/Skylake_X.html">latency numbers for a Skylake-X CPU</a> running at 4.3 Ghz:</p>

<center>

</center>

<p>The small number of cycles we tack as the penalty of for split-loading (7 in this diagram) on to the memory operations is very real when we compare it to regular L1/L2 cache latency. But once we compare it to L3 or RAM latency, it becomes abundantly clear why we are seeing diminishing returns for this optimization; the penalty is simply too small to notice at those work points.</p>

<p>Finally, for this optimization, we must never forget our moto of trust no one and nothing. Let’s double check what the current state of affairs is as far as <code>perf</code> is concerned:</p>

<div><div><pre><code><table><tbody><tr>
<td><pre>1
2
3
4
5
6
7
8
9
10
11
12
</pre></td>
<td><pre><span>$ </span>perf record <span>-Fmax</span> <span>-e</span> mem_inst_retired.split_loads <span></span>
   ./Example <span>--type-list</span> DoublePumpOvelined <span>--size-list</span> 100000 <span></span>
       <span>--max-loops</span> 1000 <span>--no-check</span>
<span>$ </span>perf report <span>--stdio</span> <span>-F</span> overhead,sym | <span>head</span> <span>-20</span>
<span># To display the perf.data header info, please use --header/--header-only options.</span>
<span># Samples: 129  of event 'mem_inst_retired.split_loads'</span>
<span># Event count (approx.): 12900387</span>
<span># Overhead  Symbol</span>
    30.23%  <span>[</span>.] DoublePumpOverlined...::Sort<span>(</span>int32<span>*</span>,int32<span>*</span>,int64,int32<span>)</span>
    28.68%  <span>[</span>.] DoublePumpOverlined...::VectorizedPartitionInPlace<span>(</span>int32<span>*</span>,int32<span>*</span>,int64<span>)</span>
    13.95%  <span>[</span>.] __memmove_avx_unaligned_erms
     0.78%  <span>[</span>.] JIT_MemSet_End
</pre></td>
</tr></tbody></table></code></pre></div></div>

<p>Seems like this moved the needle, and then some. We started with <code>86.68%</code> of <code>87,102,613</code> split-loads in our previous version of vectorized partitioning , and now we have <code>28.68%</code> of <code>12,900,387</code>. In other words: <code>(0.2668 * 12900387) / (0.8668 * 87102613)</code> gives us <code>4.55%</code>, or a <code>95.44%</code> reduction of split-load events for this version.
Not an entirely unpleasant experience.</p>

<h4 id="sub-optimization--converting-branches-to-arithmetic-1">Sub-optimization- Converting branches to arithmetic: <img title=":+1:" alt=":+1:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f44d.png" height="20" width="20">
</h4>

<p>By this time, my code contained quite a few branches to deal with various edge cases around alignment, and I pulled another rabbit out of the optimization hat that is worth mentioning: We can convert simple branches into arithmetic operations. Many times, we end up having branches with super simple code behind them; here’s a real example I used to have in my code, as part of some early version of overlinement, which we’ll try to optimize:</p>

<div>
  <div>

    <div>
<div><pre><code><table><tbody><tr>
<td><pre>1
2
3
4
5
</pre></td>
<td><pre><span>int</span> <span>leftAlign</span><span>;</span>
<span>...</span> <span>// Calculate left align here...</span>
<span>if</span> <span>(</span><span>leftAlign</span> <span>&lt;</span> <span>0</span><span>)</span> <span>{</span>
    <span>readLeft</span> <span>+=</span> <span>8</span><span>;</span>
<span>}</span>
</pre></td>
</tr></tbody></table></code></pre></div>    </div>

  </div>

  <p>This looks awfully friendly, and it is unless <code>leftAlign</code> and therefore the entire branch is determined by random data we read from the array, making the CPU mispredict this branch too often than we’d care for it to happen. In my case, I had two branches like this, and each of them was happening at a rate of <code>1/8</code>. So enough for me to care. The good news is that we can re-write this, entirely in C#, and replace the potential misprediction with a constant, predictable (and often shorter!) data dependency. Let’s start by inspecting the re-written “branch”:</p>

</div>

<div>
  <div>

    <div>
<div><pre><code><table><tbody><tr>
<td><pre>1
2
3
4
5
6
</pre></td>
<td><pre><span>int</span> <span>leftAlign</span><span>;</span>
<span>...</span> <span>// Calculate left align here...</span>
<span>// Signed arithmetic FTW</span>
<span>var</span> <span>leftAlignMask</span> <span>=</span> <span>leftAlign</span> <span>&gt;&gt;</span> <span>31</span><span>;</span>
<span>// the mask is now either all 1s or all 0s depending on leftAlign's sign!</span>
<span>readLeft</span> <span>+=</span> <span>8</span> <span>&amp;</span> <span>leftALignMask</span><span>;</span>
</pre></td>
</tr></tbody></table></code></pre></div>    </div>

  </div>

  <p>By taking the same value we were comparing to 0 and right shifting it, we are performing an arithmetic right shift. This takes the top bit, which is either <code>0/1</code> depending on <code>leftAlign</code>’s sign bit, and essentially propagates it throughout the entire 32-bit value, which is then assigned to the <code>lestAlignMask</code> variable. We’ve essentially taken what was previously the result of the comparison as part of the branch (the sign bit), transforming it into a mask. We then proceed to take the mask and use it to control the outcome of the <code>+= 8</code> operation, effectively turning it into <em>either</em> a <code>+= 8</code> -or- a <code>+= 0</code> operation, depending on the value of the mask!<br>
This turns out to be a quite effective way, again, for simple branches only, at converting a potential misprediction event costing us 15 cycles, with a 100% constant 3-4 cycles data-dependency for the CPU: It can be thought as a “signaling” mechanism where we tell the CPU not to speculate on the result of the branch but instead complete the <code>readLeft +=</code> statement only after waiting for the right-shift (<code>&gt;&gt; 31</code>) and the bitwise and (<code>&amp;</code>) operation to propagate through its pipeline.</p>

  <table>
<tbody><tr>
<td><span>Note</span></td>
<td>
<p>I referred to this as an old geezer’s optimization since modern processors already support this internally in the form of a <code>CMOV</code> instruction, which is more versatile, faster and takes up less bytes in the instruction stream while having the same “do no speculate on this” effect on the CPU. <em>The only issue</em> is we don’t have <code>CMOV</code> in the CoreCLR JIT (Mono’s JIT, peculiarly does support this both with the internal JIT and naturally with LLVM…).<br>
As a side note to this side note, I’ll add that this is such an old-dog trick that LLVM even detects such code and de-optimizes it back into a “normal” branch and then proceeds to optimize it again into <code>CMOV</code>, which I think is just a very cool thing, regardless :)</p>
</td>
</tr>
</tbody></table>

</div>

<p>I ended up replacing about 5-6 super simple/small branches this way. I won’t show direct performance numbers for this, as this is already part of the overlined version; I can’t say it improved performance considerably for my test runs, but it did reduce the jitter of those runs, which can be seen in the reduced error bars and tighter confidence intervals shown in the benchmark results above.</p>

<h3 id="coming-to-terms-with-bad-speculation">Coming to terms with bad speculation</h3>

<p>At the end of part 3, we came to a hard realization that our code is badly speculating inside the CPU. Even after simplifying the branch code in our loop in part 4, the bad speculation remained there, staring at us persistently. If you recall, we experienced a lot of bad-speculation effects when sorting the data with our vectorized code, and profiling using hardware counters showed us that while <code>InsertionSort</code> was the cause of most of the bad-speculation events (41%), our vectorized code was still responsible for 32% of them. Let’s try to think about that mean nasty branch, stuck there, in the middle of our beautiful loop:</p>

<div>
  <div>

    <div>
<div><pre><code><table><tbody><tr>
<td><pre>1
2
3
4
5
6
7
8
9
10
</pre></td>
<td><pre><span>int</span><span>*</span> <span>nextPtr</span><span>;</span>
<span>if</span> <span>((</span><span>byte</span> <span>*)</span> <span>writeRight</span> <span>-</span> <span>(</span><span>byte</span> <span>*)</span> <span>readRight</span> <span>&lt;</span> <span>N</span> <span>*</span> <span>sizeof</span><span>(</span><span>int</span><span>))</span> <span>{</span>
    <span>nextPtr</span>   <span>=</span>  <span>readRight</span><span>;</span>
    <span>readRight</span> <span>-=</span> <span>N</span><span>;</span>
<span>}</span> <span>else</span> <span>{</span>
    <span>nextPtr</span>  <span>=</span>  <span>readLeft</span><span>;</span>
    <span>readLeft</span> <span>+=</span> <span>N</span><span>;</span>
<span>}</span>

<span>PartitionBlock</span><span>(</span><span>nextPtr</span><span>,</span> <span>P</span><span>,</span> <span>pBase</span><span>,</span> <span>ref</span> <span>writeLeft</span><span>,</span> <span>ref</span> <span>writeRight</span><span>);</span>
</pre></td>
</tr></tbody></table></code></pre></div>    </div>

  </div>

  <p>Long story short: We ended up sneaking up a data-based branch into our code in the form of this side-selection logic. Whenever we try to pick a side, we would read from next is where we put the CPU in a tough spot. We’re asking it to speculate on something it <em>can’t possibly speculate on successfully</em>. Our question is: “Oh CPU, CPU in the socket, Which side is closer to being over-written of them all?”, to which the answer is completely data-driven. In other words, it depends on how the last round(s) of partitioning mutated the pointers involved in the comparison. It might sound like an easy thing for the CPU to check, but we have to remember it is attempting to execute ~100 or so instructions into the future, as it is required to speculate on the result: the previous rounds of partitioning have not yet been fully-executed, internally. The CPU guesses, at best, based on stale data, and we know, as the grand designers of this mess, that its best guess is no better here than flipping a coin. Quite sad. You have to admit it is ironic we managed to do this whole big circle around our own tails just to come-back to having a branch misprediction based on the random array data. Mis-predicting here seems unavoidable. Or is it?</p>

  <h4 id="replacing-the-branch-with-arithmetic--1">Replacing the branch with arithmetic: <img title=":-1:" alt=":-1:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f44e.png" height="20" width="20">
</h4>

  <p>Could we replace this branch with arithmetic, just like we’ve done a couple of paragraphs above? Yes we can.
Consider this alternative version:</p>
</div>

<div><div><pre><code><table><tbody><tr>
<td><pre>1
2
3
4
5
6
7
8
9
10
11
12
13
14
</pre></td>
<td><pre><span>var</span> <span>readRightMask</span> <span>=</span>
    <span>(((</span><span>byte</span><span>*)</span> <span>writeRight</span> <span>-</span> <span>(</span><span>byte</span><span>*)</span> <span>readRight</span> <span>-</span> <span>N</span><span>*</span><span>sizeof</span><span>(</span><span>int</span><span>)))</span> <span>&gt;&gt;</span> <span>63</span><span>;</span>
<span>var</span> <span>readLeftMask</span> <span>=</span>  <span>~</span><span>readRightMask</span><span>;</span>
<span>// If readRightMask is 0, we pick the left side</span>
<span>// If readLeftMask is 0, we pick the right side</span>
<span>var</span> <span>readRightMaybe</span>  <span>=</span> <span>(</span><span>ulong</span><span>)</span> <span>readRight</span> <span>&amp;</span> <span>(</span><span>ulong</span><span>)</span> <span>readRightMask</span><span>;</span>
<span>var</span> <span>readLeftMaybe</span>   <span>=</span> <span>(</span><span>ulong</span><span>)</span> <span>readLeft</span>  <span>&amp;</span> <span>(</span><span>ulong</span><span>)</span> <span>readLeftMask</span><span>;</span>

<span>PartitionBlock</span><span>((</span><span>int</span> <span>*)</span> <span>(</span><span>readLeftMaybe</span> <span>+</span> <span>readRightMaybe</span><span>),</span>
               <span>P</span><span>,</span> <span>pBase</span><span>,</span> <span>ref</span> <span>writeLeft</span><span>,</span> <span>ref</span> <span>writeRight</span><span>);</span>

<span>var</span> <span>postFixUp</span> <span>=</span> <span>-</span><span>32</span> <span>&amp;</span> <span>readRightMask</span><span>;</span>
<span>readRight</span> <span>=</span> <span>(</span><span>int</span> <span>*)</span> <span>((</span><span>byte</span> <span>*)</span> <span>readRight</span> <span>+</span> <span>postFixUp</span><span>);</span>
<span>readLeft</span>  <span>=</span> <span>(</span><span>int</span> <span>*)</span> <span>((</span><span>byte</span> <span>*)</span> <span>readLeft</span>  <span>+</span> <span>postFixUp</span> <span>+</span> <span>32</span><span>);</span>
</pre></td>
</tr></tbody></table></code></pre></div></div>

<p>What the code above does, except for causing a nauseating headache, is taking the same concept of turning branches into arithmetic from the previous section and using it to get rid of that nasty branch: We take the comparison and turn it into a negative/positive number, then proceed to use it to generate masks we use to execute the code that used to reside under the branch.</p>

<p>I don’t want to dig deep into this. While its technically sound, and does what we need it to do, it’s more important to focus on how this performs:</p>

<div>
  <div>

<ul data-uk-switcher="{connect:'#bc8fa6d9-2952-4950-addc-79dad9bb86b4'}">

	<li><a href="#" aria-expanded="true"><i></i> Scaling</a></li>

	<li><a href="#" aria-expanded="false"><i></i> Time/N</a></li>

	<li><a href="#" aria-expanded="false"><i></i> Benchmarks</a></li>

	<li><a href="#" aria-expanded="false"><i></i> Setup</a></li>

</ul>

<ul id="bc8fa6d9-2952-4950-addc-79dad9bb86b4">

	<li>
<div>
<div data-intro="Size of the sorting problem, 10..10,000,000 in powers of 10" data-position="bottom">
<div data-intro="Performance scale: Array.Sort (solid gray) is always 100%, and the other methods are scaled relative to it" data-position="left">
<div data-intro="Click legend items to show/hide series" data-position="right">
<div>
<canvas data-chart="line" height="500" width="1020">
N,100,1K,10K,100K,1M,10M
Overlined,         1   , 1   , 1  , 1   , 1    , 1
Branchless, 0.87253937, 0.951842168, 1.104715689, 1.140662148, 1.253573179, 1.379499062

<!-- 
{ 
 "data" : {
  "datasets" : [
  { 
    "backgroundColor": "rgba(66,66,66,0.35)",
    "rough": { "fillStyle": "hachure", "hachureAngle": -30, "hachureGap": 9, "fillWeight": 0.3}
  },
  { 
    "backgroundColor": "rgba(33,220,33,.9)",
    "rough": { "fillStyle": "hachure", "hachureAngle": 60, "hachureGap": 3}
  }  
  ]
 },
 "options": {
    "title": { "text": "AVX2 Branchless Sorting - Scaled to Overlined", "display": true },
    "scales": { 
      "yAxes": [{
       "ticks": {
         "fontFamily": "Indie Flower",
         "min": 0.80, 
         "callback": "ticksPercent"
        },
        "scaleLabel": {
          "labelString": "Scaling (%)",
          "display": true
        }
      }]
    }
 },
 "defaultOptions": {"scales":{"xAxes":[{"scaleLabel":{"display":"true,","labelString":"N (elements)","fontFamily":"Indie Flower"},"ticks":{"fontFamily":"Indie Flower"}}]},"legend":{"display":true,"position":"bottom","labels":{"fontFamily":"Indie Flower","fontSize":14}},"title":{"position":"top","fontFamily":"Indie Flower","fontSize":16}}
}
--> </canvas>

</div>
</div>
</div>
</div>
</div>

</li>

	<li>
<div>
<div data-intro="Size of the sorting problem, 10..10,000,000 in powers of 10" data-position="bottom">
<div data-intro="Time in nanoseconds spent sorting per element. Array.Sort (solid gray) is the baseline, again" data-position="left">
<div data-intro="Click legend items to show/hide series" data-position="right">
<p>
<canvas data-chart="line" height="0" width="0">
N,100,1K,10K,100K,1M,10M
Overlined, 20.3199,21.0354,21.6787,23.0622,23.246,24.7603
Branchless, 17.7252,20.0221,23.9488,26.3062,29.1405,34.1567

<!-- 
{ 
 "data" : {
  "datasets" : [
  { 
    "backgroundColor": "rgba(66,66,66,0.35)",
    "rough": { "fillStyle": "hachure", "hachureAngle": -30, "hachureGap": 9, "fillWeight": 0.3}
  },
  { 
    "backgroundColor": "rgba(33,220,33,.9)",
    "rough": { "fillStyle": "hachure", "hachureAngle": 60, "hachureGap": 3}
  }
  ]
 },
 "options": {
    "title": { "text": "AVX2 Jedi Sorting + Aligned - log(Time/N)", "display": true },
    "scales": { 
      "yAxes": [{ 
        "type": "logarithmic",
        "ticks": {
          "min": 15,
          "max": 35,
          "callback": "ticksNumStandaard",
          "fontFamily": "Indie Flower"          
        },
        "scaleLabel": {
          "labelString": "Time/N (ns)",
          "fontFamily": "Indie Flower",
          "display": true
        }
      }]
    }
 },
 "defaultOptions": {"scales":{"xAxes":[{"scaleLabel":{"display":"true,","labelString":"N (elements)","fontFamily":"Indie Flower"},"ticks":{"fontFamily":"Indie Flower"}}]},"legend":{"display":true,"position":"bottom","labels":{"fontFamily":"Indie Flower","fontSize":14}},"title":{"position":"top","fontFamily":"Indie Flower","fontSize":16}}
}
--> </canvas>

</p>
</div>
</div>
</div>
</div>
</li>

	<li>
<div>
<div>
      
      
      <div>
      
      <div>
      <p><span>
      <span>Loading, please wait</span>
      <span><span></span></span>
      </span>
      </p>
      <table data-json="../_posts/Bench.BlogPt5_3_Int32_-report.datatable.json" data-id-field="name" data-pagination="false" data-page-list="[9, 18]" data-intro="Each row in this table represents a benchmark result" data-position="left" data-show-pagination-switch="false" data-show-header="true" data-striped="true">
  <thead data-intro="The header can be used to sort/filter by clicking" data-position="right"><tr><th data-field="TargetMethodColumn.Method"><p><span data-intro="The name of the benchmarked method" data-position="top">
            Method<br>Name
          </span>
        </p></th><th data-field="N"><p><span data-intro="The size of the sorting problem being benchmarked (# of integers)" data-position="top">
            Problem<br>Size
            </span>
        </p></th><th data-field="TimePerNDataTable"><p><span data-intro="Time in nanoseconds spent sorting each element in the array (with confidence intervals in parenthesis)" data-position="top">
              Time /<br>Element (ns)
            </span>
        </p></th><th data-field="RatioDataTable"><p><span data-intro="Each result is scaled to its baseline (Array.Sort in this case)" data-position="top">
                  Scaling
            </span>
        </p></th><th data-field="Measurements"><p><span data-intro="Raw benchmark results visualize how stable the result it. Longest/Shortest runs marked with <span style='color: red'>Red</span>/<span style='color: green'>Green</span>" data-position="top">Measurements</span>
        </p></th></tr></thead><tbody><tr data-index="0"><td>Overlined</td><td>100</td><td>20.32 <small>(13.52 - 27.12)</small></td><td><div><p><span>100%</span></p></div></td><td><canvas data-value="6091,5747,5622,5626,6356,5924,5820,5637,5598,5459;#00AA00,5568,5664,5556,24577.5;#AA0000,6073.5,6032.5,5673.5,5787.5,5745.5,5953.5,5800.5,5641.5,24331.5,5886.5,5837.5,24262.5,24218.5,24197.5" width="252" height="25"></canvas></td></tr><tr data-index="1"><td>Branchless</td><td>100</td><td>17.73 <small>(17.20 - 18.25)</small></td><td><div><p><span>81%</span></p></div></td><td><canvas data-value="5484,5712,5560,5293,5164,5267,5208,5075,5004,5132,4996;#00AA00,5637,5375,5157,5272,5350,5232,5221,5118,5145,5420,5117,5521,5358,5299,5888;#AA0000,5569" width="243" height="25"></canvas></td></tr><tr data-index="2"><td>Overlined</td><td>1,000</td><td>21.04 <small>(18.84 - 23.23)</small></td><td><div><p><span>100%</span></p></div></td><td><canvas data-value="77387,58917,58865,57942;#00AA00,58688,58306,89462;#AA0000,58580,59299,88528,75293,59981,58476,58243,64850,61507,59188,59899,58785,59031,59588,61087,61385,57992,59978,59501" width="234" height="25"></canvas></td></tr><tr data-index="3"><td>Branchless</td><td>1,000</td><td>20.02 <small>(19.65 - 20.39)</small></td><td><div><p><span>97%</span></p></div></td><td><canvas data-value="58368.5,61913.5,61405.5,62103.5,59454.5,59340.5,61166.5,61656.5,61809.5,59229.5,61012.5,59741.5,58982.5,62144.5;#AA0000,57782.5,57993.5,61312.5,60153.5,60072.5,60493.5,58639.5,58065.5,60396.5,61007.5,57413.5;#00AA00" width="225" height="25"></canvas></td></tr><tr data-index="4"><td>Overlined</td><td>10,000</td><td>21.68 <small>(21.28 - 22.08)</small></td><td><div><p><span>100%</span></p></div></td><td><canvas data-value="668003,632018,633006,641117,643404,646315,631241,658954,645690,631709,631574,670884,667224,654434,624741,637873.5,644343.5,700918.5;#AA0000,648901.5,664097.5,672816.5,661256.5,660378.5,658320.5,645608.5,645879.5,673668.5,623098.5;#00AA00,642960.5" width="261" height="25"></canvas></td></tr><tr data-index="5"><td>Branchless</td><td>10,000</td><td>23.95 <small>(23.44 - 24.46)</small></td><td><div><p><span>111%</span></p></div></td><td><canvas data-value="708329.5,691589.5,687479.5,737840.5,747226.5,709589.5,719807.5,731992.5,689488.5,711407.5,726064.5,717795.5,695756.5,720439.5,671657;#00AA00,702999,724040,743105,718499,727476,760075;#AA0000,676328,752726,739987,719146,701892,730085,735952,736685" width="261" height="25"></canvas></td></tr><tr data-index="6"><td>Overlined</td><td>100,000</td><td>23.06 <small>(22.82 - 23.30)</small></td><td><div><p><span>100%</span></p></div></td><td><canvas data-value="7002164,7163013;#AA0000,7045874,6894263,7101705,6983282,7050598,7143577,6971846,6939983,6862371,6799916,6944549,6870305,6865179,6833450.5,6867566.5,6882426.5,6907631.5,6988553.5,6858210.5,6876902.5,6871443.5,6913996.5,6762447.5;#00AA00,6820771.5,6825350.5,6780633.5,6813222.5" width="261" height="25"></canvas></td></tr><tr data-index="7"><td>Branchless</td><td>100,000</td><td>26.31 <small>(26.05 - 26.56)</small></td><td><div><p><span>113%</span></p></div></td><td><canvas data-value="7697369.5;#00AA00,7792158.5,8007352.5,8025405.5,7853119.5,7830870.5,7960540.5,7889122.5,8023984.5,8049016.5,8021739.5,7815697.5,7990539.5,7754941.5,7808999.5,7737760,7848146,8065069;#AA0000,7994853,7882451,7867151,8052613,7725600,7851516,8044759,7940268,7826488,7860393,7717372,7820427" width="270" height="25"></canvas></td></tr><tr data-index="8"><td>Overlined</td><td>1,000,000</td><td>23.25 <small>(22.85 - 23.64)</small></td><td><div><p><span>100%</span></p></div></td><td><canvas data-value="67553755.5;#00AA00,67859754.5,67895631.5,68322124.5,68039940.5,68181009.5,67870730.5,67694280.5,67803304.5,68202765.5,67847170.5,68518605.5,68097685.5,68065861.5,67966670.5,71500106,71522967,71377544,71181501,71185127,71354072,71812670,71477090,71327342,71668034,71362267,71878922;#AA0000,71656523,71423508,71489799" width="270" height="25"></canvas></td></tr><tr data-index="9"><td>Branchless</td><td>1,000,000</td><td>29.14 <small>(29.02 - 29.26)</small></td><td><div><p><span>126%</span></p></div></td><td><canvas data-value="87349375,87693029,88172036,87637128,87816746,87715951,87418121,87969725,87838569,87473326,87427769,87869924,87732668,87519538,87992165.5,87395266.5,88375961.5;#AA0000,87534863.5,87487505.5,86633289.5,87489450.5,86675505.5,86729449.5,86629552.5,86553707.5,87684739.5,87014134.5,87004507.5,86390975.5;#00AA00" width="261" height="25"></canvas></td></tr><tr data-index="10"><td>Overlined</td><td>10,000,000</td><td>24.76 <small>(24.61 - 24.91)</small></td><td><div><p><span>100%</span></p></div></td><td><canvas data-value="745770753.5,763163008.5;#AA0000,748640820.5,744066602.5,743235605.5,743051219.5,743540209.5,743176528.5,742601287.5,743532865.5,755654169.5,743936743.5,745894636.5,740776808.5,742149631.5,739116080.5,737440536.5,738276094.5,738516569.5,737076223.5,737026122.5;#00AA00,737663083.5,738517162.5,738083823.5,739282380.5" width="225" height="25"></canvas></td></tr><tr data-index="11"><td>Branchless</td><td>10,000,000</td><td>34.16 <small>(34.12 - 34.20)</small></td><td><div><p><span>138%</span></p></div></td><td><canvas data-value="1024532262,1024162560,1023475512,1023477348,1026168017,1026112365,1025164762,1024911937,1024152365,1022938310,1025288201,1024007958,1025915729,1029892656;#AA0000,1025238783,1022747946;#00AA00,1024093201,1023023054,1023908065,1023967093,1023417564,1025586735,1027014606,1026288698,1023250679,1023524195" width="234" height="25"></canvas></td></tr></tbody>
</table></div>
      
      </div>
      
      </div>
</div>

</li>

	<li>
<div><div><pre><code><table><tbody><tr>
<td><pre>1
2
3
4
5
6
7
8
9
10
11
12
13
14
</pre></td>
<td><pre><span>BenchmarkDotNet</span><span>=</span>v0.12.0, <span>OS</span><span>=</span>clear-linux-os 32120
Intel Core i7-7700HQ CPU 2.80GHz <span>(</span>Kaby Lake<span>)</span>, 1 CPU, 4 logical and 4 physical cores
.NET Core <span>SDK</span><span>=</span>3.1.100
  <span>[</span>Host]     : .NET Core 3.1.0 <span>(</span>CoreCLR 4.700.19.56402, CoreFX 4.700.19.56404<span>)</span>, X64 RyuJIT
  Job-DEARTS : .NET Core 3.1.0 <span>(</span>CoreCLR 4.700.19.56402, CoreFX 4.700.19.56404<span>)</span>, X64 RyuJIT

<span>InvocationCount</span><span>=</span>3  <span>IterationCount</span><span>=</span>15  <span>LaunchCount</span><span>=</span>2
<span>UnrollFactor</span><span>=</span>1  <span>WarmupCount</span><span>=</span>10

<span>$ </span><span>grep</span> <span>'stepping|model|microcode'</span> /proc/cpuinfo | <span>head</span> <span>-4</span>
model           : 158
model name      : Intel<span>(</span>R<span>)</span> Core<span>(</span>TM<span>)</span> i7-7700HQ CPU @ 2.80GHz
stepping        : 9
microcode       : 0xb4
</pre></td>
</tr></tbody></table></code></pre></div></div>

</li>

</ul>

</div>

  <p>Look, I’m not here to sugar-coat it: This looks like an unmitigated disaster. But I claim that it is one we can learn a lot from in the future.
With the exception of sorting <code>&lt;= 100</code> elements, as the problem grows, the situation is getting much worse.</p>

  <p>To double-check that everything is sound, I ran <code>perf</code> recording the <code>instructions</code>, <code>branches</code> and <code>branch-misses</code> events for both versions for sorting <code>100,000</code> elements.</p>

  <p>The command line used was this:</p>

  <div>
<div><pre><code><table><tbody><tr>
<td><pre>1
2
3
4
5
6
</pre></td>
<td><pre><span>$ </span>perf record <span>-F</span> max <span>-e</span> instructions,branches,branch-misses <span></span>
    ./Example <span>--type-list</span> DoublePumpOverlined <span></span>
              <span>--size-list</span> 100000 <span>--max-loops</span> 1000 <span>--no-check</span>
<span>$ </span>perf record <span>-F</span> max <span>-e</span> instructions,branches,branch-misses <span></span>
    ./Example <span>--type-list</span> DoublePumpBranchless <span></span>
              <span>--size-list</span> 100000 <span>--max-loops</span> 1000 <span>--no-check</span>
</pre></td>
</tr></tbody></table></code></pre></div>  </div>

  <p>If you’re one of those sick people who likes to look into other people’s sorrows, here is a <a href="https://gist.github.com/damageboy/79368e350364348c6ca476492a63f052">gist with the full results</a>, if you’re more normal, and to keep things simple, I’ve processed the results and presenting them here in table form:</p>

  <center>

</center>

</div>

<p>This is pretty amazing if you think about it:</p>

<ul>
  <li>The number of branches was cut in half: This makes sense, the loop control itself is a branch instuction after all, so it remains even in the <code>Branchless</code> variant.</li>
  <li>The branches that remain in the <code>branchless</code> version are all easy to predict, and we see that the <code>branch-misses</code> counter shows us those are down to nothing.<br>
This means that there is no mistake: We succeeded in a targeted assassination of that branch; however, there was a lot of collateral damage…</li>
  <li>The verbiage of the branchless code, expressed in the <code>instructions</code> counter is definitely costing us something here:<br>
The number of executed instructions inside our partition loop have gone up by 17%, which is a lot.</li>
</ul>

<p>The slowdown we’ve measured here is directly related to NOT having <code>CMOV</code> available to us through the CoreCLR JIT. but I really don’t think that this is the entire story here. It’s hard to express this in words, but
the slope at which the branchless code is slowing down compared to the previous version is very suspicious in my eyes.<br>
There is an expression we use in Hebrew a lot for this sort of situation: “The operation was successful, but the patient died”. There is no question that this is one of those moments.
This failure to accelerate the sorting operation, and specifically the way it fails, increasingly as the problem size grows, is very telling in my eyes.
I have an idea of why this is and how we might be able to go around it. But, for today, our time is up. I’ll try and get back to this much much later in this series,
and hopefully, we’ll all be wiser for it.</p>

<hr>


        
      </section>

      

      


      
  

    </div></div></div>
    </div>
    <footer>
        <div>created by <a href="https://buildstarted.com">buildstarted</a> &copy; 2020 <a href="/about">about</a></div>
        <div>Share this page on social media: copy and paste this url https://linksfor.dev/</div>
        <div>If you prefer RSS: <a href="https://linksfor.dev/feed.xml">https://linksfor.dev/feed.xml</a></div>
        <div>Customer satisfaction guaranteed to be optional.</div>
    </footer>
    
    <script async defer>
        _dna = window._dna || {};
        _dna.siteId = "linksfor.devs";
        _dna.outlink = true;

        (function() {
            let dna = document.createElement('script');
            dna.type = 'text/javascript';
            dna.async = true;
            dna.src = '//dna.buildstarted.com/t.js';
            let s = document.getElementsByTagName('script')[0];
            s.parentNode.insertBefore(dna, s);
        })();
    </script>
    <noscript><img src="//dna.buildstarted.com/g?siteId=linksfor.devs"/></noscript>
</body>
</html>