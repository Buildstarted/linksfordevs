<!DOCTYPE html>
<html lang="en">
<head>
    <title>linksfor.dev(s)</title>
    <meta charset="utf-8">
    <link rel="alternate" type="application/rss+xml" title="Linksfor.dev(s) feed" href="https://linksfor.dev/feed.rss" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta name="google" value="notranslate">
    <link rel="stylesheet" type="text/css" href="/style.css" />
    <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon">
    <link rel="icon" href="/favicon.ico" type="image/x-icon">
    
    <meta property="article:author" content="Buildstarted"/>
    <meta property="og:site_name" content="linksfor.dev(s)" />
    <meta property="og:title" content="linksfor.dev(s)" />
    <meta property="og:type" content="website" />
    <meta property="og:description" content="A curated list of sources of development information including c#, c++, and other dev related links." />
</head>
<body>
    <div class="grid">
        <h1>
                <span style="cursor: default" title="linksfor.dev(s) has been running for 1 year! :partypopper:">üéâ</span>
<a href="/" style="color:inherit">linksfor.dev(s)</a>
        </h1>
        <title>linksfor.dev(s) - Common Systems Programming Optimizations &amp; Tricks - paulcavallaro</title>
<div class="readable">
        <h1>Common Systems Programming Optimizations &amp; Tricks - paulcavallaro</h1>
        <p>
Reading time: 15-19 minutes        </p>
        <p><a href="https://paulcavallaro.com/blog/common-systems-programming-optimizations-tricks/">https://paulcavallaro.com/blog/common-systems-programming-optimizations-tricks/</a></p>
        <hr/>
<div id="readability-page-1" class="page"><div>
  

  <div role="main">
    <div>
      

      <article>
      

<p>Today‚Äôs blog post is an overview of some common optimization techniques and neat
tricks for doing ‚Äúsystems programming‚Äù ‚Äì whatever that means today. We‚Äôll walk
through some methods to make your code run faster, be more efficient, and to
squeeze just a little more juice from whatever you got.</p>

<p>All of the examples we‚Äôll go over are also on github at
<a href="https://github.com/paulcavallaro/systems-programming">paulcavallaro/systems-programming</a>.</p>

<h2 id="cache-lines-false-sharing">Cache Lines &amp; False Sharing</h2>

<p>False sharing is a fairly well-understood problem in optimizing multi-threaded
code on modern SMP systems. The problem has been
<a href="https://software.intel.com/en-us/articles/avoiding-and-identifying-false-sharing-among-threads">written</a>
<a href="https://mechanical-sympathy.blogspot.com/2011/07/false-sharing.html">about</a>
<a href="https://dzone.com/articles/false-sharing">fairly</a>
<a href="https://herbsutter.com/2009/05/15/effective-concurrency-eliminate-false-sharing/">extensively</a>,
but the basic idea is that physical memory on a machine isn‚Äôt infinitely
granular, i.e. you can‚Äôt just read a byte.<sup id="fnref:1"><a href="#fn:1">1</a></sup> Instead, when you want to read a
single byte of memory, the processor will pull in and cache not just that byte,
but the surrounding data as well, on the assumption that it will likely be used
too. This unit of data that gets read and cached is called a ‚Äúcache line‚Äù, and
is essentially the smallest piece of memory that can be accessed.</p>

<p>As of 2019 cache lines are powers of 2, usually in the range of 32 to 256 bytes,
with 64 bytes being the most common.</p>

<p>Now, to support multiple processors on a single machine reading and writing from
the same memory in a coherent way, only one processor on a machine can have
exclusive access to a given cache line.<sup id="fnref:2"><a href="#fn:2">2</a></sup></p>

<p><em>False sharing</em> is when you accidentally put two unrelated pieces of data in the
same cache line. Now having two processors update separate chunks of data, say
counters, will interfere with one another, as each processor attempts to gain
exclusive access to the cache line that houses both fields.</p>

<p>The explanation for the name <em>false sharing</em> is that even though these two
counters shouldn‚Äôt be affecting one another from a correctness standpoint, they
are ‚Äì big air quotes incoming ‚Äì ‚Äúfalsely sharing‚Äù a cache line for no good reason.</p>

<p>One solution is to force the data onto separate cache lines, which you can do in
C/C++ by forcing the alignment of the members of a struct/class. In
<a href="https://github.com/paulcavallaro/systems-programming/blob/master/examples/cache-lines.cc">examples/cache-lines.cc</a>
we use absl‚Äôs
<a href="https://github.com/abseil/abseil-cpp/blob/fa00c321073c7ea40a4fc3dfc8a06309eae3d025/absl/base/optimization.h#L99-L148"><code>ABSL_CACHELINE_ALIGNED</code></a>
macro to achieve this.</p>

<p>To show the effect in practice, we benchmark two different structs of
<code>std::atomic&lt;int64&gt;</code> counters, <code>NormalCounters</code> and <code>CacheLineAwareCounters</code>.</p>

<div><pre><code data-lang="cpp"><span>
</span><span>
</span><span>
</span><span>
</span><span>
</span><span>
</span><span></span><span><span>struct</span></span> <span>ABSL_CACHELINE_ALIGNED</span> <span>NormalCounters</span> <span>{</span>
  <span>std</span><span>::</span><span>atomic</span><span>&lt;</span><span>int64</span><span>&gt;</span> <span>success</span><span>{</span><span><span>0</span></span><span>};</span>
  <span>std</span><span>::</span><span>atomic</span><span>&lt;</span><span>int64</span><span>&gt;</span> <span>failure</span><span>{</span><span><span>0</span></span><span>};</span>
  <span>std</span><span>::</span><span>atomic</span><span>&lt;</span><span>int64</span><span>&gt;</span> <span>okay</span><span>{</span><span><span>0</span></span><span>};</span>
  <span>std</span><span>::</span><span>atomic</span><span>&lt;</span><span>int64</span><span>&gt;</span> <span>meh</span><span>{</span><span><span>0</span></span><span>};</span>
<span>};</span>

<span>
</span><span>
</span><span>
</span><span>
</span><span>
</span><span></span><span><span>struct</span></span> <span>ABSL_CACHELINE_ALIGNED</span> <span>CacheLineAwareCounters</span> <span>{</span>
  <span>ABSL_CACHELINE_ALIGNED</span> <span>std</span><span>::</span><span>atomic</span><span>&lt;</span><span>int64</span><span>&gt;</span> <span>success</span><span>{</span><span><span>0</span></span><span>};</span>
  <span>ABSL_CACHELINE_ALIGNED</span> <span>std</span><span>::</span><span>atomic</span><span>&lt;</span><span>int64</span><span>&gt;</span> <span>failure</span><span>{</span><span><span>0</span></span><span>};</span>
  <span>ABSL_CACHELINE_ALIGNED</span> <span>std</span><span>::</span><span>atomic</span><span>&lt;</span><span>int64</span><span>&gt;</span> <span>okay</span><span>{</span><span><span>0</span></span><span>};</span>
  <span>ABSL_CACHELINE_ALIGNED</span> <span>std</span><span>::</span><span>atomic</span><span>&lt;</span><span>int64</span><span>&gt;</span> <span>meh</span><span>{</span><span><span>0</span></span><span>};</span>
<span>};</span>
</code></pre></div>

<p>The benchmark uses either 1, 2, 3, or 4 threads, each bumping a separate atomic
counter inside the struct 64K times. Here are the results on a 2013 MacBook Pro
with a Haswell processor:</p>

<div><pre><code data-lang="perl"><span>Executing</span> <span>tests</span> <span>from</span> <span>//</span><span>examples:cache</span><span>-</span><span>lines</span>
<span>-----------------------------------------------------------------------------</span>
<span>Cache</span> <span>Line</span> <span>Size:</span> <span><span>64</span></span>
<span>sizeof</span><span>(</span><span>NormalCounters</span><span>)</span> <span>=</span> <span><span>64</span></span>
<span>sizeof</span><span>(</span><span>CacheLineAwareCounters</span><span>)</span> <span>=</span> <span><span>256</span></span>
<span><span>2019</span></span><span>-</span><span>08</span><span>-</span><span><span>13</span></span> <span><span>011</span></span><span>8</span>
<span>Run</span> <span>on</span> <span>(</span><span><span>4</span></span> <span>X</span> <span><span>2800</span></span> <span>MHz</span> <span>CPU</span> <span><span>s</span></span><span>)</span>
<span>CPU</span> <span>Caches:</span>
  <span>L1</span> <span>Data</span> <span><span>32</span></span><span>K</span> <span>(</span><span>x2</span><span>)</span>
  <span>L1</span> <span>Instruction</span> <span><span>32</span></span><span>K</span> <span>(</span><span>x2</span><span>)</span>
  <span>L2</span> <span>Unified</span> <span><span>262</span></span><span>K</span> <span>(</span><span>x2</span><span>)</span>
  <span>L3</span> <span>Unified</span> <span><span>4194</span></span><span>K</span> <span>(</span><span>x1</span><span>)</span>
<span>---------------------------------------------------------------------------</span>
<span>Benchmark</span>                                    <span>Time</span>           <span>CPU</span> <span>Iterations</span>
<span>---------------------------------------------------------------------------</span>
<span>BM_NormalCounters</span><span>/</span><span>threads:<span>1</span></span>                <span><span>389</span></span> <span>us</span>        <span><span>387</span></span> <span>us</span>       <span><span>1812</span></span>
<span>BM_NormalCounters</span><span>/</span><span>threads:<span>2</span></span>               <span><span>1264</span></span> <span>us</span>       <span><span>2517</span></span> <span>us</span>        <span><span>234</span></span>
<span>BM_NormalCounters</span><span>/</span><span>threads:<span>3</span></span>               <span><span>1286</span></span> <span>us</span>       <span><span>3718</span></span> <span>us</span>        <span><span>225</span></span>
<span>BM_NormalCounters</span><span>/</span><span>threads:<span>4</span></span>               <span><span>1073</span></span> <span>us</span>       <span><span>3660</span></span> <span>us</span>        <span><span>204</span></span>
<span>BM_CacheLineAwareCounters</span><span>/</span><span>threads:<span>1</span></span>        <span><span>386</span></span> <span>us</span>        <span><span>385</span></span> <span>us</span>       <span><span>1799</span></span>
<span>BM_CacheLineAwareCounters</span><span>/</span><span>threads:<span>2</span></span>        <span><span>200</span></span> <span>us</span>        <span><span>400</span></span> <span>us</span>       <span><span>1658</span></span>
<span>BM_CacheLineAwareCounters</span><span>/</span><span>threads:<span>3</span></span>        <span><span>208</span></span> <span>us</span>        <span><span>581</span></span> <span>us</span>       <span><span>1152</span></span>
<span>BM_CacheLineAwareCounters</span><span>/</span><span>threads:<span>4</span></span>        <span><span>193</span></span> <span>us</span>        <span><span>721</span></span> <span>us</span>       <span><span>1008</span></span></code></pre></div>

<p>A note on these results: <code>Time</code> measures wall clock time per-thread and <code>CPU</code>
measures cpu time per-thread.</p>

<p>We can see that the sizes of the two structs are different, where
<code>sizeof(NormalCounters) = 64</code> and <code>sizeof(CacheLineAwareCounters) = 256</code>. This
is because the alignment constraint we put on the individual fields, such that
each member is on its own cache line. So instead of an <code>int64</code> taking up 8 bytes
like usual, it‚Äôs taking up a full cache line, which is 64 bytes on my machine.</p>

<p><img src="https://paulcavallaro.com/images/sysprog-false-sharing.png" title="Graph of CPU Time vs. # of Threads"></p>

<p>We also see that for the single threaded case, the <code>NormalCounters</code>
vs. <code>CacheLineAwareCounters</code> perform indistinguishably. But as we add more
threads, the <code>CacheLineAwareCounters</code> perform much better than the naive normal
counters that suffer from the false sharing.</p>

<p>Interestingly the wall clock time spent for <code>CacheLineAwareCounters</code> is higher
for one thread than multiple threads, which could point to perhaps some subtle
benchmarking problem, or maybe a fixed amount of delay that‚Äôs getting attributed
across more threads now, and so is smaller per-thread.</p>

<h2 id="the-magic-power-of-2-division-is-slowaloo">The Magic Power of 2: Division Is Slowaloo</h2>

<p>In current hardware, division is one of the most expensive operations, where
expensive here means ‚Äúlongest
latency‚Äù. <a href="https://www.agner.org/optimize/instruction_tables.pdf">Agner Fog‚Äôs listing of instruction latencies</a>
lists Intel Skylake‚Äôs <code>DIV</code> instruction operating on two 64 bit registers having
a latency of 35-88 cycles, compared to an <code>ADD</code> instruction operating on the
same two 64 bit registers having a latency of 1 cycle. So it would behoove us to
avoid division where some other set of operations would work.</p>

<p>One place where division is commonly used besides for actually doing division,
is in the modulo <code>%</code> operator. And a common place for the modulo operator is in
hash tables: to go from a hash to a bucket we compute <code>HASH %
TABLE_SIZE</code>. Modulo is used even more heavily in
<a href="https://en.wikipedia.org/wiki/Open_addressing">open-addressing</a> schemes since
we‚Äôre constantly remapping values back into the hash table bucket space.</p>

<p>So how do we go from using modulo to something else? Bit twiddling and the Magic
Power of 2!</p>

<p>First, let me give away the answer: We‚Äôre going to force all of our hash tables
to be a size that‚Äôs a power of 2.</p>

<p>We‚Äôll be able to take advantage of this property for replacing division with
faster bit twiddling. Also, this property is easy to maintain since we double
the size of our hash table whenever we grow to amoritize the cost of rehashing,
so the size will stay a power of 2 as we grow.</p>

<p>Now, we were using division ‚Äì or modulo ‚Äì for the purpose of mapping any hash
value to a bucket index in our hash table. Bucket indices must be strictly less
than our size, and this mapping should not lose any entropy from the hash value.</p>

<p>Instead of using division to do this, we‚Äôll use a bitmask that will ‚Äúmask‚Äù away
all set bits except those that are strictly less than our power of 2. This way
we keep all the entropy in the least significant bits, just like modulo, but
it‚Äôs much faster ‚Äì Agner Fog lists it at 1 cycle latency on the same Intel
Skylake architecture.</p>

<p>As a quick refresher on bit twiddling and to explain how we‚Äôll choose our
bitmask, let‚Äôs look at some bit patterns.</p>

<p>Since numbers are represented in binary, we know that every power of 2 number
<code>N</code> only has one bit set. For example:</p>

<div><pre><code data-lang="c"><span>Decimal</span> <span>|</span>    <span>Binary</span>
    <span><span>2</span></span>   <span>|</span> <span><span>00</span></span> <span><span>00</span></span> <span><span>00</span></span> <span><span>10</span></span>
    <span><span>8</span></span>   <span>|</span> <span><span>00</span></span> <span><span>00</span></span> <span><span>10</span></span> <span><span>00</span></span>
   <span><span>32</span></span>   <span>|</span> <span><span>00</span></span> <span><span>10</span></span> <span><span>00</span></span> <span><span>00</span></span>
  <span><span>128</span></span>   <span>|</span> <span><span>10</span></span> <span><span>00</span></span> <span><span>00</span></span> <span><span>00</span></span></code></pre></div>

<p>And that means all <code>N - 1</code>‚Äôs have every less significant bit than <code>log2(N)</code>
set. For example:</p>

<div><pre><code data-lang="c"><span>Decimal</span> <span>|</span>    <span>Binary</span>
    <span><span>1</span></span>   <span>|</span> <span><span>00</span></span> <span><span>00</span></span> <span><span>00</span></span> <span><span>01</span></span>
    <span><span>7</span></span>   <span>|</span> <span><span>00</span></span> <span><span>00</span></span> <span><span>01</span></span> <span><span>11</span></span>
   <span><span>31</span></span>   <span>|</span> <span><span>00</span></span> <span><span>01</span></span> <span><span>11</span></span> <span><span>11</span></span>
  <span><span>127</span></span>   <span>|</span> <span><span>01</span></span> <span><span>11</span></span> <span><span>11</span></span> <span><span>11</span></span></code></pre></div>

<p>So in order to replace our modulo operator in the <code>HASH % N</code> calculation, we
instead calculate <code>HASH &amp; (N - 1)</code> using bitwise AND. This will only keep the
set bits that are less significant than our <code>log_2(N)</code> bit, mapping any <code>HASH</code>
value onto a number in the interval
<code>[0, N)</code>. We can even cache this bitmask if we want so we don‚Äôt have to
recompute it in the future.</p>

<p>To show how much faster this bitmask trick is than using normal modulo operator,
I‚Äôve written a
<a href="https://github.com/paulcavallaro/systems-programming/blob/master/examples/power-of-two.cc">small benchmark</a>
to measure executing 1M modulo operations, versus executing 1M bitmasks.</p>

<div><pre><code data-lang="perl"><span>Executing</span> <span>tests</span> <span>from</span> <span>//</span><span>examples:power</span><span>-</span><span>of</span><span>-</span><span>two</span>
<span>-----------------------------------------------------------------------------</span>
<span><span>2019</span></span><span>-</span><span>08</span><span>-</span><span><span>13</span></span> <span><span>0203</span></span>
<span>Run</span> <span>on</span> <span>(</span><span><span>4</span></span> <span>X</span> <span><span>2800</span></span> <span>MHz</span> <span>CPU</span> <span><span>s</span></span><span>)</span>
<span>CPU</span> <span>Caches:</span>
  <span>L1</span> <span>Data</span> <span><span>32</span></span><span>K</span> <span>(</span><span>x2</span><span>)</span>
  <span>L1</span> <span>Instruction</span> <span><span>32</span></span><span>K</span> <span>(</span><span>x2</span><span>)</span>
  <span>L2</span> <span>Unified</span> <span><span>262</span></span><span>K</span> <span>(</span><span>x2</span><span>)</span>
  <span>L3</span> <span>Unified</span> <span><span>4194</span></span><span>K</span> <span>(</span><span>x1</span><span>)</span>
<span>--------------------------------------------------------</span>
<span>Benchmark</span>                 <span>Time</span>           <span>CPU</span> <span>Iterations</span>
<span>--------------------------------------------------------</span>
<span>BM_Mod</span>                 <span><span>9261</span></span> <span>us</span>       <span><span>9234</span></span> <span>us</span>         <span><span>75</span></span>
<span>BM_BitMask</span>              <span><span>325</span></span> <span>us</span>        <span><span>324</span></span> <span>us</span>       <span><span>1984</span></span></code></pre></div>

<p>Here we can see how the <code>DIV</code> instruction used by the modulo operator is on the
order of 28x slower, near the 35x slow down predicted by Agner Fog‚Äôs latency
numbers.</p>

<p>Since this trick is easy to do, and provides a nice win, it‚Äôs used by many
high-performance hash-tables, like absl‚Äôs ‚ÄúSwiss Tables‚Äù
<a href="https://github.com/abseil/abseil-cpp/blob/c6c3c1b498e4ee939b24be59cae29d59c3863be8/absl/container/internal/raw_hash_set.h#L471-L474">flat_hash_set and flat_hash_map</a> and <a href="https://github.com/concurrencykit/ck/blob/master/src/ck_ht.c#L145-L172">ConcurrencyKit‚Äôs ck_ht_map</a>.</p>

<h2 id="repurposing-top-bits">Repurposing Top Bits</h2>

<p>Oftentimes you want to store a bit or two of extra information along with a
pointer ‚Äì in fact it‚Äôs so common
<a href="https://en.wikipedia.org/wiki/Tagged_pointer">Wikipedia has an article about it</a>. One
way to do this is to take advantage that on many 64-bit systems, such as linux,
virtual memory addresses are only 48 bits wide, even though we use 8 bytes to
store them.</p>

<p>This means, we can just store any old crap we want to in the top 16 bits by just
masking it out whenever we want to actually dereference it. Here‚Äôs some
<a href="https://github.com/paulcavallaro/systems-programming/blob/master/examples/pointer-tagging.cc">example C++ code</a>
of using the top bit of a pointer to store whether the underlying data is
‚Äúdirty‚Äù or not.</p>

<div><pre><code data-lang="cpp"><span><span>constexpr</span></span> <span>uintptr_t</span> <span>kDirtyBit</span> <span>=</span> <span><span>0x8000000000000000</span></span><span>;</span>
<span><span>constexpr</span></span> <span>uintptr_t</span> <span>kPtrMask</span> <span>=</span> <span><span>0x7fffffffffffffff</span></span><span>;</span>
<span><span>static_assert</span></span><span>(</span><span><span>sizeof</span></span><span>(</span><span>uintptr_t</span><span>)</span> <span>==</span> <span><span>8</span></span><span>,</span> <span><span>"Only works on 64-bit systems"</span></span><span>);</span>

<span><span>template</span></span> <span>&lt;</span><span><span>typename</span></span> <span>T</span><span>&gt;</span>
<span>uintptr_t</span> <span>MarkDirty</span><span>(</span><span>T</span><span>*</span> <span>t</span><span>)</span> <span>{</span>
  <span><span>return</span></span> <span><span>reinterpret_cast</span></span><span>&lt;</span><span>uintptr_t</span><span>&gt;</span><span>(</span><span>t</span><span>)</span> <span>|</span> <span>kDirtyBit</span><span>;</span>
<span>}</span>

<span><span>template</span></span> <span>&lt;</span><span><span>typename</span></span> <span>T</span><span>&gt;</span>
<span>T</span><span>*</span> <span>GetPointer</span><span>(</span><span>uintptr_t</span> <span>t</span><span>)</span> <span>{</span>
  <span><span>return</span></span> <span><span>reinterpret_cast</span></span><span>&lt;</span><span>T</span><span>*&gt;</span><span>(</span><span>t</span> <span>&amp;</span> <span>kPtrMask</span><span>);</span>
<span>}</span>
</code></pre></div>

<p>Interestingly though, since this is a feature of Linux‚Äôs memory
management/virtual address space, it is subject to change ‚Äì and it actually
has!</p>

<p>LWN <a href="https://lwn.net/Articles/717293/">reported on the patchset in 2017</a>
implementing five-level page tables in order to support larger amounts of
addressable memory. The change, if enabled, would move Linux from 48-bit virtual
addresses to 57-bit virtual addresses, increasing virtual address space from 256
TiB to 128 PiB ‚Äì which ought to be enough for everyone üòÄ.</p>

<p>Part of why the change couldn‚Äôt be enabled by default is because various high
performance programs, notably various JavaScript engines and
<a href="http://luajit.org/">LuaJIT</a>, use this repurposing trick to pack some extra data
into pointers.</p>

<h2 id="lock-striping">Lock Striping</h2>

<p>Locks can be used for mutual exclusion when you want to have multiple threads
access shared data exclusively. The downside though is if the shared data is
frequently accessed and the critical section is non-trivial, your threads could
spend most of their time contending on the lock, instead of actually doing work.</p>

<p>One common way of solving this problem, is introducing more locks. Wait ‚Äì what?</p>

<p>Well, instead of one lock that guards all of the data, instead you have many
locks responsible for just a piece of the data. In this way we shard the data
into separate independent buckets that don‚Äôt contend with each other. Assuming
the data access is uniform-ish, increasing sharding of the data reduces the
number of threads contending on a lock proportionally.</p>

<p>Below is a
<a href="https://github.com/paulcavallaro/systems-programming/blob/master/examples/lock-striping.cc">small C++ example</a>
of two implementations of thread-safe hash set. The first implementation,
<code>ThreadSafeHashSet</code>, just uses a single lock to guard a single underlying
<code>absl::flat_hash_set</code>. The second implementation <code>LockStripedHashSet</code> has <code>N</code>
separate locks, guarding <code>N</code> separate <code>abs::flat_hash_sets</code>.</p>

<div><pre><code data-lang="cpp"><span>
</span><span></span><span><span>class</span></span><span> </span><span>ThreadSafeHashSet</span> <span>{</span>
 <span><span>public</span></span><span>:</span>
  <span><span>void</span></span> <span>Insert</span><span>(</span><span>uint64</span> <span>i</span><span>)</span> <span>{</span>
    <span>absl</span><span>::</span><span>MutexLock</span> <span>lock</span><span>(</span><span>&amp;</span><span>mu_</span><span>);</span>
    <span>hash_set_</span><span>.</span><span>insert</span><span>(</span><span>i</span><span>);</span>
  <span>}</span>

  <span><span>bool</span></span> <span>Contains</span><span>(</span><span>uint64</span> <span>i</span><span>)</span> <span>{</span>
    <span>absl</span><span>::</span><span>MutexLock</span> <span>lock</span><span>(</span><span>&amp;</span><span>mu_</span><span>);</span>
    <span><span>return</span></span> <span>hash_set_</span><span>.</span><span>contains</span><span>(</span><span>i</span><span>);</span>
  <span>}</span>

 <span><span>private</span></span><span>:</span>
  <span>absl</span><span>::</span><span>Mutex</span> <span>mu_</span><span>;</span>
  <span>absl</span><span>::</span><span>flat_hash_set</span><span>&lt;</span><span>uint64</span><span>&gt;</span> <span>hash_set_</span><span>;</span>
<span>};</span>

<span>
</span><span>
</span><span></span><span><span>template</span></span> <span>&lt;</span><span>size_t</span> <span>kNumChunks</span><span>&gt;</span>
<span><span>class</span></span><span> </span><span>LockStripedHashSet</span> <span>{</span>
 <span><span>public</span></span><span>:</span>
  <span><span>void</span></span> <span>Insert</span><span>(</span><span>uint64</span> <span>i</span><span>)</span> <span>{</span>
    <span>
</span><span></span>    <span><span>const</span></span> <span>size_t</span> <span>idx</span> <span>=</span> <span>i</span> <span>%</span> <span>kNumChunks</span><span>;</span>
    <span>absl</span><span>::</span><span>MutexLock</span> <span>lock</span><span>(</span><span>&amp;</span><span>mu_</span><span>[</span><span>idx</span><span>]);</span>
    <span>hash_set_</span><span>[</span><span>idx</span><span>].</span><span>insert</span><span>(</span><span>i</span><span>);</span>
  <span>}</span>

  <span><span>bool</span></span> <span>Contains</span><span>(</span><span>uint64</span> <span>i</span><span>)</span> <span>{</span>
    <span><span>const</span></span> <span>size_t</span> <span>idx</span> <span>=</span> <span>i</span> <span>%</span> <span>kNumChunks</span><span>;</span>
    <span>absl</span><span>::</span><span>MutexLock</span> <span>lock</span><span>(</span><span>&amp;</span><span>mu_</span><span>[</span><span>idx</span><span>]);</span>
    <span><span>return</span></span> <span>hash_set_</span><span>[</span><span>idx</span><span>].</span><span>contains</span><span>(</span><span>i</span><span>);</span>
  <span>}</span>

 <span><span>private</span></span><span>:</span>
  <span>std</span><span>::</span><span><span><span>array</span></span></span><span><span>&lt;</span></span><span><span>absl</span></span><span><span>::</span></span><span><span>Mutex</span></span><span><span>,</span></span><span> </span><span><span>kNumChunks</span></span><span><span>&gt;</span></span> <span>mu_</span><span>;</span>
  <span>std</span><span>::</span><span><span><span>array</span></span></span><span><span>&lt;</span></span><span><span>absl</span></span><span><span>::</span></span><span><span>flat_hash_set</span></span><span><span>&lt;</span></span><span><span>uint64</span></span><span><span>&gt;</span></span><span>,</span> <span>kNumChunks</span><span>&gt;</span> <span>hash_set_</span><span>;</span>
<span>};</span>
</code></pre></div>

<p>To illustrate the benefits of lock striping, we benchmark the performance of the
two thread-safe hash sets in the presence of many threads, each inserting 1M
items. For the <code>LockStripedHashSet</code> we try with 4 chunks and with 8 chunks. The
results are below:</p>

<div><pre><code data-lang="perl"><span>Executing</span> <span>tests</span> <span>from</span> <span>//</span><span>examples:lock</span><span>-</span><span>striping</span>
<span>-----------------------------------------------------------------------------</span>
<span><span>2019</span></span><span>-</span><span>08</span><span>-</span><span><span>24</span></span> <span><span>2237</span></span>
<span>Run</span> <span>on</span> <span>(</span><span><span>4</span></span> <span>X</span> <span><span>2800</span></span> <span>MHz</span> <span>CPU</span> <span><span>s</span></span><span>)</span>
<span>CPU</span> <span>Caches:</span>
  <span>L1</span> <span>Data</span> <span><span>32</span></span><span>K</span> <span>(</span><span>x2</span><span>)</span>
  <span>L1</span> <span>Instruction</span> <span><span>32</span></span><span>K</span> <span>(</span><span>x2</span><span>)</span>
  <span>L2</span> <span>Unified</span> <span><span>262</span></span><span>K</span> <span>(</span><span>x2</span><span>)</span>
  <span>L3</span> <span>Unified</span> <span><span>4194</span></span><span>K</span> <span>(</span><span>x1</span><span>)</span>
<span>--------------------------------------------------------------------------</span>
<span>Benchmark</span>                                   <span>Time</span>           <span>CPU</span> <span>Iterations</span>
<span>--------------------------------------------------------------------------</span>
<span>BM_SingleLock</span><span>/</span><span>threads:<span>1</span></span>                    <span><span>65</span></span> <span>ms</span>         <span><span>65</span></span> <span>ms</span>         <span><span>11</span></span>
<span>BM_SingleLock</span><span>/</span><span>threads:<span>2</span></span>                   <span><span>140</span></span> <span>ms</span>        <span><span>254</span></span> <span>ms</span>          <span><span>2</span></span>
<span>BM_SingleLock</span><span>/</span><span>threads:<span>3</span></span>                   <span><span>140</span></span> <span>ms</span>        <span><span>332</span></span> <span>ms</span>          <span><span>3</span></span>
<span>BM_SingleLock</span><span>/</span><span>threads:<span>4</span></span>                   <span><span>142</span></span> <span>ms</span>        <span><span>405</span></span> <span>ms</span>          <span><span>4</span></span>
<span>BM_LockStriping_4_Chunks</span><span>/</span><span>threads:<span>1</span></span>         <span><span>71</span></span> <span>ms</span>         <span><span>69</span></span> <span>ms</span>          <span><span>9</span></span>
<span>BM_LockStriping_4_Chunks</span><span>/</span><span>threads:<span>2</span></span>         <span><span>90</span></span> <span>ms</span>        <span><span>178</span></span> <span>ms</span>          <span><span>4</span></span>
<span>BM_LockStriping_4_Chunks</span><span>/</span><span>threads:<span>3</span></span>         <span><span>89</span></span> <span>ms</span>        <span><span>248</span></span> <span>ms</span>          <span><span>3</span></span>
<span>BM_LockStriping_4_Chunks</span><span>/</span><span>threads:<span>4</span></span>         <span><span>82</span></span> <span>ms</span>        <span><span>299</span></span> <span>ms</span>          <span><span>4</span></span>
<span>BM_LockStriping_8_Chunks</span><span>/</span><span>threads:<span>1</span></span>         <span><span>70</span></span> <span>ms</span>         <span><span>69</span></span> <span>ms</span>         <span><span>10</span></span>
<span>BM_LockStriping_8_Chunks</span><span>/</span><span>threads:<span>2</span></span>         <span><span>74</span></span> <span>ms</span>        <span><span>143</span></span> <span>ms</span>          <span><span>4</span></span>
<span>BM_LockStriping_8_Chunks</span><span>/</span><span>threads:<span>3</span></span>         <span><span>71</span></span> <span>ms</span>        <span><span>198</span></span> <span>ms</span>          <span><span>3</span></span>
<span>BM_LockStriping_8_Chunks</span><span>/</span><span>threads:<span>4</span></span>         <span><span>60</span></span> <span>ms</span>        <span><span>200</span></span> <span>ms</span>          <span><span>4</span></span></code></pre></div>

<p>Again, <code>Time</code> measures wall clock time per-thread and <code>CPU</code> measures cpu time
per-thread. Also note that since my machine only has 4 logical cores, this test
only goes up to 4 threads, since anything beyond that doesn‚Äôt actually introduce
any extra contention.</p>

<p><img src="https://paulcavallaro.com/images/sysprog-lock-striping.png" title="Graph of CPU Time vs. # of Threads"></p>

<p>We can see that in the single-threaded case the <code>LockStripedHashSet</code>, no matter
the chunking, performs slightly worse on both wall clock and CPU time than the
simple <code>ThreadSafeHashSet</code>.</p>

<p>However, as more threads are added, increasing the contention on the locks, the
<code>LockStripedHashSet</code> performs much better, and the 8 chunk version outperforms
the 4 chunk version at higher thread counts.</p>

<p>While lock-striping can help alleviate contention on locks, it does have the
downside of increasing storage overhead for the locks. In our example the
overhead of 7 extra locks and extra <code>absl::flat_hash_set</code> bookkeeping is
miniscule for a single instance in our benchmark, but if you replaced all the
hash sets in an application with an 8-way striped thread-safe hash set, you
could bloat its memory footprint by a significant amount.</p>

<h2 id="conclusion">Conclusion</h2>

<p>While this is nowhere near an exhaustive list of the most common systems
programming tricks, hopefully it whets your appetite to learn more, provides
some tools for improving performance in your own applications, or at least make
it easier to understand why performance sensitive code is doing what it‚Äôs doing.</p>

<p><em>Thanks to Mason Simon and Ray Yang for reviewing drafts of this post.</em></p>


      </article>

      

      

    </div>
  </div>
</div></div></div>
    </div>
    <footer>
        <div>created by buildstarted &copy; 2020 <a href="/about">about</a></div>
        <div>Share this page on social media: copy and paste this url https://linksfor.dev/</div>
        <div>If you prefer RSS: <a href="https://linksfor.dev/feed.xml">https://linksfor.dev/feed.xml</a></div>
    </footer>
    
    <script async defer>
        _dna = window._dna || {};
        _dna.siteId = "linksfor.devs";
        _dna.outlink = true;

        (function () {
            let dna = document.createElement('script');
            dna.type = 'text/javascript';
            dna.async = true;
            dna.src = '//dna.buildstarted.com/t.js';
            let s = document.getElementsByTagName('script')[0];
            s.parentNode.insertBefore(dna, s);
        })();
    </script>
    <noscript><img src="//dna.buildstarted.com/g?siteId=linksfor.devs"/></noscript>
</body>
</html>