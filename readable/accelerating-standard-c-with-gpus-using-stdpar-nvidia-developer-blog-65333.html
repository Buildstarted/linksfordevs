<!DOCTYPE html>
<html lang="en">
<head>
    <title>
Accelerating Standard C&#x2B;&#x2B; with GPUs Using stdpar | NVIDIA Developer Blog - linksfor.dev(s)    </title>
    <meta charset="utf-8">
    <link rel="alternate" type="application/rss+xml" title="Linksfor.dev(s) feed" href="https://linksfor.dev/feed.rss" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta name="google" value="notranslate">
    <link rel="stylesheet" type="text/css" href="/style.min.css" />
    <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon">
    <link rel="icon" href="/favicon.ico" type="image/x-icon">
    <meta property="og:title" content="Accelerating Standard C&#x2B;&#x2B; with GPUs Using stdpar | NVIDIA Developer Blog - linksfor.dev(s)"/>
    <meta property="article:author" content="View all posts by David Olsen"/>
    <meta property="og:description" content="Historically, accelerating your C&#x2B;&#x2B; code with GPUs has not been possible in Standard C&#x2B;&#x2B; without using language extensions or additional libraries: CUDA C&#x2B;&#x2B; requires the use of and attributes on&#x2026;"/>
    <meta property="og:type" content="website"/>
    <meta property="og:url" content="https://developer.nvidia.com/blog/accelerating-standard-c-with-gpus-using-stdpar/"/>

<meta property="og:site_name" content="linksfor.dev(s)" />
</head>
<body>
    <div class="devring" style="background: #222">
        <div style="text-align:center">Explore other dev related sites in this ring. If you would like to join this ring <a href="https://devring.club">click here</a>.</div>
        <div class="grid">
            <div style="display: grid; grid-template-columns: .5fr 1fr 1fr 1fr; text-align: center;">
                <span class="devring-title"><a href="https://devring.club/">devring.club</a></span>
                <a href="https://devring.club/sites/1/prev" class="devring-previous">Previous</a>
                <a href="https://devring.club/random" class="devring-random">Random</a>
                <a href="https://devring.club/sites/1/next" class="devring-next">Next</a>
            </div>
        </div>
    </div>
    <div class="grid">
        <h1 style="margin: unset">
<a href="/" style="color:inherit">linksfor.dev(s)</a>
        </h1>
        <title>linksfor.dev(s) - Accelerating Standard C&#x2B;&#x2B; with GPUs Using stdpar | NVIDIA Developer Blog</title>
<div class="readable">
        <h1>Accelerating Standard C&#x2B;&#x2B; with GPUs Using stdpar | NVIDIA Developer Blog</h1>
            <div>by View all posts by David Olsen</div>
            <div>Reading time: 24-30 minutes</div>
        <div>Posted here: 07 Aug 2020</div>
        <p><a href="https://developer.nvidia.com/blog/accelerating-standard-c-with-gpus-using-stdpar/">https://developer.nvidia.com/blog/accelerating-standard-c-with-gpus-using-stdpar/</a></p>
        <hr/>
<div id="readability-page-1" class="page"><div id="pf-post-view">
    <div>

                  <article id="post-18511">
                

                <div>
                  
<div>
<figure><img src="https://developer.nvidia.com/blog/wp-content/uploads/2020/06/C-standard-parallelism-3x2-blog-format-300x200.png" alt="Standard Parallellism in C++" width="300" height="200" srcset="https://developer.nvidia.com/blog/wp-content/uploads/2020/06/C-standard-parallelism-3x2-blog-format-300x200.png 300w, https://developer.nvidia.com/blog/wp-content/uploads/2020/06/C-standard-parallelism-3x2-blog-format-625x417.png 625w, https://developer.nvidia.com/blog/wp-content/uploads/2020/06/C-standard-parallelism-3x2-blog-format-173x115.png 173w, https://developer.nvidia.com/blog/wp-content/uploads/2020/06/C-standard-parallelism-3x2-blog-format-768x512.png 768w, https://developer.nvidia.com/blog/wp-content/uploads/2020/06/C-standard-parallelism-3x2-blog-format-450x300.png 450w, https://developer.nvidia.com/blog/wp-content/uploads/2020/06/C-standard-parallelism-3x2-blog-format-135x90.png 135w, https://developer.nvidia.com/blog/wp-content/uploads/2020/06/C-standard-parallelism-3x2-blog-format-362x241.png 362w, https://developer.nvidia.com/blog/wp-content/uploads/2020/06/C-standard-parallelism-3x2-blog-format-165x110.png 165w, https://developer.nvidia.com/blog/wp-content/uploads/2020/06/C-standard-parallelism-3x2-blog-format.png 786w" sizes="(max-width: 300px) 100vw, 300px"></figure>
</div>



<p>Historically, accelerating your C++ code with GPUs has not been possible in Standard C++ without using language extensions or additional libraries:</p>



<ul><li>CUDA C++ requires the use of <code>__host__</code> and <code>__device__</code> attributes on functions and the triple-chevron syntax for GPU kernel launches.</li><li>OpenACC uses #pragmas to control GPU acceleration.</li><li>Thrust lets you express parallelism portably but uses language extensions internally and only supports a limited number of CPU and GPU backends. The portability of the application is limited by the portability of the library.</li></ul>



<p>In many cases, the results of these ports are worth the effort. But what if you could get the same effect without that cost? What if you could take your Standard C++ code and accelerate on a GPU?</p>



<p>Now you can! NVIDIA recently announced NVC++, the <a href="https://developer.nvidia.com/hpc-sdk" data-wpel-link="external" target="_blank" rel="nofollow external noopener noreferrer">NVIDIA HPC SDK</a> C++ compiler. This is the first compiler to support GPU-accelerated Standard C++ with no language extensions, pragmas, directives, or non-standard libraries. You can write Standard C++, which is portable to other compilers and systems, and use NVC++ to automatically accelerate it with high-performance NVIDIA GPUs. We built it so that you can spend less time porting and more time on what really matters—solving the world’s problems with computational science.</p>



<h2>C++ Standard Parallelism</h2>



<p>C++11 introduced a memory model, concurrent execution model, and concurrency library, providing a standard way to take advantage of multicore processors. However, until recently, Standard C++ lacked higher-level facilities for parallel programming.</p>



<p>The C++17 Standard introduced higher-level parallelism features that allow users to request parallelization of Standard Library algorithms.</p>



<p>This higher-level parallelism is expressed by adding an execution policy as the first parameter to any algorithm that supports execution policies. Most of the existing Standard C++ algorithms were enhanced to support execution policies. C++17 defined several new parallel algorithms, including the useful <a href="https://en.cppreference.com/w/cpp/algorithm/reduce" data-wpel-link="external" target="_blank" rel="nofollow external noopener noreferrer">std::reduce</a> and <a href="https://en.cppreference.com/w/cpp/algorithm/transform_reduce" data-wpel-link="external" target="_blank" rel="nofollow external noopener noreferrer">std::transform_reduce</a>.</p>



<p>C++ defines four <a href="https://en.cppreference.com/w/cpp/algorithm/execution_policy_tag" data-wpel-link="external" target="_blank" rel="nofollow external noopener noreferrer">execution policies</a>:</p>



<ul><li><code>std::execution::seq</code>: Sequential execution. No parallelism is allowed.</li><li><code>std::execution::unseq</code>: Vectorized execution on the calling thread (this execution policy was added in C++20).</li><li><code>std::execution::par</code>: Parallel execution on one or more threads.</li><li><code>std::execution::par_unseq</code>: Parallel execution on one or more threads, with each thread possibly vectorized.</li></ul>



<p>When you use an execution policy other than <code>std::execution::seq</code>, you are communicating two important things to the compiler:</p>



<ul><li>You prefer but do not require that the algorithm be run in parallel. A conforming implementation may ignore the hint and run the algorithm sequentially, but a high-quality implementation takes the hint and executes in parallel when possible and prudent.</li><li>The algorithm is safe to run in parallel. For the <code>std::execution::par</code> and <code>std::execution::par_unseq</code> policies, any user-provided code—such as iterators, lambdas, or function objects passed into the algorithm—must not introduce data races if run concurrently on separate threads. For the <code>std::execution::unseq</code> and <code>std::execution::par_unseq</code> policies, any user-provided code must not introduce data races or deadlocks if multiple calls are interleaved on the same thread, which is what happens when a loop is vectorized. For more information about potential deadlocks, see the <a href="https://en.cppreference.com/w/cpp/language/memory_model#Progress_guarantee" data-wpel-link="external" target="_blank" rel="nofollow external noopener noreferrer">forward progress guarantees</a> provided by the parallel policies or watch <a href="https://www.youtube.com/watch?v=FJIn1YhPJJc" data-wpel-link="external" target="_blank" rel="nofollow external noopener noreferrer">CppCon 2018: Bryce Adelstein Lelbach “The C++ Execution Model”</a>.</li></ul>



<p>The C++ Standard grants compilers great freedom to choose if, when, and how to execute algorithms in parallel as long as the forward progress guarantees that the user requests are honored. For example, <code>std::execution::unseq</code> may be implemented with vectorization and <code>std::execution::par</code> may be implemented with a CPU thread pool. It is also possible to execute parallel algorithms on a GPU, which is a good choice for invocations with sufficient parallelism to take advantage of the processing power and memory bandwidth of modern GPU processors.</p>



<h2>NVIDIA HPC SDK</h2>



<p>The <a href="https://developer.nvidia.com/hpc-sdk" data-wpel-link="external" target="_blank" rel="nofollow external noopener noreferrer">NVIDIA HPC SDK</a> is a comprehensive suite of compilers, libraries, and tools used to GPU accelerate HPC modeling and simulation applications. With support for NVIDIA GPUs and x86-64, OpenPOWER, or Arm CPUs running Linux, the NVIDIA HPC SDK provides proven tools and technologies for building cross-platform, performance-portable, and scalable HPC applications.</p>



<p>The <a href="https://developer.nvidia.com/hpc-sdk" data-wpel-link="external" target="_blank" rel="nofollow external noopener noreferrer">NVIDIA HPC SDK</a> includes the new NVIDIA HPC C++ compiler, NVC++. NVC++ supports C++17, C++ Standard Parallelism (stdpar) for CPU and GPU, OpenACC for CPU and GPU, and OpenMP for CPU.</p>



<p>NVC++ can compile Standard C++ algorithms with the parallel execution policies <code>std::execution::par</code> or <code>std::execution::par_unseq</code> for execution on NVIDIA GPUs. An NVC++ command-line option, <code>-stdpar</code>, is used to enable GPU-accelerated C++ Parallel Algorithms. Lambdas, including generic lambdas, are fully supported in parallel algorithm invocations. No language extensions or non-standard libraries are required to enable GPU acceleration. All data movement between host memory and GPU device memory is performed implicitly and automatically under the control of CUDA Unified Memory.</p>



<p>It’s easy to automatically use GPU acceleration for C++ Parallel Algorithms with NVC++. However, there are some restrictions and limitations, which we explain later in this post.</p>



<h3>Enabling C++ Parallel Algorithms with the -stdpar option</h3>



<p>GPU acceleration of C++ Parallel Algorithms is enabled with the <code>-stdpar</code> command-line option to NVC++. If <code>-stdpar</code> is specified, almost all algorithms that use a parallel execution policy are compiled for offloading to run in parallel on an NVIDIA GPU:</p>



<pre>nvc++ -stdpar program.cpp -o program</pre>



<h2>Simple examples</h2>



<p>Here are a few simple examples to get a feel for how the C++ Parallel Algorithms work.</p>



<p>From the early days of C++, sorting items stored in an appropriate container has been relatively easy using a single call such as the following:</p>



<pre>std::sort(employees.begin(), employees.end(),
            CompareByLastName());</pre>



<p>Assuming that the comparison class <code>CompareByLastName</code> is thread-safe, which is true for most comparison functions, then parallelizing this sort is simple with C++ Parallel Algorithms. Include <code>&lt;execution&gt;</code> and add an execution policy to the function call:</p>



<pre>std::sort(std::execution::par,
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; employees.begin(), employees.end(),
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; CompareByLastName());</pre>



<p>Calculating the sum of all the elements in a container is also simple with the <code>std::accumulate</code> algorithm. Prior to C++17, transforming the data in some way while taking the sum was somewhat awkward. For example, to compute the average age of your employees, you might write the following code example:</p>



<pre>int ave_age =
&nbsp;&nbsp;&nbsp; std::accumulate(employees.begin(), employees.end(), 0,
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; [](int sum, const Employee&amp; emp){
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;return sum + emp.age();
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; })
&nbsp;&nbsp;&nbsp; / employees.size();</pre>



<p>The <code>std::transform_reduce</code> algorithm introduced in C++17 makes it simple to parallelize this code. It also results in cleaner code by separating the reduction operation, in this case <code>std::plus</code>, from the transformation operation, in this case <code>emp.age</code>:</p>



<pre>int ave_age =
&nbsp;&nbsp;&nbsp; std::transform_reduce(std::execution::par_unseq,
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; employees.begin(), employees.end(),
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 0, std::plus&lt;int&gt;(),
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[](const Employee&amp; emp){
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; return emp.age();
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; })
&nbsp;&nbsp;&nbsp; / employees.size();</pre>



<h2>Coding guidelines for GPU-accelerating C++ Parallel Algorithms</h2>



<p>GPUs are not simply CPUs with more threads. To take advantage of the massive parallelism available on GPUs, it is typical for GPU programming models to put some limitations on code to be executed on the GPU.</p>



<p>The NVC++ implementation of C++ Parallel Algorithms is no exception in this regard. In this post, we list the major limitations that apply at the time of publication. As always, NVIDIA is working across both hardware and software teams to eliminate as many of these restrictions as possible for future releases.</p>



<p>Topics covered in this section include:</p>



<ul><li>C++ Parallel Algorithms and device function annotations</li><li>C++ Parallel Algorithms and CUDA Unified Memory</li><li>C++ Parallel Algorithms and function pointers</li><li>Random-access iterators</li><li>Interoperability with the C++ Standard Library</li><li>No exceptions in GPU code</li></ul>



<h3>C++ Parallel Algorithms and device function annotations</h3>



<p>Unlike CUDA C++, functions do not need any <code>__device__</code> annotations or other special markings to be compiled for GPU execution. The NVC++ compiler walks the call graph for each source file and automatically infers which functions must be compiled for GPU execution.</p>



<p>However, this only works when the compiler can see the function definition in the same source file where the function is called. This is true for most inline functions and template functions but may fail when functions are defined in a different source file or linked in from an external library.</p>



<h3>C++ Parallel Algorithms and CUDA Unified Memory</h3>



<p>Most GPU programming models allow or require that movement of data objects between CPU memory and GPU memory be managed explicitly by the user. For example, CUDA provides <code>cudaMemcpy</code> and OpenACC has <code>#pragma acc</code> data directives to control the movement of data between CPU memory and GPU memory.</p>



<p>NVC++ relies on CUDA Unified Memory for all data movement between CPU and GPU memory. Through support in both the CUDA device driver and the NVIDIA GPU hardware, the CUDA Unified Memory manager automatically moves some types of data based on usage.</p>



<p>Currently, only data dynamically allocated on the heap in CPU code that was compiled by NVC++ can be managed automatically. Memory dynamically allocated in GPU code is only visible from GPU code and can never be accessed by the CPU.</p>



<p>Likewise, CPU and GPU stack memory and memory used for global objects on most systems cannot be automatically managed. Dynamic allocations from CPU code that was not compiled by NVC++ with the <code>-stdpar</code> option is not automatically managed by CUDA Unified Memory, even though it is on the CPU heap.</p>



<p>As a result, any pointer that is dereferenced and any object that is referenced within a C++ Parallel Algorithm invocation must refer to the CPU heap. Dereferencing a pointer to a CPU stack or a global object results in a memory violation in GPU code.</p>



<p>For example, <code>std::vector</code> uses dynamically allocated memory, which is accessible from the GPU when using <code>stdpar</code>. Iterating over the contents of <code>std::vector</code> in a C++ Parallel Algorithm works as expected:</p>



<pre>std::vector&lt;int&gt; v = ...;
std::sort(std::execution::par,
          v.begin(), v.end()); // Okay, accesses heap memory.</pre>



<p>On the other hand, <code>std::array</code> performs no dynamic allocations. Its contents are stored within the <code>std::array</code> object itself, which is often on a CPU stack. Iterating over the contents of <code>std::array</code> won’t work unless the <code>std::array</code> object itself is allocated on the heap:</p>



<pre>std::array&lt;int, 1024&gt; a = ...;
std::sort(std::execution::par,
          a.begin(), a.end()); // Fails, array is on a CPU stack.</pre>



<p>Pay particular attention to lambda captures, especially capturing data objects by reference, which may contain non-obvious pointer dereferences.</p>



<pre>void saxpy(float* x, float* y, int N, float a) {
  std::transform(std::execution::par_unseq, x, x + N, y, y,
                 [<strong>&amp;</strong>](float xi, float yi){ return a * xi + yi; });
}</pre>



<p>In the earlier example, the function parameter <code>a</code> is captured by reference. The code within the body of the lambda, which is running on the GPU, tries to access <code>a</code>, which is in the CPU stack memory. This results in a memory violation and undefined behavior. In this case, the problem can easily be fixed by changing the lambda to capture by value:</p>



<pre>void saxpy(float* x, float* y, int N, float a) {
  std::transform(std::execution::par_unseq, x, x + N, y, y,
                 [<strong>=</strong>](float xi, float yi){ return a * xi + yi; });
}</pre>



<p>With this one-character change, the lambda makes a copy of <code>a</code>, which is then copied to the GPU, and there are no attempts to reference CPU stack memory from GPU code.</p>



<h3>C++ Parallel Algorithms and function pointers</h3>



<p>Functions compiled to run on either the CPU or the GPU must be compiled into two different versions, one with the CPU machine instructions and one with the GPU machine instructions.</p>



<p>In the current implementation, a function pointer either points to the CPU or the GPU version of the functions. This causes problems if you attempt to pass function pointers between CPU and GPU code. You might inadvertently pass a pointer to the CPU version of the function to GPU code. In the future, it may be possible to support the use of function pointers automatically and seamlessly across CPU and GPU code boundaries, but it is not supported in the current implementation.</p>



<p>Function pointers can’t be passed to C++ Parallel Algorithms to be run on the GPU, and functions may not be called through a function pointer within GPU code. For example, the following code example won’t work correctly:</p>



<pre>void square(int&amp; x) { x = x * x; }
void square_all(std::vector&lt;int&gt;&amp; v) {
  std::for_each(std::execution::par_unseq,
                v.begin(), v.end(), &amp;square);
}</pre>



<p>It passes a pointer to the CPU version of the function square to a parallel <code>for_each</code> algorithm invocation. When the algorithm is parallelized and offloaded to the GPU, the program fails to resolve the function pointer to the GPU version of <code>square</code>.</p>



<p>You can often solve this issue by using a function object, which is an object with a function call operator. The function object’s call operator is resolved at compile time to the GPU version of the function, instead of being resolved at run time to the incorrect CPU version of the function as in the previous example. For example, the following code example works:</p>



<pre>struct squared {
  void operator()(int&amp; x) const { x = x * x; }
};
void square_all(std::vector&lt;int&gt;&amp; v) {
  std::for_each(std::execution::par_unseq,
                v.begin(), v.end(), squared{});
}</pre>



<p>Another possible workaround is to change the function to a lambda, because a lambda is implemented as a nameless function object:</p>



<pre>void square_all(std::vector&lt;int&gt;&amp; v) {
&nbsp; std::for_each(std::execution::par_unseq, v.begin(), v.end(),
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; [](int&amp; x) { x = x * x; });
}</pre>



<p>If the function in question is too big to be converted to a function object or a lambda, then it should be possible to wrap the call to the function in a lambda:</p>



<pre>void compute(int&amp; x) {
// Assume lots and lots of code here.
}
void compute_all(std::vector&lt;int&gt;&amp; v) {
  std::for_each(std::execution::par_unseq, v.begin(), v.end(),
                [](int&amp; x) { compute(x); });
}</pre>



<p>No function pointers are used in this example.</p>



<p>The restriction on calling a function through a function pointer unfortunately means passing polymorphic objects from CPU code to GPU-accelerated C++ Parallel Algorithms is not currently supported, as virtual tables are implemented using function pointers.</p>



<h3>Random-access iterators</h3>



<p>The C++ Standard requires that the iterators passed to most C++ Parallel Algorithms be forward iterators. However, stdpar on GPUs only works with random-access iterators. Passing a forward iterator or a bidirectional iterator to a GPU-accelerated C++ Parallel Algorithm results in a compilation error. Passing raw pointers that point to the heap or Standard Library random-access iterators to the algorithms has the best performance, but most other random-access iterators work correctly.</p>



<h3>Interoperability with the C++ Standard Library</h3>



<p>Large parts of the C++ Standard Library can be used with stdpar on GPUs.</p>



<ul>
	<li><code>std::atomic&lt;T&gt;</code> objects within GPU code work provided that <code>T</code> is a four-byte or eight-byte integer type. <code>std::atomic&lt;T&gt;</code> objects can be accessed from both CPU and GPU code provided the object is on the heap.</li>
	<li>Math functions that operate on floating-point types—such as sin, cos, log, and most of the other functions declared in <code>&lt;cmath&gt;</code>—can be used in GPU code and resolve to the same implementations that are used in CUDA C++ programs.</li>
	<li><code>std::complex</code>, <code>std::tuple</code>, <code>std::pair</code>, <code>std::optional</code>, <code>std::variant</code>, and <code>&lt;type_traits&gt;</code>, are supported and work as expected in GPU code.</li>
</ul>



<p>The parts of the C++ Standard Library that aren’t supported in GPU code include I/O functions and in general any function that accesses the CPU operating system. As a special case, basic <code>printf</code> calls can be used within GPU code and leverage the same implementation that is used in CUDA C++.</p>



<h3>No exceptions in GPU code</h3>



<p>As with most other GPU programming models, throwing and catching C++ exceptions is not supported within C++ Parallel Algorithm invocations that are offloaded to the GPU.</p>



<p>Unlike some other GPU programming models where try/catch blocks and throw expressions are compilation errors, exception code does compile but with non-standard behavior. Catch clauses are ignored, and throw expressions abort the GPU kernel if executed. Exceptions in CPU code work without restrictions.</p>



<h2>Larger example: LULESH</h2>



<p>The <a href="https://github.com/LLNL/LULESH" data-wpel-link="external" target="_blank" rel="nofollow external noopener noreferrer">LULESH</a> hydrodynamics mini-app was developed at Lawrence Livermore National Laboratory to stress test compilers and to model the expected performance of their full-sized production hydrodynamics applications. It is about 9,000 lines of C++ code, of which 2,800 lines are the core computation that should be parallelized. The app has been ported to many different parallel programming models, including MPI, OpenMP, OpenACC, CUDA C++, RAJA, and Kokkos.</p>



<p>We ported LULESH to C++ Parallel Algorithms and made the port available on <a href="https://github.com/LLNL/LULESH/tree/2.0.2-dev/stdpar" data-wpel-link="external" target="_blank" rel="nofollow external noopener noreferrer">LULESH’s GitHub repository</a>. To compile it, install the <a href="https://developer.nvidia.com/hpc-sdk" data-wpel-link="external" target="_blank" rel="nofollow external noopener noreferrer">NVIDIA HPC SDK</a>, check out the 2.0.2-dev branch of the LULESH repository, go to the correct directory, and run make.</p>



<pre>git clone --branch 2.0.2-dev https://github.com/LLNL/LULESH.git
cd LULESH/stdpar/build
make run</pre>



<p>While LULESH is too large to show the entire source code in this post, here are some key sections that demonstrate the use of stdpar.</p>



<p>The LULESH code has many loops with large bodies and no loop-carried dependencies, making them good candidates for parallelization. Most of these were easily converted into calls to <code>std::for_each_n</code> with the <code>std::execution::par</code> policy, where the body of the lambda passed to <code>std::for_each_n</code> is identical to the original loop body.</p>



<p>The function <a href="https://github.com/LLNL/LULESH/blob/master/lulesh.cc#L1770-L1920" target="_blank" rel="noreferrer noopener nofollow external" data-wpel-link="external">CalcMonotonicQRegionForElems</a> is an example of this. The loop header written for OpenMP looks like the following code example:</p>



<pre>#pragma omp parallel for firstprivate(qlc_monoq, qqc_monoq, 
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; monoq_limiter_mult, monoq_max_slope, ptiny)
&nbsp; for ( Index_t i = 0 ; i &lt; domain.regElemSize(r); ++i ) {</pre>



<p>This loop header becomes <a href="https://github.com/LLNL/LULESH/blob/2.0.2-dev/stdpar/src/lulesh.cc#L1555-L1756" data-wpel-link="external" target="_blank" rel="nofollow external noopener noreferrer">the following</a>:</p>



<pre>std::for_each_n(
&nbsp; std::execution::par, counting_iterator(0), domain.regElemSize(r),
&nbsp;&nbsp;&nbsp; [=, &amp;domain](Index_t i) {</pre>



<p>The loop body, which in this case is almost 200 lines long, becomes the body of the lambda but is otherwise unchanged.</p>



<p>In a number of places, an explicit for loop was changed to use C++ Parallel Algorithms that better express the intent of the code, such as the function <a href="https://github.com/LLNL/LULESH/blob/master/lulesh.cc#L2022-L2027" data-wpel-link="external" target="_blank" rel="nofollow external noopener noreferrer">CalcPressureForElems</a>:</p>



<pre>#pragma omp parallel for firstprivate(length)
for (Index_t i = 0; i &lt; length ; ++i) {
  Real_t c1s = Real_t(2.0)/Real_t(3.0) ;
  bvc[i] = c1s * (compression[i] + Real_t(1.));
  pbvc[i] = c1s;
}</pre>



<p>This function was rewritten as <a href="https://github.com/LLNL/LULESH/blob/2.0.2-dev/stdpar/src/lulesh.cc#L1825-L1830" data-wpel-link="external" target="_blank" rel="nofollow external noopener noreferrer">the following</a>:</p>



<pre>constexpr Real_t cls = Real_t(2.0) / Real_t(3.0);
std::transform(std::execution::par,
  compression, compression + length, bvc,
  [=](Real_t compression_i) {
    return cls * (compression_i + Real_t(1.0));
  });
std::fill(std::execution::par, pbvc, pbvc + length, cls);</pre>



<p>There are a few cases where the improvements in code clarity, and therefore productivity, are dramatic. The function <code>CalcHydroConstraintForElems</code> finds the minimum value from a set of computed values.</p>



<p>The <a href="https://github.com/LLNL/LULESH/blob/master/lulesh.cc#L2521-L2571" data-wpel-link="external" target="_blank" rel="nofollow external noopener noreferrer">OpenMP version of the function</a> is 52 lines long, with a temporary array to hold the minimum value found by each thread. This can be written in Standard C++ and run in parallel on the GPU with a single call to <code>std::transform_reduce</code>, letting the compiler take care of all the complications of doing a parallel reduction. This change shrinks the previous 52 lines into the following <a href="https://github.com/LLNL/LULESH/blob/2.0.2-dev/stdpar/src/lulesh.cc#L2232-L2242" data-wpel-link="external" target="_blank" rel="nofollow external noopener noreferrer">12 lines</a>:</p>



<pre>dthydro = std::transform_reduce(std::execution::par,
  counting_iterator(0), counting_iterator(length),
  dthydro, [](Real_t a, Real_t b) { return a &lt; b ? a : b; },
  [=, &amp;domain](Index_t i) {
     Index_t indx = regElemlist[i];
     if (domain.vdov(indx) == Real_t(0.0)) {
      return std::numeric_limits&lt;Real_t&gt;::max();
     } else {
      return dvovmax /
             (std::abs(domain.vdov(indx)) + Real_t(1.e-20));
     }
  });</pre>



<p>When the C++ Parallel Algorithm version of LULESH is compiled for a single A100 GPU, the application runs almost seven times faster than when the same code is compiled to run on all 40 CPU cores on a dual-socket Skylake system, as shown in Figure 1. The performance of the parallel algorithm version on A100 is almost identical to the OpenACC version on A100.</p>



<div>
<figure><a href="https://developer.nvidia.com/blog/wp-content/uploads/2020/06/a100_lulesh_stdpar_benchmark.pdf" data-wpel-link="internal"><img src="https://developer.nvidia.com/blog/wp-content/uploads/2020/06/a100_lulesh_stdpar_benchmark-pdf.jpg" alt="" width="800" height="600"></a>
<figcaption><em>Figure 1.</em> <em>Relative performance of different implementations of LULESH. The left column is the Standard C++ Parallel Algorithms version run on all the CPU cores of a 40-core dual-socket Skylake system. The other two columns were run on a single A100 GPU. The middle column is the Standard C++ Parallel Algorithms version, the same code as the left column. The right column is the OpenACC version of LULESH.</em></figcaption>
</figure>
</div>



<h2>Getting started with C++ Parallel Algorithms for GPUs</h2>



<p>To get started, download and install the <a href="https://developer.nvidia.com/hpc-sdk" data-wpel-link="external" target="_blank" rel="nofollow external noopener noreferrer">NVIDIA HPC SDK</a> on your x86-64, OpenPOWER, or Arm CPU-based system running a supported version of Linux.</p>



<p>The NVIDIA HPC SDK is freely downloadable and includes a perpetual use license for all NVIDIA Registered Developers, including access to future release updates as they are issued. After you have the NVIDIA HPC SDK installed on your system, the NVC++ compiler is available under the /opt/nvidia/hpc_sdk directory structure.</p>



<ul><li>To use the&nbsp;compilers, including NVC++ on a Linux/x86-64 system, add the directory <code>/opt/nvidia/hpc_sdk/Linux_x86_64/20.5/compilers/bin</code> to your path.</li><li>On an OpenPOWER or Arm CPU-based system, replace <code>Linux_x86_64</code> with <code>Linux_ppc64le</code> or <code>Linux_aarch64</code>, respectively.</li></ul>



<h3>Supported NVIDIA GPUs</h3>



<p>The NVC++ compiler can automatically offload C++ Parallel Algorithms to NVIDIA GPUs based on the Volta, Turing, or Ampere architectures. These architectures include features—such as independent thread scheduling and hardware optimizations for CUDA Unified Memory—that were specifically designed to support high-performance, general-purpose parallel programming models like the C++ Parallel Algorithms.</p>



<p>The NVC++ compiler provides limited support for C++ Parallel Algorithms on the Pascal architecture, which does not have the <a href="https://developer.nvidia.com/blog/inside-volta/" data-wpel-link="internal">independent thread scheduling</a> necessary to properly support the <code>std::execution::par</code> policy. When compiling for the Pascal architecture (<code>-gpu=cc60</code>), NVC++ compiles algorithms with the <code>std::execution::par</code> policy for the CPU. Only algorithms with the <code>std::execution::par_unseq</code> policy can run on Pascal GPUs.</p>



<p>By default, NVC++ auto-detects and generates GPU code for the type of GPU that is installed on the system on which the compiler is running. To generate code for a specific GPU architecture, which may be necessary when the application is compiled and run on different systems, add the <code>-gpu=cc<em>XX</em></code> command-line option. Currently, the compiler can generate executables targeted for only one GPU architecture. The use of multiple <code>-gpu=cc<em>XX</em></code> options in a single compilation results in an error from the compiler.</p>



<h3>Supported CUDA versions</h3>



<p>The NVC++ compiler is built on CUDA libraries and technologies and uses CUDA to accelerate&nbsp; C++ Parallel Algorithms to NVIDIA GPUs. A GPU-accelerated system on which NVC++-compiled applications are to be run must have a CUDA 10.1 or newer device driver installed.</p>



<p>The NVIDIA HPC SDK compilers ship with an integrated CUDA toolchain, header files, and libraries to use during compilation, so it is not necessary to have the CUDA Toolkit installed on the system.</p>



<p>When <code>-stdpar</code> is specified, NVC++ compiles using the CUDA toolchain version that matches the CUDA driver installed on the system on which compilation is performed. To compile using a different version of the CUDA toolchain, use the <code>-gpu=cuda<em>X.Y</em></code> option. For example, use the <code>-gpu=cuda11.0</code> option to specify that your program should be compiled for a CUDA 11.0 system using the 11.0 toolchain.</p>



<h2>Productivity, performance, and portability</h2>



<p>The recently announced NVC++ compiler included in the NVIDIA HPC SDK enables you, for the first time, to program NVIDIA GPUs using completely standard and fully portable C++ constructs. C++ Parallel Algorithm invocations instrumented with appropriate execution policies are automatically parallelized and offloaded to NVIDIA GPUs. The resulting programming environment increases GPU programmer productivity, provides an easy on-ramp to GPU computing for new users, and delivers performance portability across a wide variety of Standard C++ implementations.</p>

                    

                    
                </div>
            </article>
        
    </div>
</div></div></div>
    </div>
    <footer>
        <div>created by <a href="https://buildstarted.com">buildstarted</a> &copy; 2020 <a href="/about">about</a></div>
        <div>Share this page on social media: copy and paste this url https://linksfor.dev/</div>
        <div>If you prefer RSS: <a href="https://linksfor.dev/feed.xml">https://linksfor.dev/feed.xml</a></div>
        <div>Customer satisfaction guaranteed to be optional.</div>
    </footer>
    
    <script async defer>
        _dna = window._dna || {};
        _dna.siteId = "linksfor.devs";
        _dna.outlink = true;

        (function() {
            let dna = document.createElement('script');
            dna.type = 'text/javascript';
            dna.async = true;
            dna.src = '//dna.buildstarted.com/t.js';
            let s = document.getElementsByTagName('script')[0];
            s.parentNode.insertBefore(dna, s);
        })();
    </script>
    <noscript><img src="//dna.buildstarted.com/g?siteId=linksfor.devs"/></noscript>
</body>
</html>