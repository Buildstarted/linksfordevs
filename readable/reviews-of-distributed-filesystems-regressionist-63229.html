<!DOCTYPE html>
<html lang="en">
<head>
    <title>
Reviews of distributed filesystems | Regressionist - linksfor.dev(s)    </title>
    <meta charset="utf-8">
    <link rel="alternate" type="application/rss+xml" title="Linksfor.dev(s) feed" href="https://linksfor.dev/feed.rss" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta name="google" value="notranslate">
    <link rel="stylesheet" type="text/css" href="/style.css" />
    <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon">
    <link rel="icon" href="/favicon.ico" type="image/x-icon">
    <meta property="og:title" content="Reviews of distributed filesystems | Regressionist - linksfor.dev(s)"/>
    <meta property="article:author" content="by regadmin"/>
    <meta property="og:description" content="I have a lot of data to work with, and I want to do it with just my mismatched bunch of servers, desktops, SSDs, and spinning disks. My equipment is old, so I want a filesystem that is robust not only to the failure of any drive, but also to the failure of any one machine. My preference is to build a hyper-converged system, where each machine hosts data in addition to working on compute jobs. Following are reviews I found on the main open-source distributed filesystems out there:"/>
    <meta property="og:type" content="website"/>
    <meta property="og:url" content="http://www.regressionist.com/2020/06/20/reviews-of-distributed-filesystems/"/>

<meta property="og:site_name" content="linksfor.dev(s)" />
</head>
<body>
    <div class="devring" style="background: #222">
        <div style="text-align:center">Explore other dev related sites in this ring. If you would like to join this ring <a href="https://devring.club">click here</a>.</div>
        <div class="grid">
            <div style="display: grid; grid-template-columns: .5fr 1fr 1fr 1fr; text-align: center;">
                <span class="devring-title"><a href="https://devring.club/">devring.club</a></span>
                <a href="https://devring.club/sites/1/prev" class="devring-previous">Previous</a>
                <a href="https://devring.club/random" class="devring-random">Random</a>
                <a href="https://devring.club/sites/1/next" class="devring-next">Next</a>
            </div>
        </div>
    </div>
    <div class="grid">
        <h1>
<a href="/" style="color:inherit">linksfor.dev(s)</a>
        </h1>
        <title>linksfor.dev(s) - Reviews of distributed filesystems | Regressionist</title>
<div class="readable">
        <h1>Reviews of distributed filesystems | Regressionist</h1>
            <div>by by regadmin</div>
            <div>Reading time: 18-23 minutes</div>
        <div>Posted here: 22 Jun 2020</div>
        <p><a href="http://www.regressionist.com/2020/06/20/reviews-of-distributed-filesystems/">http://www.regressionist.com/2020/06/20/reviews-of-distributed-filesystems/</a></p>
        <hr/>
<div id="readability-page-1" class="page"><div>
			
<figure><img src="http://www.regressionist.com/wp-content/uploads/2020/06/filesystems-1.png" alt="" srcset="http://www.regressionist.com/wp-content/uploads/2020/06/filesystems-1.png 1010w, http://www.regressionist.com/wp-content/uploads/2020/06/filesystems-1-300x213.png 300w, http://www.regressionist.com/wp-content/uploads/2020/06/filesystems-1-768x546.png 768w, http://www.regressionist.com/wp-content/uploads/2020/06/filesystems-1-624x444.png 624w" sizes="(max-width: 1010px) 100vw, 1010px"></figure>



<p>I have a lot of data to work with, and I want to do it with just my mismatched bunch of servers, desktops, SSDs, and spinning disks. My equipment is old, so I want a filesystem that is robust not only to the failure of any drive, but also to the failure of any one machine. My preference is to build a <a href="https://www.computerweekly.com/feature/Hyper-converged-infrastructure-vs-NAS-SAN-shared-storage">hyper-converged</a> system, where each machine hosts data in addition to working on compute jobs. Following are reviews I found on the main open-source distributed filesystems out there:</p>



<ul><li><a href="#Gluster">Gluster</a></li><li><a href="#Lustre">Lustre</a></li><li><a href="#BeeGFS">BeeGFS/FhGFS</a></li><li><a href="#OrangeFS">OrangeFS</a></li><li><a href="#LizardFS">LizardFS/MooseFS</a></li><li><a href="#Ceph">Ceph/CephFS</a></li><li><a href="#Tahoe">Tahoe-LAFS</a></li></ul>



<h2 id="Gluster">Gluster</h2>



<blockquote><p><em>Avoid GlusterFS.</em></p><cite><a href="https://www.jdieter.net/posts/2012/03/31/glusterfs-madness/">Jonathan Dieter</a></cite></blockquote>



<blockquote><p>Not gluster, it requires low latency and is difficult to autoscale. Thus, true high availability is difficult to achieve.</p><cite><a href="https://www.reddit.com/r/sysadmin/comments/5uulqm/best_distributed_file_system_glusterfs_vs_ceph_vs/ddwyszz/">hi11111</a></cite></blockquote>



<blockquote><p>as someone who watched a product launch fail, probably 90% because of glusters inability to scale in size without massive upgrades to the hardware, stay away</p><cite><a href="https://www.reddit.com/r/sysadmin/comments/5uulqm/best_distributed_file_system_glusterfs_vs_ceph_vs/ddx98fl/">netburnr2</a></cite></blockquote>



<blockquote><p>GlusterFS is latency dependent. Since self-heal checks are done when establishing the FD and the client connects to all the servers in the volume simultaneously, high latency (mult-zone) replication is not normally advisable. Each lookup will query both sides of the replica. A simple directory listing of 100 files across a 200ms connection would require 400 round trips totaling 80 seconds. A single drupal page could take around 20 minutes.</p><cite><small><a href="https://joejulian.name/post/glusterfs-replication-dos-and-donts/">Joe Julian</a></small></cite></blockquote>



<blockquote><div><p>I only have experience running Lustre and Gluster. Gluster was a mess, to be honest, though that was several years ago. Lustre (and we run Intel Enterprise Lustre) has been pretty solid. Most HPC outfits run lustre over ZFS, actually, so you get the benefits of both.</p><p>Ceph and Gluster can’t achieve the kind of performance required for HPC scratch. Hadoop can, but if you are managing a general purpose HPC cluster, nearly all software out there is expecting to write to a posix filesystem.</p></div><cite><a href="https://news.ycombinator.com/item?id=14168100">zhynn</a></cite></blockquote>



<blockquote><div><p>People may use Gluster for HPC; I don’t know. But it really isn’t suited.</p><p>I run two Gluster clusters for more boring enterprisey reliability.</p><p>It is still kind of a mess – there are rough edges all over. Log messages make zero sense by themselves – you either need to learn what they’re indicative of or read the code. Frequently, typoing configuration commands (configuration is mostly imperative command line driven, not via configuration files) so that it fails, actually does make changes, such that you’ll have to unwind what it did before you can try again, etc.</p><p>All that said, Gluster is pretty solid for our sue – distributed, fault-tolerant POSIX storage. But I wouldn’t use it in an HPC environment.</p></div><cite><a href="https://news.ycombinator.com/vote?id=14169763&amp;how=up&amp;auth=f279f2662b87aef7bfb8eda7122c23f8ed1b4d80&amp;goto=item%3Fid%3D14169763#14169763"></a><br><a href="https://news.ycombinator.com/item?id=14169763">__jal</a></cite></blockquote>



<blockquote><div><p>currently i would give MooseFS, orangefs or lizardfs a try.</p><p>Had experience with ceph and glusterfs and both I can not recommend.</p></div><cite><a href="https://news.ycombinator.com/item?id=11166191">poelzi</a></cite></blockquote>



<blockquote><p>I tried GlusterFS a few years ago and it took a good while to figure out the right setup but in the end had disappointing small file performance.</p><cite><a href="https://news.ycombinator.com/vote?id=11166255&amp;how=up&amp;auth=e092c1cbbe3ed659ca6f8d4275c2d499753fe549&amp;goto=item%3Fid%3D11166255#11166255"></a><br><a href="https://news.ycombinator.com/item?id=11166255">eis</a></cite></blockquote>



<blockquote><p>I also tried ceph and gluster before settling on moosefs a couple years ago — gluster was slow for filesystem operations on a lot of files and it would get into a state where some files weren’t replicated properly with seemingly no problems with the network for physical servers.</p><cite><a href="https://news.ycombinator.com/item?id=11166797">mattbillenstein</a></cite></blockquote>



<blockquote><p>Actually GlusterFS is close to my needs, however the “easy to install” and actually bad performance on many small files is a road blocker for us. However it runs well, even on just 3 servers.</p><cite><a href="https://news.ycombinator.com/item?id=11167890">merb</a></cite></blockquote>



<blockquote><p>GlusterFS seems good on paper; but is really inherently unstable and isn’t designed for what you are intending to do. Also high availability has too many parts to really care it highly available</p><cite><a href="https://www.reddit.com/r/sysadmin/comments/5uulqm/best_distributed_file_system_glusterfs_vs_ceph_vs/ddzfc6g/">joe8mofo</a></cite></blockquote>



<blockquote><p>Can’t use random drives. That’s why I didn’t run GlusterFS originally. You need to add drives in bunches, and that’s just not going to be practical. I’ve got literally dozens of drives ranging from 450gb to 8tb, with every size in between.</p><cite><a href="https://www.reddit.com/r/DataHoarder/comments/ayrn7x/migrating_from_lizardfs_considering_options/ei30si8/">wintersdark</a></cite></blockquote>



<h2 id="Lustre">Lustre</h2>



<p>Lustre began at CMU and appears to have been owned in some sense by Cluster File Systems, Sun Microsystems, Oracle, Xyratex, Seagate, Whamcloud, Intel, and DataDirect Networks. Lustre is complex. I decided it wasn’t for me after watching some <a href="https://lustre.ornl.gov/lustre101-courses/">Lustre 101</a> course videos.</p>



<blockquote><p>While companies like DDN can make Lustre easier to install and maintain, it is by its very nature a difficult beast to tame</p><cite><a href="https://www.nextplatform.com/2017/04/20/intel-shuts-lustre-file-system-business/">Timothy Prickett Morgan</a></cite></blockquote>



<blockquote><div><p>Lustre is a pure-bread racehorse. It is not something the general masses should ever take a ride on; or even get up close and personal with. It will bite you; kick you; piss on you; crap on you; stomp on you; and generally try to kill you. It is cranky. It is not user friendly. It doesn’t play nice with others.</p><p>But if you do know how to ride it; you have track to ride it on; and it decides to let you ride; you are in for a real treat. It is a rush that can’t be beat. Even though after that ride your body will ache from the beating it’s been given.</p><p>In the right workloads Lustre can’t be beat. I’ve seen it completely transform large HPC clusters from dogs to kentucky derby winners. But as mentioned by many it is a complex beast that needs the right people; right hardware; and right workloads to make it run.</p><p>We used it for many years for large geospatial big data workloads. Huge images to many 1000’s of HPC nodes. Worked great. But it was highly sensitive to the technical staff running it; and ultimately it became too “cranky” to use in production mode without Gandalf and his wizard army to keep it from eating itself.</p><p>So we now run on NFS over IB and happily give up some performance for 24*365 uptime.</p></div><cite><a href="https://news.ycombinator.com/item?id=14168321">sanguy</a></cite></blockquote>



<blockquote><p>In my experience, Lustre on a typical medium-size (several hundred node) cluster has been more reliable than NFS (at least once the cluster vendor was out of the way), and you can actually use it for MPI-IO, which is crucial. It’s been quite straightforward to operate, at least with a simple LNET setup. I don’t understand this stuff about racehorses. NFS has been easily flattened in various circumstances, too.</p><cite><a href="https://news.ycombinator.com/item?id=14174380">gnufx</a></cite></blockquote>



<blockquote><p>Actually Lustre gives good “out of the box” performance… I’ve used a few HPC clusters and have almost never felt the need to issue commands to configure striping, etc.</p><cite><a href="https://news.ycombinator.com/item?id=14169345">frozenport</a></cite></blockquote>



<blockquote><div><p>Lustre is for large HPC clusters. It’s not for your desktop; it’s not for your video editing suite. It is the only way to provide a filesystem that scales to support, for instance, a compute job of 100k ranks, all writing checkpoints and snapshots periodically. No, Hadoop doesn’t do it. Ceph is for performance-and-scaling-insensitive cloud installs.</p><p>Lustre is for when you expect to saturage 100 100Gb IB links to storage. it works remarkably well for its use case (though even on HPC clusters, MDS performance can be a problem).</p></div><cite><a href="https://news.ycombinator.com/item?id=14167201">markhahn</a></cite></blockquote>



<blockquote><p>Well, NFS isn’t really cache coherent, and hence not POSIX compliant. Lustre is, but pays for it with an amazingly complicated architecture.</p><cite><a href="https://news.ycombinator.com/item?id=14169273">jabl</a></cite></blockquote>



<blockquote><p>The only serious open-source competitors to Lustre are Ceph and glusterfs. But Ceph is too unstable, and glusterfs 3.0 is based off of distributed hash tables and so is not strongly consistent.</p><cite><a href="https://news.ycombinator.com/item?id=14170167">reality_czech</a></cite></blockquote>



<blockquote><div><p>We used Lustre, pNFS and GPFS at Stanford on HPC gear (DDN, Panasas and some enterprise COTS). Luster has a lot of moving parts and config. Most folks tend to use Puppet/Chef and/or Rocks distro to deploy clusters in a somewhat sane manner. (Sometimes AFS too but not much.)</p><p>These days, Ceph/Gluster might work, but Lustre’s proven.</p></div><cite><a href="https://news.ycombinator.com/item?id=14170383">anon263626</a></cite></blockquote>



<blockquote><div><p>Lustre is a monstrosity – badly designed, poorly implemented, very hard to configure and keep running or even get adequate performance under other than a single limited use case.</p><p>Good riddance to bad rubbish.</p></div><cite><a href="https://news.ycombinator.com/item?id=14166622">pinewurst</a></cite></blockquote>



<blockquote><p>Lustre was definitely a beast (in a good sense), but we’d occasionally get bit by a work load high in metadata operations – which would bring Lustre to its knees due to latency.</p><cite><a href="https://news.ycombinator.com/item?id=14168566">lo_stronzo</a></cite></blockquote>



<blockquote><p>right, so they took a vertical that no one else wanted and served it kind of badly. I don’t know about the last few years, but Lustre used to be famous for eating up a huge amount of time in the field, even given that its only supposed to be relevant for really large files.</p><cite><a href="https://news.ycombinator.com/vote?id=14167207&amp;how=up&amp;auth=5b35f0e0965753e7647153da4b45c8fae6651775&amp;goto=item%3Fid%3D14167207#14167207"></a><br><a href="https://news.ycombinator.com/item?id=14167207">convolvatron</a></cite></blockquote>



<h2 id="BeeGFS">BeeGFS/FhGFS</h2>



<p>BeeGFS can use infiniband. It doesn’t support erasure coding. It uses a “buddy mirroring” system for redundancy. It can have multiple metadata servers, which removes that as a bottleneck.</p>



<blockquote><p>Actually Lustre isn’t the only game in town for this. BeeGFS (<a href="http://beegfs.com/">http://beegfs.com</a>) does a very good job at this as well, has better small to large scaling, understandable by mere-mortal error messages, doesn’t require a specific (ancient) kernel or distro for the server …</p><cite><a href="https://news.ycombinator.com/item?id=14168244">hpcjoe</a></cite></blockquote>



<blockquote><p>We used BeeGFS since it was called FhGFS on a cluster of 32 nodes for a total capacity ranging from 256tb-1Pb. BeeGFS is very easy to configure and maintain, outperforming Lustre for every task we threw it at. In fact, we never transitioned lustre into production. Lustre was often slower than plain NFS for the cases where all storage could be mounted by a single node(!).</p><cite><a href="https://news.ycombinator.com/item?id=14167165">usernam</a></cite></blockquote>



<blockquote><div><p>BeeGFS is great. I originally evaluated it when it was FhGFS and am still involved with a large production BeeGFS cluster deployment.</p><p>Any knucklehead can install it and it works transparently with excellent performance. It’s currently not enterprise-grade fault tolerant – that’s the worst thing I can say about it.</p></div><cite><a href="https://news.ycombinator.com/item?id=14167677">pinewurst</a></cite></blockquote>



<blockquote><div><p>BeeGFS v6 has metadata mirroring via buddy groups, and data mirroring the same way. A few things are still not there, but it is rapidly getting closer.</p><p>Being open source helps as well.</p><p>Definitely recommend it, and have deployed it and supported it many places in my previous life</p></div><cite><a href="https://news.ycombinator.com/item?id=14168322">hpcjoe</a></cite></blockquote>



<blockquote><div><p>MooseFS and BeeGFS don’t really seem to be in the same category of filesystem though.</p><p>BeeGFS comes from the HPC world where it is all about performance, while MooseFS seems more focused on high reliability even in the face of entire machines coming and going.</p><p>BeeGFS stores files striped over multiple machines and can use infiniband natively which gives us a system where bandwidth to individual files can reach a bit over 1GB/s (best case) and aggregated bandwidth can reach 30GB/s from a very mixed and unoptimized bioinformatics workload, a lot more when just testing the raw bandwidth.</p><p>Since BeeGFS uses an underlying filesystem on each storage target you can of course run raid, zfs or whatever it takes to make you comfortable that the individual storage targets aren’t going to be lost – which is what it takes for data to be unavailable.</p><p>If you want some extra reliability in BeeGFS it also supports mirroring so you only lose data if you fully lose two storage targets. We can’t really afford to run with full mirroring for our 3PB though.</p><p>We are very happy with it for our HPC environment but I’m not sure how well it works in an AWS setup.</p></div><cite><a href="https://news.ycombinator.com/vote?id=11168238&amp;how=up&amp;auth=37396cf963ad17bd279154c48ca99411d96efda1&amp;goto=item%3Fid%3D11168238#11168238"></a><br><a href="https://news.ycombinator.com/item?id=11168238">birc_a</a></cite></blockquote>



<blockquote><div><p>The company I work for has been running BeeGFS for 6 years with no issues. Write and read speeds on Dell storage hardware match or exceed the NetApp appliances we use.</p><p>We do occasionally have issues where ALL storage nodes must be rebooted but other than it works great.</p></div><cite><a href="https://news.ycombinator.com/item?id=11169367">shiftpgdn</a></cite></blockquote>



<blockquote><p>I will say that I know two people that have independently tried fairly large trials (400TB+) of Lustre, GlusterFS, and BeeGFS, and BeeGFS was their eventual favorite.</p><cite><a href="https://news.ycombinator.com/item?id=11168025">craigyk</a></cite></blockquote>



<blockquote><p>Parallel File Systems with exceptional metadata handling such as BeeGFS are perhaps the best option for TensorFlow on HPC.</p><cite><a href="https://blog.exxactcorp.com/examining-hpc-storage-performance-on-deep-learning-workloads-with-beegfs-parallel-file-system/">Exxact Corporation</a></cite></blockquote>



<blockquote><p>If good I/O performance is also important, then definitely also take a look at BeeGFS.</p><cite><a href="https://www.reddit.com/r/sysadmin/comments/5uulqm/best_distributed_file_system_glusterfs_vs_ceph_vs/ddxlozo/">netzvolk</a></cite></blockquote>



<h2 id="OrangeFS">OrangeFS</h2>



<blockquote><p>I also looked at <a href="http://www.orangefs.org/">OrangeFS</a> (originally PVFS2), but it doesn’t seem to provide replication, and, in preliminary testing, it was over ten times slower than the alternatives.</p><cite><a href="https://www.jdieter.net/posts/2018/07/21/small-file-performance-on-distributed-filesystems-round-2/">Jonathan Dieter</a></cite></blockquote>



<h2 id="LizardFS">LizardFS/MooseFS</h2>



<div><p>MooseFS development seemed to stagnate some years ago. LizardFS was a fork that got most of the new development. LizardFS has been the free version with all the features, and MooseFS has only offered those features (like erasure coding and hot backup metadata servers) to those who get the professional license. However, in recent years, MooseFS seems to be the one with all the updates and bugfixes.</p><p>In both of these, all the metadata is stored in RAM.</p></div>



<blockquote><div><p>So far I heard a lot about GlusterFS and Ceph being great but so far I only seen MooseFS in production.</p><p>MooseFS seems to do its job well, the only problem is that by default it lacks HA. The master server is very important since it keeps track of where all data is located, so if it is gone your data is gone. You can set up one or more metalogger nodes, which sole purpose is to backup the metadata information. You can also manually promote metalogger to master.</p><p>They have commercial offering that includes HA, but I never used so hard to comment about it.</p><p>You could also set up HA yourself using corosync and peacemaker, but it’s a bit challenging task.</p></div><cite><a href="https://news.ycombinator.com/item?id=14167152">takeda</a></cite></blockquote>



<blockquote><p>Moosefs is pragmatic — written in pretty tight C, performant, and the web UI for seeing the state of the cluster and how files are replicating is very nice. We have a small cluster with maybe a dozen nodes running for almost 2 years now with no hiccups…</p><cite><a href="https://news.ycombinator.com/item?id=11166797">mattbillenstein</a></cite></blockquote>



<blockquote><p>So, six months on, LizardFS has served us well, and will hopefully continue to serve us for the next (few? many?) years.</p><cite><a href="https://www.jdieter.net/posts/2016/09/30/from-nfs-to-lizardfs/">Jonathan Dieter</a></cite></blockquote>



<blockquote><p>MooseFS seems miles ahead of LizardFS today, waaay faster read/write (same hardware as LizardFS), and way better speeds for reading small files, which finally makes it possible to use as home folders in our studio.</p><cite><a href="https://github.com/lizardfs/lizardfs/issues/805">hradec</a></cite></blockquote>



<blockquote><div><p>I used moosefs, which the free version is now called lizardfs, in a POC with about 50tb and it performed VERY well and was simple to maintain.</p><p>The POC compared it to glusterfs and cephfs. Moosefs won the POC, but wasn’t selected because of the vendor support. For a small, non production workload, lizardfs would be my choice.</p></div><cite><a href="https://www.reddit.com/r/sysadmin/comments/5uulqm/best_distributed_file_system_glusterfs_vs_ceph_vs/ddxsd86/">r3dk0w</a></cite></blockquote>



<blockquote><p>LizardFS has got decent OSX support, using it right now. Installation is not THAT easy (used this <a rel="noreferrer noopener" href="https://github.com/lizardfs/lizardfs/blob/master/create-osx-package.sh" target="_blank">https://github.com/lizardfs/lizardfs/blob/master/create-osx-package.sh</a>). There should possibly be some guide or something on their site. Anyway, it just rocks on OSX, ubuntus and some windows we have (via NFS, native client is closed source). Constantly expanding, nice tiering (hot data on SSD, colder on HDD). It is about twice as fast as Gluster in our tests.</p><cite><a href="https://www.reddit.com/r/sysadmin/comments/5uulqm/best_distributed_file_system_glusterfs_vs_ceph_vs/de45v8h/">affidavitdonda</a></cite></blockquote>



<blockquote><div><p>After working with Ceph for 11 months I came to conclusion that it utterly sucks so I suggest to avoid it. I tried XtreemFS, RozoFS and QuantcastFS but found them not good enough either.</p><p>I wholeheartedly recommend LizardFS which is a fork of now proprietary MooseFS. LizardFS features data integrity, monitoring and superior performance with very few dependencies.</p><p>2019 update: situation has changed and LizardFS is not actively maintained any more.</p><p>MooseFS is stronger than ever and free from most LizardFS bugs. MooseFS is well maintained and it is faster than LizardFS.</p></div><cite><a href="https://stackoverflow.com/questions/17425153/distributed-file-systems-gridfs-vs-glusterfs-vs-ceph-vs-hekafs-benchmarks/27612096#27612096">Onlyjob</a></cite></blockquote>



<h2 id="Ceph">Ceph/CephFS</h2>



<blockquote><p>After CephFS’s amazing performance in the single-client mode, I was looking forward to some incredible results, but it really didn’t scale as well as I had hoped, though it was still competitive with the other distributed filesystems. Once again, LizardFS has shown that when it comes to metadata operations, it’s really hard to beat, but its aggregate read and write performance were disappointing. And, once again, GlusterFS really struggled with the test. I wish it would have worked with the performance tuning for small files enabled, as we might have seen better results.</p><cite><a href="https://www.jdieter.net/posts/2018/07/21/small-file-performance-on-distributed-filesystems-round-2/">Jonathan Dieter</a></cite></blockquote>



<blockquote><div><p>Take, for example, my favorite storage system Ceph. As I understand it was originally going to be CephFS, with multiple metadata servers and lots of distributed POSIX goodness. However, in the 10+ years its been in development, the parts that have gotten tons of traction and have widespread use are seemingly one-off side projects from the underlying storage system: object storage and the RBD block device interfaces. Only in the past 12 months is CephFS becoming production ready. But only with a single metadata server, and the multiple metadata servers are still being debugged.</p><p>With Ceph, some of these timing issues are that the market for object store and network-based block devices are dwarf the market for distributed POSIX. But I bring it up to point out that distributed POSIX is also just a really really hard problem, with limited use cases. It’s super convenient for getting an existing Unix executable to run on lots of machines at once. But that convenience may not be worth the challenges it imposes on the infrastructure.</p></div><cite><a href="https://news.ycombinator.com/item?id=14167054">epistasis</a></cite></blockquote>



<blockquote><p>CephFS simply didn’t gain as much traction because it made sense to just store objects in many cases, and let something else worry about what is stored and where. A massive distributed file system is not nearly as necessary as people make it out to be for a lot of different workloads.</p><cite><a href="https://news.ycombinator.com/item?id=14167130">X-Istence</a></cite></blockquote>



<blockquote><div><p>I’ve played around with Ceph, I really like its features and strong consistency.</p><p>But its weakness is that it is a complex beast to setup and maintain. If you think that configuring and maintaining a Hadoop cluster is hard, then Ceph is twice as hard. Ceph has really good documentation, but all the knobs you have to read and play around with is still too much.</p></div><cite><a href="https://news.ycombinator.com/item?id=14167273">olavgg</a></cite></blockquote>



<blockquote><p>I’ll echo that, we had a project that at one time, even with a couple experienced Ceph admins, suffered meltdown after meltdown on Ceph until we just took the time to move the project and workload over to HDFS. Our admins learned to setup and administer HDFS from scratch and have had far fewer issues.</p><cite><a href="https://news.ycombinator.com/item?id=14169364">bane</a></cite></blockquote>



<blockquote><p>I’ve run a ceph deployment for about 18 months now, it’s good. ops-wise it’s like anything, a few quirks to learn – but it’s very solid. I played around with a couple of puppet modules and ended up on ceph-deploy as well, makes life quite easy.</p><cite><a href="https://news.ycombinator.com/item?id=11167397">angrygoat</a></cite></blockquote>



<blockquote><p>Doing some work with the Ceph code in the (G)iant timeframe, almost two years ago now, my assessment was that the code was under heavy revision and strong project oversight wasn’t obviously present. I wouldn’t have been in any hurry to use it in production. Today the project says the latest release(s) are of production grade.</p><cite><a href="https://www.reddit.com/r/sysadmin/comments/5uulqm/best_distributed_file_system_glusterfs_vs_ceph_vs/ddxu6lb/">pdp10</a></cite></blockquote>



<blockquote><p>Ceph is an object-based scale-out distributed storage platform with geo-replica capabilities. CEPH provides a filesystem interface and a block storage interface. It is complicated to setup, but works very well for a distributed object (Dropbox replacement)/filesystem. I would recommend Ceph for your use cases, but it will take some time to architect and setup properly</p><cite><a href="https://www.reddit.com/r/sysadmin/comments/5uulqm/best_distributed_file_system_glusterfs_vs_ceph_vs/ddzfc6g/">joe8mofo</a></cite></blockquote>



<blockquote><p>Probably not a drama for you but Ceph falls apart above 200 nodes. It’s a great tool but if you want to scale it big, be warned.</p><cite><a href="https://www.reddit.com/r/sysadmin/comments/5uulqm/best_distributed_file_system_glusterfs_vs_ceph_vs/ddxknzn/">dreadpiratewombat</a></cite></blockquote>



<blockquote><p>Tip: Ceph prefers uniform hardware across pools. If you are adding drives of dissimilar size, you can adjust their weights. However, for best performance, consider a CRUSH hierarchy with drives of the same type/size.</p><cite><a href="https://docs.ceph.com/docs/master/rados/operations/add-or-rm-osds/">ceph.com</a></cite></blockquote>



<h2 id="Tahoe">Tahoe-LAFS</h2>



<p>Erasure coding, focus on security with encrypted data</p>



<blockquote><p>I use Tahoe-LAFS for all of my distributed FS stuff, and I really do love it. One minor downside is that the introducer server is a SPOF, but they can be backed up/spun up and distributed introducers are on the roadmap for the next year.</p><cite><a href="https://news.ycombinator.com/vote?id=14167606&amp;how=up&amp;auth=4c35010aa2b7748d83df7aaea54662ed83d8e877&amp;goto=item%3Fid%3D14167606#14167606"></a><br><a href="https://news.ycombinator.com/item?id=14167606">thraway2016</a></cite></blockquote>



<blockquote><p>Almost to my bewilderment I learned the Tahoe Lafs distributed filesystem is written in Python.</p><cite><a href="https://www.reddit.com/r/rust/comments/f8oweg/tahoe_lafs/">commandline_be</a></cite></blockquote>



<h2>RozoFS</h2>



<p>All I can find are press releases.</p>



<h2>Additional info</h2>



<p>Marlan Marinov gave a presentation entitled, “<a href="https://www.slideshare.net/azilian/comparison-of-foss-distributed-storage">Comparison of FOSS distributed filesystems</a>.” His slides were very useful to read, although he says that BeeGFS uses FUSE, which I don’t believe is correct.</p>



<figure><img src="http://www.regressionist.com/wp-content/uploads/2020/06/filesystem_complexity.png" alt="" srcset="http://www.regressionist.com/wp-content/uploads/2020/06/filesystem_complexity.png 625w, http://www.regressionist.com/wp-content/uploads/2020/06/filesystem_complexity-300x162.png 300w" sizes="(max-width: 625px) 100vw, 625px"></figure>
					</div></div></div>
    </div>
    <footer>
        <div>created by <a href="https://buildstarted.com">buildstarted</a> &copy; 2020 <a href="/about">about</a></div>
        <div>Share this page on social media: copy and paste this url https://linksfor.dev/</div>
        <div>If you prefer RSS: <a href="https://linksfor.dev/feed.xml">https://linksfor.dev/feed.xml</a></div>
        <div>Customer satisfaction guaranteed to be optional.</div>
    </footer>
    
    <script async defer>
        _dna = window._dna || {};
        _dna.siteId = "linksfor.devs";
        _dna.outlink = true;

        (function() {
            let dna = document.createElement('script');
            dna.type = 'text/javascript';
            dna.async = true;
            dna.src = '//dna.buildstarted.com/t.js';
            let s = document.getElementsByTagName('script')[0];
            s.parentNode.insertBefore(dna, s);
        })();
    </script>
    <noscript><img src="//dna.buildstarted.com/g?siteId=linksfor.devs"/></noscript>
</body>
</html>