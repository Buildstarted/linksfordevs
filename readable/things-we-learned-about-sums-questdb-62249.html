<!DOCTYPE html>
<html lang="en">
<head>
    <title>
Things we learned about sums &#xB7; QuestDB - linksfor.dev(s)    </title>
    <meta charset="utf-8">
    <link rel="alternate" type="application/rss+xml" title="Linksfor.dev(s) feed" href="https://linksfor.dev/feed.rss" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta name="google" value="notranslate">
    <link rel="stylesheet" type="text/css" href="/style.css" />
    <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon">
    <link rel="icon" href="/favicon.ico" type="image/x-icon">
    <meta property="og:title" content="Things we learned about sums &#xB7; QuestDB - linksfor.dev(s)"/>
    <meta property="og:description" content="In the world of databases, benchmarking performance has always been the hottest"/>
    <meta property="og:type" content="website"/>
    <meta property="og:url" content="https://questdb.io/blog/2020/05/12/interesting-things-we-learned-about-sums"/>

<meta property="og:site_name" content="linksfor.dev(s)" />
</head>
<body>
    <div class="grid">
        <h1>
<a href="/" style="color:inherit">linksfor.dev(s)</a>
        </h1>
        <title>linksfor.dev(s) - Things we learned about sums &#xB7; QuestDB</title>
<div class="readable">
        <h1>Things we learned about sums &#xB7; QuestDB</h1>
            <div>Reading time: 9-12 minutes</div>
        <div>Posted here: 29 May 2020</div>
        <p><a href="https://questdb.io/blog/2020/05/12/interesting-things-we-learned-about-sums">https://questdb.io/blog/2020/05/12/interesting-things-we-learned-about-sums</a></p>
        <hr/>
<div id="readability-page-1" class="page"><div><div><p><span><p><img src="https://questdb.io/blog/assets/road-runner.png" alt="alt-text"></p>
<p>In the world of databases, benchmarking performance has always been the hottest
topic. Who is faster for data ingestion and queries? About a month ago we
announced a new release with SIMD aggregations on
<a href="https://news.ycombinator.com/item?id=22803504">HackerNews</a> and
<a href="https://www.reddit.com/r/programming/comments/fwlk0k/questdb_using_simd_to_aggregate_billions_of/">Reddit</a>.
Fast. But were those results numerically accurate?</p>
<p>Speed is not everything. Some of the feedback we have received pointed us toward
the accuracy of our results. This is something typically overlooked in the
space, but our sums turned out to be "naive", with small errors for large
computations. By compounding a very small error over and over through a set of
operations, it can eventually become significant enough for people to start
worrying about it.</p>
<p>We then went on to include an accurate summation algorithm (such as "Kahan" and
"Neumaier" compensated sums). Now that we're doing the sums accurately, we
wanted to see how it affected performance. There is typically a trade-off
between speed and accuracy. However, by extracting even more performance out of
QuestDB (see below for how we did it), we managed to compute accurate sums as
fast as naive ones! Since comparisons to Clickhouse have been our most frequent
question, we have run the numbers and <a href="#comparison-with-clickhouse">here</a> is
what we got, 2x faster for summing 1bn doubles will nulls.</p>
<p>All of this is included in our new
<a href="https://github.com/questdb/questdb/releases/tag/4.2.1">release 4.2.1</a></p>
<p>You can find our repository on <a href="https://github.com/questdb/questdb">GitHub</a>. All
your
<a href="https://github.com/questdb/questdb/issues?q=is%3Aissue+is%3Aopen+sort%3Aupdated-desc">issues</a>,
<a href="https://github.com/questdb/questdb/pulls?q=is%3Apr+is%3Aopen+sort%3Aupdated-desc">pull-requests</a>
and <a href="https://github.com/questdb/questdb">stars</a> are welcome
<span>🙂</span>.</p>
<h3>How did we get there? TL;DR</h3>
<p>We used prefetch and co-routines techniques to pull data from RAM to cache in
parallel with other CPU instructions. Our performance was previously limited by
memory bandwidth - using these techniques would address this and allow us to
compute accurate sums as fast as naive sums.</p>
<p>With the help of prefetch we implemented the fastest and most accurate summation
we have ever <a href="#comparison-with-clickhouse">tested</a> - 68ms over 1bn double values
with nulls (versus 139ms for Clickhouse). We believe this is a significant
advance in terms of performance for accurate summations, and will help
developers handling intensive computations with large datasets.</p>
<h3>Contents</h3>
<ul>
<li>An <a href="#inaccurate-summation">introductory example</a> of the problem with summing
doubles.</li>
<li>A <a href="#float-representation-and-truncation-accuracy-loss">quick glance</a> at
floating points inaccuracies.</li>
<li>A <a href="#kahans-algorithm-for-compensated-summation">presentation</a> of the Kahan
algorithm.</li>
<li>Our <a href="#implementation-with-simd-instructions">compensated sum implementation</a>
using SIMD instructions.</li>
<li>A <a href="#comparison-with-clickhouse">benchmark versus Clickhouse</a> for naive and
accurate summation methods.</li>
</ul>
<h3>Inaccurate summation?</h3>
<p>Before we dig in, some of you might wonder how an addition can be inaccurate as
opposed to simply right or wrong.</p>
<p>CPUs are poor at dealing with floating-point values. Arithmetics are almost
always wrong, with a worst-case error proportional to the number of operations
<code>n</code>. As floating-point operations are intransitive, the order in which you
perform them also has an impact on accuracy.</p>
<p>Here is an example:</p>
<pre><code><span><span>public</span> <span>static</span> <span>void</span> <span>main</span><span>(String[] args)</span> </span>{
    System.out.println(<span>5.1</span>+<span>9.2</span>);
}
</code></pre>
<p>We ask to add <code>5.1</code> to <code>9.2</code>. The result should be <code>14.3</code>, but we get the
following instead.</p>
<pre><code><span>14.299999999999999</span>
</code></pre>
<p>It is a small difference (only <code>0.000000000000001</code>), but it is still wrong. To
make matters worse, this error can be compounded.</p>
<pre><code><span><span>public</span> <span>static</span> <span>void</span> <span>main</span><span>(String[] args)</span> </span>{
    <span>double</span> a = <span>5.1</span>+<span>9.2</span>;
    <span>double</span> b = a + <span>3.5</span>;
    <span>double</span> c = <span>14.3</span> + <span>3.5</span>;
    System.out.println(<span>"The result is: "</span> + b);
    System.out.print(<span>"But we expected: "</span> + c);
}
</code></pre>
<pre><code><span>The result is:</span> <span>17.799999999999997</span>
<span>But we expected:</span> <span>17.8</span>
</code></pre>
<p>The error has just grown to <code>0.000000000000003</code> and will keep on growing as we
add operations.</p>
<h3>Float representation and truncation accuracy loss</h3>
<p>Decimal numbers are not accurately stored. This is well documented already, for
example on
<a href="https://stackoverflow.com/questions/588004/is-floating-point-math-broken/588014#588014">StackOverlow</a>
or <a href="https://0.30000000000000004.com/">here</a>.</p>
<p>Consequently, operations on floating points will return inaccurate results. This
is not the only problem. Performing operations is also likely to introduce more
errors and to grow the total error over time. One such case is once the result
of an operation has to be truncated to fit the original format. Here is a
simplified example of the <strong>truncation</strong> that happens when adding floats of
different orders of magnitude.</p>
<blockquote>
<p>For the below example we will be using base 10 and expressing the exponent as
a number rather than a binary for sake of simplicity. We assume 5 significant
digits.</p>
</blockquote>
<p>We start with both our numbers expressed in scientific notation.</p>
<p><img src="https://questdb.io/blog/assets/sum-1.png" alt="alt-text"></p>
<p>Let's expand into decimal notation and place them on a similar scale so all
digits fit.</p>
<p><img src="https://questdb.io/blog/assets/sum-2.png" alt="alt-text"></p>
<p>Now, let us express this sum back as one number in scientific notation. We have
to <code>truncate</code> the result back to 5 significant digits.</p>
<p><img src="https://questdb.io/blog/assets/sum-3.png" alt="alt-text"></p>
<p>The result is incorrect. In fact, it is as if we did not sum anything.</p>
<h3>Kahan's algorithm for compensated summation</h3>
<p>Compensated sum maintains a sum of accumulated errors and uses it to attempt to
correct the (inaccurate) sum by the total error amount. It does so by trying to
adjust each new number by the total accumulated error.</p>
<p>The main Compensated summation algorithm is the
<a href="https://en.wikipedia.org/wiki/Kahan_summation_algorithm" target="_blank">Kahan</a>
sum. It runs in 4 steps:</p>
<ul>
<li>Subtract the <code>running error</code> from the new <code>number</code> to get the
<code>adjusted number</code>. If this is the first number, then the running error is 0.</li>
<li>Add the <code>adjusted number</code> to the <code>running total</code> and truncate to the number of
significant digits. This is the <code>truncated result</code>.</li>
<li>Calculate the <code>new running error</code> as
<code>(truncated result - running total) - adjusted number</code>.</li>
<li>Assign the <code>truncated result</code> as the new <code>running total</code>.</li>
</ul>
<p>This works because of addition transitivity rules.</p>
<h3>Implementation with SIMD instructions</h3>
<p>Now, the interesting bit! QuestDB implements the same 4-step algorithm as Kahan.
However, it uses vectorized instructions to make things a lot faster. The idea
came from Zach Bjornson who wrote about this on
<a href="http://blog.zachbjornson.com/2019/08/11/fast-float-summation.html" target="_blank">
his blog</a>.</p>
<p>Here is our implementation in details:</p>
<p>We first define our vectors:</p>
<pre><code>Vec8d inputVec;
<span>const</span> <span>int</span> step = <span>8</span>;
<span>const</span> auto *lim = d + count;
<span>const</span> auto remainder = (int32_t) (count - (count / step) * step);
<span>const</span> auto *lim_vec = lim - remainder;
Vec8d sumVec = <span>0</span>.;
Vec8d yVec;
Vec8d cVec = <span>0</span>.;
Vec8db bVec;
Vec8q nancount = <span>0</span>;
Vec8d tVec;
</code></pre>
<p>Then we load vectors with data. What's happening below is exactly Kahan's
algorithm. However, instead of summing individual values, we are summing vectors
of 8 values each.</p>
<pre><code><span>for</span> (; d &lt; lim_vec; d += step) {
    _mm_prefetch(d + <span>63</span> * step, _MM_HINT_T1);
    inputVec.load(d);
    bVec = is_nan(inputVec);
    nancount = if_add(bVec, nancount, <span>1</span>);
    yVec = select(bVec, <span>0</span>, inputVec - cVec);
    tVec = sumVec + yVec;
    cVec = (tVec - sumVec) - yVec;
    sumVec = tVec;
}
</code></pre>
<p>The strategically placed <code>prefetch</code> relies on CPU pipelining. The goal is to
have the CPU fetching the next chunk of data from RAM to cache while we are
calculating the current vector.</p>
<p>Lastly, we use <code>horizontal_add</code> to sum all values into a scalar value. Again, we
recognise Kahan's sum algorithm.</p>
<pre><code><span>double</span> sum = horizontal_add(sumVec);
<span>double</span> c = horizontal_add(cVec);
<span>int</span> nans = horizontal_add(nancount);
<span>for</span> (; d &lt; lim; d++) {
      <span>double</span> x = *d;
    <span>if</span> (x == x) {
        auto y = x - c;
        auto t = sum + y;
        c = (t - sum) -y;
        sum = t;
    } <span>else</span> {
        nans++;
    }
}
</code></pre>
<h3>Comparison with Clickhouse</h3>
<p>We compared how performance behaves when switching from naive (inaccurate) sum
to Kahan compensated sum.</p>
<h4>Hardware</h4>
<p>We run all databases on an <code>c5.metal</code> AWS instance, which has two Intel 8275CL
24-core CPUs and 192GB of memory. QuestDB was running on 16 threads. As we
showed in a
<a href="https://questdb.io/blog/2020/05/12/2020-04-02-using-simd-to-aggregate-billions-of-rows-per-second.md">previous article</a>,
adding more threads does not improve performance beyond a certain point.
Clickhouse was running using all cores as per default configuration, however we
increased the memory limit from the default value from 10GB to 40GB
<code>&lt;max_memory_usage&gt;40000000000&lt;/max_memory_usage&gt;</code>.</p>
<h4>Test data</h4>
<p>We generated two test files using our
<a href="https://questdb.io/docs/functionsRandomValueGenerators.md">random generation functions</a> and
exported the results to CSV. We then imported the CSV individually in the
databases.</p>
<pre><code><span>SELECT</span> <span>rnd_double</span>() <span>FROM</span> <span>long_sequence</span>(<span>1</span>_000_000_000l); <span>SELECT</span> <span>rnd_double</span>(<span>2</span>) <span>FROM</span> <span>long_sequence</span>(<span>1</span>_000_000_000l); </code></pre>
<h4>Storage engine</h4>
<ul>
<li><strong>QuestDB</strong>: on disk</li>
<li><strong>Clickhouse</strong>: in memory (using the <code>memory()</code> engine)</li>
</ul>
<h4>Commands</h4>
<h5>With null</h5>
<table>
<thead>
<tr><th>Description</th><th>QuestDB</th><th>Clickhouse</th></tr>
</thead>
<tbody>
<tr><td>DDL</td><td><code>CREATE TABLE test_double AS(SELECT rnd_double() FROM long_sequence(1000000000L);</code></td><td><code>CREATE TABLE test_double (val Nullable(Float64)) Engine=Memory;</code></td></tr>
<tr><td>Import</td><td>Not required</td><td><code>clickhouse-client --query="INSERT INTO test_double FORMAT CSVWithNames;" &lt; test_double.csv</code></td></tr>
<tr><td>Naive sum</td><td><code>SELECT sum(val) FROM test_double;</code></td><td><code>SELECT sum(val) FROM test_double;</code></td></tr>
<tr><td>Kahan sum</td><td><code>SELECT ksum(val) FROM test_double;</code></td><td><code>SELECT sumKahan(val) FROM test_double;</code></td></tr>
</tbody>
</table>
<h5>Non-null</h5>
<p>For non-null values, we adjusted the commands as follows</p>
<ul>
<li>use <code>test_double_not_nul.csv</code> instead of <code>test_double.csv</code>.</li>
<li>for Clickhouse, skip declaring val as <code>nullable</code>:
<code>CREATE TABLE test_double_not_null (val Float64) Engine=Memory;</code>.</li>
<li>for QuestDB, replace <code>rnd_double()</code> by <code>rnd_double(2)</code> at the DDL step.</li>
</ul>
<h4>Results</h4>
<p>We ran each query several times for both QuestDB and Clickhouse and kept the
best result.</p>
<p>Without null values, both databases sum naively at roughly the same speed. With
Kahan summation, QuestDB performs at the same speed while Clickhouse's
performance drops by ~40%.</p>
<p><img src="https://questdb.io/blog/assets/kahan-naive-not-null.png" alt="alt-text"></p>
<p>As we include null values, Clickhouse's performance degrades by 28% and 50% for
naive and Kahan summation, respectively.</p>
<p><img src="https://questdb.io/blog/assets/kahan-naive-null.png" alt="alt-text"></p>
<h3>Concluding remarks</h3>
<p>It is useful to stabilize aggregation with compensated sums. We learned that
vector-based calculation produce different arithmetic errors compared to
non-vector calculations. The way the aggregation is executed by multiple threads
is not constant. This can cause results to be different from one SQL run to
another, if the sum is accuracy naive. Through compensated sums, the results are
consistent and more accurate.</p>
<p>It was also both interesting and surprising to be able to quantify the effect of
prefetch on what is essentially sequential memory read.</p>
<p>Your support means a lot to us! If you like content like this, what we do, and where we're going, please <a href="https://serieux-saucisson-79115.herokuapp.com/">join our community</a> and give us a <a href="https://github.com/questdb/questdb">star️</a> on GitHub.</p>
</span></p></div></div></div></div>
    </div>
    <footer>
        <div>created by buildstarted &copy; 2020 <a href="/about">about</a></div>
        <div>Share this page on social media: copy and paste this url https://linksfor.dev/</div>
        <div>If you prefer RSS: <a href="https://linksfor.dev/feed.xml">https://linksfor.dev/feed.xml</a></div>
		<div>Customer satisfaction guaranteed to be optional.</div>
    </footer>
    
    <script async defer>
        _dna = window._dna || {};
        _dna.siteId = "linksfor.devs";
        _dna.outlink = true;

        (function () {
            let dna = document.createElement('script');
            dna.type = 'text/javascript';
            dna.async = true;
            dna.src = '//dna.buildstarted.com/t.js';
            let s = document.getElementsByTagName('script')[0];
            s.parentNode.insertBefore(dna, s);
        })();
    </script>
    <noscript><img src="//dna.buildstarted.com/g?siteId=linksfor.devs"/></noscript>
</body>
</html>