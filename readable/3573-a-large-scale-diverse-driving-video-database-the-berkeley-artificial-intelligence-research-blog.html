<!DOCTYPE html>
<html lang="en">
<head>
    <title>
A Large-scale Diverse Driving Video Database &#x2013; The Berkeley Artificial Intelligence Research Blog -
linksfor.dev(s)
    </title>
    <link rel="alternate" type="application/rss+xml" title="Linksfor.dev(s) feed" href="https://linksfor.dev/feed.rss" />
    <meta charset="utf-8">
    <meta name="Description" content="A curated source of links that devs might find interesting. Updated around the clock." />
    <meta name="google" value="notranslate">
    <link rel="stylesheet" type="text/css" href="/style.css" />
    <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon">
    <link rel="icon" href="/favicon.ico" type="image/x-icon">
</head>
<body>
    <div class="grid">
        
<div class="readable">
    <div id="readOverlay" class="style-ebook"><div id="readInner" class="margin-medium size-medium"><h1>A Large-scale Diverse Driving Video Database – The Berkeley Artificial Intelligence Research Blog</h1><div><div id="" class="post-content"><p><strong>Update 06/18/2018</strong>: please also check <a href="http://bair.berkeley.edu/blog/2018/06/18/bdd-update/">our follow-up blog post</a> after
reading this.</p><p>TL;DR, we released the largest and most diverse driving video dataset with rich
annotations called BDD100K. You can access the data for research now at <a href="http://bdd-data.berkeley.edu/">http://bdd-data.berkeley.edu</a>.  We  have
recently released <a href="https://arxiv.org/abs/1805.04687">an arXiv
report</a> on it. And there is still time to participate in <a href="http://bdd-data.berkeley.edu/wad-2018.html">our CVPR 2018 challenges</a>!</p><h2 id="large-scale-diverse-driving-video-pick-four">Large-scale, Diverse, Driving, Video: Pick Four</h2><p>Autonomous driving is poised to change the life in every community. However,
recent events show that it is not clear yet how a man-made perception system can
avoid even seemingly obvious mistakes when a driving system is deployed in the
real world. As computer vision researchers, we are interested in exploring the
frontiers of perception algorithms for self-driving to make it safer. To design
and test potential algorithms, we would like to make use of all the information
from the data collected by a real driving platform. Such data has four major
properties: it is large-scale, diverse, captured on the street, and with
temporal information. Data diversity is especially important to test the
robustness of perception algorithms. However, current open datasets can only
cover a subset of the properties described above. Therefore, with the help of <a href="https://www.getnexar.com/">Nexar</a>, we are releasing the BDD100K
database, which is the largest and most diverse open driving video dataset so
far for computer vision research. This project is organized and sponsored by <a href="https://deepdrive.berkeley.edu/">Berkeley DeepDrive</a> Industry
Consortium, which investigates state-of-the-art technologies in computer vision
and machine learning for automotive applications.</p><p><img width="750" src="http://bair.berkeley.edu/static/blog/bdd/geo_distribution.jpg"><br><i>
Locations of a random video subset.
</i></p><p>As suggested in the name, our dataset consists of 100,000 videos. Each video is
about 40 seconds long, 720p, and 30 fps. The videos also come with GPS/IMU
information recorded by cell-phones to show rough driving trajectories. Our
videos were collected from diverse locations in the United States, as shown in
the figure above. Our database covers different weather conditions, including
sunny, overcast, and rainy, as well as  different times of day including daytime
and nighttime. The table below summarizes comparisons with previous datasets,
which shows our dataset is much larger and more diverse.</p><p><img width="750" src="http://bair.berkeley.edu/static/blog/bdd/table0.png"><br><i>
Comparisons with some other street scene datasets. It is hard to fairly compare
# images between datasets, but we list them here as a rough reference.
# Sequences are lists as a reference for diversity, but different datasets have different sequence lengths.
</i></p><p>The videos and their trajectories can be useful for imitation learning of
driving policies, as in our <a href="https://arxiv.org/abs/1612.01079">CVPR 2017
paper</a>. To facilitate computer vision research on our large-scale dataset, we
also provide basic annotations on the video keyframes, as detailed in the next
section. You can download the data and annotations now at <a href="http://bdd-data.berkeley.edu">http://bdd-data.berkeley.edu</a>.</p><h2 id="annotations">Annotations</h2><p>We sample a keyframe at the 10th second from each video and provide annotations
for those keyframes. They are labeled at several levels: image tagging, road
object bounding boxes, drivable areas, lane markings, and full-frame instance
segmentation. These annotations will help us understand the diversity of the
data and object statistics in different types of scenes. We will discuss the
labeling process in a different blog post. More information about the
annotations can be found in our <a href="https://arxiv.org/abs/1805.04687">arXiv
report</a>.</p><p><img width="750" src="http://bair.berkeley.edu/static/blog/bdd/annotation_examples.png"><br><i>
Overview of our annotations.
</i></p><h3 id="road-object-detection">Road Object Detection</h3><p>We label object bounding boxes for objects that commonly appear on the road on
all of the 100,000 keyframes to understand the distribution of the objects and
their locations. The bar chart below shows the object counts. There are also
other ways to play with the statistics in our annotations. For example, we can
compare the object counts under different weather conditions or in different
types of scenes. This chart also shows the diverse set of objects that appear in
our dataset, and the scale of our dataset –  more than 1 million cars. The
reader should be reminded here that those are distinct objects with distinct
appearances and contexts.</p><p><img width="750" src="http://bair.berkeley.edu/static/blog/bdd/bbox_instance.png"><br><i>
Statistics of different types of objects.
</i></p><p>Our dataset is also suitable for studying some particular domains. For example,
if you are interested in detecting and avoiding pedestrians on the streets, you
also have a reason to study our dataset since it contains more pedestrian
instances than previous specialized datasets as shown in the table below.</p><p><img width="600" src="http://bair.berkeley.edu/static/blog/bdd/table1.png"><br><i>
Comparisons with other pedestrian datasets regarding training set size.
</i></p><h3 id="lane-markings">Lane Markings</h3><p>Lane markings are important road instructions for human drivers. They are also
critical cues of driving direction and localization for the autonomous driving
systems when GPS or maps does not have accurate global coverage. We divide the
lane markings into two types based on how they instruct the vehicles in the
lanes. Vertical lane markings (marked in red in the figures below) indicate
markings that are  along the driving direction of their lanes. Parallel lane
markings (marked in blue in the figures below) indicate those that are  for the
vehicles in the lanes to stop. We also provide attributes for the markings such
as solid vs. dashed and double vs. single.</p><p><img width="750" src="http://bair.berkeley.edu/static/blog/bdd/lane_markings.png"></p><p>If you are ready to try out your lane marking prediction algorithms, please look
no further. Here is the comparison with existing lane marking datasets.</p><p><img width="750" src="http://bair.berkeley.edu/static/blog/bdd/table2.png"></p><h3 id="drivable-areas">Drivable Areas</h3><p>Whether we can drive on a road does not only depend on lane markings and traffic
devices. It also depends on the complicated interactions with other objects
sharing the road. In the end, it  is important to understand which area can be
driven on. To investigate this problem, we also provide segmentation annotations
of drivable areas as shown below. We divide  the drivable areas into two
categories based on the trajectories of the ego vehicle: direct drivable, and
alternative drivable. Direct drivable, marked in  red, means the ego vehicle has
the road priority and can keep driving in that area. Alternative drivable,
marked in  blue, means the ego vehicle can drive in the area, but has to be
cautious since the road priority  potentially belongs to other vehicles.</p><p><img width="750" src="http://bair.berkeley.edu/static/blog/bdd/drivable_area.png"></p><h3 id="full-frame-segmentation">Full-frame Segmentation</h3><p>It has been shown on Cityscapes dataset that full-frame fine instance
segmentation can greatly bolster research in dense prediction and object
detection, which are pillars of a wide range of computer vision applications. As
our videos are in a different domain, we provide instance segmentation
annotations as well to compare the domain shift relative by different datasets.
It can be expensive and laborious to obtain full pixel-level segmentation.
Fortunately, with our own labeling tool, the labeling cost could be reduced by
50%. In the end, we label a subset of 10K images with full-frame instance
segmentation. Our label set is compatible with the training annotations in
Cityscapes to make it easier to study domain shift between the datasets.</p><p><img width="750" src="http://bair.berkeley.edu/static/blog/bdd/segmentation.jpg"></p><h2 id="driving-challenges">Driving Challenges</h2><p>We are hosting <a href="http://bdd-data.berkeley.edu/wad-2018.html">three
challenges</a> in CVPR 2018 Workshop on Autonomous Driving based on our data:
road object detection, drivable area prediction, and domain adaptation of
semantic segmentation. The detection task requires your algorithm to find all of
the target objects in our testing images and drivable area prediction requires
segmenting the areas a car can drive in. In domain adaptation, the testing data
is collected in China. Systems are thus challenged to get models learned in the
US to work in the crowded streets in Beijing, China. You can submit your results
now after <a href="http://bdd-data.berkeley.edu/login.html">logging in our
online submission portal</a>. Make sure to check out <a href="https://github.com/ucbdrive/bdd-data">our toolkit</a> to jump start your
participation.</p><p>Join our CVPR workshop challenges to claim your cash prizes!!!</p><h2 id="future-work">Future Work</h2><p>The perception system for self-driving is by no means only about monocular
videos. It may also include panorama and stereo videos as well as  other types
of sensors like LiDAR and radar. We hope to provide and study those
multi-modality sensor data as well in the near future.</p><h2 id="reference-links">Reference Links</h2><p><a href="https://core.ac.uk/download/pdf/4875878.pdf"> Caltech,
</a><a href="http://www.cvlibs.net/publications/Geiger2012CVPR.pdf"> KITTI,
</a><a href="https://arxiv.org/abs/1702.05693"> CityPerson,
</a><a href="https://arxiv.org/abs/1604.01685"> Cityscapes,
</a><a href="https://arxiv.org/pdf/1803.06184v1.pdf"> ApolloScape,
</a><a href="https://research.mapillary.com/img/publications/ICCV17a.pdf"> Mapillary,
</a><a href="https://arxiv.org/abs/1411.7113"> Caltech Lanes Dataset,
</a><a href="https://ieeexplore.ieee.org/document/6232144/"> Road Marking Dataset,
</a><a href="http://www.cvlibs.net/projects/autonomous_vision_survey/literature/Fritsch2013ITSC.pdf"> KITTI Road,
</a><a href="https://arxiv.org/abs/1710.06288"> VPGNet</a></p></div></div></div></div>
</div>
    </div>
    <footer>
        <div>created by buildstarted &copy; 2020 <a href="/about">about</a></div>
        <div>Share this page on social media: copy and paste this url https://linksfor.dev/</div>
        <div>If you prefer RSS: <a href="https://linksfor.dev/feed.xml">https://linksfor.dev/feed.xml</a></div>
    </footer>
    
    <script>
        (function () {
            var COLLECT_URL = "https://dna.buildstarted.com/t";
            var SITE_ID = "linksfor.devs";
            var GLOBAL_VAR_NAME = "__DNA__";

            window[GLOBAL_VAR_NAME] = {};

            window[GLOBAL_VAR_NAME].sendPageView = function () {
                var path = location.pathname;
                var referrer = document.referrer;

                var url = COLLECT_URL + "?siteid=" + SITE_ID + "&p=" + encodeURIComponent(path) + "&r=" + encodeURIComponent(referrer);

                try { fetch(url, { method: "GET" }); } catch (e) { }
            };

            window[GLOBAL_VAR_NAME].sendPageView();
        })();
    </script>
</body>
</html>