<!DOCTYPE html>
<html lang="en">
<head>
    <title>
Fixing Random, part 40 of 40 - linksfor.dev(s)    </title>
    <meta charset="utf-8">
    <link rel="alternate" type="application/rss+xml" title="Linksfor.dev(s) feed" href="https://linksfor.dev/feed.rss" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta name="google" value="notranslate">
    <link rel="stylesheet" type="text/css" href="/style.css" />
    <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon">
    <link rel="icon" href="/favicon.ico" type="image/x-icon">
    <meta property="og:title" content="Fixing Random, part 40 of 40 - linksfor.dev(s)"/>
    <meta property="og:description" content="All right, let&#x2019;s finish this thing off! First, I want to summarize, second I want to describe a whole lot of interesting stuff that I did not get to, and third, I want to give a selection of papers&#x2026;"/>
    <meta property="og:type" content="website"/>
    <meta property="og:url" content="https://ericlippert.com/2019/07/26/fixing-random-part-40-of-40/"/>

<meta property="og:site_name" content="linksfor.dev(s)" />
</head>
<body>
    <div class="grid">
        <h1>
                <span style="cursor: default" title="linksfor.dev(s) has been running for 1 year! :partypopper:">üéâ</span>
<a href="/" style="color:inherit">linksfor.dev(s)</a>
        </h1>
        <title>linksfor.dev(s) - Fixing Random, part 40 of 40</title>
<div class="readable">
        <h1>Fixing Random, part 40 of 40</h1>
        <p>
Reading time: 10-13 minutes        </p>
        <p><a href="https://ericlippert.com/2019/07/26/fixing-random-part-40-of-40/">https://ericlippert.com/2019/07/26/fixing-random-part-40-of-40/</a></p>
        <hr/>
<div id="readability-page-1" class="page"><div>
		<p data-adtags-visited="true">All right, let‚Äôs finish this thing off!</p>
<p data-adtags-visited="true">First, I want to <strong>summarize</strong>, second I want to describe <strong>a whole lot of interesting stuff that I did not get to</strong>, and third, I want to give <strong>a selection of papers</strong> and further reading that inspired me in this series.</p>
<hr>
<p data-adtags-visited="true">If you come away with nothing else from this series, the key message is: probabilistic programming is important, it is too hard today, and we can do a lot better than we are doing. We need to build better tools that leverage the type system and support line-of-business programmers who need to do probabilistic work, the same way that we built better tools for programmers who needed to use sequences, or databases, or asynchrony.</p>
<hr>
<ul>
<li>We started this series with me complaining about <code>System.Random</code>, hence the name of the series. Even though some of the implementation details have finally improved after only some decades of misleading developers, we are still dealing with random numbers like it was 1972.</li>
<li>The abstraction that we‚Äôre missing is to make ‚Äúvalue drawn from a random distribution‚Äù a part of the type system, the same way that ‚Äúfunction that returns a value‚Äù, or ‚Äúsequence of values‚Äù, or ‚Äúvalue that might be null‚Äù is part of the type system.</li>
<li>The type we want is something like a sequence enumeration, but instead of a ‚Äúmove next‚Äù operation, we‚Äôve got a ‚Äúsample‚Äù operation.</li>
<li>If we stick to simple distributions where the support is finite and small and the ‚Äúweights‚Äù are integers, and restrict ourselves to pure functions, we can create new distributions from old ones using the same operations that we use to create new sequences from old ones: the LINQ operators <code>Select</code>, <code>SelectMany</code> and <code>Where</code>.</li>
<li>Moreover, we can compute distributions exactly at runtime, without doing rejection sampling or other clever techniques.</li>
<li>And we can even use query comprehension syntax, which makes me happy.</li>
<li>From this we can see that probability distributions are monads; <strong>P(A)</strong> is just <code>IDistribution&lt;A&gt;</code></li>
<li>We also see that conditional probabilities <strong>P(B given A)</strong> are just <code>Func&lt;A, IDistribution&lt;B&gt;&gt;</code> ‚Äî they are likelihood functions.</li>
<li>The <code>SelectMany</code>operation on the probability monad lets us combine a likelihood function with a prior probability.</li>
<li><strong>The <code>Where</code> operation lets us compute a posterior given a prior, a likelihood and an observation.</strong></li>
<li>This is an extremely useful result; even domain experts like doctors often do not have good intuitions about how an observation should change our opinions about the probability of an event having occurred. A positive result on a test of a rare disease may be only weak evidence if the test failure rate is close to the disease rate.</li>
<li>Can we put these features in the language as well as the type system? Abusing LINQ is clever but maybe not the best from a usability perspective.</li>
<li>We could in fact embed <code>sample</code> and <code>condition</code> operators in the C# language, just as we embedded <code>await</code>. We can then write an imperative workflow, and have the compiler generate a method that returns the correct probability distribution, just as <code>await</code> allows us to write an asynchronous workflow and the compiler generates a method that returns a task!</li>
<li>Unfortunately, real-world probability problems are seldom discrete, small, and completely analyzable; they‚Äôre more often continuous and approximated.</li>
<li>Implementing <code>Sample</code> efficiently on arbitrary distributions turns out to be a hard problem.</li>
<li>We can use our tools to generate Markov chains.</li>
<li>We can use Markov chains to implement <code>Sample</code> using the Metropolis Algorithm</li>
<li>If we have a continuous prior (like a mint that produces coins of a certain quality with a certain probability) and a discrete likelihood (like the probability of a coin flip coming up heads), we can use this technique to compute a continuous posterior given an observation of one or more flips.</li>
<li>This is a very useful technique in many real-world applications.</li>
<li>Computing the expected value of a distribution given a function is a tricky problem in of itself, but there are a variety of techniques we can use.</li>
<li>And if we can do that, we can solve some high-dimensional integral calculus problems!</li>
</ul>
<hr>
<p data-adtags-visited="true">That was rather a lot, and we still did not get to everything I wanted to.</p>
<p data-adtags-visited="true">By far the biggest thing that I did not get to, and that I may return to in another series of posts, is: <strong>the connection between <code>await</code> as an operator and <code>sample</code>as an operator&nbsp; is deeper than you might think.</strong></p>
<p data-adtags-visited="true">As I noted above, you can put <code>sample</code> and <code>condition</code> operations in a language and have the compiler build a method that when run, generates a simple, discrete distribution. But it turns out <strong>we can actually do a pretty good job of dealing with sample operators on non-discrete distributions as well,</strong> by having the compiler be smart about using some of the techniques that we discussed in this series for continuous distributions.</p>
<p data-adtags-visited="true">What you really need is the ability to pick a sample, run a little bit of a routine, remember the result, back up a bit and try a different sample, and so on; from this <strong>we can build a distribution of program traces</strong>, and from that we can build an approximation of the distribution of the output of a probabilistic method!</p>
<p data-adtags-visited="true">This kind of control flow is tricky; it‚Äôs sort of a generalization of coroutines where you‚Äôre allowed to re-run code that you‚Äôve run before, but with <em>different</em> values for the variables to see what happens.</p>
<p data-adtags-visited="true">Obviously it is crucially important that the methods be pure! It‚Äôs also crucially important that you spend most of your time exploring high-likelihood control flows, because the number of unlikely control flows is nigh infinite. If this sounds a bit like training up an ML model and then using that model in production later, that‚Äôs because it is basically the same thing, but applied to programs.</p>
<p data-adtags-visited="true">I know what you‚Äôre thinking: <em>that sounds bonkers.</em> But ‚Äî &nbsp;here is the thing that really got me motivated to write this series in the first place ‚Äî<strong> we actually did it.</strong></p>
<p data-adtags-visited="true">A while back my colleague Michael and I hacked together (ha ha) an implementation of multi-shot-continuations for <em>Hack</em>, our PHP variant and showed that we could in fact do probabilistic workflows where we have a distribution, and we sample from it some number of times, and trace out what happens in the program as we do so.</p>
<p data-adtags-visited="true">I then went on to work on other projects, but in the meanwhile <strong>a team of people who understand statistics far, far better than I do actually built an industrial-strength probabilistic control flow language with a sample operator.&nbsp;</strong></p>
<p data-adtags-visited="true">You can read all about it at their paper <a href="https://research.fb.com/wp-content/uploads/2019/06/HackPPL-A-Universal-Probabilistic-Programming-Language.pdf">Hack PPL: A Universal Probabilistic Programming Language</a>.</p>
<hr>
<p data-adtags-visited="true">Another point that I very much wanted to get to in this series but did not is: <strong>we can do the same thing in C#, and in fact we can do it today</strong>.</p>
<p data-adtags-visited="true">The C# team added the ability to return types other than <code>Task&lt;T&gt;</code> from asynchronous workflows; and it turns out you only need to abuse that feature a small amount to convince C# to ‚Äúgo back a bit‚Äù in the workflow ‚Äì back to the previous <code>await</code> operation, which becomes a stand-in for <code>sample</code> ‚Äî and re-run portions of it with different sampled values. <strong>The C# team could add probabilistic workflows to C# tomorrow.</strong></p>
<p data-adtags-visited="true">The C# team has historically done a great job of finding useful monads and embedding them into the control flow of the language; monadic probabilistic workflows with multi-shot continuations could be the next one.<em> How about it team? </em></p>
<hr>
<p data-adtags-visited="true">Finally, here‚Äôs a very incomplete list of papers and web sites that were inspiring to me in writing this series. I learned a lot, and there is plenty more to learn; I‚Äôve just scratched the surface here.</p>
<ul>
<li>The idea that we can treat probability distributions like LINQ queries was given to me by my friend and director Erik Meijer; his fun and accessible paper <a href="https://queue.acm.org/detail.cfm?id=3055303">Making Money Using Math</a> hits many of the same points I did in this series, but I did it with a lot more code.</li>
<li>The design of <a href="https://kotlinlang.org/docs/reference/coroutines-overview.html">coroutines in Kotlin</a> was a great inspiration to me; they‚Äôve done a great job of making features that you would normally think of as being part of the language proper, like <code>yield</code> and <code>await</code>, into library methods. The first thing I did in my multi-shot coroutine hack was verify that I could simulate those features. (I was also very pleased to discover that much of this work was implemented by my old colleague Nikov from the C# team!)</li>
<li><a href="https://arxiv.org/pdf/1809.10756.pdf">An introduction to probabilistic programming</a> is a book-length work that uses a Lisp-like language with sample and observe primitives.</li>
<li><a href="https://cocolab.stanford.edu/papers/GoodmanEtAl2008-UncertaintyInArtificialIntelligence.pdf">Church</a> is another Lisp-like language often used in academic studies of PPLs.</li>
<li>The <a href="http://webppl.org/">webppl.org</a> web site has a Javascript-based implementation of a probabilistic programming language and a lot of great supporting information at <a href="http://dippl.org/">dippl.org</a>.</li>
</ul>
<p data-adtags-visited="true">The next few papers are more technical.</p>
<ul>
<li><a href="http://www.randomhacks.net/files/build-your-own-probability-monads.pdf">Build Your Own Probability Monads</a> is a good overview for the Haskell programmers out there, as are <a href="http://mlg.eng.cam.ac.uk/pub/pdf/SciGhaGor15.pdf">Practical Probabilistic Programming With Monads</a> and &nbsp;&nbsp;<a href="https://www.cs.tufts.edu/~nr/pubs/pmonad.pdf">Stochastic Lambda Calculus and Monads of Probability Distributions</a>.</li>
<li><a href="http://proceedings.mlr.press/v15/wingate11a/wingate11a.pdf">Lightweight Implementations of Probabilistic Programming Languages Via Transformational Compilation</a> is a good overview of how you can use MCMC techniques on program traces. <a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/final.pdf">A provably correct sampler for probabilistic programs</a> goes into some of the correctness problems faced by PPL implementations, and <a href="https://web.stanford.edu/~ngoodman/papers/aistats2014-shred.pdf">Generating Efficient MCMC Kernels</a> goes into some of the performance problems.</li>
<li>Another fascinating area that I wanted to explore in this series is: we know it is bad enough to try to debug programs that have yields and awaits in them; how on earth do you debug programs that are actually running the same code paths maybe thousands of times when exploring the sample space of a workflow? Here‚Äôs an interesting paper on <a href="https://homes.cs.washington.edu/~cnandi/docs/mapl-cr.pdf">debugging probabilistic workflows</a>.</li>
</ul>
<p data-adtags-visited="true">And these are some good ones for the math:</p>
<ul>
<li>These MIT course notes on <a href="https://ocw.mit.edu/courses/mathematics/18-05-introduction-to-probability-and-statistics-spring-2014/readings/MIT18_05S14_Reading13a.pdf">Bayesian updating of continuous priors</a> was a good primer for my episodes on coin flipping. And the lecture notes <a href="http://ib.berkeley.edu/labs/slatkin/eriq/classes/guest_lect/mc_lecture_notes.pdf">MC Methods and Importance Sampling</a> are what it says on the tin.</li>
<li><a href="http://web1.sph.emory.edu/users/hwu30/teaching/statcomp/papers/chibGreenbergMH.pdf">Understanding the Metropolis-Hastings Algorithm</a> gives a bunch of the underlying math.</li>
<li>The enormous book <a href="https://www.inference.org.uk/itprnn/book.pdf">Information Theory, Inference and Learning Algorithms</a> was invaluable in getting me up to speed on math I had not done since my undergrad days.</li>
</ul>
<p data-adtags-visited="true">There are many more that I am forgetting, and I‚Äôll add to this list as I recall them.</p>
<hr>
<p data-adtags-visited="true">All right, <strong>that was super fun;</strong> I am off on my annual vacation where I have no phone or internet, so I‚Äôm going to take a bit of a break from blogging; we‚Äôll see you in a month or so for more fabulous adventures in coding!</p>

			
			
						</div></div></div>
    </div>
    <footer>
        <div>created by buildstarted &copy; 2020 <a href="/about">about</a></div>
        <div>Share this page on social media: copy and paste this url https://linksfor.dev/</div>
        <div>If you prefer RSS: <a href="https://linksfor.dev/feed.xml">https://linksfor.dev/feed.xml</a></div>
    </footer>
    
    <script async defer>
        _dna = window._dna || {};
        _dna.siteId = "linksfor.devs";
        _dna.outlink = true;

        (function () {
            let dna = document.createElement('script');
            dna.type = 'text/javascript';
            dna.async = true;
            dna.src = '//dna.buildstarted.com/t.js';
            let s = document.getElementsByTagName('script')[0];
            s.parentNode.insertBefore(dna, s);
        })();
    </script>
    <noscript><img src="//dna.buildstarted.com/g?siteId=linksfor.devs"/></noscript>
</body>
</html>