<!DOCTYPE html>
<html lang="en">
<head>
    <title>
TechEmpower Framework Benchmarks -
linksfor.dev(s)
    </title>
    <link rel="alternate" type="application/rss+xml" title="Linksfor.dev(s) feed" href="https://linksfor.dev/feed.rss" />
    <meta charset="utf-8">
    <meta name="Description" content="A curated source of links that devs might find interesting. Updated around the clock." />
    <meta name="google" value="notranslate">
    <link rel="stylesheet" type="text/css" href="/style.css" />
    <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon">
    <link rel="icon" href="/favicon.ico" type="image/x-icon">
</head>
<body>
    <div class="grid">
        
<div class="readable">
    <div id="readOverlay" class="style-ebook"><div id="readInner" class="margin-medium size-medium"><h1>TechEmpower Framework Benchmarks</h1><div><div class="centeredcontent body-div body-intro active"><h1>Introduction</h1><p>This is a performance comparison of many web application frameworks executing fundamental tasks such as JSON serialization, database access, and server-side template composition. Each framework is operating in a realistic production configuration. Results are captured on cloud instances and on physical hardware. The test implementations are largely community-contributed and all source is available at the <a href="https://github.com/TechEmpower/FrameworkBenchmarks">GitHub repository</a>.</p><p class="italicnote">Note: We use the word "framework" loosely to refer to platforms, micro-frameworks, and full-stack frameworks.</p><p>In a <a href="//www.techempower.com/blog/2013/03/28/frameworks-round-1/">March 2013 blog entry</a>, we published the results of comparing the performance of several web application frameworks executing simple but representative tasks: serializing JSON objects and querying databases.  Since then, community input has been tremendous.  We—speaking now for all contributors to the project—have been regularly updating the test implementations, expanding coverage, and capturing results in semi-regular updates that we call "rounds."</p><h1>Results</h1><p>View the latest results from <a href="#section=data-r18" class="select-data-r18">Round 18</a>.  Or check out the <a href="#section=previous-rounds">previous rounds</a>.</p><h1>Making improvements</h1><p>We expect that all frameworks' tests could be improved with community input.  For that reason, we are extremely happy to receive <a href="https://github.com/TechEmpower/FrameworkBenchmarks/pulls">pull requests</a> from fans of any framework.  We would like our tests for every framework to perform optimally, so we invite you to please join in.</p><h1>What's to come</h1><p>Feedback has been continuous and we plan to keep updating the project in several ways, such as:</p><ul><li>Coverage of more frameworks.  Thanks to community contributions to-date, the number of frameworks covered has already grown quite large.  We're happy to add more if you submit a pull request.</li><li><a href="https://github.com/TechEmpower/FrameworkBenchmarks/issues/133">Additional test types</a>.</li><li>Tests on more types of hardware.</li><li>Enhancements to this results web site.</li></ul><h2>Additional resources</h2></div><div class="centeredcontent body-div body-previous-rounds"><h1>Current and Previous Rounds</h1><p><i><a href="#section=data-r18" class="select-data-r18">Round 18</a> —</i> This round included several <a href="https://github.com/TechEmpower/FrameworkBenchmarks/issues/4630">requirements clarifications</a> such as specificying how often implementations are required to recompute the response <code>Date</code> header and stricter validation.</p><p><i><a href="#section=data-r17" class="select-data-r17">Round 17</a> —</i> Another <a href="https://tfb-status.techempower.com/">Continuous Benchmarking</a> run promoted to an official round, Round 17 now includes 179 frameworks.  In this round, we permitted Postgres query pipelining, which has created a stratification of database tests.  We are optimistic that over time, more test implementations will be able to leverage this capability.</p><p><i><a href="#section=data-r16" class="select-data-r16">Round 16</a> —</i> Now Dockerized and running on a new 10-gigabit powered hardware environment, Round 16 of the Framework Benchmarks project brings new performance highs and increased stability.</p><p><i><a href="#section=data-r15" class="select-data-r15">Round 15</a> —</i> The project exceeded 3,000 stars on GitHub and has processed nearly 2,500 pull requests.  Continuous benchmarking results are now available on the <a href="https://tfb-status.techempower.com/">Results dashboard</a>.</p><p><i><a href="#section=data-r14" class="select-data-r14">Round 14</a> —</i> Adoption of the <a href="https://github.com/facebook/mention-bot">mention-bot from Facebook</a> has proven useful in notifying project participants of changes to their contributions.  Continuous benchmarking provided a means for several community previews in this round, and we expect that to continue going forward.  Note that this round was conducted only on physical hardware within the ServerCentral environment; tests on the cloud environment will return for Round 15.</p><p><i><a href="#section=data-r13" class="select-data-r13">Round 13</a> —</i> Microsoft's <a href="http://asp.net/">ASP.NET</a> team delivers the most impressive improvement we've seen in this project—a 85,000% increase in plaintext results for ASP.NET Core—making it a top-performing framework at the fundamentals of HTTP request routing.  Round 13 also sees new hardware and cloud environments from <a href="https://www.ServerCentral.com/">ServerCentral</a> and <a href="https://azure.microsoft.com/">Microsoft Azure</a>.</p><p><i><a href="#section=data-r12" class="select-data-r12">Round 12</a> —</i> Marking the last round on the Peak environment, Round 12 sees some especially high Plaintext scores.</p><p><i><a href="#section=data-r11" class="select-data-r11">Round 11</a> —</i> 26 more frameworks, three more languages, and the volume cranked to 11.</p><p><i><a href="#section=data-r10" class="select-data-r10">Round 10</a> —</i> Significant restructuring of the project's infrastructure, including re-organization of the project's directory structure and integration with <a href="https://travis-ci.org/">Travis CI</a> for rapid review of pull requests, and the addition of numerous frameworks.</p><p><i><a href="#section=data-r9" class="select-data-r9">Round 9</a> —</i> Thanks to the contribution of a 10-gigabit testing environment by <a href="//www.peakhosting.com/">Peak Hosting</a>, the network barrier that frustrated top-performing frameworks in previous rounds has been removed. The Dell R720xd servers in this new environment feature dual Xeon E5-2660 v2 processors and illustrate how the spectrum of frameworks scale to forty processor cores.</p><p><i><a href="#section=data-r8" class="select-data-r8">Round 8</a> —</i> Six more frameworks contributed by the community takes the total count to 90 frameworks and 230 permutations (variations of configuration).  Meanwhile, several implementations have been updated and the highest-performance platforms jockey for the top spot on each test's charts.</p><p><i><a href="#section=data-r7" class="select-data-r7">Round 7</a> —</i> After a several month hiatus, another large batch of frameworks have been added by the community.  Even after consolidating a few, Round 7 counts 84 frameworks and over 200 test permutations!  This round also was the first to use a community-review process.  Future rounds will see roughly one week of preview and review by the community prior to release to the public here.</p><p><i><a href="#section=data-r6" class="select-data-r6">Round 6</a> —</i> Still more tests were contributed by the developer community, bringing the number of frameworks to 74!  Round 6 also introduces an "plaintext" test type that exercises HTTP pipelining and higher client-side concurrency levels.</p><p><i><a href="#section=data-r5" class="select-data-r5">Round 5</a> —</i> The developer community comes through with the addition of ASP.NET tests ready to run on Windows.  This round is the first with Windows tests, and we seek assistance from Windows experts to apply additional tuning to bring the results to parity with the Linux tests.  Round 5 also introduces an "update" test type to exercise ORM and database writes.</p><p><i><a href="#section=data-r4" class="select-data-r4">Round 4</a> —</i> With 57 frameworks in the benchmark suite, we've added a filter control allowing you to narrow your view to only the frameworks you want to see.  Round 4 also introduces the "Fortune" test to exercise server-side templates and collections.</p><p><i><a href="#section=data-r3" class="select-data-r3">Round 3</a> —</i> We created this stand-alone site for comparing the results data captured across many web application frameworks.  Even more frameworks have been contributed by the community and the testing methodology was changed slightly thanks to enhancements to the testing tool named <a href="https://github.com/wg/wrk">Wrk</a>.</p><p><i><a href="//www.techempower.com/blog/2013/04/05/frameworks-round-2/">Round 2</a> —</i> In April, we published a follow-up blog entry named "Frameworks Round 2" where we incorporated changes suggested and contributed by the community.</p><p><i><a href="//www.techempower.com/blog/2013/03/28/frameworks-round-1/">Round 1</a> —</i> In a March 2013 blog entry, we published the results of comparing the performance of several web application frameworks executing simple but representative tasks: serializing JSON objects and querying databases.  The community reaction was terrific.  We are flattered by the volume of feedback.  We received dozens of comments, suggestions, questions, criticisms, and most importantly, GitHub pull requests at <a href="https://github.com/TechEmpower/FrameworkBenchmarks">the repository</a> we set up for this project.</p><h2>Unofficial Results</h2><p>We operate a continuously-running benchmarking environment.  You can see unofficial results as they are collected at the <a href="https://tfb-status.techempower.com/">TFB Results Dashboard</a>.</p></div><div class="centeredcontent body-div body-motivation"><h1>Motivation</h1><p>Choosing a web application framework involves evaluation of many factors. While comparatively easy to measure, performance is frequently given little consideration. We hope to help change that.</p><p>Application performance can be directly mapped to hosting dollars, and for companies both large and small, hosting costs can be a pain point. Weak performance can also cause premature and costly scale pain by requiring earlier optimization efforts and increased architectural complexity. Finally, slow applications yield poor user experience and may suffer penalties levied by search engines.</p><p>What if building an application on one framework meant that at the very best your hardware is suitable for one tenth as much load as it would be had you chosen a different framework? The differences aren't always that extreme, but in some cases, they might be. Especially with several modern high-performance frameworks offering respectable developer efficiency, <b>it's worth knowing what you're getting into.</b></p><h1>Terminology</h1><dl><dt>framework</dt><dd>We use the word <i>framework</i> loosely to refer to any HTTP server implementation upon which you could build a web application—a full-stack framework, a micro-framework, or even a web platform such as Rack, Servlet, or plain PHP.</dd><dt>platform</dt><dd>For us, <i>platforms</i> are broadly defined as anything situated between the programming language and the web framework (examples are Servlet, Netty, and Rack).  By comparison to a full-stack framework or micro-framework, a <i>platform</i> may include a bare-bones HTTP server implementation with rudimentary request routing and virtually none of the higher-order functionality of frameworks such as form validation, input sanitization, templating, JSON serialization, and database connectivity.  Frameworks are often built on top of platforms.  To be thorough, and to compute framework overhead, we test several platforms <i>as if</i> they were frameworks.</dd><dt>permutation</dt><dd>A combination of attributes that compose a full technology stack being tested (take node.js for example, we might have one permutation with <i>MongoDB</i> and another with <i>MySQL</i>).  Some frameworks have seen many permutations contributed by the community; others only one or few.</dd><dt>test type</dt><dd>One of the workloads we exercise, such as JSON serialization, single-query, multiple-query, fortunes, data updates, and plaintext.</dd><dt>test</dt><dd>An individual <i>test</i> is a measurement of the performance of a permutation's implementation of a test type.  For example, a test might be measuring <i>Wicket paired with MySQL running the single-query test type</i>.</dd><dt>implementation</dt><dd>Sometimes called "test implementations," these are the bodies of code and configuration created to test permutations according to <a href="//github.com/TechEmpower/FrameworkBenchmarks/wiki/Project-Information-Framework-Tests-Overview">the requirements</a>.  These are frequently contributed by fans, advocates, or the maintainers of frameworks.  Together with the <i>toolset</i>, test implementations are the meat of this project.</dd><dt>toolset</dt><dd>A set of Python scripts that run our tests.</dd><dt>run</dt><dd>An execution of the benchmark toolset across the suite of test implementations, either in full or in part, in order to capture results for any purpose.</dd><dt>preview</dt><dd>A capture of data from a run used by project participants to sanity-check prior to an official <i>round</i>.</dd><dt>round</dt><dd>A posting of "official" results on this web site.  This is mostly for ease of consumption by readers and good-spirited &amp; healthy competitive bragging rights.  For in-depth analysis, we encourage you to examine the source code and run the tests on your own hardware.</dd></dl><h1>Expected questions</h1><p>We expect that you might have a bunch of questions.  Here are some that we're anticipating.  But please contact us if you have a question we're not dealing with here or just want to tell us we're doing it wrong.</p><h2>Frameworks and configuration</h2><ol><li><i>"You call x a framework, but it's a platform."</i>  See the terminology section above.  We are using the word "framework" loosely to refer to anything found on the spectrum ranging from full-stack frameworks, micro-frameworks, to platforms.  If it's used to build web applications, it probably qualifies.  That said, we understand that comparing a full-stack framework versus platforms or vice-versa is unusual.  We feel it's valuable to be able to compare these, for example to understand the performance overhead of additional abstraction.  You can use the filters in the results viewer to adjust the rows you see in the charts.</li><li><i>"You configured framework x incorrectly, and that explains the numbers you're seeing."</i> Whoops! Please let us know how we can fix it, or submit a <a href="https://github.com/TechEmpower/FrameworkBenchmarks">GitHub</a> pull request, so we can get it right.</li><li><i>"Why include this <span class="colorname" framework="gemini">Gemini</span> framework I've never heard of?"</i> We have included our in-house Java web framework, Gemini, in our tests. We've done so because it's of interest to us. You can consider it a stand-in for any relatively lightweight minimal-locking Java framework. While we're proud of how it performs among the well-established field, this exercise is not about Gemini.</li><li><i>"Why don't you test framework X?"</i> We'd love to, if we can find the time. Even better, craft the test implementation yourself and submit a <a href="https://github.com/TechEmpower/FrameworkBenchmarks">GitHub</a> pull request so we can get it in there faster!</li><li><i>"Some frameworks use process-level concurrency; have you accounted for that?"</i> Yes, we've attempted to use production-grade configuration settings for all frameworks, including those that rely on process-level concurrency.  For the EC2 tests, for example, such frameworks are configured to utilize the two virtual cores provided on an c3.large (in previous rounds, m1.large) instance.  For the i7 tests, they are configured to use the eight hyper-threading cores of our hardware's i7 CPUs.</li><li><i>"Have you enabled <a href="http://php.net/manual/en/book.apc.php">APC</a> for the PHP tests?"</i> Yes, the PHP tests run with APC and PHP-FPM on nginx.</li><li><i>"Why are you using a (slightly) old version of framework X?"</i> It's nothing personal! With so many frameworks we have a never-ending game of whack-a-mole. If you think an update will affect the results, please let us know (or better yet, submit a <a href="https://github.com/TechEmpower/FrameworkBenchmarks">GitHub</a> pull request) and we'll get it updated!</li><li><i>"It's unfair and possibly even incorrect to compare X and Y!"</i> It may be alarming at first to see the full results table, where one may evaluate frameworks vs platforms; MySQL vs Postgres; Go vs Python; ORM vs raw database connectivity; and any number of other possibly irrational comparisons.  Many readers desire the ability to compare these and other permutations.  If you prefer to view an unpolluted subset, you may use the filters available at the top of the results page.  We believe that comparing frameworks with plausible and diverse technology stacks, despite the number of variables, is precisely the value of this project.  With sufficient time and effort, we hope to continuously broaden the test permutations.  But we recommend against ignoring the data on the basis of concerns about multi-variable comparisons.  Read more opinion on this at <a href="http://tiamat.tsotech.com/unfair-comparisons">Brian Hauer's personal blog</a>.</li><li><i>"If you are testing production deployments, why is logging disabled?"</i> At present, we have elected to run tests with logging features disabled.  Although this is <i>not</i> consistent with production deployments, we avoid a few complications related to logging, most notably disk capacity and consistent granularity of logging across all test implementations.  In spot tests, we have not observed significant performance impact from logging when enabled.  If there is strong community consensus that logging is necessary, we will reconsider this.</li><li><i>"Tell me about the Windows configuration."</i> We are very thankful to the community members who have contributed Windows tests.  In fact, nearly the entirety of the Windows configuration has been contributed by subject-matter experts from the community.  Thanks to their effort, we now have tests covering both Windows paired with Linux databases and Windows paired with Microsoft SQL Server.  As with all aspects of this project, we welcome continued input and tuning by other experts.  If you have advice on better tuning the Windows tests, please submit <a href="https://github.com/TechEmpower/FrameworkBenchmarks">GitHub</a> issues or pull requests.</li></ol><h2>The tests</h2><ol start="11"><li><i>"Framework X has in-memory caching, why don't you use that?"</i> In-memory caching, as provided by some frameworks, yields higher performance than repeatedly hitting a database, but isn't available in all frameworks, so we omitted in-memory caching from these tests. Cache tests are planned for later rounds.</li><li><i>"What about other caching approaches, then?"</i> Remote-memory or near-memory caching, as provided by Memcached and similar solutions, also improves performance and we would like to conduct future tests simulating a more expensive query operation versus Memcached. However, curiously, in spot tests, some frameworks paired with Memcached were conspicuously slower than other frameworks directly querying the authoritative MySQL database (recognizing, of course, that MySQL had its entire data-set in its own memory cache). For simple "get row ID n" and "get all rows" style fetches, a fast framework paired with MySQL may be faster and easier to work with versus a slow framework paired with Memcached.</li><li><i>"Why doesn't your test include more substantial algorithmic work?"</i> Great suggestion. We hope to in the future!</li><li><i>"What about <a href="http://en.wikipedia.org/wiki/Reverse_proxy">reverse proxy</a> options such as Varnish?"</i> We are expressly not using reverse proxies on this project.  There are other benchmark projects that evaluate the performance of reverse proxy software.  This project measures the performance of web applications in any scenario where requests reach the application server.  Given that objective, allowing the web application to avoid doing the work thanks to a reverse proxy would invalidate the results.  If it's difficult to conceptualize the value of measuring performance beyond the reverse proxy, imagine a scenario where every response provides user-specific and varying data.  It's also notable that some platforms respond with sufficient performance to potentially render a reverse proxy unnecessary.</li><li><i>"Do all the database tests use connection pooling?"</i> Yes, our expectation is that all tests use connection pooling.</li><li><i>"How is each test run?"</i> Each test is executed as follows:
<ol><li>Restart the database servers.</li><li>Start the platform and framework using their start-up mechanisms.</li><li>Run a 5-second <b>primer</b> at 8 client-concurrency to verify that the server is in fact running.  These results are not captured.</li><li>Run a 15-second <b>warmup</b> at 256 client-concurrency to allow lazy-initialization to execute and just-in-time compilation to run.  These results are not captured.</li><li>Run a 15-second <b>captured test</b> for each of the concurrency levels (or iteration counts) exercised by the test type.  Concurrency-variable test types are tested at 16, 32, 64, 128, 256, and 512 client-side concurrency.  The high-concurrency <i>plaintext</i> test type is tested at 256, 1,024, 4,096, and 16,384 client-side concurrency.</li><li>Stop the platform and framework.</li></ol></li><li><i>"Hold on, 15 seconds is not enough to gather useful data."</i>  This is a reasonable concern.  But in examining the data, we have seen no evidence that the results have changed by reducing the individual test durations from 60 seconds to 15 seconds.  The duration reduction was made necessary by the growing number of test permutations and a target that the full suite complete in less than one day.  With additional effort, we aim to build a continuously-running test environment that will pull the latest source and begin a new run as soon as a previous run completes.  When we have such an environment ready, we will be comfortable with multi-day execution times, so we plan to extend the duration of each test when that happens.</li><li><i>"Also, a 15-second warmup is not sufficient."</i> On the contrary, we have not yet seen evidence suggesting that any additional warmup time is beneficial to any framework.  In fact, for frameworks based on JIT platforms such as the Java Virtual Machine (JVM), spot tests show that the JIT has even completed its work already after just the <i>primer</i> and before the warmup starts—the warmup (256-concurrency) and real 256-concurrency tests yield results that are separated only by test noise. However, as with test durations, we intend to increase the duration of the warmup when we have a continuously-running test environment.</li></ol><h2>Environment</h2><ol start="19"><li><i>"What is Wrk?"</i> Although many web performance tests use ApacheBench from Apache to generate HTTP requests, we now use <a href="https://github.com/wg/wrk">Wrk</a> for this project. ApacheBench remains a single-threaded tool, meaning that for higher-performance test scenarios, ApacheBench itself is a limiting factor. Wrk is a multithreaded tool that provides a similar function, allowing tests to run for a prescribed amount of time (rather than limited to a number of requests) and providing us result data including total requests completed and latency information.</li><li><i>"Doesn't benchmarking on Amazon EC2 invalidate the results?"</i> Our opinion is that doing so confirms precisely what we're trying to test: performance of web applications within realistic production environments. Selecting EC2 as a platform also allows the tests to be readily verified by anyone interested in doing so. However, we've also executed tests on our Core i7 (Sandy Bridge) workstations running Ubuntu as a non-virtualized comparison. Doing so confirmed our suspicion that the ranked order and relative performance across frameworks is mostly consistent between EC2 and physical hardware. That is, while the EC2 instances were slower than the physical hardware, they were slower by roughly the same proportion across the spectrum of frameworks.</li><li><i>"Tell me about your physical hardware."</i> For the tests we refer to as "i7" tests, we're using our office workstations.  These use Intel i7-2600K processors, making them a little antiquated, to be honest.  These are connected via an unmanaged low-cost gigabit Ethernet switch.  In previous rounds, we used a two-machine configuration where the load-generation and database role coexisted.  Although these two roles were not crowding one another out (neither role was starved for CPU time), as of Round 7, we are using a three-machine configuration for the physical hardware tests.  The machine roles are:
<ul><li>Application server, which hosts the application code and web server, where applicable.</li><li>Database server, which hosts the common databases.  Starting with Round 5, we equipped the database server with a Samsung 840 Pro SSD.</li><li>Load generator, which makes HTTP requests to the Application server via the Wrk load generation tool.</li></ul></li><li><i>"What is Resin? Why aren't you using Tomcat for the Java frameworks?"</i> Resin is a Java application server. The GPL version that we used for our tests is a relatively lightweight Servlet container. We tested on Tomcat as well but ultimately dropped Tomcat from our tests because Resin was slightly faster across all Servlet-based frameworks.</li><li><i>"Do you run any warmups before collecting results data?"</i> Yes. See "how is each test run" above. Every test is preceded by a warmup and brief (several seconds) cooldown prior to gathering test data.</li></ol><h2>Results</h2><ol start="24"><li><i>"I am about to start a new web application project; how should I interpret these results?"</i> Most importantly, recognize that performance data should be one part of your decision-making process.  High-performance web applications reduce hosting costs and improve user experience.  Additionally, recognize that while we have aimed to select test types that represent workloads that are common for web applications, nothing beats conducting performance tests yourself for the specific workload of your application.  In addition to performance, consider other requirements such as your language and platform preference; your invested knowledge in one or more of the frameworks we've tested; and the documentation and support provided by the framework's community.  Combined with an examination of <a href="https://github.com/TechEmpower/FrameworkBenchmarks">the source code</a>, the results seen here should help you identify a platform and framework that is high-performance while still meeting your other requirements.</li><li><i>"Why are the leaderboards for JSON Serialization and Plaintext so different on EC2 versus i7?"</i> Put briefly, for fast frameworks on our i7 physical hardware, the limiting factor for the JSON test is our gigabit Ethernet; whereas on EC2, the limit is the CPU. Assuming proper response headers are provided, at approximately 200,000 non-pipelined and 550,000 pipelined responses per second and above, the network is saturated.</li><li><i>"Where did earlier rounds go?"</i> To better capture HTTP errors reported by Wrk, we have restructured the format of our results.json file. The test tool changed at Round 2 and some framework IDs were changed at Round 3. As a result, the results.json for Rounds 1 and 2 would have required manual editing and we opted to simply remove the previous rounds from this site. You can still see those rounds at our blog: <a href="//www.techempower.com/blog/2013/03/28/frameworks-round-1/">Round 1</a>, <a href="//www.techempower.com/blog/2013/04/05/frameworks-round-2/">Round 2</a>.</li><li><i>"What does 'Did not complete' mean?"</i> Starting with Round 9, we have added validation checks to confirm that implementations are behaving as we have specified in <a href="//github.com/TechEmpower/FrameworkBenchmarks/wiki/Project-Information-Framework-Tests-Overview">the requirements section</a> of this site.  An implementation that does not return the correct results, bypasses some of the requirements, or even formats the results in a manner inconsistent with the requirements will be marked as "Did not complete."  We have solicited corrections from prior contributors and have attempted to address many of these, but it will take more time for all implementations to be correct.  If you are a project participant and your contribution is marked as "Did not complete," please help us resolve this by contacting us at the <a href="https://github.com/TechEmpower/FrameworkBenchmarks">GitHub repository</a>.  We may ultimately need a pull request from you, but we'd be happy to help you understand what specifically is triggering a validation error with your implementation.</li><li><i>"Why are Stripped test implementations hidden by default?"</i> Since the introduction of Stripped test implementations, we have debated whether they should be included at all.  A Stripped test implementation is one that is specially crafted to excel at our benchmark.  By comparison, a "Realistic" test implementation should be demonstrative of the general-purpose, best-practices compliant, and production-class approach for the given framework.  We have <a href="https://github.com/TechEmpower/FrameworkBenchmarks/issues/2192">decided to hide Stripped tests by default</a> because we feel that while their results have some value, that value is exceedingly low for the vast majority of consumers of the data.  You may still view the results for Stripped tests by enabling the Stripped Implementation Approach in the filters control panel.</li><li><i>"What exactly causes a test implementation to be classified as Stripped?"</i> It's not possible to paint an exact picture, but conceptually, a Stripped implementation is characterized by being configured or engineered expressly to the requirements of our benchmark tests.  By comparison, a Realistic implementation will use a production-grade configuration of a general-purpose web application framework that meets our requirements.  When we first introduced the notion of Stripped, it referred to <i>configurations</i> of otherwise normal software that had been stripped of some normal behaviors.  For example, removing some middleware from Rails or Django.  However, we have <a href="https://github.com/TechEmpower/FrameworkBenchmarks/issues/2783">broadened the definition of Stripped</a> to also refer to bespoke software that has been crafted expressly to meet our test types' requirements.</li><li><i>"I have collected results from my own test environment; can I visualize them in a manner similar to this web site?"</i> Yes, use the <a href="#section=test" class="select-test">test results visualization</a> feature to visualize results you've gathered.  Be aware that you need to specify the per-test duration setting (in seconds) and that only known frameworks will be rendered.</li></ol><h2>Contributions</h2><ol start="31"><li><i>"Do you accept contributions?"</i> Absolutely!  Please visit the project's <a href="https://github.com/TechEmpower/FrameworkBenchmarks">GitHub repository</a> to join the project.  In fact, the majority of the test implementations are community-contributed.</li><li><i>"You emphasize production-grade software. Do you accept early builds of frameworks or toy projects?"</i> Actually, yes, we are quite liberal with accepting contributions, including those that don't meet the target of production-grade.  However, in many cases, such implementations will be marked as "Stripped," meaning they are not recommended for use in real-world production projects.  They will still be measured and can be made visible by enabling Stripped in the filters control panel.</li><li><i>"I am a contributor and my framework hasn't shown up in an official round yet. How do I know how my contribution will performn in your test environment?"</i> We have a benchmark environment that is continuously running the benchmark suite, which we imaginatively named "Continuous Benchmarking™" (not actually trademarked).  The continuous benchmark runner posts results to the <a href="https://tfb-status.techempower.com/">TFB Results Dashboard</a>.  If you have made a contribution, you should see its results posted to the dashboard within a few days to a week, when the next regular continuous run completes.</li></ol><h1>Join the conversation</h1></div></div></div></div>
</div>
    </div>
    <footer>
        <div>created by buildstarted &copy; 2020 <a href="/about">about</a></div>
        <div>Share this page on social media: copy and paste this url https://linksfor.dev/</div>
        <div>If you prefer RSS: <a href="https://linksfor.dev/feed.xml">https://linksfor.dev/feed.xml</a></div>
    </footer>
    
    <script>
        (function () {
            var COLLECT_URL = "https://dna.buildstarted.com/t";
            var SITE_ID = "linksfor.devs";
            var GLOBAL_VAR_NAME = "__DNA__";

            window[GLOBAL_VAR_NAME] = {};

            window[GLOBAL_VAR_NAME].sendPageView = function () {
                var path = location.pathname;
                var referrer = document.referrer;

                var url = COLLECT_URL + "?siteid=" + SITE_ID + "&p=" + encodeURIComponent(path) + "&r=" + encodeURIComponent(referrer);

                try { fetch(url, { method: "GET" }); } catch (e) { }
            };

            window[GLOBAL_VAR_NAME].sendPageView();
        })();
    </script>
</body>
</html>