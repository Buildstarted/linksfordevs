<!DOCTYPE html>
<html lang="en">
<head>
    <title>
Learning to Drive Smoothly in Minutes - linksfor.dev(s)    </title>
    <meta charset="utf-8">
    <link rel="alternate" type="application/rss+xml" title="Linksfor.dev(s) feed" href="https://linksfor.dev/feed.rss" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta name="google" value="notranslate">
    <link rel="stylesheet" type="text/css" href="/style.css" />
    <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon">
    <link rel="icon" href="/favicon.ico" type="image/x-icon">
    <meta property="og:title" content="Learning to Drive Smoothly in Minutes - linksfor.dev(s)"/>
    <meta property="article:author" content="Antonin RAFFIN"/>
    <meta property="og:description" content="Reinforcement Learning on a Small Racing Car"/>
    <meta property="og:type" content="website"/>
    <meta property="og:url" content="https://towardsdatascience.com/learning-to-drive-smoothly-in-minutes-450a7cdb35f4"/>

<meta property="og:site_name" content="linksfor.dev(s)" />
</head>
<body>
    <div class="grid">
        <h1>
                <span style="cursor: default" title="linksfor.dev(s) has been running for 1 year! :partypopper:">🎉</span>
<a href="/" style="color:inherit">linksfor.dev(s)</a>
        </h1>
        <title>linksfor.dev(s) - Learning to Drive Smoothly in Minutes</title>
<div class="readable">
        <h1>Learning to Drive Smoothly in Minutes</h1>
            <div>by Antonin RAFFIN</div>
            <div>Reading time: 13-17 minutes</div>
        <div>Posted here: 27 Feb 2019</div>
        <p><a href="https://towardsdatascience.com/learning-to-drive-smoothly-in-minutes-450a7cdb35f4">https://towardsdatascience.com/learning-to-drive-smoothly-in-minutes-450a7cdb35f4</a></p>
        <hr/>
<div id="readability-page-1" class="page"><section><div><div><p id="f175"><h2>Reinforcement Learning on a Small Racing Car</h2></p><div><div><div><p><a href="https://towardsdatascience.com/@araffin?source=post_page-----450a7cdb35f4----------------------" rel="noopener"><img alt="Antonin RAFFIN" src="https://miro.medium.com/fit/c/96/96/1*_gm_U2uwQNA0DsSNXVlOLQ.jpeg" width="48" height="48"></a></p></div></div></div></div></div><div><figure><div><div><p><img src="https://miro.medium.com/max/60/1*CBCj13inKdUEJmwkQerXGg.png?q=20" width="1815" height="718" role="presentation"></p><p><img width="1815" height="718" role="presentation" src="https://miro.medium.com/max/1815/1*CBCj13inKdUEJmwkQerXGg.png"></p></div></div><figcaption data-selectable-paragraph="">Donkey Car in action with teleoperation control panel in the unity simulator</figcaption></figure></div><div><div><p id="bd7a" data-selectable-paragraph="">In this post, we will see how to train an autonomous racing car in minutes and how to smooth its control. The method, based on Reinforcement Learning (RL) and presented here in simulation (Donkey Car simulator), was designed to be applicable in the real world. It builds on the work of a startup named <a href="https://wayve.ai/" target="_blank" rel="noopener nofollow">Wayve.ai</a> that focuses on autonomous driving.</p><p id="a6db" data-selectable-paragraph="">The code and simulator used in this article are open source and public. Please check the <a href="https://github.com/araffin/learning-to-drive-in-5-minutes" target="_blank" rel="noopener nofollow">associated GitHub repository</a> for more information ;) (pre-trained controllers are also available for download)</p><h2 id="c59d" data-selectable-paragraph="">Video</h2><figure><div></div></figure><h2 id="1668" data-selectable-paragraph="">GitHub Repository: Reproduce the Results</h2><h2 id="cf54" data-selectable-paragraph="">Introduction: Racing Car Competition</h2><p id="21d9" data-selectable-paragraph="">Since <a href="https://diyrobocars.com/" target="_blank" rel="noopener nofollow">DIY Robocars</a> creation few years ago, numerous autonomous racing car competitions exist now (e.g. <a href="http://toulouse-robot-race.org/" target="_blank" rel="noopener nofollow">Toulouse Robot Race</a>, <a href="http://www.ironcar.org/" target="_blank" rel="noopener nofollow">Iron Car,</a> …). In those, the goal is simple: you have a racing car and it must go as fast as possible while staying on a track, given only an image from its on-board camera as input.</p><figure><div><div><div><p><img src="https://miro.medium.com/freeze/max/60/1*z7xGwQwzLI-5PoyxsfvqrQ.gif?q=20" width="600" height="450" role="presentation"></p><p><img width="600" height="450" role="presentation"></p></div></div></div><figcaption data-selectable-paragraph="">The Warehouse level, inspired by DIY Robocars</figcaption></figure><p id="47c9" data-selectable-paragraph="">Self-driving challenges are a good way to get into robotics. To facilitate learning, the <a href="http://www.donkeycar.com/" target="_blank" rel="noopener nofollow">Donkey Car</a>, an open source self-driving platform was developed. In its ecosystem, there is now a <a href="https://github.com/tawnkramer/sdsandbox/tree/donkey" target="_blank" rel="noopener nofollow">unity simulator</a> featuring that small robot. We will be testing the proposed approach on this Donkey Car.</p><h2 id="36a5" data-selectable-paragraph="">Outline</h2><p id="342b" data-selectable-paragraph="">After briefly reviewing the different methods used in small autonomous car competitions, we will present what is reinforcement learning and then go into the details of our approach.</p><figure><div><div><div><div><p><img src="https://miro.medium.com/max/60/1*Cv0mRiU_J9ji_gq_3Bl4cQ.png?q=20" width="1224" height="645" role="presentation"></p><p><img width="1224" height="645" role="presentation"></p></div></div></div></div><figcaption data-selectable-paragraph=""><a href="https://becominghuman.ai/autonomous-racing-robot-with-an-arduino-a-raspberry-pi-and-a-pi-camera-3e72819e1e63" target="_blank" rel="noopener nofollow">Autonomous Racing Robot With an Arduino, a Raspberry Pi and a Pi Camera</a></figcaption></figure><h2 id="186f" data-selectable-paragraph="">Methods Used in the Self-Driving Competitions: Line Following and Behavior Cloning</h2><p id="4712" data-selectable-paragraph="">Before presenting RL, we will first quickly review what are the different solutions currently used in the RC car competitions.</p><p id="2b90" data-selectable-paragraph="">In a <a href="https://becominghuman.ai/autonomous-racing-robot-with-an-arduino-a-raspberry-pi-and-a-pi-camera-3e72819e1e63" target="_blank" rel="noopener nofollow">previous blog post</a>, I’ve described a first approach to drive autonomously, that combines <em>computer vision and a PID controller</em>. Although the idea is simple and applicable to many settings, it requires manual labeling of data (to tell the car where the center of the track is) which is costly and exhausting (trust me, manual labeling is not fun!).</p><figure><div><div><div><div><p><img src="https://miro.medium.com/max/60/1*ZYn2p9RDul0Iiy1yqAfJ3w.png?q=20" width="704" height="322" role="presentation"></p><p><img width="704" height="322" role="presentation"></p></div></div></div></div><figcaption data-selectable-paragraph=""><a href="https://www.youtube.com/watch?v=xhI71ZdSh6k" target="_blank" rel="noopener nofollow">Predicting where is the center of the track</a></figcaption></figure><p id="0a9b" data-selectable-paragraph="">As an other approach, lots of competitors use supervised learning to <em>reproduce a human driver behavior</em>. For that, a human needs to drive manually the car during several laps, recording camera image and associated control input from the joystick. Then, a model is trained to reproduce the human driving. However, this technique is not really robust, requires homogeneous driving and retraining for each track, because it generalizes quite badly.</p><h2 id="9dd1" data-selectable-paragraph="">What Is Reinforcement Learning (RL) and Why Should We Use It?</h2><p id="40a0" data-selectable-paragraph="">In view of the above issues, reinforcement learning (RL) appears to be an interesting alternative.</p><p id="c476" data-selectable-paragraph="">In a reinforcement learning setting, an agent (or robot) acts on its environment and receives a reward as feedback. It can be a positive reward (the robot did something good) or negative reward (the robot should be penalized).</p><p id="1463" data-selectable-paragraph=""><em>The goal of the robot is to maximize the cumulative reward.</em> To do so, it learns, through interaction with the world, what is called a policy (or behavior/controller) that maps its sensory input to actions.</p><p id="839e" data-selectable-paragraph="">In our case, <em>the input is the camera image and the actions are the throttle and steering angle</em>. So if we model the reward in a way that the car stays on the track and maximizes its velocity, we’re done!</p><figure><div><div><div><p><img src="https://miro.medium.com/max/60/1*vhcFaoSKKU4hLsnCVqY-8g.png?q=20" width="616" height="537" role="presentation"></p><p><img width="616" height="537" role="presentation"></p></div></div></div><figcaption data-selectable-paragraph=""><a href="https://github.com/hill-a/stable-baselines" target="_blank" rel="noopener nofollow">Stable-Baselines: an easy to use reinforcement learning library</a></figcaption></figure><p id="6bd7" data-selectable-paragraph="">That is the beauty of <a href="https://spinningup.openai.com/en/latest/" target="_blank" rel="noopener nofollow">reinforcement learning</a>, you need very little assumption (here only designing a reward function) and it will optimize directly what you want (go fast on the track to win the race!).</p><p id="2db0" data-selectable-paragraph="">Note: This is not the first blog post about reinforcement learning on a small self-driving car, but compared to <a href="https://flyyufelix.github.io/2018/09/11/donkey-rl-simulation.html" target="_blank" rel="noopener nofollow">previous approaches</a>,<em> the presented technique takes only minutes</em><strong> </strong>(and not hours) to learn a good and smooth control policy (~5 to 10 minutes for a smooth controller, ~20 minutes for a very smooth one).</p><p id="08ad" data-selectable-paragraph="">Now that we have briefly presented what is RL, we will go into the details, starting by dissecting the Wayve.ai approach, the base of our method.</p><h2 id="1c59" data-selectable-paragraph="">Learning to Drive in a Day — Key Elements of Wayve.ai Approach</h2><p id="1a37" data-selectable-paragraph=""><a href="https://wayve.ai/blog/learning-to-drive-in-a-day-with-reinforcement-learning" target="_blank" rel="noopener nofollow">Wayve.ai</a> describes a method to train a self-driving car in the real world on a simple road. This method is composed of several key elements.</p><figure><div><div><div><p><img src="https://miro.medium.com/max/60/1*tf2jFvzymcIakY3vfaXoag.jpeg?q=20" width="480" height="360" role="presentation"></p><p><img width="480" height="360" role="presentation"></p></div></div></div><figcaption data-selectable-paragraph=""><a href="https://www.youtube.com/watch?v=eRwTbRtnT1I" target="_blank" rel="noopener nofollow">Wayve.ai approach</a>: learning to drive in a day</figcaption></figure><p id="8848" data-selectable-paragraph="">First, they <em>train a feature extractor</em> (here a Variational Auto-Encoder or <em>VAE</em>) to compress the image to a lower dimensional space. The model is trained to reconstruct the input image but contains a bottleneck that forces it to compress the information.</p><p id="7dfd" data-selectable-paragraph=""><strong>This step of extracting relevant information from raw data is called </strong><a href="https://github.com/araffin/robotics-rl-srl" target="_blank" rel="noopener nofollow"><strong>State Representation Learning (SRL)</strong></a>, and was my main <a href="https://openreview.net/forum?id=Hkl-di09FQ" target="_blank" rel="noopener nofollow">research topic</a>. That notably allows to reduce the search space and therefore accelerate training. Below is a diagram that shows the connection between SRL and end-to-end reinforcement learning, that is to say, learn directly the control policy from pixels.</p><p id="e4e1" data-selectable-paragraph="">Note: training an auto-encoder <a href="https://arxiv.org/pdf/1802.04181.pdf" target="_blank" rel="noopener nofollow">is not the only solution</a> to extract useful features, you can also train for instance an <a href="https://github.com/araffin/srl-zoo" target="_blank" rel="noopener nofollow">inverse dynamics model</a>.</p><figure><div><div><div><div><p><img src="https://miro.medium.com/max/60/1*1y7Z_TxH7j6fDhSQexl1Iw.png?q=20" width="789" height="412" role="presentation"></p><p><img width="789" height="412" role="presentation"></p></div></div></div></div><figcaption data-selectable-paragraph=""><a href="https://openreview.net/forum?id=Hkl-di09FQ" target="_blank" rel="noopener nofollow">Decoupling Feature Extraction from Policy Learning</a></figcaption></figure><p id="64e8" data-selectable-paragraph="">The second key element is the use of an RL algorithm named <a href="https://stable-baselines.readthedocs.io/en/master/modules/ddpg.html" target="_blank" rel="noopener nofollow">Deep Deterministic Policy Gradient (DDPG)</a>, which learns a control policy using the VAE features as input. This policy is updated after each episode. One important aspect of the algorithm is that <em>it has a memory, called replay buffer</em><strong>, </strong>where its interactions with its environment are recorded and can be “replayed” afterward. So, even when the car does not interact with the world, it can sample experience from this buffer to update its policy.</p><p id="dc05" data-selectable-paragraph="">The car is trained to maximize the number of meters traveled before human intervention. And that is the final key ingredient: the human operator ends the episode as soon as the car starts going off the road. This<strong> </strong><em>early termination</em> is really important (as shown by <a href="https://xbpeng.github.io/projects/DeepMimic/index.html" target="_blank" rel="noopener nofollow">Deep Mimic</a>) and prevents the car from exploring regions that are not interesting to solve the task.</p></div></div></section><section><div><div><h2 id="ef78" data-selectable-paragraph="">Learning to Drive in Minutes — The Updated Approach</h2><p id="99a7" data-selectable-paragraph="">Although Wayve.ai technique may work in principle, it has some issues that needs to be addressed to apply it to a self-driving RC car.</p><p id="b570" data-selectable-paragraph="">First, because the feature extractor (VAE) is trained after each episode, the distribution of features is not stationary. That is to say, <em>the features are changing over time</em> and can lead to instabilities in the policy training. Also, training a VAE on a laptop (without a GPU) is quite slow, so we would like to avoid retraining the VAE after each episode.</p><p id="709c" data-selectable-paragraph="">To address those two problems, I decided to <em>train a VAE beforehand</em> and used Google <a href="https://colab.research.google.com/drive/1mF2abRb_yi4UNqYXVBF-t4FuCy6fl1c1#scrollTo=9bIR_N7R11XI" target="_blank" rel="noopener nofollow">Colab notebook</a> to preserve my computer. That way, the policy is trained with a fixed feature extractor.</p><p id="0912" data-selectable-paragraph="">In the image below, we <a href="https://github.com/araffin/srl-zoo" target="_blank" rel="noopener nofollow">explore what the VAE has learned</a>. We navigate in its latent space (using the sliders) and observe the reconstructed image.</p><figure><div><div><div><p><img src="https://miro.medium.com/freeze/max/60/1*EvSBN0zynU-RuWbhXWfdhQ.gif?q=20" width="600" height="381" role="presentation"></p><p><img width="600" height="381" role="presentation"></p></div></div></div><figcaption data-selectable-paragraph=""><a href="https://github.com/araffin/srl-zoo" target="_blank" rel="noopener nofollow">Exploring the latent space</a> learned by the VAE</figcaption></figure><p id="37d4" data-selectable-paragraph="">Then, DDPG is known to be <a href="https://arxiv.org/abs/1709.06560" target="_blank" rel="noopener nofollow">unstable</a> (in the sense that its performance can drop catastrophically during training) and is quite hard to tune. Fortunately, a recent algorithm named <a href="https://stable-baselines.readthedocs.io/en/master/modules/sac.html" target="_blank" rel="noopener nofollow"><strong>Soft Actor-Critic</strong></a><strong> (SAC) has equivalent performances and is much easier to tune</strong>*.</p><p id="5066" data-selectable-paragraph="">*during my experiments, I tried <a href="https://stable-baselines.readthedocs.io/en/master/modules/ppo2.html" target="_blank" rel="noopener nofollow">PPO</a>, SAC and DDPG. DDPG and SAC were giving the best results in few episodes but SAC was simpler to tune.</p><p id="58a1" data-selectable-paragraph="">For this project, I used the Soft Actor-Critic (SAC) implementation I wrote for <a href="https://github.com/hill-a/stable-baselines" target="_blank" rel="noopener nofollow">stable-baselines</a><strong> </strong>(if you are working with RL, I definitely recommend you to take a look ;) ), that has the <a href="https://bair.berkeley.edu/blog/2018/12/14/sac/" target="_blank" rel="noopener nofollow">latest improvements</a> of the algorithm in it.</p><p id="8221" data-selectable-paragraph="">Finally, I updated the reward function and the action space to smooth the control and maximize the speed.</p><h2 id="4a3b" data-selectable-paragraph="">Reward Function: Go Fast but Stay on the Track!</h2><p id="1d1f" data-selectable-paragraph="">The robot car does not have any odometry (nor speed sensor), so the number of meters traveled (nor speed) cannot be used as a reward.</p><p id="077d" data-selectable-paragraph="">Therefore, I decided to give a “life bonus” at each time-step (i.e., a +1 reward for staying on the track) and penalize the robot, using a <strong>“</strong>crash penalty<strong>”</strong> (-10 reward) for leaving the track. Additionally, I found it beneficial to also punish the car for getting off the road too fast: an additional negative reward proportional to the throttle is added to the crash penalty.</p><p id="38ef" data-selectable-paragraph="">Finally, because<strong> </strong>we want to go fast as it is a racing car, I added a “throttle bonus” proportional to the current throttle.<strong> </strong>That way, <strong>the robot will try to stay on the track and maximize its speed at the same time</strong>.</p><p id="d7bc" data-selectable-paragraph="">To sum it up:</p><figure><div><div><div><div><p><img src="https://miro.medium.com/max/60/1*Yay32inVCySHAV9sspqQAw.png?q=20" width="1090" height="104" role="presentation"></p><p><img width="1090" height="104" role="presentation"></p></div></div></div></div></figure><p id="19e3" data-selectable-paragraph="">where w1 and w2 are just constant that allows to balance the objectives (with w1 &lt;&lt; 10 and w2 &lt;&lt; 1 because they are secondary objectives)</p><h2 id="c2ab" data-selectable-paragraph="">Avoiding Shaky Control: Learning to Drive Smoothly</h2><blockquote><p id="a09b" data-selectable-paragraph="">The world is not really stochastic. If you’ve noticed — a robot doesn’t just spontaneously start shaking. Unless you hook up an RL algorithm to it. — Emo Todorov</p></blockquote><figure><div><div><div><p><img src="https://miro.medium.com/freeze/max/60/1*20LZ6UZUHUjaCarRgNHfEw.gif?q=20" width="640" height="242" role="presentation"></p><p><img width="640" height="242" role="presentation"></p></div></div></div><figcaption data-selectable-paragraph="">Left: Shaky Control — Right: Smooth Control using the proposed technique</figcaption></figure><p id="f8ad" data-selectable-paragraph="">If you apply the presented approach so far, it will work: the car will stay on the track and will try to go fast. However, <strong>you will probably end up with a shaky control: </strong>the car will oscillate as shown in the above image, because it has no incentive not to do so, it just tries to maximize its reward.</p><p id="9a5a" data-selectable-paragraph=""><strong>The solution to smooth the control is to constrain the change in steering angle while augmenting the input with the history of previous commands </strong>(steering and throttle). That way, <em>you impose continuity</em> in the steering.</p><p id="4247" data-selectable-paragraph="">As an example, if the current car steering angle is 0° and it tries suddenly to steer at 90°, the continuity constrain will only allow it to steer at 40° for instance. Hence, the difference between two consecutive steering commands stays in a given range. This additional constrain comes at a cost of a little more training.</p><p id="97e2" data-selectable-paragraph="">I passed several days trying to solve that issue before finding a satisfying solution<strong>, so here is what I tried that did not work</strong>:</p><ul><li id="b6a0" data-selectable-paragraph="">output relative steering instead of absolute steering: produces oscillations with lower frequency</li><li id="37a6" data-selectable-paragraph="">add a continuity penalty (penalize the robot for high changes in steering): the robot does not optimize the right thing, it works sometimes but then don’t stay on the track. If the cost on that penalty is too low, it just ignores it.</li><li id="1c2e" data-selectable-paragraph="">limit the maximum steering: the car cannot stay on the track anymore in the sharpest turns</li><li id="2fad" data-selectable-paragraph=""><a href="https://danieltakeshi.github.io/2016/11/25/frame-skipping-and-preprocessing-for-deep-q-networks-on-atari-2600-games/" target="_blank" rel="noopener nofollow">stack several frames </a>to give some velocity information: produces oscillation of lower frequency</li></ul><p id="9d09" data-selectable-paragraph="">Note: recently, <a href="http://robotics.sciencemag.org/content/4/26/eaau5872" target="_blank" rel="noopener nofollow">researchers from ETH Zurich</a> suggested to use curriculum learning to have continuous and energy-efficient control. This could be a second solution (although a bit harder to tune).</p><h2 id="5df6" data-selectable-paragraph="">Recap of the Approach</h2><p id="dfb3" data-selectable-paragraph="">In our approach, we decouple policy learning from feature extraction and add an additional constrain to smooth the control.</p><p id="4f61" data-selectable-paragraph="">First, a human collects data by driving manually the car (10k images in ~5 minutes of manual driving). Those images are used to train a VAE.</p><p id="59e4" data-selectable-paragraph="">Then, we alternate between exploration episodes (a stochastic policy is used) and policy training (done when the human puts the car back on the track to optimize the time spent).</p><p id="a2eb" data-selectable-paragraph="">For training the policy, the images are first encoded using a VAE (here with a latent space of dimension 64) and concatenated with an history of the last ten actions taken (throttle and steering) creating a 84D feature vector.</p><p id="3691" data-selectable-paragraph="">The control policy is represented by a neural network (2 fully-connected layers of 32 and 16 units, with ReLU or ELU activation function).</p><p id="5efc" data-selectable-paragraph="">This controller outputs steering angle and throttle. We constrain the throttle to be in a given range and also limits the difference between current and previous steering angle.</p><h2 id="d18f" data-selectable-paragraph="">Conclusion</h2><p id="0914" data-selectable-paragraph="">In this article, we have presented an approach to learn a smooth control policy for the Donkey Car in minutes, using only a camera.</p><p id="aeb4" data-selectable-paragraph="">As this method is designed to be applied in the real world, this is definitely my next step in this project: test the approach on a real RC car* (see below). This will require to shrink the VAE model (the policy network is already quite small) in order to make it run on a raspberry pi.</p><p id="f5af" data-selectable-paragraph="">That’s all for today, don’t hesitate to test the code, comment or ask questions, and remember, sharing is caring ;)!</p><p id="91a9" data-selectable-paragraph="">*the wayve.ai approach was <a href="https://www.youtube.com/watch?v=6JUjDw9tfD4" target="_blank" rel="noopener nofollow">reproduced by Roma Sokolkov on a real RC car</a>, however that does not include the latest improvements for smooth control</p><figure><div></div></figure><h2 id="793a" data-selectable-paragraph="">Acknowledgments</h2><p id="fecb" data-selectable-paragraph="">This work would not have been possible without <a href="https://github.com/r7vme/learning-to-drive-in-a-day" target="_blank" rel="noopener nofollow">Roma Sokolkov</a>’s re-implementation of Wayve.ai approach, <a href="https://github.com/tawnkramer" target="_blank" rel="noopener nofollow">Tawn Kramer</a>’s Donkey Car simulator, <a href="https://flyyufelix.github.io/2018/09/11/donkey-rl-simulation.html" target="_blank" rel="noopener nofollow">Felix Yu</a>’s blog post for inspiration, <a href="https://github.com/hardmaru/WorldModelsExperiments" target="_blank" rel="noopener nofollow">David Ha</a> for his VAE implementation, <a href="https://github.com/hill-a/stable-baselines" target="_blank" rel="noopener nofollow">Stable-Baselines</a> and its <a href="https://github.com/araffin/rl-baselines-zoo" target="_blank" rel="noopener nofollow">model zoo</a> for SAC implementation and training scripts, the <a href="https://github.com/sergionr2/RacingRobot" target="_blank" rel="noopener nofollow">Racing Robot project</a> for the teleoperation and the <a href="https://github.com/araffin/robotics-rl-srl" target="_blank" rel="noopener nofollow">S-RL Toolbox</a> for VAE debugging and training tools.</p><p id="ba86" data-selectable-paragraph="">I would like also to thanks Roma, Sebastian, Tawn, Florence, Johannes, Jonas, Gabriel, Alvaro, Arthur and Sergio for the feedbacks.</p><h2 id="d47b" data-selectable-paragraph="">Appendix: Learning a State Representation</h2><p id="f09f" data-selectable-paragraph=""><em>Influence of the latent space dimension and number of samples</em></p><p id="c032" data-selectable-paragraph="">The latent space dimension of the VAE just need to be big enough so the VAE manages to reconstruct the important part of the input image. For instance, there were no huge differences in the resulting control policy between a 64D and 512D VAE.</p><p id="d900" data-selectable-paragraph="">One important thing is not really the number of samples but rather the diversity and the representativeness of samples. If your training images does not cover all the environment diversity, then you need more samples.</p><p id="52de" data-selectable-paragraph=""><em>Can we learn a control policy from random features?</em></p><p id="e971" data-selectable-paragraph="">I tried to fix the weights of the VAE right after initialization and then learn a policy on those random features. However, that did not work.</p><p id="935a" data-selectable-paragraph=""><em>Comparison with learning from pixels</em></p><p id="9528" data-selectable-paragraph="">I did not have the time (because my laptop does not have a GPU) to compare the approach from learning the policy directly from pixels. However, I’ll be interested in the results if someone could do that using my codebase.</p><p id="0ae2" data-selectable-paragraph=""><em>What is the minimal policy that works?</em></p><p id="b636" data-selectable-paragraph="">A one-layer mlp works. I tried also with a linear policy, however, I did not succeed to obtain a good controller.</p></div></div></section></div></div>
    </div>
    <footer>
        <div>created by buildstarted &copy; 2020 <a href="/about">about</a></div>
        <div>Share this page on social media: copy and paste this url https://linksfor.dev/</div>
        <div>If you prefer RSS: <a href="https://linksfor.dev/feed.xml">https://linksfor.dev/feed.xml</a></div>
    </footer>
    
</body>
</html>