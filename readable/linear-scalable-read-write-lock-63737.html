<!DOCTYPE html>
<html lang="en">
<head>
    <title>
Linear scalable read-write lock - linksfor.dev(s)    </title>
    <meta charset="utf-8">
    <link rel="alternate" type="application/rss+xml" title="Linksfor.dev(s) feed" href="https://linksfor.dev/feed.rss" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta name="google" value="notranslate">
    <link rel="stylesheet" type="text/css" href="/style.css" />
    <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon">
    <link rel="icon" href="/favicon.ico" type="image/x-icon">
    <meta property="og:title" content="Linear scalable read-write lock - linksfor.dev(s)"/>
    <meta property="article:author" content="Lu Pan"/>
    <meta property="og:description" content="The basic concept of a read-write lock is simple. It allows multiple readers to&#xA;access the resource simultaneously, but at most one thread can have exclusive&#xA;ownership of the lock (a.k.a write lock). It&#x27;s supposed to be an optimization,&#xA;comparing to simple mutex (e.g. std::mutex). As in theory, multiple readers do&#xA;not contend. Naively, read performance should scale linearly. However, this is&#xA;not the case in practice for most read-write locks, due to cacheline&#xA;pingpong-ing.&#xA;&#xA;CPU cacheLet&#x27;s say N "/>
    <meta property="og:type" content="website"/>
    <meta property="og:url" content="https://blog.the-pans.com/shared-mutex/"/>

<meta property="og:site_name" content="linksfor.dev(s)" />
</head>
<body>
    <div class="devring" style="background: #222">
        <div style="text-align:center">Explore other dev related sites in this ring. If you would like to join this ring <a href="https://devring.club">click here</a>.</div>
        <div class="grid">
            <div style="display: grid; grid-template-columns: .5fr 1fr 1fr 1fr; text-align: center;">
                <span class="devring-title"><a href="https://devring.club/">devring.club</a></span>
                <a href="https://devring.club/sites/1/prev" class="devring-previous">Previous</a>
                <a href="https://devring.club/random" class="devring-random">Random</a>
                <a href="https://devring.club/sites/1/next" class="devring-next">Next</a>
            </div>
        </div>
    </div>
    <div class="grid">
        <h1>
<a href="/" style="color:inherit">linksfor.dev(s)</a>
        </h1>
        <title>linksfor.dev(s) - Linear scalable read-write lock</title>
<div class="readable">
        <h1>Linear scalable read-write lock</h1>
            <div>by Lu Pan</div>
            <div>Reading time: 7-8 minutes</div>
        <div>Posted here: 03 Jul 2020</div>
        <p><a href="https://blog.the-pans.com/shared-mutex/">https://blog.the-pans.com/shared-mutex/</a></p>
        <hr/>
<div id="readability-page-1" class="page"><section>
				<!--kg-card-begin: markdown--><p>The basic concept of a read-write lock is simple. It allows multiple readers to access the resource simultaneously, but at most one thread can have exclusive ownership of the lock (a.k.a write lock). It's supposed to be an optimization, comparing to simple mutex (e.g. <code>std::mutex</code>). As in theory, multiple readers do not contend. Naively, read performance should scale linearly. However, this is not the case in practice for most read-write locks, due to cacheline pingpong-ing.</p>
<!--kg-card-end: markdown--><figure><img src="https://blog.the-pans.com/content/images/2020/07/Screen-Shot-2020-07-01-at-2.21.45-PM.png"><figcaption>CPU cache</figcaption></figure><!--kg-card-begin: markdown--><p>Let's say N threads are running on N cores, all trying to get shared ownership of the mutex (read lock). Because we need to keep a count of the number of active readers (to exclude writers), each <code>lock_shared()</code> call needs to mutate the mutex state.</p>
<!--kg-card-end: markdown--><figure><img src="https://blog.the-pans.com/content/images/2020/07/Screen-Shot-2020-07-01-at-2.27.11-PM.png"></figure><p>And CPU needs to keep cache coherent. It needs to invalidate cachelines in other cores of the same address. </p><figure><img src="https://blog.the-pans.com/content/images/2020/07/Screen-Shot-2020-07-01-at-2.29.29-PM.png"></figure><p>Now if Core 1 wants to get the lock in shared mode, it will get a cache miss. An L1 cache hit takes about 4 CPU cycles. Main memory access usually takes 20x more CPU cycles if not more. In Jeff Dean's famous latency table, L1 cache hit takes about 0.5 ns, and main memory access is about 100 ns. In this way, the performance doesn't scale linearly as you throw more readers to the data. By increasing the number of threads, you often end up with 10x power assumption and maybe 20% performance improvment over single reader. </p><!--kg-card-begin: markdown-->
<p><code>folly</code> is Facebook's C++ open source library. <a href="https://github.com/facebook/folly/blob/master/folly/SharedMutex.h"><code>folly::SharedMutex</code></a> solves this problem by storing active readers in a global static storage. It stripes the shared storage, so each reader takes a whole cacheline. It tries to have a sticky routing from a given core to the slot in the shared memory for reducing search overhead. Each slot stores the address of the mutex. In this way, when another reader comes in, it only needs to mutate the shared storage, and leaves the mutex state unchanged.</p>
<p>Voil√†! No more cacheline pingpong-ing!</p>
<!--kg-card-end: markdown--><figure><img src="https://blog.the-pans.com/content/images/2020/07/Screen-Shot-2020-07-01-at-2.47.35-PM.png"></figure><!--kg-card-begin: markdown--><p>However, it does come with a downside. Because <em>all</em> deferred readers share the same block of static memory, this means</p>
<blockquote>
<p>acquisition of any lock in write mode can conflict with acquisition of any lock in shared mode<br>
(<a href="https://github.com/facebook/folly/blob/master/folly/SharedMutex.h">https://github.com/facebook/folly/blob/master/folly/SharedMutex.h</a>)</p>
</blockquote>
<p>E.g. A writer, of a given mutex, might have most of the deferred reader array pre-fetched in memory, but readers of other mutex might cause the cacheline to be invalidated. And most of the time the write and the read are not even contending on the same lock object. But the cacheline gets invalidated anyway.</p>
<p>The benefit of SharedMutex is most significant when there are many concurrent readers, and there are actual contention. For cases where actual concurrent reads is rare, spin lock works better. That's why <code>folly::SharedMutex</code> actually degenerates to an inlined read-write lock (which stores the reader count inline) when there are no concurrent readers detected. This is done dynamically at runtime.</p>
<h2 id="details">Details</h2>
<p>The devil is in the details.</p>
<h4 id="howtoblock">How to block</h4>
<p>How do we block when we need to, E.g. reader waiting for an active writer, or writer waiting for multiple outstanding readers? You don't want to call <a href="https://man7.org/linux/man-pages/man2/futex.2.html">FUTEX_WAIT</a> every time you block, as system calls are expensive. <code>folly::SharedMutex</code> blocks in following order</p>
<ol>
<li>spin with <code>pause</code> instruction</li>
<li><code>std::this_thread::yield()</code></li>
<li>FUTEX_WAIT</li>
</ol>
<p>If there's no contention or the contention is short lived, spinning wait is the fastest, with no system calls. <code>PAUSE</code> instruction is used to avoid expensive pipeline flush when exiting the loop. (<a href="https://c9x.me/x86/html/file_module_x86_id_232.html">https://c9x.me/x86/html/file_module_x86_id_232.html</a>)</p>
<p>If it exhausts the spinning wait tries, it moves on to <a href="https://linux.die.net/man/2/sched_yield">sched_yield()</a>. It will give the other thread opportunities to release the lock and avoid burning through too many CPU cycles busy waiting.</p>
<p>At the very end, it calls FUTEX_WAIT if both spin-wait and yield are exhausted. All the states related to a given mutex is encoded in the 32bit futex and the external deferred reader array.</p>
<h4 id="howlockunlockworks">How lock/unlock works</h4>
<p>All the synchronizations required are done based on a single 32bit atomic futex. A common pattern of how lock works looks like:</p>
<ol>
<li>block until <code>state</code> shows no contention with <code>acquire</code> semantic</li>
<li>if the lock is inlined, perform CAS on the atomic state. Only return, when CAS succeeds (except this is try_lock, or timeout is allowed).</li>
<li>if the lock is not inlined
<ul>
<li>if not already set, perform CAS on the atomic state to block other exclusive operations (e.g. set the "has_deferred_reader" bit in the state for <code>lock_shared()</code>, if not already set)</li>
<li>create deferred reader slot in global static memory</li>
<li>read state again with <code>acquire</code> semantic</li>
<li>if the "has_deferred_reader" bit is still set, return success</li>
<li>retry the whole operation</li>
</ul>
</li>
</ol>
<p>Setting the "has_deferred_reader" bit will block the acquisition of the lock in exclusive mode. However it's possible that after a deferred reader slot is written, the previous last deferred reader exists the lock and clears the "has_deferred_reader" bit. Or even worse, some writer might have even started acquiring the lock in exclusive mode. That's why we need to read the atomic state again to check the "has_deferred_reader" before we can return from <code>lock_shared()</code>. If we have the deferred reader registered, and "has_deferred_reader" is set in the state, it's impossible for another writer to successfully get exclusive ownership of the lock. So we can safely return.</p>
<p>For <code>lock()</code> (lock exclusive), it sometimes needs to wait for all the deferred readers to exit the lock. But for deferred readers, we are not keeping a refcount inline anymore. So <code>folly::SharedMutex</code> in this case, would move all the deferred readers back inline, and then <code>FUTEX_WAIT</code> on the state until the refcount reaches zero.</p>
<p>Unlock just calls <code>FUTEX_WAKE</code>. But again, the devil is in the details. Whom do we wake up? How many threads do we wake up?</p>
<h4 id="whomtowakeup">Whom to wake up</h4>
<p>If there are multiple writers waiting for the mutex, when the last reader exits, it's most efficient to just wake up one writer instead of all of them. This is done using a special bit from the atomic state. Only the first writer waiter will set this bit, and only this thread will be waken up.</p>
<p>On the other hand, if there are multiple readers waiting for the active writer to exit the lock, all the them will be waken up at once.</p>
<h4 id="whatifwerunoutofslots">What if we run out of slots</h4>
<p>If no slots are available, readers are inlined. They are refcounted inside the 32bit atomic futex.</p>
<h2 id="benchmark">Benchmark</h2>
<p>You can run the <a href="https://github.com/facebook/folly/blob/master/folly/test/SharedMutexTest.cpp">benchmark</a> on your own hardware. On my machine (32KB L1 cache per core, dual-socket, 24 cores with hyperthreading enabled), <code>folly::SharedMutex</code> is better than other mutexes in most of the cases, and in some cases significantly faster. When it's slower than other mutexes, it's usually only few ns slower.</p>
<!--kg-card-end: markdown--><figure><img src="https://blog.the-pans.com/content/images/2020/07/Screen-Shot-2020-07-01-at-4.57.26-PM.png"><figcaption>benchmark</figcaption></figure>
			</section></div></div>
    </div>
    <footer>
        <div>created by buildstarted &copy; 2020 <a href="/about">about</a></div>
        <div>Share this page on social media: copy and paste this url https://linksfor.dev/</div>
        <div>If you prefer RSS: <a href="https://linksfor.dev/feed.xml">https://linksfor.dev/feed.xml</a></div>
        <div>Customer satisfaction guaranteed to be optional.</div>
    </footer>
    
    <script async defer>
        _dna = window._dna || {};
        _dna.siteId = "linksfor.devs";
        _dna.outlink = true;

        (function () {
            let dna = document.createElement('script');
            dna.type = 'text/javascript';
            dna.async = true;
            dna.src = '//dna.buildstarted.com/t.js';
            let s = document.getElementsByTagName('script')[0];
            s.parentNode.insertBefore(dna, s);
        })();
    </script>
    <noscript><img src="//dna.buildstarted.com/g?siteId=linksfor.devs" /></noscript>
</body>
</html>