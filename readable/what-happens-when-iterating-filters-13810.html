<!DOCTYPE html>
<html lang="en">
<head>
    <title>
What happens when iterating filters? - linksfor.dev(s)    </title>
    <meta charset="utf-8">
    <link rel="alternate" type="application/rss+xml" title="Linksfor.dev(s) feed" href="https://linksfor.dev/feed.rss" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta name="google" value="notranslate">
    <link rel="stylesheet" type="text/css" href="/style.min.css" />
    <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon">
    <link rel="icon" href="/favicon.ico" type="image/x-icon">
    <meta property="og:title" content="What happens when iterating filters? - linksfor.dev(s)"/>
    <meta property="og:description" content="Casey Muratori posted on his blog about half-pixel interpolation filters this week, where he ends up focusing on a particular criterion: whether the filter in question is stable under repeated appl&#x2026;"/>
    <meta property="og:type" content="website"/>
    <meta property="og:url" content="https://fgiesen.wordpress.com/2019/04/08/what-happens-when-iterating-filters/"/>

<meta property="og:site_name" content="linksfor.dev(s)" />
</head>
<body>
    <div class="devring" style="background: #222">
        <div style="text-align:center">Explore other dev related sites in this ring. If you would like to join this ring <a href="https://devring.club">click here</a>.</div>
        <div class="grid">
            <div style="display: grid; grid-template-columns: .5fr 1fr 1fr 1fr; text-align: center;">
                <span class="devring-title"><a href="https://devring.club/">devring.club</a></span>
                <a href="https://devring.club/sites/1/prev" class="devring-previous">Previous</a>
                <a href="https://devring.club/random" class="devring-random">Random</a>
                <a href="https://devring.club/sites/1/next" class="devring-next">Next</a>
            </div>
        </div>
    </div>
    <div class="grid">
        <h1 style="margin: unset">
<a href="/" style="color:inherit">linksfor.dev(s)</a>
        </h1>
        <title>linksfor.dev(s) - What happens when iterating filters?</title>
<div class="readable">
        <h1>What happens when iterating filters?</h1>
            <div>Reading time: 7-9 minutes</div>
        <div>Posted here: 08 Apr 2019</div>
        <p><a href="https://fgiesen.wordpress.com/2019/04/08/what-happens-when-iterating-filters/">https://fgiesen.wordpress.com/2019/04/08/what-happens-when-iterating-filters/</a></p>
        <hr/>
<div id="readability-page-1" class="page"><div>
								<p>Casey Muratori posted on his blog about <a href="https://caseymuratori.com/blog_0035">half-pixel interpolation filters</a> this week, where he ends up focusing on a particular criterion: whether the filter in question is stable under repeated application or not.</p>
<p>There are many things about filters that are more an art than a science, especially where perceptual factors are concerned, but this particular question is both free of tricky perceptual evaluations and firmly in the realm of things we have excellent theory for, albeit one that will require me to start with a linear algebra infodump. So let’s get into it!</p>
<h3>Analyzing iterated linear maps</h3>
<p>Take any vector space V over some field <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BF%7D&amp;bg=f9f7f5&amp;fg=444444&amp;s=0" alt="\mathbb{F}" title="\mathbb{F}"> and any linear map <img src="https://s0.wp.com/latex.php?latex=T+%3A+V+%5Crightarrow+V&amp;bg=f9f7f5&amp;fg=444444&amp;s=0" alt="T : V \rightarrow V" title="T : V \rightarrow V"> from that space to itself. An <em>eigenvector</em> v of T is a nonzero element of V such that <img src="https://s0.wp.com/latex.php?latex=T%28v%29+%3D+Tv+%3D+%5Clambda+v&amp;bg=f9f7f5&amp;fg=444444&amp;s=0" alt="T(v) = Tv = \lambda v" title="T(v) = Tv = \lambda v"> for some <img src="https://s0.wp.com/latex.php?latex=%5Clambda+%5Cin+%5Cmathbb%7BF%7D&amp;bg=f9f7f5&amp;fg=444444&amp;s=0" alt="\lambda \in \mathbb{F}" title="\lambda \in \mathbb{F}"> – that is, the result of applying the map T to v is a scaled version of v itself. The scale factor λ is the corresponding <em>eigenvalue</em>.</p>
<p>Now when we’re iterating the map T multiple times, eigenvectors of T behave in a very simple way under the iterated map: we know that applying T to v gives back a scaled version of v, and then linearity of T allows us to conclude that:<br>
<img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+T%5E2%28v%29+%3D+T%28T%28v%29%29+%3D+T%28Tv%29+%3D+T%28%5Clambda+v%29+%3D+%5Clambda+T%28v%29+%3D+%5Clambda%5E2+v&amp;bg=f9f7f5&amp;fg=444444&amp;s=0" alt="\displaystyle T^2(v) = T(T(v)) = T(Tv) = T(\lambda v) = \lambda T(v) = \lambda^2 v" title="\displaystyle T^2(v) = T(T(v)) = T(Tv) = T(\lambda v) = \lambda T(v) = \lambda^2 v"><br>
and more generally <img src="https://s0.wp.com/latex.php?latex=T%5Ek%28v%29+%3D+%5Clambda%5Ek+v&amp;bg=f9f7f5&amp;fg=444444&amp;s=0" alt="T^k(v) = \lambda^k v" title="T^k(v) = \lambda^k v"> for any <img src="https://s0.wp.com/latex.php?latex=k+%5Cin+%5Cmathbb%7BN%7D&amp;bg=f9f7f5&amp;fg=444444&amp;s=0" alt="k \in \mathbb{N}" title="k \in \mathbb{N}">.</p>
<p>The best possible case is that we find lots of eigenvectors – enough to fully characterize the map purely by what it does on its eigenvectors. For example, if V is a finite-dimensional vector space with <img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7Bdim%7D%28V%29%3Dn&amp;bg=f9f7f5&amp;fg=444444&amp;s=0" alt="\mathrm{dim}(V)=n" title="\mathrm{dim}(V)=n">, then if we can find n linearly independent eigenvectors, we’re golden: we can select a basis entirely made of eigenvectors, and then written in that basis, T will have a very simple form: we will have <img src="https://s0.wp.com/latex.php?latex=T+%3D+Q+%5CLambda+Q%5E%7B-1%7D&amp;bg=f9f7f5&amp;fg=444444&amp;s=0" alt="T = Q \Lambda Q^{-1}" title="T = Q \Lambda Q^{-1}"> where <img src="https://s0.wp.com/latex.php?latex=%5CLambda+%3D+%5Cmathrm%7Bdiag%7D%28%5Clambda_1%2C+%5Cdots%2C+%5Clambda_n%29&amp;bg=f9f7f5&amp;fg=444444&amp;s=0" alt="\Lambda = \mathrm{diag}(\lambda_1, \dots, \lambda_n)" title="\Lambda = \mathrm{diag}(\lambda_1, \dots, \lambda_n)"> for some Q (whose columns contain n linearly independent eigenvectors of T).</p>
<p>That is, in the right basis (made of eigenvectors), T is just a diagonal matrix, which is to say, a (non-uniform) scale. This makes analysis of repeated applications of T easy, since:<br>
<img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+T%5E2+%3D+Q+%5CLambda+Q%5E%7B-1%7D+Q+%5CLambda+Q%5E%7B-1%7D+%3D+Q+%5CLambda%5E2+Q%5E%7B-1%7D&amp;bg=f9f7f5&amp;fg=444444&amp;s=0" alt="\displaystyle T^2 = Q \Lambda Q^{-1} Q \Lambda Q^{-1} = Q \Lambda^2 Q^{-1}" title="\displaystyle T^2 = Q \Lambda Q^{-1} Q \Lambda Q^{-1} = Q \Lambda^2 Q^{-1}"><br>
and in general<br>
<img src="https://s0.wp.com/latex.php?latex=T%5Ek+%3D+Q+%5CLambda%5Ek+Q%5E%7B-1%7D&amp;bg=f9f7f5&amp;fg=444444&amp;s=0" alt="T^k = Q \Lambda^k Q^{-1}" title="T^k = Q \Lambda^k Q^{-1}"> and <img src="https://s0.wp.com/latex.php?latex=%5CLambda%5Ek+%3D+%5Cmathrm%7Bdiag%7D%28%5Clambda_1%5Ek%2C+%5Cdots%2C+%5Clambda_n%5Ek%29&amp;bg=f9f7f5&amp;fg=444444&amp;s=0" alt="\Lambda^k = \mathrm{diag}(\lambda_1^k, \dots, \lambda_n^k)" title="\Lambda^k = \mathrm{diag}(\lambda_1^k, \dots, \lambda_n^k)">: viewed in the basis made of eigenvectors, repeated application of T is just repeated scaling, and behaviour over lots of iterations ultimately just hinges on what the eigenvalues are.</p>
<p>Not every matrix can be written that way; ones that can are called <em>diagonalizable</em>. But there is a very important class of transforms (and now we allow infinite-dimensional spaces again) that is guaranteed to be diagonalizable: so called self-adjoint transforms. In the finite-dimensional real case, these correspond to symmetric matrices (matrices A such that <img src="https://s0.wp.com/latex.php?latex=A+%3D+A%5ET&amp;bg=f9f7f5&amp;fg=444444&amp;s=0" alt="A = A^T" title="A = A^T">). Such transforms are guaranteed to be diagonalizable, and even better, their eigenvectors are guaranteed to be pairwise orthogonal to each other, meaning the transform Q is an orthogonal matrix (a rotation or reflection), which among other things makes the whole process numerically quite well-behaved.</p>
<p>As a small aside, if you’ve ever wondered why iterative solvers for linear systems usually require symmetric (or, in the complex case, Hermitian) matrices: this is why. If a matrix is symmetric, it it diagonalizable, which allows us to build an iterative process to solve linear equations that we can analyze easily and <em>know</em> will converge (if we do it right). It’s not that we can’t possibly do anything iterative on non-symmetric linear systems; it just becomes a lot trickier to make any guarantees, especially if we allow arbitrary matrices. (Which could be quite pathological.)</p>
<p>Anyway, that’s a bit of background on eigendecompositions of linear maps. But what does any of this have to do with filtering?</p>
<h3>Enter convolution</h3>
<p>Convolution itself is a <em>bilinear map</em>, meaning it’s linear in both arguments. That means that if we fix either of the arguments, we get a linear map. Suppose we have a FIR filter f given by its coefficients <img src="https://s0.wp.com/latex.php?latex=%28f_0%2C+f_1%2C+%5Cdots%2C+f_%7Bm-1%7D%29&amp;bg=f9f7f5&amp;fg=444444&amp;s=0" alt="(f_0, f_1, \dots, f_{m-1})" title="(f_0, f_1, \dots, f_{m-1})">. Then we can define an associated linear map <img src="https://s0.wp.com/latex.php?latex=T_f&amp;bg=f9f7f5&amp;fg=444444&amp;s=0" alt="T_f" title="T_f"> on a suitable space, say something like <img src="https://s0.wp.com/latex.php?latex=T_f+%3A+%5Cell%5E%5Cinfty%28%5Cmathbb%7BC%7D%29+%5Crightarrow+%5Cell%5E%5Cinfty%28%5Cmathbb%7BC%7D%29&amp;bg=f9f7f5&amp;fg=444444&amp;s=0" alt="T_f : \ell^\infty(\mathbb{C}) \rightarrow \ell^\infty(\mathbb{C})" title="T_f : \ell^\infty(\mathbb{C}) \rightarrow \ell^\infty(\mathbb{C})"> (writing <img src="https://s0.wp.com/latex.php?latex=%5Cell%5E%5Cinfty%28%5Cmathbb%7BC%7D%29&amp;bg=f9f7f5&amp;fg=444444&amp;s=0" alt="\ell^\infty(\mathbb{C})" title="\ell^\infty(\mathbb{C})"> for the set of bounded sequences of complex numbers) by setting<br>
<img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+T_f%28x%29+%3D+T_f+x+%3A%3D+f+%2A+x&amp;bg=f9f7f5&amp;fg=444444&amp;s=0" alt="\displaystyle T_f(x) = T_f x := f * x" title="\displaystyle T_f(x) = T_f x := f * x">.</p>
<p>If this is all a bit dense on notation for you, all I’m doing here is holding one of the two arguments to the convolution operator constant, and trying to at least specify what set our map is working on (in this case, bounded sequences of real numbers).</p>
<p>And now we’re just about ready for the punchline: we now have a linear map from a set to itself, although in this case we’re dealing with infinite sequences, not finite ones. Luckily the notions of eigenvectors (eigensequences in this case) and eigenvalues generalize just fine. What’s even better is that for <em>all</em> discrete convolutions, we get a full complement of eigensequences, and we know exactly what they are. Namely, define the family of sequences <img src="https://s0.wp.com/latex.php?latex=e_%5Comega&amp;bg=f9f7f5&amp;fg=444444&amp;s=0" alt="e_\omega" title="e_\omega"> by:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+e_%5Comega%5Bn%5D+%3D+%5Cexp%28i+%5Comega+n%29+%3D+%5Ccos%28%5Comega+n%29+%2B+i+%5Csin%28%5Comega+n%29&amp;bg=f9f7f5&amp;fg=444444&amp;s=0" alt="\displaystyle e_\omega[n] = \exp(i \omega n) = \cos(\omega n) + i \sin(\omega n)" title="\displaystyle e_\omega[n] = \exp(i \omega n) = \cos(\omega n) + i \sin(\omega n)"></p>
<p>That’s a cosine wave with frequency ω in the real part and the corresponding sine wave in the imaginary part, if you are so inclined, although I much prefer to stick with the complex exponentials, especially when doing algebra (it makes things easier). Anyway, if we apply our FIR filter f to that signal, we get (this is just expanding out the definition of discrete convolution for our filter and input signal, using the convention that unqualified summation is over all values of k where the sum is well-defined)</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%28T_f+e_%5Comega%29%5Bn%5D+%3D+%5Csum_k+f_k+%5Cexp%28i+%5Comega+%28n-k%29%29&amp;bg=f9f7f5&amp;fg=444444&amp;s=0" alt="\displaystyle (T_f e_\omega)[n] = \sum_k f_k \exp(i \omega (n-k))" title="\displaystyle (T_f e_\omega)[n] = \sum_k f_k \exp(i \omega (n-k))"></p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%3D+%5Cexp%28i+%5Comega+n%29+%5Cunderbrace%7B%5Csum_k+f_k+%5Cexp%28-i+%5Comega+k%29%7D_%7B%3D%3A%5Chat%7Bf%7D%28%5Comega%29%7D&amp;bg=f9f7f5&amp;fg=444444&amp;s=0" alt="\displaystyle = \exp(i \omega n) \underbrace{\sum_k f_k \exp(-i \omega k)}_{=:\hat{f}(\omega)}" title="\displaystyle = \exp(i \omega n) \underbrace{\sum_k f_k \exp(-i \omega k)}_{=:\hat{f}(\omega)}"><br>
<img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%3D+%5Chat%7Bf%7D%28%5Comega%29+%5Cexp%28i+%5Comega+n%29&amp;bg=f9f7f5&amp;fg=444444&amp;s=0" alt="\displaystyle = \hat{f}(\omega) \exp(i \omega n)" title="\displaystyle = \hat{f}(\omega) \exp(i \omega n)"></p>
<p>There’s very little that happens here. The first line is just expanding the definition; then in the second line we use the properties of the exponential function (and the linearity of sums) to pull out the constant factor of <img src="https://s0.wp.com/latex.php?latex=%5Cexp%28i+%5Comega+n%29&amp;bg=f9f7f5&amp;fg=444444&amp;s=0" alt="\exp(i \omega n)" title="\exp(i \omega n)">. And it turns out the entire rest of the formula doesn’t depend on n at all, so it turns into a constant factor for the whole sequence. It does depend on f and ω, so we label it <img src="https://s0.wp.com/latex.php?latex=%5Chat%7Bf%7D%28%5Comega%29&amp;bg=f9f7f5&amp;fg=444444&amp;s=0" alt="\hat{f}(\omega)" title="\hat{f}(\omega)">. The final line states exactly what we wanted, namely that the result of applying <img src="https://s0.wp.com/latex.php?latex=T_f&amp;bg=f9f7f5&amp;fg=444444&amp;s=0" alt="T_f" title="T_f"> to <img src="https://s0.wp.com/latex.php?latex=e_%5Comega&amp;bg=f9f7f5&amp;fg=444444&amp;s=0" alt="e_\omega" title="e_\omega"> is just a scaled copy of <img src="https://s0.wp.com/latex.php?latex=e_%5Comega&amp;bg=f9f7f5&amp;fg=444444&amp;s=0" alt="e_\omega" title="e_\omega"> itself—we have an eigensequence (with eigenvalue <img src="https://s0.wp.com/latex.php?latex=%5Chat%7Bf%7D%28%5Comega%29&amp;bg=f9f7f5&amp;fg=444444&amp;s=0" alt="\hat{f}(\omega)" title="\hat{f}(\omega)">).</p>
<p>Also note that the formula for the eigenvalue isn’t particularly scary either in our case, since we’re dealing with a FIR filter f, meaning it’s a regular finite sum:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5Chat%7Bf%7D%28%5Comega%29+%3D+%5Csum_%7Bk%3D0%7D%5E%7Bm-1%7D+f_k+%5Cexp%28-i+%5Comega+k%29&amp;bg=f9f7f5&amp;fg=444444&amp;s=0" alt="\displaystyle \hat{f}(\omega) = \sum_{k=0}^{m-1} f_k \exp(-i \omega k)" title="\displaystyle \hat{f}(\omega) = \sum_{k=0}^{m-1} f_k \exp(-i \omega k)"></p>
<p>Oh, and there’s one more minor detail I’ve neglected to mention so far: that’s just the <a href="https://en.wikipedia.org/wiki/Discrete-time_Fourier_transform">discrete-time Fourier transform</a> (DTFT, not to be confused with the DFT, although they’re related) of f. Yup, we started out with a digital FIR filter, asked what happens when we iterate it a bunch, did a brief detour into linear algebra, and ended up in Fourier theory.</p>
<p>Long story short, if you want to know whether a linear digital filter is stable under repeated application, you want to look at its eigenvalues, which in turn are just given by its frequency response. In particular, for any given frequency ω, we have exactly three options:</p>

<p>The proof for all three cases is simply observing that k-fold application of the filter f to the signal <img src="https://s0.wp.com/latex.php?latex=e_%5Comega&amp;bg=f9f7f5&amp;fg=444444&amp;s=0" alt="e_\omega" title="e_\omega"> will result in the signal <img src="https://s0.wp.com/latex.php?latex=%28%5Chat%7Bf%7D%28%5Comega%29%29%5Ek+e_%5Comega&amp;bg=f9f7f5&amp;fg=444444&amp;s=0" alt="(\hat{f}(\omega))^k e_\omega" title="(\hat{f}(\omega))^k e_\omega">. To generalize this to a wider class of signals (not just complex exponentials) we would need to represent said signals as sum of complex exponentials, which is exactly what Fourier series are all about; so it can be done, but I won’t bother with the details here, since they’re out of the scope of this post.</p>
<p>Therefore, all you need to know about the stability of a given filter under repeated application is contained in its Fourier transform. I’ll try to do another post soon that shows the Fourier transforms of the filters Casey mentioned (or their magnitude response anyway, which is what we care about) and touches on other aspects such as the effect of rounding and quantization, but we’re at a good stopping point right now, so I’ll end this post here.</p>
											</div></div></div>
    </div>
    <footer>
        <div>created by <a href="https://buildstarted.com">buildstarted</a> &copy; 2020 <a href="/about">about</a></div>
        <div>Share this page on social media: copy and paste this url https://linksfor.dev/</div>
        <div>If you prefer RSS: <a href="https://linksfor.dev/feed.xml">https://linksfor.dev/feed.xml</a></div>
        <div>Customer satisfaction guaranteed to be optional.</div>
    </footer>
    
    <script async defer>
        _dna = window._dna || {};
        _dna.siteId = "linksfor.devs";
        _dna.outlink = true;

        (function() {
            let dna = document.createElement('script');
            dna.type = 'text/javascript';
            dna.async = true;
            dna.src = '//dna.buildstarted.com/t.js';
            let s = document.getElementsByTagName('script')[0];
            s.parentNode.insertBefore(dna, s);
        })();
    </script>
    <noscript><img src="//dna.buildstarted.com/g?siteId=linksfor.devs"/></noscript>
</body>
</html>