<!DOCTYPE html>
<html lang="en">
<head>
    <title>linksfor.dev(s)</title>
    <meta charset="utf-8">
    <link rel="alternate" type="application/rss+xml" title="Linksfor.dev(s) feed" href="https://linksfor.dev/feed.rss" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta name="google" value="notranslate">
    <link rel="stylesheet" type="text/css" href="/style.css" />
    <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon">
    <link rel="icon" href="/favicon.ico" type="image/x-icon">
    
    <meta property="article:author" content="Buildstarted"/>
    <meta property="og:site_name" content="linksfor.dev(s)" />
    <meta property="og:title" content="linksfor.dev(s)" />
    <meta property="og:type" content="website" />
    <meta property="og:description" content="A curated list of sources of development information including c#, c++, and other dev related links." />
</head>
<body>
    <div class="grid">
        <h1>
                <span style="cursor: default" title="linksfor.dev(s) has been running for 1 year! :partypopper:">ðŸŽ‰</span>
<a href="/" style="color:inherit">linksfor.dev(s)</a>
        </h1>
        <title>linksfor.dev(s) - dotnet/performance</title>
<div class="readable">
        <h1>dotnet/performance</h1>
        <p>
Reading time: 20-25 minutes        </p>
        <p><a href="https://github.com/dotnet/performance/blob/master/docs/microbenchmark-design-guidelines.md">https://github.com/dotnet/performance/blob/master/docs/microbenchmark-design-guidelines.md</a></p>
        <hr/>
<div id="readability-page-1" class="page"><div id="readme">
    <article itemprop="text">
<h2>General Overview</h2>
<ul>
<li>Choose a good name because you won't be able to change it after the first results are uploaded to the Reporting System.</li>
<li>Move initialization logic to a Setup method. Don't forget to clean up the resources in a corresponding Cleanup method.</li>
<li>If your benchmark needs input data, it should be always exactly the same data. Use <code>ValuesGenerator</code> to generate the data.</li>
<li>Benchmark should have one test case per one code path of the benchmarked code. Not more.</li>
<li>Focus on the most common use cases, not edge cases. Performance of error cases is almost never interesting.</li>
<li>Make sure the benchmark has no side-effects.</li>
<li>Return the result from the benchmark to prevent dead code elimination.</li>
<li>Try to avoid adding loops to your benchmark, BenchmarkDotNet is going to invoke the benchmark many times for you instead.</li>
</ul>
<h2>Table of Contents</h2>
<ul>
<li><a href="#Mindset">Mindset</a>
<ul>
<li><a href="#Benchmarks-are-not-Unit-Tests">Benchmarks are not Unit Tests</a></li>
<li><a href="#Benchmarks-are-Immutable">Benchmarks are Immutable</a></li>
</ul>
</li>
<li><a href="#BenchmarkDotNet">BenchmarkDotNet</a></li>
<li><a href="#Setup">Setup</a>
<ul>
<li><a href="#GlobalSetup">GlobalSetup</a></li>
<li><a href="#IterationSetup">IterationSetup</a></li>
<li><a href="#OperationsPerInvoke">OperationsPerInvoke</a></li>
</ul>
</li>
<li><a href="#Test-Cases">Test Cases</a>
<ul>
<li><a href="#Code-Paths">Code Paths</a>
<ul>
<li><a href="#Array.Reverse">Array.Reverse</a></li>
<li><a href="#Buffer.CopyMemory">Buffer.CopyMemory</a></li>
</ul>
</li>
<li><a href="#Always-the-same-input-data">Always the same input data</a></li>
<li><a href="#BenchmarkDotNet">BenchmarkDotNet</a>
<ul>
<li><a href="#Arguments">Arguments</a></li>
<li><a href="#Params">Params</a></li>
<li><a href="#ArgumentsSource">ArgumentsSource</a></li>
<li><a href="#Generic-benchmarks">Generic benchmarks</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#Best-Practices">Best Practices</a>
<ul>
<li><a href="#Single-Responsibility-Principle">Single Responsibility Principle</a></li>
<li><a href="#No-Side-Effects">No Side-Effects</a></li>
<li><a href="#Dead-Code-Elimination">Dead Code Elimination</a></li>
<li><a href="#Loops">Loops</a></li>
<li><a href="#Method-Inlining">Method inlining</a></li>
</ul>
</li>
</ul>
<h2>Mindset</h2>
<p>Writing Benchmarks is much different than writing Unit Tests. So before you start coding, you need to change your mindset first.</p>
<h3>Benchmarks are not Unit Tests</h3>
<p>When writing Unit Tests, we ideally want to test all methods and properties of the given type. We also test both the happy and unhappy paths. The result of every Unit Test run is a single value: passed or failed.</p>
<p>Benchmarks are different. First of all, the result of a benchmark run is never a single value. It's a whole distribution, described with values like mean, standard deviation, min, max and so on. To get a meaningful distribution, the benchmark has to be executed many, many times. <strong>This takes a lot of time</strong>. With the current <a href="https://github.com/dotnet/performance/blob/51d8f8483b139bb1edde97f917fa436671693f6f/src/harness/BenchmarkDotNet.Extensions/RecommendedConfig.cs#L17-L20">recommended settings</a> used in this repository, it takes on average six seconds to run a single benchmark.
The public surface of .NET Standard 2.0 API has tens of thousands of methods. If we had 1 benchmark for every public method, it would take two and a half days to run the benchmarks. Not to speak about the time it would take to analyze the results, filter the false positives, etc..</p>
<p>This is only one of the reasons why writing Benchmarks is different than writing Unit Tests.</p>
<p>The goal of benchmarking is to test the performance of all the methods that are frequently used (hot paths) and should be performant. <strong>The focus should be on the most common use cases, not edge cases</strong>.</p>
<h3>Benchmarks are Immutable</h3>
<p>The results of benchmark runs are exported to an internal Reporting System. Every benchmark is identified using the following <code>xUnit</code> ID pattern:</p>
<pre><code>namespace.typeName.methodName(paramName: paramValue)
</code></pre>
<p>To be able to track the performance over time, we <strong>must NOT change the ID</strong>. It means that every change to a namespace, type name, method name, parameter name|value is considered to be a breaking change and must be avoided if possible.</p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/dotnet/performance/blob/master/docs/img/benchmark_id_rename.png"><img src="https://github.com/dotnet/performance/raw/master/docs/img/benchmark_id_rename.png" alt="Rename"></a></p>
<p>The Reporting System is used to monitor the performance changes and detect regressions. If the body of the benchmark is changed, the old name is kept and the reported time increases, it's going to be marked as regression and require a dedicated performance investigation. We want to avoid such false alarms and hence we want to reduce the number of changes in existing benchmarks to a minimum.</p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/dotnet/performance/blob/master/docs/img/benchmark_id_regression.png"><img src="https://github.com/dotnet/performance/raw/master/docs/img/benchmark_id_regression.png" alt="Regression"></a></p>
<p>If you have some good reasons for changing the implementation of the benchmark you should remove the old one and introduce a new one with a new name.</p>
<h2>BenchmarkDotNet</h2>
<p>BenchmarkDotNet is the benchmarking harness used in this repository. If you are new to BenchmarkDotNet, you should read <a href="https://github.com/dotnet/performance/blob/master/docs/benchmarkdotnet.md">this introduction to BenchmarkDotNet</a>.</p>
<p>Key things that you need to remember:</p>
<ul>
<li>BenchmarkDotNet <strong>does not require the user to provide the number of iterations and invocations per iteration</strong>, it implements a smart heuristic based on standard error and runs the benchmark until the results are stable.</li>
<li>BenchmarkDotNet runs every benchmark in a separate process, process isolation allows avoiding side-effects. The more memory allocated by given benchmark, the bigger the difference for in-proc vs out-proc execution.</li>
<li>BenchmarkDotNet was designed to make accurate nano-benchmarks with repeatable results possible, to achieve that it does many things, including overhead calculation and subtraction (it benchmarks an empty method with the same signature and subtract the average value from results).</li>
<li>BenchmarkDotNet removes outliers by default (this repo is configured to remove only the upper outliers)</li>
<li>BenchmarkDotNet creates a type which derives from type with benchmarks. So the type with benchmarks must <strong>not</strong> be <strong>sealed</strong> and it can <strong>not</strong> be <strong>static</strong> and it has to be <strong>public</strong>. It also has to be a <code>class</code> (no structs support).</li>
</ul>
<p><strong>Note:</strong> If you are not sure what invocation or iteration means, please read <a href="https://benchmarkdotnet.org/articles/guides/how-it-works.html" rel="nofollow">this doc</a> that explains how BenchmarkDotNet works.</p>
<h2>Setup</h2>
<p>Let's write a benchmark that measures the performance of reversing an array of 1 000 integers:</p>
<div><pre>[<span>Benchmark</span>]
<span>public</span> <span>int</span>[] <span>Reverse</span>()
{
    <span>int</span>[] <span>array</span> <span>=</span> <span>Enumerable</span>.<span>Range</span>(<span>0</span>, <span>1_000</span>).<span>ToArray</span>();

    <span>Array</span>.<span>Reverse</span>(<span>array</span>);

    <span>return</span> <span>array</span>;
}</pre></div>
<p>Profile it using the <a href="https://github.com/dotnet/performance/blob/master/docs/benchmarkdotnet.md#Profiling">ETW Profiler</a>:</p>
<div><pre>dotnet run -c Release -f netcoreapp3.0 --filter *.Reverse --profiler ETW</pre></div>
<p>And open the produced trace file with <a href="https://github.com/Microsoft/perfview">PerfView</a>:</p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/dotnet/performance/blob/master/docs/img/setup_array_reverse_profiler.png"><img src="https://github.com/dotnet/performance/raw/master/docs/img/setup_array_reverse_profiler.png" alt="Regression"></a></p>
<p>As you can see, reversing the array took only <code>26.6%</code> of the benchmark execution time! The rest was spent on executing the array creation logic. What does it mean? It means that the given benchmark is measuring the performance of creating and reversing the array. Not just reversing the array.</p>
<p>This is why <strong>the initialization logic should be always separated from the benchmark</strong>.</p>
<h3>GlobalSetup</h3>
<p>A public method marked with <code>[GlobalSetup]</code> attribute is going to be executed <strong>exactly once</strong>, before running the benchmark for the first time.</p>
<div><pre><span>private</span> <span>int</span>[] <span>_array</span>;

[<span>GlobalSetup</span>]
<span>public</span> <span>void</span> <span>SetupReverse</span>() <span>=&gt;</span> <span>_array</span> <span>=</span> <span>Enumerable</span>.<span>Range</span>(<span>0</span>, <span>1000</span>).<span>ToArray</span>();

[<span>Benchmark</span>]
<span>public</span> <span>void</span> <span>Reverse</span>() <span>=&gt;</span> <span>Array</span>.<span>Reverse</span>(<span>_array</span>);</pre></div>
<p>Any method marked <code>[GlobalSetup]</code> which does not specify the <code>Target(s)</code> benchmark name(s) is executed for every benchmark declared in a given class.</p>
<p>In case you want to have a class with multiple benchmarks and multiple, dedicated setup methods you need to use <code>Target</code> or <code>Targets</code> properties of the setup attribute:</p>
<div><pre>[<span>GlobalSetup</span>(<span>Target</span> <span>=</span> <span>nameof</span>(<span>Reverse</span>))]
<span>public</span> <span>void</span> <span>SetupReverse</span>()

[GlobalSetup(Targets <span>=</span> <span>new</span> [] { <span>nameof</span>(<span>Array</span>), <span>nameof</span>(<span>Span</span>), <span>nameof</span>(<span>ReadOnlySpan</span>)})]
public void SetupArray()</pre></div>
<p><strong>Note:</strong> If you need to clean up resources after the benchmark run (an example of required cleanup would be any files created on the disk by the benchmark process), you should use the corresponding <code>[GlobalCleanup]</code> attribute. It's going to be executed only once, after all benchmark iterations.</p>
<h3>IterationSetup</h3>
<p>If your benchmark requires a clean state for every invocation, you need to use the <code>[IterationSetup]</code> attribute. Unfortunately, just using the <code>[IterationSetup]</code> attribute is not enough to get stable results. You also need to make sure that the benchmark itself performs enough of computations for a single invocation to run longer than 100ms. <strong>If you don't, your benchmark will be entirely invalid.</strong></p>
<div><pre>[<span>Params</span>(<span>1000</span> <span>*</span> <span>1000</span> <span>*</span> <span>200</span>)] <span><span>//</span> allows for stable iteration around 200ms</span>
<span>public</span> <span>int</span> <span>NumberOfBytes</span> { <span>get</span>; <span>set</span>; }

<span>private</span> <span>byte</span>[] <span>_source</span>, <span>_destination</span>;

[<span>GlobalSetup</span>]
<span>public</span> <span>void</span> <span>Setup</span>()
{
    <span>_source</span> <span>=</span> <span>ValuesGenerator</span>.<span>Array</span>&lt;<span>byte</span>&gt;(<span>NumberOfBytes</span>);
    <span>_destination</span> <span>=</span> <span>new</span> <span>byte</span>[<span>Base64</span>.<span>GetMaxEncodedToUtf8Length</span>(<span>NumberOfBytes</span>)];
}

[<span>IterationSetup</span>(<span>Target</span> <span>=</span> <span>nameof</span>(<span>Base64EncodeInPlace</span>))]
<span>public</span> <span>void</span> <span>SetupBase64EncodeInPlace</span>() <span>=&gt;</span> <span>Array</span>.<span>Copy</span>(<span>_source</span>, <span>_destination</span>, <span>_source</span>.<span>Length</span>);

[<span>Benchmark</span>]
<span>public</span> <span>OperationStatus</span> <span>Base64EncodeInPlace</span>() <span>=&gt;</span> <span>Base64</span>.<span>EncodeToUtf8InPlace</span>(<span>_destination</span>, <span>_source</span>.<span>Length</span>, <span>out</span> <span>_</span>);</pre></div>
<p><strong>Note:</strong> If you need to clean up resources after every benchmark run, you should use the corresponding <code>[IterationCleanup]</code> attribute. It's going to be executed once after every iteration.</p>
<p>If you want to get a better understanding of it, you should read <a href="https://aakinshin.net/posts/stopwatch/#pitfalls" rel="nofollow">this blog post</a> about stopwatch and follow the GitHub discussions in this <a href="https://github.com/dotnet/BenchmarkDotNet/issues/730">issue</a> and <a href="https://github.com/dotnet/BenchmarkDotNet/pull/760">PR</a>.</p>
<p><ins><strong>If using <code>[GlobalSetup]</code> is enough, you should NOT be using <code>[IterationSetup]</code></strong></ins></p>
<h3>OperationsPerInvoke</h3>
<p><code>[GlobalSetup]</code> and <code>[IterationSetup]</code> might not be enough if you want to setup some nano-benchmarks.</p>
<p>A good example is <code>Slicing</code> a <code>Span</code>. <code>Span</code> is a stack-only type, so we can not have a <code>[GlobalSetup]</code> method which writes it to a field of a <code>class</code>.</p>
<div><pre><span>class</span> <span>WontCompile</span>
{
    <span>Span</span>&lt;<span>byte</span>&gt; <span>_span</span>; <span><span>//</span> compiler error</span>

    [<span>GlobalSetup</span>]
    <span>public</span> <span>void</span> <span>Setup</span>() <span>=&gt;</span> <span>_span</span> <span>=</span> <span>new</span> <span>Span</span>&lt;<span>byte</span>&gt;(<span>new</span> <span>byte</span>[<span>10</span>]);
}</pre></div>
<p>The current implementation of BenchmarkDotNet does not allow to run benchmarks defined in structs (including <code>ref struct</code>).</p>
<p>If we write a following benchmark:</p>
<div><pre><span>private</span> <span>byte</span>[] <span>_nonEmptyArray</span> <span>=</span> <span>new</span> <span>byte</span>[<span>10</span>];

[<span>Benchmark</span>]
<span>public</span> <span>Span</span>&lt;<span>byte</span>&gt; <span>Slice</span>()
{
    <span>Span</span>&lt;<span>byte</span>&gt; <span>span</span> <span>=</span> <span>new</span> <span>Span</span>&lt;<span>byte</span>&gt;(<span>_nonEmptyArray</span>);

    <span>return</span> <span>span</span>.<span>Slice</span>(<span>span</span>.<span>Length</span> <span>/</span> <span>2</span>);
}</pre></div>
<p>The benchmark is not going to measure the performance of <code>Slice</code> operation, but similar to previous example of reversing an array it's going to measure the performance of creating a <code>Span</code> from <code>Array</code> and the <code>Slice</code> operation.</p>
<p>To solve this problem we can use <code>OperationsPerInvoke</code> property of the <code>[Benchmark]</code> attribute.</p>
<div><pre><span>private</span> <span>byte</span>[] <span>_nonEmptyArray</span> <span>=</span> <span>new</span> <span>byte</span>[<span>18</span>];

[<span>Benchmark</span>(<span>OperationsPerInvoke</span> <span>=</span> <span>16</span>)]
<span>public</span> <span>Span</span>&lt;<span>byte</span>&gt; <span>Slice</span>()
{
    <span>Span</span>&lt;<span>byte</span>&gt; <span>span</span> <span>=</span> <span>new</span> <span>Span</span>&lt;<span>byte</span>&gt;(<span>_nonEmptyArray</span>); <span><span>//</span> create it once</span>

    <span><span>//</span> perform OperationsPerInvoke-many operations</span>
    <span><span>//</span> without introducing a loop, which would add an extra overhead</span>
    <span>span</span> <span>=</span> <span>span</span>.<span>Slice</span>(<span>1</span>); <span>span</span> <span>=</span> <span>span</span>.<span>Slice</span>(<span>1</span>); <span>span</span> <span>=</span> <span>span</span>.<span>Slice</span>(<span>1</span>); <span>span</span> <span>=</span> <span>span</span>.<span>Slice</span>(<span>1</span>);
    <span>span</span> <span>=</span> <span>span</span>.<span>Slice</span>(<span>1</span>); <span>span</span> <span>=</span> <span>span</span>.<span>Slice</span>(<span>1</span>); <span>span</span> <span>=</span> <span>span</span>.<span>Slice</span>(<span>1</span>); <span>span</span> <span>=</span> <span>span</span>.<span>Slice</span>(<span>1</span>);
    <span>span</span> <span>=</span> <span>span</span>.<span>Slice</span>(<span>1</span>); <span>span</span> <span>=</span> <span>span</span>.<span>Slice</span>(<span>1</span>); <span>span</span> <span>=</span> <span>span</span>.<span>Slice</span>(<span>1</span>); <span>span</span> <span>=</span> <span>span</span>.<span>Slice</span>(<span>1</span>);
    <span>span</span> <span>=</span> <span>span</span>.<span>Slice</span>(<span>1</span>); <span>span</span> <span>=</span> <span>span</span>.<span>Slice</span>(<span>1</span>); <span>span</span> <span>=</span> <span>span</span>.<span>Slice</span>(<span>1</span>); <span>span</span> <span>=</span> <span>span</span>.<span>Slice</span>(<span>1</span>);

    <span>return</span> <span>span</span>;
}</pre></div>
<p>BenchmarkDotNet is going to scale the result by the number provided in <code>OperationsPerInvoke</code> so the cost of creating the <code>Span</code> is going to be amortized:</p>
<pre><code>reportedResult = 1/16*SpanCtor + 1*Slice
</code></pre>
<p><strong>Note:</strong> <code>OperationsPerInvoke</code> should be big enough to amortize the "setup" cost.</p>
<h2>Test Cases</h2>
<h3>Code Paths</h3>
<p>When it comes to choosing different test cases for a benchmark, we should be using the internal perspective of the system (<a href="https://en.wikipedia.org/wiki/White-box_testing" rel="nofollow">White-box testing</a>) and <strong>choose test cases that test different code paths</strong>.</p>
<h4>Array.Reverse</h4>
<p>Let's consider the example of reversing an array and take a look at the <a href="https://github.com/dotnet/coreclr/blob/085398b5c6913b03180c855f56b967e2a7d6edde/src/System.Private.CoreLib/shared/System/Array.cs#L1347-L1371">current implementation</a>:</p>
<div><pre><span>public</span> <span>static</span> <span>void</span> <span>Reverse</span>&lt;<span>T</span>&gt;(<span>T</span>[] <span>array</span>, <span>int</span> <span>index</span>, <span>int</span> <span>length</span>)
{
    <span>if</span> (<span>array</span> <span>==</span> <span>null</span>)
        <span>ThrowHelper</span>.<span>ThrowArgumentNullException</span>(<span>ExceptionArgument</span>.<span>array</span>);
    <span>if</span> (<span>index</span> <span>&lt;</span> <span>0</span>)
        <span>ThrowHelper</span>.<span>ThrowIndexArgumentOutOfRange_NeedNonNegNumException</span>();
    <span>if</span> (<span>length</span> <span>&lt;</span> <span>0</span>)
        <span>ThrowHelper</span>.<span>ThrowLengthArgumentOutOfRange_ArgumentOutOfRange_NeedNonNegNum</span>();
    <span>if</span> (<span>array</span><span>!</span>.<span>Length</span> <span>-</span> <span>index</span> <span>&lt;</span> <span>length</span>) <span><span>//</span> TODO-NULLABLE: Remove ! when [DoesNotReturn] respected</span>
        <span>ThrowHelper</span>.<span>ThrowArgumentException</span>(<span>ExceptionResource</span>.<span>Argument_InvalidOffLen</span>);

    <span>if</span> (<span>length</span> <span>&lt;=</span> <span>1</span>)
        <span>return</span>;

    <span>ref</span> <span>T</span> <span>first</span> <span>=</span> <span>ref</span> <span>Unsafe</span>.<span>Add</span>(<span>ref</span> <span>Unsafe</span>.<span>As</span>&lt;<span>byte</span>, <span>T</span>&gt;(<span>ref</span> <span>array</span>.<span>GetRawSzArrayData</span>()), <span>index</span>);
    <span>ref</span> <span>T</span> <span>last</span> <span>=</span> <span>ref</span> <span>Unsafe</span>.<span>Add</span>(<span>ref</span> <span>Unsafe</span>.<span>Add</span>(<span>ref</span> <span>first</span>, <span>length</span>), <span>-</span><span>1</span>);
    <span>do</span>
    {
        <span>T</span> <span>temp</span> <span>=</span> <span>first</span>;
        <span>first</span> <span>=</span> <span>last</span>;
        <span>last</span> <span>=</span> <span>temp</span>;
        <span>first</span> <span>=</span> <span>ref</span> <span>Unsafe</span>.<span>Add</span>(<span>ref</span> <span>first</span>, <span>1</span>);
        <span>last</span> <span>=</span> <span>ref</span> <span>Unsafe</span>.<span>Add</span>(<span>ref</span> <span>last</span>, <span>-</span><span>1</span>);
    } <span>while</span> (<span>Unsafe</span>.<span>IsAddressLessThan</span>(<span>ref</span> <span>first</span>, <span>ref</span> <span>last</span>));
}</pre></div>
<p>Does it make sense to test the code paths that throw?</p>
<ul>
<li>No, because we would be measuring the performance of throwing and catching the exceptions. That was not the goal of this benchmark.</li>
<li>No, because throwing exceptions should be exceptional and <a href="https://docs.microsoft.com/en-US/visualstudio/profiling/da0007-avoid-using-exceptions-for-control-flow?view=vs-2019" rel="nofollow">exceptions should not be used to control flow</a>. It's an edge case, we should focus on <a href="#Benchmarks-are-not-Unit-Tests">common use cases, not edge cases</a>.</li>
</ul>
<p>Should we test the code path for an array with one or zero elements?</p>
<ul>
<li>No, because it does not perform any actual work. We would be benchmarking a branch and return from the method. If <code>Reverse</code> is inlinable, such a benchmark would be measuring the performance of <code>if (length &lt;= 1)</code> and the throw checks.</li>
<li>No, because it's not a common case. Moreover, it's very unlikely that removing this check from the code would pass the <a href="https://github.com/dotnet/runtime">dotnet/runtime</a> repository code review and regress the performance in the future.</li>
</ul>
<p>So what is left? A loop that does the actual work. Does it make sense to test it for arrays of different sizes?</p>
<ul>
<li>No, because the loop has <code>O(n)</code> complexity and testing different sizes is just going to produce the results that confirm the linear time. It won't add any value but instead, prolong the time it takes to run the benchmarks.</li>
</ul>
<h4>Buffer.CopyMemory</h4>
<p>Let's consider another example: copying blocks of memory. Part of the <a href="https://github.com/dotnet/coreclr/blob/085398b5c6913b03180c855f56b967e2a7d6edde/src/System.Private.CoreLib/shared/System/Buffer.cs#L143-L155">current implementation</a>:</p>
<div><pre><span>internal</span> <span>static</span> <span>unsafe</span> <span>void</span> <span>Memmove</span>(byte* dest, byte* src, <span>nuint</span> <span>len</span>)
{
    <span><span>//</span> P/Invoke into the native version when the buffers are overlapping.</span>
    <span>if</span> (((<span>nuint</span>)<span>dest</span> <span>-</span> (<span>nuint</span>)<span>src</span> <span>&lt;</span> <span>len</span>) <span>||</span> ((<span>nuint</span>)<span>src</span> <span>-</span> (<span>nuint</span>)<span>dest</span> <span>&lt;</span> <span>len</span>))
    {
        <span>goto</span> <span>PInvoke</span>;
    }

    <span>byte</span><span>*</span> <span>srcEnd</span> <span>=</span> <span>src</span> <span>+</span> <span>len</span>;
    <span>byte</span><span>*</span> <span>destEnd</span> <span>=</span> <span>dest</span> <span>+</span> <span>len</span>;

    <span>if</span> (<span>len</span> <span>&lt;=</span> <span>16</span>) <span>goto</span> <span>MCPY02</span>;
    <span>if</span> (<span>len</span> <span>&gt;</span> <span>64</span>) <span>goto</span> <span>MCPY05</span>;

    <span>MCPY00</span>:
    <span><span>//</span> omitted for brevity</span>
    <span>MCPY01</span>:
    <span><span>//</span> omitted for brevity</span>
    <span>MCPY02</span>:
    <span><span>//</span> omitted for brevity</span>
    <span>MCPY03</span>:
    <span><span>//</span> omitted for brevity</span>
    <span>MCPY04</span>:
    <span><span>//</span> omitted for brevity</span>

    <span>MCPY05</span>:
    <span><span>//</span> PInvoke to the native version when the copy length exceeds the threshold.</span>
    <span>if</span> (<span>len</span> <span>&gt;</span> <span>MemmoveNativeThreshold</span>)
        <span>goto</span> <span>PInvoke</span>;

    <span>MCPY06</span>:</pre></div>
<p>Should we test the performance of copying overlapping buffers?</p>
<ul>
<li>No, because it's an edge case.</li>
<li>No, because the same code path is executed for big buffers (<code>len &gt; 64 &amp;&amp; len &gt; MemmoveNativeThreshold</code>)</li>
</ul>
<p>What should we test? This method is commonly used, so we should test all four code paths:</p>
<ul>
<li>small buffers (<code>len &lt;= 16</code>)</li>
<li>medium size buffers (<code>len &gt; 16 &amp;&amp; len &lt;= 64</code>)</li>
<li>big buffers (<code>len &gt; 64 &amp;&amp; len &lt; MemmoveNativeThreshold</code>)</li>
<li>very big buffers (<code>len &gt; 64 &amp;&amp; len &gt; MemmoveNativeThreshold</code>)</li>
</ul>
<p>One important thing is to leave a comment in the source code and explain our decision. So nobody removes an important test case in the future and we don't forget why we added it ourselves. An example:</p>
<div><pre>[<span>Params</span>(
    <span>128</span>, <span><span>//</span> stackalloc path</span>
    <span>1024</span> <span>*</span> <span>256</span>)] <span><span>//</span> ArrayPool.Shared.Rent without allocation </span>
<span>public</span> <span>int</span> <span>Count</span> { <span>get</span>; <span>set</span>; }</pre></div>
<h3>Always the same input data</h3>
<p>To allow for apples to apples comparison over time, the input data should be always the same.</p>
<p><code>ValuesGenerator</code> class from <code>BenchmarkDotNet.Extensions</code> library exposes following methods that guarantee to always produce the same data:</p>
<div><pre><span>T</span> <span>GetNonDefaultValue</span>&lt;<span>T</span>&gt;();
<span>T</span>[] <span>ArrayOfUniqueValues</span>&lt;<span>T</span>&gt;(<span>int</span> <span>count</span>);
<span>T</span>[] <span>Array</span>&lt;<span>T</span>&gt;(<span>int</span> <span>count</span>);
<span>Dictionary</span>&lt;<span>TKey</span>, <span>TValue</span>&gt; <span>Dictionary</span>&lt;<span>TKey</span>, <span>TValue</span>&gt;(<span>int</span> <span>count</span>)</pre></div>
<p>As of today, the <code>T</code> can be: <code>byte</code>, <code>char</code>, <code>int</code>, <code>double</code>, <code>bool</code> and <code>string</code>. Extending <code>T</code> to more types is very welcomed!</p>
<p><strong>Note:</strong> <code>ValuesGenerator</code> is simply always creating a new instance of <code>Random</code> with a constant seed. It's a crucial component and its correctness is verified using  <a href="https://github.com/dotnet/performance/blob/master/src/tests/harness/BenchmarkDotNet.Extensions.Tests/UniqueValuesGeneratorTests.cs">Unit Tests</a>.</p>
<p><strong>Note:</strong> Please don't use <code>Random</code> directly in the benchmarks, do use <code>ValuesGenerator</code> and extend it with missing features when needed.</p>
<h3>BenchmarkDotNet</h3>
<h4>Arguments</h4>
<p><code>[Arguments]</code> is xUnit's <code>[InlineData]</code> counterpart.</p>
<div><pre>[<span>Benchmark</span>]
[<span>Arguments</span>(<span>100</span>)]
<span>public</span> <span>void</span> <span>Method</span>(<span>int</span> <span>length</span>)</pre></div>
<p><strong>Note:</strong> BenchmarkDotNet supports passing arguments by <code>ref</code> to the benchmarked method.</p>
<h4>Params</h4>
<p>If you need to use the argument in the setup method, then instead of using <code>Arguments</code> you should use <code>Params</code>. A <code>[Params]</code> field|property can be accessed during the setup/cleanup (<a href="https://benchmarkdotnet.org/Advanced/Params.htm" rel="nofollow">docs</a>). Example:</p>
<div><pre><span>int</span>[] <span>_array</span>;

[<span>Params</span>(
    <span>100</span>, <span><span>//</span> comment explaining why it's needed to have this test case</span>
    <span>1_000</span>, <span><span>//</span> comment explaining why it's needed to have this test case</span>
    <span>10_000</span>)] <span><span>//</span> comment explaining why it's needed to have this test case</span>
<span>public</span> <span>int</span> <span>Size</span> { <span>get</span>; <span>set</span>; }

[<span>GlobalSetup</span>]
<span>public</span> <span>void</span> <span>PrepareArray</span>() <span>=&gt;</span> <span>_array</span> <span>=</span> <span>new</span> <span>int</span>[<span>Size</span>];</pre></div>
<p>However, <strong><code>[Params]</code> are applied to all the benchmarks in given class</strong>. So if few benchmarks require different <code>[Params]</code> values they have to be split into separate types (current BenchmarkDotNet implementation limitation).</p>
<h4>ArgumentsSource</h4>
<p><code>[ArgumentsSource]</code> is xUnit's <code>[MemberData]</code> counterpart.</p>
<div><pre><span><span>//</span> for single argument it's an IEnumerable of objects (object), not IEnumerable of arrays of objects (object[])</span>
<span>public</span> <span>IEnumerable</span>&lt;<span>object</span>&gt; <span>Cultures</span>()
{
    <span>yield</span> <span>return</span> <span>new</span> <span>CultureInfo</span>(<span><span>"</span>fr<span>"</span></span>);
    <span>yield</span> <span>return</span> <span>new</span> <span>CultureInfo</span>(<span><span>"</span>da<span>"</span></span>);
    <span>yield</span> <span>return</span> <span>new</span> <span>CultureInfo</span>(<span><span>"</span>ja<span>"</span></span>);
    <span>yield</span> <span>return</span> <span>new</span> <span>CultureInfo</span>(<span><span>"</span><span>"</span></span>);
}

[<span>Benchmark</span>]
[<span>ArgumentsSource</span>(<span>nameof</span>(<span>Cultures</span>))]
<span>public</span> <span>DateTime</span> <span>Parse</span>(<span>CultureInfo</span> <span>cultureInfo</span>)
    <span>=&gt;</span> <span>DateTime</span>.<span>Parse</span>(<span><span>"</span>10/10/2010 12:00:00 AM<span>"</span></span>, <span>cultureInfo</span>);</pre></div>
<p><strong>Note:</strong> If you need to use the argument in the setup method, then instead of using <code>ArgumentsSource</code> you should use <code>ParamsSource</code>.</p>
<p><strong>Note:</strong> <code>[ArgumentsSource]</code> methods are only called once per execution of the benchmark, and the cached results are used per invocation.</p>
<p><strong>Note:</strong> the time spent for initializing an argument is not included in the time reported by the harness. It means that you can have a custom complex type with initialization logic in it's ctor and setup the benchmark in that way:</p>
<p>An example from <a href="https://github.com/dotnet/performance/blob/d13b517422b40c2c4e0c78934a5e1c4b54420372/src/benchmarks/micro/corefx/System.Drawing/Perf_Image_Load.cs#L71-L85">System.Drawing.Perf_Image_Load</a></p>
<div><pre>[<span>Benchmark</span>]
[<span>ArgumentsSource</span>(<span>nameof</span>(<span>ImageFormats</span>))]
<span>public</span> <span>void</span> <span>Bitmap_FromStream</span>(<span>ImageTestData</span> <span>format</span>)
{
    <span>using</span> (<span>new</span> <span>Bitmap</span>(<span>format</span>.<span>Stream</span>))
    {
    }
}

<span>public</span> <span>IEnumerable</span>&lt;<span>object</span>&gt; <span>ImageFormats</span>() <span>=&gt;</span> <span>return</span> <span>new</span> [] 
{
    <span>new</span> <span>ImageTestData</span>(<span>ImageFormat</span>.<span>Bmp</span>),
    <span>new</span> <span>ImageTestData</span>(<span>ImageFormat</span>.<span>Jpeg</span>),
    <span>new</span> <span>ImageTestData</span>(<span>ImageFormat</span>.<span>Png</span>),
    <span>new</span> <span>ImageTestData</span>(<span>ImageFormat</span>.<span>Gif</span>)
};

<span>public</span> <span>class</span> <span>ImageTestData</span>
{
    <span>public</span> <span>Stream</span> <span>Stream</span> { <span>get</span>; }
    <span>private</span> <span>string</span> <span>FormatName</span> { <span>get</span>; }

    <span>public</span> <span>ImageTestData</span>(<span>ImageFormat</span> <span>format</span>)
    {
        <span>Stream</span> <span>=</span> <span>CreateTestImage</span>(<span>format</span>); <span><span>//</span> another way to setup your benchmark(s)</span>
        <span>FormatName</span> <span>=</span> <span>format</span>.<span>ToString</span>();
    }

    <span><span>//</span> the value returned by ToString is used in the text representation of Benchmark ID in our reporting system</span>
    <span>public</span> <span>override</span> <span>string</span> <span>ToString</span>() <span>=&gt;</span> <span>FormatName</span>;

    <span>private</span> <span>static</span> <span>Stream</span> <span>CreateTestImage</span>(<span>ImageFormat</span> <span>format</span>)
    <span><span>//</span> omitted for brevity</span>
}</pre></div>
<h4>Generic benchmarks</h4>
<p>BenchmarkDotNet supports generic classes. It allows for having a dedicated test case for Value and Reference Types.</p>
<p>An example from <a href="https://github.com/dotnet/performance/blob/dabac287ff09d06c756ee316e4dfe28c78698635/src/benchmarks/micro/coreclr/System.Reflection/Activator.cs">Activator</a> benchmarks:</p>
<div><pre>[<span>GenericTypeArguments</span>(<span>typeof</span>(<span>EmptyStruct</span>))] <span><span>//</span> value type</span>
[<span>GenericTypeArguments</span>(<span>typeof</span>(<span>EmptyClass</span>))] <span><span>//</span> reference type</span>
<span>public</span> <span>class</span> <span>Activator</span>&lt;<span>T</span>&gt;
{
    [<span>Benchmark</span>]
    <span>public</span> <span>T</span> <span>CreateInstanceGeneric</span>() <span>=&gt;</span> <span>System</span>.<span>Activator</span>.<span>CreateInstance</span>&lt;<span>T</span>&gt;();

    [<span>Benchmark</span>]
    <span>public</span> <span>object</span> <span>CreateInstanceType</span>() <span>=&gt;</span> <span>System</span>.<span>Activator</span>.<span>CreateInstance</span>(<span>typeof</span>(<span>T</span>));
}

<span>public</span> <span>class</span> <span>EmptyClass</span> { }
<span>public</span> <span>struct</span> <span>EmptyStruct</span> { }</pre></div>
<h2>Best Practices</h2>
<h3>Single Responsibility Principle</h3>
<p>The benchmarks should follow the <a href="https://en.wikipedia.org/wiki/Single_responsibility_principle" rel="nofollow">Single Responsibility Principle</a> as other methods do. It means that a single benchmark should do a single thing.</p>
<p>An example of a benchmark that violates the rule:</p>
<div><pre>[<span>Benchmark</span>]
<span>public</span> <span>void</span> <span>CompressDecompress</span>(<span>bool</span> <span>compress</span>, <span>Stream</span> <span>compressionStream</span>, <span>byte</span>[] <span>uncompressedData</span>)
{
    <span>if</span> (<span>compress</span>)
    {
        <span>compressionStream</span>.<span>Write</span>(<span>uncompressedData</span>, <span>0</span>, <span>uncompressedData</span>.<span>Length</span>);
    }
    <span>else</span>
    {
        <span>compressionStream</span>.<span>Read</span>(<span>uncompressedData</span>, <span>0</span>, <span>uncompressedData</span>.<span>Length</span>);
    }
}</pre></div>
<p>In this case, there should be one benchmark for compression and another for the decompression.</p>
<h3>No Side-Effects</h3>
<p>Let's consider following benchmark:</p>
<div><pre><span>List</span>&lt;<span>int</span>&gt; <span>_numbers</span> <span>=</span> <span>new</span> <span>List</span>&lt;<span>int</span>&gt;();

[<span>Benchmark</span>]
<span>public</span> <span>void</span> <span>Add</span>() <span>=&gt;</span> <span>_numbers</span>.<span>Add</span>(<span>12345</span>);</pre></div>
<p>In this particular benchmark, the list is growing with every benchmark invocation. <code>List&lt;T&gt;</code> is internally using an <code>Array</code> to store all the elements. When the array is not big enough to store one more element, a two times bigger array is allocated and all elements are copied from the old array to the new one. It means that every next <code>Add</code> operation takes more time. We might also get <code>OutOfMemoryException</code> at some point in time.</p>
<p><strong>Benchmarks should not have any side effects</strong>.</p>
<h3>Dead Code Elimination</h3>
<p>To prevent from <a href="https://en.wikipedia.org/wiki/Dead_code_elimination" rel="nofollow">dead code elimination</a> BenchmarkDotNet consumes the result returned from a benchmark and writes it to a <code>volatile</code> field (<a href="https://github.com/dotnet/BenchmarkDotNet/blob/94863ab4d024eca04d061423e5aad498feff386b/src/BenchmarkDotNet/Engines/Consumer.cs">code</a>).</p>
<p>The only thing that you need to remember is to <strong>return the result from the benchmark</strong>. This is required practice even if you know that the JIT will not eliminate code in a particular case.</p>
<h3>Loops</h3>
<p>BenchmarkDotNet does not require the user to provide the number of <strong>invocations per iteration</strong>. This value is determined by BenchmarkDotNet during the Pilot Experiment Stage, based on the <code>IterationTime</code> setting.</p>
<p>It means, that you don't have to write a loop inside the benchmark and repeat the operation many times. The harness is going to do this for you.</p>
<div><pre>[<span>Benchmark</span>]
[<span>Arguments</span>(<span><span>"</span>12345<span>"</span></span>)]
<span>public</span> <span>int</span> <span>ParseOk</span>(<span>string</span> <span>value</span>) <span>=&gt;</span> <span>int</span>.<span>Parse</span>(<span>value</span>);

[<span>Benchmark</span>]
[<span>Arguments</span>(<span><span>"</span>12345<span>"</span></span>)]
<span>public</span> <span>int</span> <span>ParseNotOk</span>(<span>string</span> <span>value</span>)
{
    <span>int</span> <span>result</span> <span>=</span> <span>0</span>;
    <span>for</span> (<span>int</span> <span>i</span> <span>=</span> <span>0</span>; <span>i</span> <span>&lt;</span> <span>1000</span>; <span>i</span><span>++</span>)
    {
        <span>result</span> <span>+=</span> <span>int</span>.<span>Parse</span>(<span>value</span>);
    }
    <span>return</span> <span>result</span>;
}</pre></div>
<p>By relying on the BDN mechanism you are going to avoid loop alignment issues. BenchmarkDotNet handles that on its own side when your benchmark does not have a loop.</p>
<h3>Method Inlining</h3>
<p>BenchmarkDotNet prevents from inlining the benchmarked method by wrapping it into a delegate (delegates can not be inlined as of today). The cost of delegate invocation is excluded by a separate run for Overhead calculation.</p>
<p>The benchmark methods don't need to have <code>[MethodImpl(MethodImplOptions.NoInlining)]</code> attribute applied.</p>
</article>
  </div></div></div>
    </div>
    <footer>
        <div>created by buildstarted &copy; 2020 <a href="/about">about</a></div>
        <div>Share this page on social media: copy and paste this url https://linksfor.dev/</div>
        <div>If you prefer RSS: <a href="https://linksfor.dev/feed.xml">https://linksfor.dev/feed.xml</a></div>
    </footer>
    
    <script async defer>
        _dna = window._dna || {};
        _dna.siteId = "linksfor.devs";
        _dna.outlink = true;

        (function () {
            let dna = document.createElement('script');
            dna.type = 'text/javascript';
            dna.async = true;
            dna.src = '//dna.buildstarted.com/t.js';
            let s = document.getElementsByTagName('script')[0];
            s.parentNode.insertBefore(dna, s);
        })();
    </script>
    <noscript><img src="//dna.buildstarted.com/g?siteId=linksfor.devs"/></noscript>
</body>
</html>