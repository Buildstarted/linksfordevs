<!DOCTYPE html>
<html lang="en">
<head>
    <title>
An Introduction to Apache Airflow - linksfor.dev(s)    </title>
    <meta charset="utf-8">
    <link rel="alternate" type="application/rss+xml" title="Linksfor.dev(s) feed" href="https://linksfor.dev/feed.rss" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta name="google" value="notranslate">
    <link rel="stylesheet" type="text/css" href="/style.css" />
    <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon">
    <link rel="icon" href="/favicon.ico" type="image/x-icon">
    <meta property="og:title" content="An Introduction to Apache Airflow - linksfor.dev(s)"/>
    <meta property="og:description" content="Airflow is a platform created by the community to programmatically author, schedule, and monitor workflows. In this blog we will underestand the basics of airflow"/>
    <meta property="og:type" content="website"/>
    <meta property="og:url" content="https://bhavaniravi.com/blog/apache-airflow-introduction/"/>

<meta property="og:site_name" content="linksfor.dev(s)" />
</head>
<body>
    <div class="grid">
        <h1>
<a href="/" style="color:inherit">linksfor.dev(s)</a>
        </h1>
        <title>linksfor.dev(s) - An Introduction to Apache Airflow</title>
<div class="readable">
        <h1>An Introduction to Apache Airflow</h1>
            <div>Reading time: 5-6 minutes</div>
        <div>Posted here: 29 May 2020</div>
        <p><a href="https://bhavaniravi.com/blog/apache-airflow-introduction/">https://bhavaniravi.com/blog/apache-airflow-introduction/</a></p>
        <hr/>
<div id="readability-page-1" class="page"><div id="___gatsby"><div tabindex="-1" role="group" id="gatsby-focus-wrapper"><div><article><div>
<h2>What is Airflow?</h2>
<blockquote>
<p>Airflow is a platform created by the community to programmatically author, schedule, and monitor workflows. </p>
</blockquote>
<p>Machine learning is the hot topic of the industry. It won't be so cool if not for the data processing involved</p>
<p>Airflow is an ETL(Extract, Transform, Load) workflow orchestration tool, used in data transformation pipelines.</p>
<h2>Uses of Airflow</h2>
<p>Imagine you have an ML model that does twitter sentiment analysis. Now you want to run that model for your favorite people on twitter for their tweets every day. Such a workflow would look something like this.</p>
<figure>
<p><img src="https://i.imgur.com/qgSw3xW.png"></p>
</figure>
<p>As you can see, the data flows from one end of the pipeline to the other end. There can be branches, but no cycles.</p>
<h2>What problems does Airflow solve?</h2>
<p>Crons are an age-old way of scheduling tasks.</p>
<ol>
<li>With cron creating and maintaining a relationship between tasks is a nightmare, whereas, in Airflow, it is as simple as writing Python code.</li>
<li>Cron needs external support to log, track, and manage tasks. Airflow UI to track and monitor the workflow execution</li>
<li>Cron jobs are not reproducible unless externally configured. The Airflow keeps an audit trail of all tasks executed.</li>
<li>Scalable</li>
</ol>
<h2>How to define a workflow in Airflow?</h2>
<p>Workflows are defined using Python files. </p>
<h3>DAG</h3>
<p>Airflow provides <code>DAG</code> Python class to create a Directed Acyclic Graph, a representation of the workflow.</p>

        <deckgo-highlight-code language="python" theme="vscode" terminal="carbon">
          <code slot="code">from Airflow.models import DAG
from airflow.utils.dates import days_ago

args = {
    'start_date': days_ago(0),
}

dag = DAG(
    dag_id='example_bash_operator',
    default_args=args,
    schedule_interval='* * * * *',
)</code>
        </deckgo-highlight-code>
      
<p><code>start_date</code> enables you to run a task on a particular date.</p>
<p><code>Schedule_interval</code> is the interval in which each workflow is supposed to run. <code>'* * * * *'</code> means the tasks need to run every minute. Don't scratch your brain over this syntax. You can play around with these using <a href="https://crontab.guru/">https://crontab.guru/</a>. </p>
<h3>Operator</h3>
<p><code>Operators</code> define the nodes of the DAG. Each operator is an independent task. </p>
<p>In the following example, we use two Operators </p>

        <deckgo-highlight-code language="python" theme="vscode" terminal="carbon">
          <code slot="code">from airflow.operators.bash_operator import BashOperator
from airflow.operators.python_operator import PythonOperator</code>
        </deckgo-highlight-code>
      
<ol>
<li><code>PythonOperator</code> which calls a python function</li>
</ol>

        <deckgo-highlight-code language="python" theme="vscode" terminal="carbon">
          <code slot="code">def print_function():
    print ("Hey I am a task")

run_this_last = PythonOperator(
    task_id='run_this_last',
    dag=dag,
    python_callable=print_function
)</code>
        </deckgo-highlight-code>
      
<ol start="2">
<li><code>BashOperator</code> which runs a bash command</li>
</ol>

        <deckgo-highlight-code language="python" theme="vscode" terminal="carbon">
          <code slot="code">run_this = BashOperator(
    task_id='run_after_loop',
    bash_command='echo 1',
    dag=dag,
)</code>
        </deckgo-highlight-code>
      
<ol start="3">
<li>The tasks are linked together using <code>&gt;&gt;</code> python operator.</li>
</ol>

        <deckgo-highlight-code language="python" theme="vscode" terminal="carbon">
          <code slot="code">run_this &gt;&gt; run_this_last</code>
        </deckgo-highlight-code>
      
<p>A sample DAG with branches would look something like this. </p>
<figure>
<p><img src="https://i.imgur.com/VyqpE8n.png"></p>
</figure>
<h2>Airflow Architecture</h2>
<figure>
<p><img src="https://i.imgur.com/UT38Lok.png"></p>
</figure>
<p>Airflow has 4 major components.</p>
<h3>Webserver</h3>
<p>The webserver is the component that is responsible for handling all the UI and REST APIs. </p>
<h3>Scheduler</h3>
<p>Scheduler goes through the DAGs every <code>n</code> seconds and schedules the task to be executed.</p>
<p>The scheduler also has an internal component called <strong>Executor</strong>. The executor is responsible for spinning up workers and executing the task to completion.</p>
<h3>Worker</h3>
<p>Workers run the task that is being handed over by the executor.</p>
<h2>Types of Executor</h2>
<h3>SequentialExecutor</h3>
<p>SequentialExecutor runs only one task at a time. The workers run the same machine as the scheduler is. </p>
<h4>Pros</h4>
<ol>
<li>Simple and easy to setup</li>
<li>Good for testing DAGs during development</li>
</ol>
<h4>Cons</h4>
<ol>
<li>Not scalable</li>
<li>It cannot run multiple tasks at the same time.</li>
<li>Not suitable for production</li>
</ol>
<h3>LocalExecutor</h3>
<p>LocalExecutor is the same as the Sequential Executor, except it can run multiple tasks at a time.</p>
<h4>Pros</h4>
<ol>
<li>Can run multiple tasks</li>
<li>Good for running DAGs during development</li>
</ol>
<h4>Cons</h4>
<ol>
<li>Not scalable</li>
<li>Single point of failure</li>
<li>Not suitable for production</li>
</ol>
<h3>CeleryExecutor</h3>
<p>Celery is used for running distributed asynchronous python tasks. </p>
<p>Hence, CeleryExecutor has been a part of Airflow for a long time, even before Kubernetes. </p>
<p>CeleryExecutors has a fixed number of workers running to pick-up the tasks as they get scheduled.</p>
<h4>Pros</h4>
<ol>
<li>It provides scalability.</li>
<li>Celery manages the workers. In case of a failure, Celery spins up a new one.</li>
</ol>
<h4>Cons</h4>
<ol>
<li>Celery needs RabbitMQ/Redis to for queuing the task, which is reinventing the wheel of what Airflow already supports.</li>
<li>The above dependency also makes the setup complex.</li>
</ol>
<h3>KubernetesExecutor</h3>
<p>KubernetesExecutor runs each task in an individual Kubernetes pod. Unlike CeleryCelery, <a href="https://bhavaniravi.com/blog/deploying-airflow-on-kubernetes/">it spins up worker pods on demand</a>, hence enabling maximum usage of resources.</p>
<h4>Pros</h4>
<ol>
<li>It Combines the pros of scalability and simplicity of CeleryExecutor and LocalExecutor.</li>
<li>Fine-grained control over resources allocated to tasks. One can define the amount of CPU/memory required at a task level.</li>
</ol>
<h4>Cons</h4>
<ol>
<li>Kubernetes is new to Airflow, and the documentation is not straightforward.</li>
</ol>
<hr>
<p>Now that we have understood Airflow's basics let's learn how to write our workflow in the next post. </p></div></article><hr><hr><hr><ul><li><a rel="prev" href="https://bhavaniravi.com/blog/secrets-as-python-file">‚Üê <!-- -->Are your Secrets Safe In Python?</a></li><li></li></ul><p><em><p>I recently moved my blogs from medium. Find more of my writing<a target="_blank" rel="noopener noreferrer" href="https://medium.com/@bhavaniravi"> here.</a></p></em></p></div></div></div></div></div>
    </div>
    <footer>
        <div>created by buildstarted &copy; 2020 <a href="/about">about</a></div>
        <div>Share this page on social media: copy and paste this url https://linksfor.dev/</div>
        <div>If you prefer RSS: <a href="https://linksfor.dev/feed.xml">https://linksfor.dev/feed.xml</a></div>
		<div>Customer satisfaction guaranteed to be optional.</div>
    </footer>
    
    <script async defer>
        _dna = window._dna || {};
        _dna.siteId = "linksfor.devs";
        _dna.outlink = true;

        (function () {
            let dna = document.createElement('script');
            dna.type = 'text/javascript';
            dna.async = true;
            dna.src = '//dna.buildstarted.com/t.js';
            let s = document.getElementsByTagName('script')[0];
            s.parentNode.insertBefore(dna, s);
        })();
    </script>
    <noscript><img src="//dna.buildstarted.com/g?siteId=linksfor.devs"/></noscript>
</body>
</html>