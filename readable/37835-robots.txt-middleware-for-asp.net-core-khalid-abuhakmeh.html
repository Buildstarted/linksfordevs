<!DOCTYPE html>
<html lang="en">
<head>
    <title>
Robots.txt Middleware For ASP.NET Core | Khalid Abuhakmeh -
linksfor.dev(s)
    </title>
    <link rel="alternate" type="application/rss+xml" title="Linksfor.dev(s) feed" href="https://linksfor.dev/feed.rss" />
    <meta charset="utf-8">
    <meta name="Description" content="A curated source of links that devs might find interesting. Updated around the clock." />
    <meta name="google" value="notranslate">
    <link rel="stylesheet" type="text/css" href="/style.css" />
    <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon">
    <link rel="icon" href="/favicon.ico" type="image/x-icon">
</head>
<body>
    <div class="grid">
        
<div class="readable">
    <div id="readOverlay" class="style-ebook"><div id="readInner" class="margin-medium size-medium"><h1>Robots.txt Middleware For ASP.NET Core  |
      Khalid Abuhakmeh</h1><div><div class="post-content"><p>As a web developer, it’s important to remind ourselves that we push our work onto the internet for the world to see. Our development team utilizes Windows Azure for many environments, and all those environments need to be publicly accessible. There are ways to limit accessibility on Windows Azure, but the eyes looking at our work can vary, so having a hard constraint becomes unmanageable for us.</p><p>The accessibility of our environments helps stakeholders see iterations of our progress faster and provide feedback. In turn, that feedback goes back into our development, making the deliverable better for everyone.</p><p>In practice, we usually have three environments in Windows Azure App Services: Development, Staging, and Production. We want all these to be visible to <em>people</em>, but we all know people don’t just view the web. The majority of web users are automated processes that scan your site, and many other sites to provide search results. I put a generous amount of effort into a previous blog post and proud of it. <a href="/search-experiences-for-your-aspnet-core-apps-with-elasticsearch">If you want to learn how to build a great search experience in your ASP.NET Core application, I highly recommend it</a>.</p><p>In this post, I’ll explain why a <code class="highlighter-rouge">robots.txt</code> is essential for your public-facing ASP.NET Core applications. Additionally, you’ll learn how to write or generate a few variations of the file. Finally, you’ll learn how to serve a specific file based on your current hosting environment.</p><p>This post is by no means limited to Windows Azure users and works anywhere you host your ASP.NET Core applications.</p><p>If you want the code, you can download it from my GitHub repository. It targets ASP.NET Core 3.0 but could be modified to support lower SDK versions.</p><p><strong><a href="https://github.com/khalidabuhakmeh/RobotsTxt">Download the Project</a></strong></p><h2 id="what-is-a-robotstxt">What is a Robots.txt</h2><p>As mentioned in the previous section, most of the web’s users are automated processes. These processes are known as Web crawlers and operated by data collection and search companies like Google, Facebook, and Microsoft.</p><p>Wikipedia sums it perfectly:</p><blockquote><p>A Web crawler, sometimes called a spider or spiderbot and often shortened to crawler, is an Internet bot that systematically browses the World Wide Web, typically for the purpose of Web indexing (web spidering). <cite>–<a href="https://en.wikipedia.org/wiki/Web_crawler">Wikipedia</a></cite></p></blockquote><p>While algorithms can differ, most crawlers start at your homepage and navigate the links on your site in an attempt to find the most content. However, the exciting fact is, you can talk to these crawlers!</p><p>If talking to spiders makes your skin crawl, don’t worry, I promise there won’t be any insects involved. Instead, there is a simple plain text file called a <code class="highlighter-rouge">robots.txt</code> file. This file contains instructions for crawlers, telling them where to look in your site for meaningful content, and even where <em>not</em> to look. As reiterated on Wikipedia:</p><blockquote><p>The robots exclusion standard, also known as the robots exclusion protocol or simply robots.txt, is a standard used by websites to communicate with web crawlers and other web robots. The standard specifies how to inform the web robot about which areas of the website should not be processed or scanned.<cite>–<a href="https://en.wikipedia.org/wiki/Robots_exclusion_standard">Wikipedia</a></cite></p></blockquote><p>It’s important to note that the instructions in the file are no guarantee that a crawler obeys your wishes. There are bad bots that crawl sites for many other reasons, especially if your site contains valuable information.</p><h2 id="why-it-is-important">Why It is Important</h2><p>I’m glad you are asking the question:</p><blockquote><p>Why is it essential to have a <code class="highlighter-rouge">robots.txt</code> file for my ASP.NET Core application?</p></blockquote><p>While you may not have content within your application, you still want individuals to find your primary domain. Pointing crawlers to the specific pages to you and your users can make for a more significant impact.</p><p><strong>In my case, we have multiple domains that can have very similar content. Crawlers may mistake the variations in domains having similar content as an attempt to create spam sites. The duplication of sites can hurt the overall ranking of a site in search engine results.</strong></p><p><strong>Even worse, your non-primary sites could rank higher than your primary! Imagine users entering critical information into your development environment, only for it to be lost. Yikes!</strong></p><h2 id="generating-your-own-robotstxt">Generating Your Own Robots.txt</h2><p>A <code class="highlighter-rouge">robots.txt</code> has a few components, but the essential parts are who and what.
Whom do you want to scan your site?
What parts of the site do you want to be indexed?
Let’s take a simple <code class="highlighter-rouge">robots.txt</code> file:</p><pre><code class="language-txt">User-Agent: *
Allow: /
</code></pre><p>The first line of the file defines what <code class="highlighter-rouge">User-Agent</code> can scan the site. The <code class="highlighter-rouge">User-Agent</code> is a string value passed from the client. For example, Google’s bot passes a value of <code class="highlighter-rouge">Googlebot</code>, but they also have an <a href="https://support.google.com/webmasters/answer/1061943?hl%3Den">army of bots</a>.</p><p>The second line of the file tells the bot what it can index. The example informs the bot to start at the root path and work its way everywhere it can. You can also <code class="highlighter-rouge">Disallow</code> specific paths, but don’t assume this takes the place of securing your site. As mentioned before, bots do not have to respect the <code class="highlighter-rouge">robots.txt</code> file.</p><p>I recommend generating a few variations using <a href="https://en.ryte.com/free-tools/robots-txt-generator/#allowing">this online tool</a>. As the tool mentions, you can damage your site’s search results if done incorrectly, so be careful and think through the file you’ll be serving.</p><p><img src="https://d33wubrfki0l68.cloudfront.net/1f411275cecf4d969ef2e8601b245eeae948dafc/8e302/assets/images/generated/robotstxt/robotstxt_generator-712-b8b6fb.png" alt="serving the https://d33wubrfki0l68.cloudfront.net/05fffa36524262cdffb4f70a41c5bc9fccc2d0b8/fde2c/robots.txt" srcset="https://d33wubrfki0l68.cloudfront.net/52c1fb4226af5a750400c687c160e7b5211faf1b/3285a/assets/images/generated/robotstxt/robotstxt_generator-400-b8b6fb.png 400w, https://d33wubrfki0l68.cloudfront.net/822d3e00f78fab2b8e57e3f1bf9f345afeae611c/f04f4/assets/images/generated/robotstxt/robotstxt_generator-600-b8b6fb.png 600w, https://d33wubrfki0l68.cloudfront.net/1f411275cecf4d969ef2e8601b245eeae948dafc/8e302/assets/images/generated/robotstxt/robotstxt_generator-712-b8b6fb.png 712w"></p><h2 id="the-code">The Code</h2><p>You’ve learned a lot up to this point:</p><ul><li>You know what a crawler is</li><li>How to talk to them via a <code class="highlighter-rouge">robots.txt</code> file</li><li>Have generated a few variations</li></ul><p>In this section, I’ll show you how to write a piece of middleware that takes inspiration for ASP.NET Core’s use of environments. Our goal is to serve a <code class="highlighter-rouge">robots.txt</code> file unique to each environment we have.</p><p>We want to tell bots to stay away from our development environments while boosting the importance of our production sites.</p><h3 id="robotstxt-per-environment">Robots.txt Per Environment</h3><p>The first step is to realize that ASP.NET Core provides an <code class="highlighter-rouge">Environment</code> mechanism. By default, we get <code class="highlighter-rouge">Development</code>, <code class="highlighter-rouge">Staging</code>, and <code class="highlighter-rouge">Production</code>. We can create any environments, but let’s start here.</p><p><img src="https://d33wubrfki0l68.cloudfront.net/e6c0d069504232714d2af71a17360ed322442bc2/1a5f2/assets/images/generated/robotstxt/environments_aspnetcore-800-729527.png" alt="asp.net core environments" srcset="https://d33wubrfki0l68.cloudfront.net/290f5b2dcdcaf0453160d3274c34f5b1492a6369/2fc04/assets/images/generated/robotstxt/environments_aspnetcore-400-729527.png 400w, https://d33wubrfki0l68.cloudfront.net/da211bb9232e2ed966bdad63822b3329ce564ab6/19960/assets/images/generated/robotstxt/environments_aspnetcore-600-729527.png 600w, https://d33wubrfki0l68.cloudfront.net/e6c0d069504232714d2af71a17360ed322442bc2/1a5f2/assets/images/generated/robotstxt/environments_aspnetcore-800-729527.png 800w, https://d33wubrfki0l68.cloudfront.net/10791b6640cdbc33756e00332ae20fecc24cf79c/dd3c4/assets/images/generated/robotstxt/environments_aspnetcore-851-729527.png 851w"></p><p>Our robots.txt files can match these environments:</p><pre><code class="language-txt">robots.txt
robots.Production.txt
robots.Development.txt
</code></pre><p>The <code class="highlighter-rouge">robots.txt</code> is our fallback, but the other files are specific to the environments. ASP.NET Core reads the current environment value from the environmental variable <code class="highlighter-rouge">ASPNETCORE_ENVIRONMENT</code> which can be set at runtime or via launch settings.</p><h3 id="robotstxt-middleware-code">Robots.txt Middleware Code</h3><p>The middleware is straight-forward. It scans our directory for the files based on our environments. In my sample, I place the files in the content path of the project, and not the webroot path. You could place it anywhere, but in general, I don’t want the static file middleware to serve the incorrect file accidentally.</p><div class="language-c# highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">public</span><span class="k">static</span><span class="k">class</span><span class="nc">RobotsTxtMiddlewareExtensions</span><span class="p">{</span><span class="k">public</span><span class="k">static</span><span class="n">IApplicationBuilder</span><span class="nf">UseRobotsTxt</span><span class="p">(</span><span class="k">this</span><span class="n">IApplicationBuilder</span><span class="n">builder</span><span class="p">,</span><span class="n">IWebHostEnvironment</span><span class="n">env</span><span class="p">,</span><span class="kt">string</span><span class="n">rootPath</span><span class="p">=</span><span class="k">null</span><span class="p">)</span><span class="p">{</span><span class="k">return</span><span class="n">builder</span><span class="p">.</span><span class="nf">MapWhen</span><span class="p">(</span><span class="n">ctx</span><span class="p">=&gt;</span><span class="n">ctx</span><span class="p">.</span><span class="n">Request</span><span class="p">.</span><span class="n">Path</span><span class="p">.</span><span class="nf">StartsWithSegments</span><span class="p">(</span><span class="s">"/robots.txt"</span><span class="p">),</span><span class="n">b</span><span class="p">=&gt;</span><span class="n">b</span><span class="p">.</span><span class="n">UseMiddleware</span><span class="p">&lt;</span><span class="n">RobotsTxtMiddleware</span><span class="p">&gt;(</span><span class="n">env</span><span class="p">.</span><span class="n">EnvironmentName</span><span class="p">,</span><span class="n">rootPath</span><span class="p">??</span><span class="n">env</span><span class="p">.</span><span class="n">ContentRootPath</span><span class="p">));</span><span class="p">}</span><span class="p">}</span><span class="k">public</span><span class="k">class</span><span class="nc">RobotsTxtMiddleware</span><span class="p">{</span><span class="k">const</span><span class="kt">string</span><span class="n">Default</span><span class="p">=</span><span class="s">@"User-Agent: *\nAllow: /"</span><span class="p">;</span><span class="k">private</span><span class="k">readonly</span><span class="n">RequestDelegate</span><span class="n">next</span><span class="p">;</span><span class="k">private</span><span class="k">readonly</span><span class="kt">string</span><span class="n">environmentName</span><span class="p">;</span><span class="k">private</span><span class="k">readonly</span><span class="kt">string</span><span class="n">rootPath</span><span class="p">;</span><span class="k">public</span><span class="nf">RobotsTxtMiddleware</span><span class="p">(</span><span class="n">RequestDelegate</span><span class="n">next</span><span class="p">,</span><span class="kt">string</span><span class="n">environmentName</span><span class="p">,</span><span class="kt">string</span><span class="n">rootPath</span><span class="p">)</span><span class="p">{</span><span class="k">this</span><span class="p">.</span><span class="n">next</span><span class="p">=</span><span class="n">next</span><span class="p">;</span><span class="k">this</span><span class="p">.</span><span class="n">environmentName</span><span class="p">=</span><span class="n">environmentName</span><span class="p">;</span><span class="k">this</span><span class="p">.</span><span class="n">rootPath</span><span class="p">=</span><span class="n">rootPath</span><span class="p">;</span><span class="p">}</span><span class="k">public</span><span class="k">async</span><span class="n">Task</span><span class="nf">InvokeAsync</span><span class="p">(</span><span class="n">HttpContext</span><span class="n">context</span><span class="p">)</span><span class="p">{</span><span class="k">if</span><span class="p">(</span><span class="n">context</span><span class="p">.</span><span class="n">Request</span><span class="p">.</span><span class="n">Path</span><span class="p">.</span><span class="nf">StartsWithSegments</span><span class="p">(</span><span class="s">"/robots.txt"</span><span class="p">))</span><span class="p">{</span><span class="kt">var</span><span class="n">generalRobotsTxt</span><span class="p">=</span><span class="n">Path</span><span class="p">.</span><span class="nf">Combine</span><span class="p">(</span><span class="n">rootPath</span><span class="p">,</span><span class="s">"robots.txt"</span><span class="p">);</span><span class="kt">var</span><span class="n">environmentRobotsTxt</span><span class="p">=</span><span class="n">Path</span><span class="p">.</span><span class="nf">Combine</span><span class="p">(</span><span class="n">rootPath</span><span class="p">,</span><span class="s">$"robots.</span><span class="p">{</span><span class="n">environmentName</span><span class="p">}</span><span class="s">.txt"</span><span class="p">);</span><span class="kt">string</span><span class="n">output</span><span class="p">;</span><span class="c1">// try environment first</span><span class="k">if</span><span class="p">(</span><span class="n">File</span><span class="p">.</span><span class="nf">Exists</span><span class="p">(</span><span class="n">environmentRobotsTxt</span><span class="p">))</span><span class="p">{</span><span class="n">output</span><span class="p">=</span><span class="k">await</span><span class="n">File</span><span class="p">.</span><span class="nf">ReadAllTextAsync</span><span class="p">(</span><span class="n">environmentRobotsTxt</span><span class="p">);</span><span class="p">}</span><span class="c1">// then robots.txt</span><span class="k">else</span><span class="k">if</span><span class="p">(</span><span class="n">File</span><span class="p">.</span><span class="nf">Exists</span><span class="p">(</span><span class="n">generalRobotsTxt</span><span class="p">))</span><span class="p">{</span><span class="n">output</span><span class="p">=</span><span class="k">await</span><span class="n">File</span><span class="p">.</span><span class="nf">ReadAllTextAsync</span><span class="p">(</span><span class="n">generalRobotsTxt</span><span class="p">);</span><span class="p">}</span><span class="c1">// then just a general default</span><span class="k">else</span><span class="p">{</span><span class="n">output</span><span class="p">=</span><span class="n">Default</span><span class="p">;</span><span class="p">}</span><span class="n">context</span><span class="p">.</span><span class="n">Response</span><span class="p">.</span><span class="n">ContentType</span><span class="p">=</span><span class="s">"text/plain"</span><span class="p">;</span><span class="k">await</span><span class="n">context</span><span class="p">.</span><span class="n">Response</span><span class="p">.</span><span class="nf">WriteAsync</span><span class="p">(</span><span class="n">output</span><span class="p">);</span><span class="p">}</span><span class="k">else</span><span class="p">{</span><span class="k">await</span><span class="nf">next</span><span class="p">(</span><span class="n">context</span><span class="p">);</span><span class="p">}</span><span class="p">}</span><span class="p">}</span></code></pre></div></div><h3 id="calling-robotstxt-in-startupcs">Calling Robots.Txt In Startup.cs</h3><p>Like any other middleware in an ASP.NET Core application, we need to register it in our middleware pipeline.</p><div class="language-c# highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">public</span><span class="k">void</span><span class="nf">Configure</span><span class="p">(</span><span class="n">IApplicationBuilder</span><span class="n">app</span><span class="p">,</span><span class="n">IWebHostEnvironment</span><span class="n">env</span><span class="p">)</span><span class="p">{</span><span class="k">if</span><span class="p">(</span><span class="n">env</span><span class="p">.</span><span class="nf">IsDevelopment</span><span class="p">())</span><span class="p">{</span><span class="n">app</span><span class="p">.</span><span class="nf">UseDeveloperExceptionPage</span><span class="p">();</span><span class="p">}</span><span class="k">else</span><span class="p">{</span><span class="n">app</span><span class="p">.</span><span class="nf">UseExceptionHandler</span><span class="p">(</span><span class="s">"/Error"</span><span class="p">);</span><span class="n">app</span><span class="p">.</span><span class="nf">UseHsts</span><span class="p">();</span><span class="p">}</span><span class="n">app</span><span class="p">.</span><span class="nf">UseHttpsRedirection</span><span class="p">();</span><span class="c1">// Register Our Middleware</span><span class="n">app</span><span class="p">.</span><span class="nf">UseRobotsTxt</span><span class="p">(</span><span class="n">env</span><span class="p">);</span><span class="n">app</span><span class="p">.</span><span class="nf">UseStaticFiles</span><span class="p">();</span><span class="n">app</span><span class="p">.</span><span class="nf">UseRouting</span><span class="p">();</span><span class="n">app</span><span class="p">.</span><span class="nf">UseAuthorization</span><span class="p">();</span><span class="n">app</span><span class="p">.</span><span class="nf">UseEndpoints</span><span class="p">(</span><span class="n">endpoints</span><span class="p">=&gt;</span><span class="p">{</span><span class="n">endpoints</span><span class="p">.</span><span class="nf">MapRazorPages</span><span class="p">();</span><span class="p">});</span><span class="p">}</span></code></pre></div></div><p>This particular middleware takes <code class="highlighter-rouge">IWebHostEnvironment</code> as a constructor parameter. Passing in the <code class="highlighter-rouge">IWebHostEnvironment</code> allows the middleware to read values like the environment name and if a root path is not provided at registration.</p><h3 id="serving-robotstxt">Serving robots.txt</h3><p>By navigating to <code class="highlighter-rouge">/robots.txt</code> we see the file based on our environment.</p><p><img src="https://d33wubrfki0l68.cloudfront.net/e8f6188178c776bb8bd9d373b17ed8d8579a1733/b6b04/assets/images/generated/robotstxt/site_robotstxt_visit-556-3e9309.png" alt="serving the https://d33wubrfki0l68.cloudfront.net/05fffa36524262cdffb4f70a41c5bc9fccc2d0b8/fde2c/robots.txt" srcset="https://d33wubrfki0l68.cloudfront.net/7b18c07ebd07d2330429559fb5baa505250dce96/590ff/assets/images/generated/robotstxt/site_robotstxt_visit-400-3e9309.png 400w, https://d33wubrfki0l68.cloudfront.net/e8f6188178c776bb8bd9d373b17ed8d8579a1733/b6b04/assets/images/generated/robotstxt/site_robotstxt_visit-556-3e9309.png 556w"></p><p>Hooray! We did it! By setting a breakpoint, we see we hit the middleware for the environment we expect.</p><p><img src="https://d33wubrfki0l68.cloudfront.net/97693bf92aa2fe9e2dcfbdf0c00e6de02e14a2c2/2eb25/assets/images/generated/robotstxt/csharp_environment_breakpoint-780-d6f2bb.png" alt="middleware breakpoint" srcset="https://d33wubrfki0l68.cloudfront.net/23233a3ce9c6ec3513c9119e748e7ace4c808353/4dda3/assets/images/generated/robotstxt/csharp_environment_breakpoint-400-d6f2bb.png 400w, https://d33wubrfki0l68.cloudfront.net/1779723c9a68ac41b640b8978f9670f476c6a252/ecefd/assets/images/generated/robotstxt/csharp_environment_breakpoint-600-d6f2bb.png 600w, https://d33wubrfki0l68.cloudfront.net/97693bf92aa2fe9e2dcfbdf0c00e6de02e14a2c2/2eb25/assets/images/generated/robotstxt/csharp_environment_breakpoint-780-d6f2bb.png 780w"></p><p>To download the project, go to my <a href="https://github.com/khalidabuhakmeh/RobotsTxt">GitHub repository page</a>. Note, you’ll need the <a href="https://dot.net">.NET Core 3.0 SDK</a>.</p><h2 id="conclusion">Conclusion</h2><p>While the code may seem trivial, serving a correct <code class="highlighter-rouge">robots.txt</code> can have an enormous impact on the success of your site. I cannot recommend enough you think about who sees your content, including bots. It’s also important to know that most visitors find your site through a search, so in reality, your most important user is a bot.</p><p><img src="https://media3.giphy.com/media/8vtm3YCdxtUvjTn0U3/giphy.webp?cid=5a38a5a2981250c94e6088ed6ea239d67c617329e10766ea&amp;rid=giphy.webp" alt="bender bot neat"></p></div></div></div></div>
</div>
    </div>
    <footer>
        <div>created by buildstarted &copy; 2020 <a href="/about">about</a></div>
        <div>Share this page on social media: copy and paste this url https://linksfor.dev/</div>
        <div>If you prefer RSS: <a href="https://linksfor.dev/feed.xml">https://linksfor.dev/feed.xml</a></div>
    </footer>
    
    <script>
        (function () {
            var COLLECT_URL = "https://dna.buildstarted.com/t";
            var SITE_ID = "linksfor.devs";
            var GLOBAL_VAR_NAME = "__DNA__";

            window[GLOBAL_VAR_NAME] = {};

            window[GLOBAL_VAR_NAME].sendPageView = function () {
                var path = location.pathname;
                var referrer = document.referrer;

                var url = COLLECT_URL + "?siteid=" + SITE_ID + "&p=" + encodeURIComponent(path) + "&r=" + encodeURIComponent(referrer);

                try { fetch(url, { method: "GET" }); } catch (e) { }
            };

            window[GLOBAL_VAR_NAME].sendPageView();
        })();
    </script>
</body>
</html>