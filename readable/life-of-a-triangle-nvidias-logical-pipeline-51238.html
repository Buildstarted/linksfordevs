<!DOCTYPE html>
<html lang="en">
<head>
    <title>
Life of a triangle - NVIDIA&#x27;s logical pipeline - linksfor.dev(s)    </title>
    <meta charset="utf-8">
    <link rel="alternate" type="application/rss+xml" title="Linksfor.dev(s) feed" href="https://linksfor.dev/feed.rss" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta name="google" value="notranslate">
    <link rel="stylesheet" type="text/css" href="/style.css" />
    <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon">
    <link rel="icon" href="/favicon.ico" type="image/x-icon">
    <meta property="og:title" content="Life of a triangle - NVIDIA&#x27;s logical pipeline - linksfor.dev(s)"/>
    <meta property="og:description" content="Since the release of the ground breaking Fermi architecture almost 5 years have gone by, it might be time to refresh the principle graphics architecture beneath it."/>
    <meta property="og:type" content="website"/>
    <meta property="og:url" content="https://developer.nvidia.com/content/life-triangle-nvidias-logical-pipeline"/>

<meta property="og:site_name" content="linksfor.dev(s)" />
</head>
<body>
    <div class="devring" style="background: #222">
        <div style="text-align:center">Explore other dev related sites in this ring. If you would like to join this ring <a href="https://devring.club">click here</a>.</div>
        <div class="grid">
            <div style="display: grid; grid-template-columns: .5fr 1fr 1fr 1fr; text-align: center;">
                <span class="devring-title"><a href="https://devring.club/">devring.club</a></span>
                <a href="https://devring.club/sites/1/prev" class="devring-previous">Previous</a>
                <a href="https://devring.club/random" class="devring-random">Random</a>
                <a href="https://devring.club/sites/1/next" class="devring-next">Next</a>
            </div>
        </div>
    </div>
    <div class="grid">
        <h1>
<a href="/" style="color:inherit">linksfor.dev(s)</a>
        </h1>
        <title>linksfor.dev(s) - Life of a triangle - NVIDIA&#x27;s logical pipeline</title>
<div class="readable">
        <h1>Life of a triangle - NVIDIA&#x27;s logical pipeline</h1>
            <div>Reading time: 12-15 minutes</div>
        <div>Posted here: 04 Feb 2020</div>
        <p><a href="https://developer.nvidia.com/content/life-triangle-nvidias-logical-pipeline">https://developer.nvidia.com/content/life-triangle-nvidias-logical-pipeline</a></p>
        <hr/>
<div id="readability-page-1" class="page"><div>

        
        <section>
                    
          
                    <a id="main-content"></a>

            <div>
    <section id="block-system-main">

      
  <div id="node-872761">

  
      
      
  
      <p>
      By <span>Christoph Kubisch</span>, posted Mar 16 2015 at 12:52PM    </p>
  
  <div>
    <div><div><div><p>
Since the release of the ground breaking Fermi architecture almost 5 years have gone by, it 
might be time to refresh the principle graphics architecture beneath it. Fermi was the first 
NVIDIA GPU implementing a fully scalable graphics engine and its core architecture can be 
found in Kepler as well as Maxwell. The following article and especially the “compressed 
pipeline knowledge” image below should serve as a primer based on the various public 
materials, such as whitepapers or GTC tutorials about the GPU architecture. This article 
focuses on the graphics viewpoint on how the GPU works, although some principles such as 
how shader program code gets executed is the same for compute.
</p>

<ul>
<li><a href="http://www.hardwarebg.com/b4k/files/nvidia_gf100_whitepaper.pdf">Fermi Whitepaper</a></li>
<li><a href="http://www.nvidia.com/content/PDF/kepler/NVIDIA-Kepler-GK110-Architecture-Whitepaper.pdf">Kepler Whitepaper</a></li>
<li><a href="http://international.download.nvidia.com/geforce-com/international/pdfs/GeForce_GTX_980_Whitepaper_FINAL.PDF">Maxwell Whitepaper</a></li>
<li><a href="http://www.highperformancegraphics.org/previous/www_2010/media/Hot3D/HPG2010_Hot3D_NVIDIA.pdf">Fast Tessellated Rendering on Fermi GF100</a></li>
<li><a href="http://on-demand.gputechconf.com/gtc/2013/presentations/S3466-Programming-Guidelines-GPU-Architecture.pdf">Programming Guidelines and GPU Architecture Reasons Behind Them</a></li>
</ul>

<h3>
Pipeline Architecture Image 
</h3>

<p><img alt="NVIDIA's logical pipeline" src="https://developer.nvidia.com/sites/default/files/akamai/gameworks/images/lifeofatriangle/fermipipeline.png" width="100%"></p>






<h3>
GPUs are super parallel work distributors
</h3>

<p>
Why all this complexity? In graphics we have to deal with data amplification that creates lots 
of variable workloads. Each drawcall may generate a different amount of triangles. The amount 
of vertices after clipping is different from what our triangles were originally made of. After
 back-face and depth culling, not all triangles may need pixels on the screen. The screen size 
 of a triangle can mean it requires millions of pixels or none at all. 
</p>

<p>
As a consequence modern GPUs let their primitives (triangles, lines, points) follow a logical pipeline,
 not a physical pipeline. In the old days before G80's unified architecture (think DX9 hardware, ps3, 
 xbox360), the pipeline was represented on the chip with the different stages and work would run through
 it one after another. G80 essentially reused some units for both vertex and fragment shader computations,
 depending on the load, but it still had a serial process for the primitives/rasterization and so on.
 With Fermi the pipeline became fully parallel, which means the chip implements a logical pipeline 
 (the steps a triangle goes through) by reusing multiple engines on the chip. 
 </p>

<p>
Let's say we have two triangles A and B. Parts of their work could be in different logical pipeline steps. 
A has already been transformed and needs to be rasterized. Some of its pixels could be running pixel-shader 
instructions already, while others are being rejected by depth-buffer (Z-cull), others could be already 
being written to framebuffer, and some may actually wait. And next to all that, we could be fetching the 
vertices of triangle B. So while each triangle has to go through the logical steps, lots of them could be 
actively processed at different steps of their lifetime. The job (get drawcall's triangles on screen) is 
split into many smaller tasks and even subtasks that can run in parallel. Each task is scheduled to the 
resources that are available, which is not limited to tasks of a certain type (vertex-shading parallel 
to pixel-shading). 
</p>


<p>
Think of a river that fans out. Parallel pipeline streams, that are independent of each other, everyone 
on their own time line, some may branch more than others. If we would color-code the units of a GPU based
 on the triangle, or drawcall it's currently working on, it would be multi-color blinkenlights :) 
 </p>
 
 
<h3>GPU architecture</h3>

<p><img alt="GPU architecture" src="https://developer.nvidia.com/sites/default/files/akamai/gameworks/images/lifeofatriangle/fermipipeline_maxwell_gpu.png" width="50%"></p>

<p>
Since Fermi NVIDIA has a similar principle architecture. There is a <strong>Giga Thread Engine</strong> which manages all
 the work that's going on. The GPU is partitioned into multiple <strong>GPCs</strong> (Graphics Processing Cluster),
 each has multiple <strong>SMs</strong> (Streaming Multiprocessor) and one <strong>Raster Engine</strong>. There is lots of interconnects 
 in this process, most notably a <strong>Crossbar</strong> that allows work migration across GPCs or other functional 
 units like <strong>ROP</strong> (render output unit) subsystems.
</p>

<p>
The work that a programmer thinks of (shader program execution) is done on the SMs. It contains many 
<strong>Cores</strong> which do the math operations for the threads. One thread could be a vertex-, or pixel-shader 
invocation for example. Those cores and other units are driven by <strong>Warp Schedulers</strong>, which manage a group 
of 32 threads as warp and hand over the instructions to be performed to <strong>Dispatch Units</strong>. The code logic
 is handled by the scheduler and not inside a core itself, which just sees something like <i>"sum register 
 4234 with register 4235 and store in 4230"</i> from the dispatcher. A core itself is rather dumb, compared 
 to a CPU where a core is pretty smart. The GPU puts the smartness into higher levels, it conducts the 
 work of an entire ensemble (or multiple if you will). 
</p>

<p>
How many of these units are actually on the GPU (how many SMs per GPC, how many GPCs..) depends on the 
chip configuration itself. As you can see above GM204 has 4 GPCs with each 4 SMs, but Tegra X1 for example
 has 1 GPC and 2 SMs, both with Maxwell design. The SM design itself (number of cores, instruction units, 
 schedulers...) has also changed over time from generation to generation (see first image) and helped making 
 the chips so efficient they can be scaled from high-end desktop to notebook to mobile.
 </p>
  

<h3>The logical pipeline</h3>

<p>
For the sake of simplicity several details are omitted. We assume the drawcall references some index- 
and vertexbuffer that is already filled with data and lives in the DRAM of the GPU and uses only vertex- 
and pixelshader (GL: fragmentshader).
</p>


<p><img alt="The logical pipeline" src="https://developer.nvidia.com/sites/default/files/akamai/gameworks/images/lifeofatriangle/fermipipeline_begin.png"></p>
<br>

<ol>

<li>
The program makes a <strong>drawcall</strong> in the graphics api (DX or GL). This reaches the driver at some point which does a bit of 
validation to check if things are "legal" and inserts the command in a GPU-readable encoding inside a <strong>pushbuffer</strong>. 
A lot of bottlenecks can happen here on the CPU side of things, which is why it is important programmers use apis well,
 and techniques that leverage the power of today's GPUs.
</li> 
 
<li>
After a while or explicit "flush" calls, the driver has buffered up enough work in a pushbuffer and sends it to be 
processed by the GPU (with some involvement of the OS). The <strong>Host Interface</strong> of the GPU picks up the commands which are processed via the <strong>Front End</strong>.
</li>


<li>
We start our work distribution in the <strong>Primitive Distributor</strong> by processing the indices in the indexbuffer and generating triangle work batches that 
we send out to multiple GPCs.
</li>

<p><img alt="The logical pipeline" src="https://developer.nvidia.com/sites/default/files/akamai/gameworks/images/lifeofatriangle/fermipipeline_sm.png"></p>
<br>


<li>
Within a GPC, the <strong>Poly Morph Engine</strong> of one of the SMs takes care of fetching the vertex data from the triangle indices (<strong>Vertex Fetch</strong>).
</li>

<li>
After the data has been fetched, warps of 32 threads are scheduled inside the SM and will be working on the vertices.
</li>

<li>
The SM's warp scheduler issues the instructions for the entire warp in-order. The threads run each instruction
 in lock-step and can be masked out individually if they should not actively execute it. There can be multiple 
 reasons for requiring such masking. For example when the current instruction is part of the "if (true)" branch 
 and the thread specific data evaluated "false", or when a loop's termination criteria was reached in one thread
 but not another. Therefore having lots of branch divergence in a shader can increase the time spent for all 
 threads in the warp significantly. Threads cannot advance individually, only as a warp! Warps, however, 
 are independent of each other.
 </li>
  
  
<li>
The warp's instruction may be completed at once or may take several dispatch turns. For example the SM typically 
has less units for load/store than doing basic math operations.
</li>

<li>
As some instructions take longer to complete than others, especially memory loads, the warp scheduler 
may simply switch to another warp that is not waiting for memory. This is the key concept how GPUs 
overcome latency of memory reads, they simply switch out groups of active threads. To make this 
switching very fast, all threads managed by the scheduler have their own registers in the register-file. 
The more registers a shader program needs, the less threads/warps have space. The less warps we can 
switch between, the less useful work we can do while waiting for instructions to complete (foremost memory fetches).
</li>

<p><img alt="The logical pipeline" src="https://developer.nvidia.com/sites/default/files/akamai/gameworks/images/lifeofatriangle/fermipipeline_memoryflow.png"></p>
<br>
<li>
Once the warp has completed all instructions of the vertex-shader, it's results are being processed
 by <strong>Viewport Transform</strong>. The triangle gets clipped by the clipspace volume and is ready for rasterization.
 We use L1 and L2 Caches for all this cross-task communication data.
 </li>
 
 
 <p><img alt="The logical pipeline" src="https://developer.nvidia.com/sites/default/files/akamai/gameworks/images/lifeofatriangle/fermipipeline_raster.png"></p>
<br>

 <li>
 Now it gets exciting, our triangle is about to be chopped up and potentially leaving the GPC it currently 
 lives on. The bounding box of the triangle is used to decide which raster engines need to work on it, 
 as each engine covers multiple tiles of the screen. It sends out the triangle to one or multiple GPCs
 via the <strong>Work Distribution Crossbar</strong>. We effectively split our triangle into lots of smaller jobs now.
 </li>

  
 <p><img alt="The logical pipeline" src="https://developer.nvidia.com/sites/default/files/akamai/gameworks/images/lifeofatriangle/fermipipeline_mid.png"></p>
 <br>
 
 
 <li>
<strong>Attribute Setup</strong> at the target SM will ensure that the interpolants (for example the outputs we generated in
 a vertex-shader) are in a pixel shader friendly format.
 </li>
 
 
 <li>
The <strong>Raster Engine</strong> of a GPC works on the triangle it received and generates the pixel information for those 
sections that it is responsible for (also handles back-face culling and Z-cull).
 </li>

 <li>
Again we batch up 32 pixel threads, or better say 8 times 2x2 pixel quads, which is the smallest 
unit we will always work with in pixel shaders. This 2x2 quad allows us to calculate derivatives 
for things like texture mip map filtering (big change in texture coordinates within quad causes 
higher mip). Those threads within the 2x2 quad whose sample locations are not actually covering
 the triangle, are masked out (gl_HelperInvocation). One of the local SM's warp scheduler will 
 manage the pixel-shading task.
 </li>
 
  
 <li>
The same warp scheduler instruction game, that we had in the vertex-shader logical stage, is now 
performed on the pixel-shader threads. The lock-step processing is particularly handy because 
we can access the values within a pixel quad almost for free, as all threads are guaranteed to 
have their data computed up to the same instruction point (<a href="https://www.opengl.org/registry/specs/NV/shader_thread_group.txt">NV_shader_thread_group</a>).
 </li> 


<p><img alt="The logical pipeline" src="https://developer.nvidia.com/sites/default/files/akamai/gameworks/images/lifeofatriangle/fermipipeline_end.png"></p>
<br>

<li>
Are we there yet? Almost, our pixel-shader has completed the calculation of the colors to be written to the rendertargets 
and we also have a depth value. At this point we have to take the original api ordering of triangles into account 
before we hand that data over to one of the ROP (render output unit) subsystems, which in itself has multiple 
ROP units. Here depth-testing, blending with the framebuffer and so on is performed. These operations need to happen 
atomically (one color/depth set at a time) to ensure we don't have one triangle's color and another triangle's depth 
value when both cover the same pixel. 
NVIDIA typically applies memory compression, to reduce memory bandwidth requirements, which increases "effective" bandwidth (see <a href="http://international.download.nvidia.com/geforce-com/international/pdfs/GeForce_GTX_980_Whitepaper_FINAL.PDF">GTX 980 pdf</a>).
</li>
</ol>

<p>
Puh! we are done, we have written some pixel into a rendertarget. I hope this information was helpful 
to understand some of the work/data flow within a GPU. It may also help understand another side-effect 
of why synchronization with CPU is really hurtful. One has to wait until everything is finished and no 
new work is submitted (all units become idle), that means when sending new work, it takes a while until 
everything is fully under load again, especially on the big GPUs.
</p>

<p>
In the image below you can see how we rendered a CAD model and colored it by the different SMs or warp 
ids that contributed to the image (<a href="https://www.opengl.org/registry/specs/NV/shader_thread_group.txt">NV_shader_thread_group</a>). The result would not be frame-coherent, 
as the work distribution will vary frame to frame. The scene was rendered using many drawcalls, of 
which several may also be processed in parallel (using NSIGHT you can see some of that drawcall 
parallelism as well). 
</p>


<p><img alt="The logical pipeline" src="https://developer.nvidia.com/sites/default/files/akamai/gameworks/images/lifeofatriangle/fermipipeline_distribution.png"></p>
<br>

<h3>Further reading</h3>
<ul>
<li><a href="https://fgiesen.wordpress.com/2011/07/09/a-trip-through-the-graphics-pipeline-2011-index/"> A trip through the graphics-pipeline </a>  by Fabian Giesen </li>
<li><a href="http://on-demand.gputechconf.com/gtc/2013/video/S3466-Performance-Optimization-Guidelines-GPU-Architecture-Details.mp4"> Performance Optimization Guidelines and the GPU Architecture behind them</a>  by Paulius Micikevicius </li>
<li><a href="http://graphics.stanford.edu/papers/pomegranate/">Pomegranate: A Fully Scalable Graphics Architecture</a> describes the concept of parallel stages and work distribution between them.</li>
</ul>







 
 














 </div></div></div>  </div>

  
  
</div>

</section>
  </div>
                  </section>

        
      </div></div></div>
    </div>
    <footer>
        <div>created by buildstarted &copy; 2020 <a href="/about">about</a></div>
        <div>Share this page on social media: copy and paste this url https://linksfor.dev/</div>
        <div>If you prefer RSS: <a href="https://linksfor.dev/feed.xml">https://linksfor.dev/feed.xml</a></div>
        <div>Customer satisfaction guaranteed to be optional.</div>
    </footer>
    
    <script async defer>
        _dna = window._dna || {};
        _dna.siteId = "linksfor.devs";
        _dna.outlink = true;

        (function() {
            let dna = document.createElement('script');
            dna.type = 'text/javascript';
            dna.async = true;
            dna.src = '//dna.buildstarted.com/t.js';
            let s = document.getElementsByTagName('script')[0];
            s.parentNode.insertBefore(dna, s);
        })();
    </script>
    <noscript><img src="//dna.buildstarted.com/g?siteId=linksfor.devs"/></noscript>
</body>
</html>