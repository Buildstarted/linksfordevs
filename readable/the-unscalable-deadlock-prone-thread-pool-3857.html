<!DOCTYPE html>
<html lang="en">
<head>
    <title>
The unscalable, deadlock-prone, thread pool - Paul Khuong: some Lisp - linksfor.dev(s)    </title>
    <meta charset="utf-8">
    <link rel="alternate" type="application/rss+xml" title="Linksfor.dev(s) feed" href="https://linksfor.dev/feed.rss" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta name="google" value="notranslate">
    <link rel="stylesheet" type="text/css" href="/style.css" />
    <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon">
    <link rel="icon" href="/favicon.ico" type="image/x-icon">
    <meta property="og:title" content="The unscalable, deadlock-prone, thread pool - Paul Khuong: some Lisp - linksfor.dev(s)"/>
    <meta property="article:author" content="Paul Khuong"/>
    <meta property="og:description" content="Paul Khuong&#x27;s personal blog. Some Lisp, some optimisation, mathematical or computer."/>
    <meta property="og:type" content="website"/>
    <meta property="og:url" content="https://pvk.ca/Blog/2019/02/25/the-unscalable-thread-pool/"/>

<meta property="og:site_name" content="linksfor.dev(s)" />
</head>
<body>
	<div class="devring" style="background: #222">
		<div class="grid">
			<div style="display: grid; grid-template-columns: .5fr 1fr 1fr 1fr; text-align: center;">
				<span class="devring-title">devring.club</span>
				<a href="https://devring.club/site/1/previous" class="devring-previous">Previous</a>
				<a href="https://devring.club/random" class="devring-random">Random</a>
				<a href="https://devring.club/sites/1/next" class="devring-next">Next</a>
			</div>
		</div>
	</div>
    <div class="grid">
        <h1>
<a href="/" style="color:inherit">linksfor.dev(s)</a>
        </h1>
        <title>linksfor.dev(s) - The unscalable, deadlock-prone, thread pool - Paul Khuong: some Lisp</title>
<div class="readable">
        <h1>The unscalable, deadlock-prone, thread pool - Paul Khuong: some Lisp</h1>
            <div>by Paul Khuong</div>
            <div>Reading time: 12-15 minutes</div>
        <div>Posted here: 27 Feb 2019</div>
        <p><a href="https://pvk.ca/Blog/2019/02/25/the-unscalable-thread-pool/">https://pvk.ca/Blog/2019/02/25/the-unscalable-thread-pool/</a></p>
        <hr/>
<div id="readability-page-1" class="page"><div><p><small>Epistemic Status: I’ve seen thread pools fail this way multiple
times, am confident the pool-per-state approach is an improvement, and
have confirmed with others they’ve also successfully used it in anger.
While I’ve thought about this issue several times over ~4 years and
pool-per-state seems like a good fix, I’m not convinced it’s
undominated and hope to hear about better approaches.</small></p>
<p>Thread pools tend to only offer a sparse interface:
<a href="https://docs.python.org/3/library/concurrent.futures.html#concurrent.futures.Executor.submit">pass a closure or a function and its arguments to the pool</a>,
<a href="https://github.com/silentbicycle/loom/blob/master/loom.h#L46">and that function</a>
<a href="https://github.com/lmj/lparallel/blob/9c11f40018155a472c540b63684049acc9b36e15/src/kernel/core.lisp#L374">will be called, eventually</a>.<sup id="fnref:convenience"><a href="#fn:convenience">1</a></sup> Functions can do
anything, so this interface should offer all the expressive power one
could need. Experience tells me otherwise.</p>
<p>The standard pool interface is so impoverished that it is nearly
impossible to use correctly in complex programs, and leads us down
design dead-ends. I would actually argue it’s better to work with raw
threads than to even have <del>generic</del> amorphous thread pools: the former force us
to stop and think about resource requirements (and lets the OS’s real
scheduler help us along), instead of making us pretend we only care
about CPU usage. I claim thread pools aren’t scalable because, with
the exception of CPU time, they actively hinder the development of
programs that achieve high resource utilisation.</p>
<p>This post comes in two parts. First, the story of a simple program
that’s parallelised with a thread pool, then hits a wall as a wider set
of resources becomes scarce. Second, a solution I like for that kind of
program: an explicit state machine, where each state gets a dedicated
queue that is aware of the state’s resource requirements.</p>
<h2 id="stages-of-parallelisation">Stages of parallelisation</h2>
<p>We start with a simple program that processes independent work units,
a serial loop that pulls in work (e.g., files in a directory), or wait
for requests on a socket, one work unit at a time.</p>
<p>At some point, there’s enough work to think about parallelisation, and
we choose threads.<sup id="fnref:processes"><a href="#fn:processes">2</a></sup> To keep things simple, we simply spawn
a thread per work unit. Load increases further, and we observe that
we spend more time switching between threads or contending on shared
data than doing actual work. We could use a semaphore to limit the
number of work units we process concurrently, but we might as well
just push work units to a thread pool and recycle threads instead of
wasting resources on a thread-per-request model. We can even start
thinking about queueing disciplines, admission control, backpressure,
etc. Experienced developers will often jump directly to this stage
after the serial loop.</p>
<p>The 80s saw a lot of research on generalising this “flat” parallelism
model to nested parallelism, where work units can spawn additional
requests and wait for the results (e.g., to recursively explore
sub-branches of a search tree). Nested parallelism seems like a good
fit for contemporary network services: we often respond to a request
by sending simpler requests downstream, before merging and munging the
responses and sending the result back to the original requestor. That
may be why futures and promises are so popular these days.</p>
<p>I believe that, for most programs, the futures model is an excellent
answer to the wrong question. The moment we perform I/O (be it
network, disk, or even with hardware accelerators) in order to
generate a result, running at scale will have to mean controlling
more resources than just CPU, and both the futures and the generic
thread pool models fall short.</p>
<p>The issue is that futures only work well when a waiter can help along
the value it needs, with task stealing, while thread pools implement a
trivial scheduler (dedicate a thread to a function until that function
returns) that must be oblivious to resource requirements, since it
handles opaque functions.</p>
<p>Once we have futures that might be blocked on I/O, we can’t
guarantee a waiter will achieve anything by lending CPU time to its
children. We could help sibling tasks, but that way stack overflows
lie.</p>
<p>The deficiency of flat generic thread pools is more subtle. Obviously, one
doesn’t want to take a tight thread pool, with one thread per core,
and waste it on synchronous I/O. We’ll simply kick off I/O
asynchronously, and re-enqueue the continuation on the pool upon
completion!</p>
<p>Instead of doing</p>
<pre><code>A, I/O, B
</code></pre>
<p>in one function, we’ll split the work in two functions and a callback</p>
<pre><code>A, initiate asynchronous I/O
On I/O completion: enqueue B in thread pool
B
</code></pre>
<p>The problem here is that it’s easy to create too many asynchronous
requests, and run out of memory, DOS the target, or delay the rest of
the computation for too long. As soon as the I/O requests has been
initiated in <code>A</code>, the function returns to the thread pool, which will
just execute more instances of <code>A</code> and initiate even more I/O.</p>
<p>At first, when the program doesn’t heavily utilise any resource in
particular, there’s an easy solution: limit the total number of
in-flight work units with a semaphore. Note that I wrote work unit,
not function calls. We want to track logical requests that we started
processing, but for which there is still work to do (e.g., the
response hasn’t been sent back yet).</p>
<p>I’ve seen two ways to cap in-flight work units. One’s buggy, the other
doesn’t generalise.</p>
<p>The buggy implementation acquires a semaphore in the first stage of
request handling (<code>A</code>) and releases it in the last stage (<code>B</code>). The
bug is that, by the time we’re executing <code>A</code>, we’re already using up a
slot in the thread pool, so we might be preventing <code>B</code>s from
executing. We have a lock ordering problem: <code>A</code> acquires a thread
pool slot before acquiring the in-flight semaphore, but <code>B</code> needs to
acquire a slot before releasing the same semaphore. If you’ve seen
code that deadlocks when the thread pool is too small, this was
probably part of the problem.</p>
<p>The correct implementation acquires the semaphore before enqueueing a
new work unit, before shipping a call to <code>A</code> to the thread pool (and
releases it at the end of processing, in <code>B</code>). This only works because
we can assume that the first thing <code>A</code> does is to acquire the
semaphore. As our code becomes more efficient, we’ll want to more
finely track the utilisation of multiple resources, and
pre-acquisition won’t suffice. For example, we might want to limit
network requests going to individual hosts, independently from disk
reads or writes, or from database transactions.</p>
<h2 id="resource-aware-thread-pools">Resource-aware thread pools</h2>
<p>The core issue with thread pools is that the only thing they can do is
run opaque functions in a dedicated thread, so the only way to reserve
resources is to already be running in a dedicated thread. However, the
one resource that every function needs is a thread on which to run, thus
any correct lock order must acquire the thread last.</p>
<p>We care about reserving resources because, as our code becomes more
efficient and scales up, it will start saturating resources that used
to be virtually infinite. Unfortunately, classical thread pools can
only control CPU usage, and actively hinder correct resource
throttling. If we can’t guarantee we won’t overwhelm the supply of a
given resource (e.g., read IOPS), we must accept wasteful
overprovisioning.</p>
<p>Once the problem has been identified, the solution becomes obvious:
make sure the work we push to thread pools describes the resources
to acquire before running the code in a dedicated thread.</p>
<p>My favourite approach assigns one global thread pool (queue) to each
function or processing step. The arguments to the functions will
change, but the code is always the same, so the resource requirements
are also well understood. This does mean that we incur complexity to
decide how many threads or cores each pool is allowed to use. However,
I find that the resulting programs are better understandable at a high
level: it’s much easier to write code that traverses and describes the
work waiting at different stages when each stage has a dedicated
thread pool queue. They’re also easier to model as queueing systems,
which helps answer “what if?” questions without actually implementing
the hypothesis.</p>
<p>In increasing order of annoyingness, I’d divide resources to acquire in
four classes.</p>
<ol>
<li>Resources that may be seamlessly<sup id="fnref:thrashing"><a href="#fn:thrashing">3</a></sup> shared or timesliced, like CPU.</li>
<li>Resources that are acquired for the duration of a single function
call or processing step, like DB connections.</li>
<li>Resources that are acquired in one function call, then released in
another thread pool invocation, like DB transactions, or asynchronous
I/O semaphores.</li>
<li>Resources that may only be released after temporarily using more of
it, or by cancelling work: memory.</li>
</ol>
<p>We don’t really have to think about the first class of resources, at
least when it comes to correctness. However, repeatedly running the
same code on a given core tends to improve performance, compared to
running all sorts of code on all cores.</p>
<p>The second class of resources may be acquired once our code is running
in a thread pool, so one could pretend it doesn’t exist. However, it
is more efficient to batch acquisition, and execute a bunch of calls
that all need a given resource (e.g., a DB connection from a
connection pool) before releasing it, instead of repetitively
acquiring and releasing the same resource in back-to-back function
calls, or blocking multiple workers on the same
bottleneck.<sup id="fnref:SEDA"><a href="#fn:SEDA">4</a></sup> More importantly, the property of always being
acquired and released in the same function invocation, is a global
one: as soon as even one piece of code acquires a given resource and
releases in another thread pool call (e.g., acquires a DB connection,
initiates an asynchronous network call, writes the result of the call
to the DB, and releases the connection), we must always treat that
resource as being in the third, more annoying, class. Having explicit
stages with fixed resource requirements helps us confirm resources
are classified correctly.</p>
<p>The third class of resources <em>must</em> be acquired in a way that
preserves forward progress in the rest of the system. In particular,
we must never have all workers waiting for resources of this third
class. In most cases, it suffices to make sure there at least as many
workers as there are queues or stages, and to only let each stage run
the initial resource acquisition code in one worker at a time.
However, it can pay off to be smart when different queued items
require different resources, instead of always trying to satisfy
resource requirements in FIFO order.</p>
<p>The fourth class of resources is essentially heap memory. Memory is
special because the only way to release it is often to complete the
computation. However, moving the computation forward will use even
more heap. In general, my only solution is to impose a hard cap on the
total number of in-flight work units, and to make sure it’s easy to
tweak that limit at runtime, in disaster scenarios. If we still run
close to the memory capacity with that limit, the code can either
crash (and perhaps restart with a lower in-flight cap), or try to
cancel work that’s already in progress. Neither option is very
appealing.</p>
<p>There are some easier cases. For example, I find that temporary bumps
in heap usage can be caused by parsing large responses from
idempotent (<code>GET</code>) requests. It would be nice if networking subsystems
tracked memory usage to dynamically throttle requests, or
even cancel and retry idempotent ones.</p>
<p>Once we’ve done the work of explicitly writing out the processing
steps in our program as well as their individual resource
requirements, it makes sense to let that topology drive the structure
of the code.</p>
<p>Over time, we’ll gain more confidence in that topology and bake it in
our program to improve performance. For example, rather than limiting
the number of in-flight requests with a semaphore, we can have a
fixed-size allocation pool of request objects. We can also
selectively use bounded ring buffers once we know we wish to impose a
limit on queue size. Similarly, when a sequence (or subgraph) of
processing steps is fully synchronous or retires in order, we can
control both the queue size and the number of in-flight work units
with a <a href="https://lmax-exchange.github.io/disruptor/">disruptor</a>, which
should also improve locality and throughput under load. These
transformations are easy to apply once we know what the movement of
data and resource looks like. However, they also ossify the structure
of the program, so I only think about such improvements if they
provide a system property I know I need (e.g., a limit on the number
of in-flight requests), or once the code is functional and we have
load-testing data.</p>
<p>Complex programs are often best understood as state machines. These
state machines can be implicit, or explicit. I prefer the latter. I
claim that it’s also preferable to have one thread pool<sup id="fnref:queue"><a href="#fn:queue">5</a></sup> per
explicit state than to dump all sorts of state transition logic
in a shared pool. If writing functions that process flat tables is
data-oriented programming, I suppose I’m arguing for data-oriented
state machines.</p>

</div></div></div>
    </div>
    <footer>
        <div>created by buildstarted &copy; 2020 <a href="/about">about</a></div>
        <div>Share this page on social media: copy and paste this url https://linksfor.dev/</div>
        <div>If you prefer RSS: <a href="https://linksfor.dev/feed.xml">https://linksfor.dev/feed.xml</a></div>
		<div>Customer satisfaction guaranteed to be optional.</div>
    </footer>
    
    <script async defer>
        _dna = window._dna || {};
        _dna.siteId = "linksfor.devs";
        _dna.outlink = true;

        (function () {
            let dna = document.createElement('script');
            dna.type = 'text/javascript';
            dna.async = true;
            dna.src = '//dna.buildstarted.com/t.js';
            let s = document.getElementsByTagName('script')[0];
            s.parentNode.insertBefore(dna, s);
        })();
    </script>
    <noscript><img src="//dna.buildstarted.com/g?siteId=linksfor.devs"/></noscript>
</body>
</html>