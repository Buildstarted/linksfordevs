<!DOCTYPE html>
<html lang="en">
<head>
    <title>
A Concurrency Cost Hierarchy - linksfor.dev(s)    </title>
    <meta charset="utf-8">
    <link rel="alternate" type="application/rss+xml" title="Linksfor.dev(s) feed" href="https://linksfor.dev/feed.rss" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta name="google" value="notranslate">
    <link rel="stylesheet" type="text/css" href="/style.css" />
    <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon">
    <link rel="icon" href="/favicon.ico" type="image/x-icon">
    <meta property="og:title" content="A Concurrency Cost Hierarchy - linksfor.dev(s)"/>
    <meta property="article:author" content="Travis Downs"/>
    <meta property="og:description" content="A blog about low-level software and hardware performance."/>
    <meta property="og:type" content="website"/>
    <meta property="og:url" content="https://travisdowns.github.io/blog/2020/07/06/concurrency-costs.html"/>

<meta property="og:site_name" content="linksfor.dev(s)" />
</head>
<body>
    <div class="devring" style="background: #222">
        <div style="text-align:center">Explore other dev related sites in this ring. If you would like to join this ring <a href="https://devring.club">click here</a>.</div>
        <div class="grid">
            <div style="display: grid; grid-template-columns: .5fr 1fr 1fr 1fr; text-align: center;">
                <span class="devring-title"><a href="https://devring.club/">devring.club</a></span>
                <a href="https://devring.club/sites/1/prev" class="devring-previous">Previous</a>
                <a href="https://devring.club/random" class="devring-random">Random</a>
                <a href="https://devring.club/sites/1/next" class="devring-next">Next</a>
            </div>
        </div>
    </div>
    <div class="grid">
        <h1>
<a href="/" style="color:inherit">linksfor.dev(s)</a>
        </h1>
        <title>linksfor.dev(s) - A Concurrency Cost Hierarchy</title>
<div class="readable">
        <h1>A Concurrency Cost Hierarchy</h1>
            <div>by Travis Downs</div>
            <div>Reading time: 45-57 minutes</div>
        <div>Posted here: 06 Jul 2020</div>
        <p><a href="https://travisdowns.github.io/blog/2020/07/06/concurrency-costs.html">https://travisdowns.github.io/blog/2020/07/06/concurrency-costs.html</a></p>
        <hr/>
<div id="readability-page-1" class="page"><div aria-label="Content">
      <div>
        <article itemscope="" itemtype="http://schema.org/BlogPosting">

  

  <div itemprop="articleBody">
    
<!-- boilerplate 
page.assets: /assets/concurrency-costs
assetpath: /assets/concurrency-costs
tablepath: /misc/tables/concurrency-costs
-->

<h2 id="introduction">Introduction</h2>

<p>Concurrency is hard to get <em>correct</em>, at least for those of us unlucky enough to be writing in languages which expose directly the guts of concurrent hardware: threads and shared memory. Getting concurrency correct <em>and</em> fast is hard, too. Your knowledge about single-threaded optimization often won’t help you: at a micro (instruction) level we can’t simply apply the usual rules of μops, dependency chains, throughput limits, and so on. The rules are different.</p>

<p>If that first paragraph got your hopes up, this second one is here to dash them: I’m not actually going to do a deep dive into the very low level aspects of concurrent performance. There are a lot of things we just don’t know about how atomic instructions and fences execute, and we’ll save that for another day.</p>

<p>Instead, I’m going to describe a higher level taxonomy that I use to think about concurrent performance. We’ll group the performance of concurrent operations into six broad <em>levels</em> running from fast to slow, with each level differing from its neighbors by roughly an order of magnitude in performance.</p>

<p>I often mind myself thinking in terms of these categories when I need high performance concurrency: what is the best level I can practically achieve for the given problem? Keeping the levels in mind is useful both during initial design (sometimes a small change in requirements or high level design can allow you to achieve a better level), and also while evaluating existing systems (to better understand existing performance and evaluate the path of least resistance to improvements).</p>

<h3 id="a-real-world-example">A “Real World” Example</h3>

<p>I don’t want this to be totally abstract, so we will use a real-world-if-you-squint<sup id="fnref:realworld"><a href="#fn:realworld">1</a></sup> running example throughout: safely incrementing an integer counter across threads. By <em>safely</em> I mean without losing increments, producing out-of-thin air values, frying your RAM or making more than a minor rip in space-time.</p>

<h3 id="source-and-results">Source and Results</h3>

<p>The source for every benchmark here is <a href="https://github.com/travisdowns/concurrency-hierarchy-bench">available</a>, so you can follow along and even reproduce the results or run the benchmarks on your own hardware. All of the results discussed here (and more) are available in the same repository, and each plot includes a <code>[data table]</code> link to the specific subset used to generate the plot.</p>

<h3 id="hardware">Hardware</h3>

<p>All of the performance results are provided for several different hardware platforms: Intel Skylake, Ice Lake, Amazon Graviton and Graviton 2. However except when I explicitly mention other hardware, the prose refers to the results on Skylake. Although the specific numbers vary, most of the qualitative relationships hold for the hardware hardware too, but <em>not always</em>. Not only does the hardware vary, but the OS and library implementations will vary as well.</p>

<p>It’s almost inevitable that this will be used to compare across hardware (“wow, Graviton 2 sure kicks Graviton 1’s ass”), but that’s not my goal here. The benchamrks are written primarily to tease apart the characteristics of the different levels, and <em>not</em> as a hardware shootout.</p>

<p>Find below the details of the hardware used:</p>

<table>
  <thead>
    <tr>
      <th>Micro-architecture</th>
      <th>ISA</th>
      <th>Model</th>
      <th>Tested Frequency</th>
      <th>Cores</th>
      <th>OS</th>
      <th>Instance Type</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Skylake</td>
      <td>x86</td>
      <td>i7-6700HQ</td>
      <td>2.6 GHz</td>
      <td>4</td>
      <td>Ubuntu 20.04</td>
      <td>&nbsp;</td>
    </tr>
    <tr>
      <td>Ice Lake</td>
      <td>x86</td>
      <td>i5-1035G4</td>
      <td>3.3 GHz</td>
      <td>4</td>
      <td>Ubuntu 19.10</td>
      <td>&nbsp;</td>
    </tr>
    <tr>
      <td>Graviton</td>
      <td>AArch64</td>
      <td>Coretex-A72</td>
      <td>2.3 GHz</td>
      <td>16</td>
      <td>Ubuntu 20.04</td>
      <td>a1.4xlarge</td>
    </tr>
    <tr>
      <td>Graviton 2</td>
      <td>AArch64</td>
      <td>Neoverse N1</td>
      <td>2.5 GHz</td>
      <td>16<sup id="fnref:g2cores"><a href="#fn:g2cores">2</a></sup></td>
      <td>Ubuntu 20.04</td>
      <td>c6g.4xlarge</td>
    </tr>
  </tbody>
</table>

<h2 id="level-2-contended-atomics">Level 2: Contended Atomics</h2>

<p>You’d probably expect this hierarchy to be introduced from fast to slow, or vice-versa, but we’re all about defying expectations here and we are going to start in the <em>middle</em> and work our way outwards. The middle (rounding down) turns out to be <em>level 2</em> and that’s where we will jump in.</p>

<p>The most elementary way to safely modify any shared object is to use a lock. It mostly <em>just works</em> for any type of object, no matter its structure or the nature of the modifications. Almost any mainstream CPU from the last thirty years has some type of locking<sup id="fnref:parisc"><a href="#fn:parisc">3</a></sup> instruction accessible to userspace.</p>

<p>So our baseline increment implementation will use a simple mutex of type <code>T</code> to protect a plain integer variable:</p>

<div><div><pre><code><span>T</span> <span>lock</span><span>;</span>
<span>uint64_t</span> <span>counter</span><span>;</span>

<span>void</span> <span>bench</span><span>(</span><span>size_t</span> <span>iters</span><span>)</span> <span>{</span>
    <span>while</span> <span>(</span><span>iters</span><span>--</span><span>)</span> <span>{</span>
        <span>std</span><span>::</span><span>lock_guard</span><span>&lt;</span><span>T</span><span>&gt;</span> <span>holder</span><span>(</span><span>lock</span><span>);</span>
        <span>counter</span><span>++</span><span>;</span>
    <span>}</span>
<span>}</span>
</code></pre></div></div>

<p>We’ll call this implementation <em><abbr title="Uses a std::mutex and std::lock_guard to protect a plain integer counter.">mutex add</abbr></em>, and on my 4 CPU Skylake-S i7-6700HQ machine, when I use the vanilla <code>std::mutex</code> I get the following results for 2 to 4 threads:</p>



<p>The reported value is the median of all trials, and the vertical black error lines at the top of each bar indicate the <em>interdecile range</em>, i.e. the values at the 10th and 90th percentile. Where the error bars don’t show up, it means there no difference between the p10 and p90 values at all, at least within the limits of the reporting resolution (100 picoseconds).</p>

<p>This shows that the baseline contended cost to modify an integer protected by a lock starts at about 125 nanoseconds for two threads, and grows somewhat with increasing thread count.</p>

<p>I can already hear someone saying: <em>If you are just modifying a single 64-bit integer, skip the lock and just directly use the atomic operations that most ISAs support!</em></p>

<p>Sure, let’s add a couple of variants that do that. The <code>std::atomic&lt;T&gt;</code> template makes this easy: we can wrap any type meeting some basic requirements and then manipulate it atomically. The easiest of all is to use <code>std::atomic&lt;uint64&gt;::operator++()</code><sup id="fnref:post"><a href="#fn:post">4</a></sup> and this gives us <em><abbr title="Uses an atomic increment on a single shared counter.">atomic add</abbr></em>:</p>

<div><div><pre><code><span>std</span><span>::</span><span>atomic</span><span>&lt;</span><span>uint64_t</span><span>&gt;</span> <span>atomic_counter</span><span>{};</span>

<span>void</span> <span>atomic_add</span><span>(</span><span>size_t</span> <span>iters</span><span>)</span> <span>{</span>
    <span>while</span> <span>(</span><span>iters</span><span>--</span><span>)</span> <span>{</span>
        <span>atomic_counter</span><span>++</span><span>;</span>
    <span>}</span>
<span>}</span>
</code></pre></div></div>

<p>The other common approach would be to use <a href="https://en.wikipedia.org/wiki/Compare-and-swap">compare and swap (<abbr title="Compare-and-swap: an atomic operation implemented on x86 and other CPUs">CAS</abbr>)</a> to load the existing value, add one and then <abbr title="Compare-and-swap: an atomic operation implemented on x86 and other CPUs">CAS</abbr> it back if it hasn’t changed. If it <em>has</em> changed, the increment raced with another thread and we try again.</p>

<p>Note that even if you use increment at the source level, the assembly might actually end up using <abbr title="Compare-and-swap: an atomic operation implemented on x86 and other CPUs">CAS</abbr> if your hardware doesn’t support atomic increment<sup id="fnref:atomicsup"><a href="#fn:atomicsup">5</a></sup>, or if your compiler or runtime just don’t take advantage of atomic operations even though they are available (e.g., see what even the newest version of <a href="https://godbolt.org/z/5h4K7y">icc does</a> for atomic increment, and what Java did for years<sup id="fnref:java"><a href="#fn:java">6</a></sup>). This caveat doesn’t apply to any of our tested platforms, however.</p>

<p>Let’s add a counter implementation that uses <abbr title="Compare-and-swap: an atomic operation implemented on x86 and other CPUs">CAS</abbr> as described above, and we’ll call it <em><abbr title="Uses a CAS loop to increment a single shared counter.">cas add</abbr></em>:</p>

<div><div><pre><code><span>std</span><span>::</span><span>atomic</span><span>&lt;</span><span>uint64_t</span><span>&gt;</span> <span>cas_counter</span><span>;</span>

<span>void</span> <span>cas_add</span><span>(</span><span>size_t</span> <span>iters</span><span>)</span> <span>{</span>
    <span>while</span> <span>(</span><span>iters</span><span>--</span><span>)</span> <span>{</span>
        <span>uint64_t</span> <span>v</span> <span>=</span> <span>cas_counter</span><span>.</span><span>load</span><span>();</span>
        <span>while</span> <span>(</span><span>!</span><span>cas_counter</span><span>.</span><span>compare_exchange_weak</span><span>(</span><span>v</span><span>,</span> <span>v</span> <span>+</span> <span>1</span><span>))</span>
            <span>;</span>
    <span>}</span>
<span>}</span>
</code></pre></div></div>

<p>Here’s what these look like alongside our existing <code>std::mutex</code> benchmark:</p>



<p>The first takeaway is that, at least in this <em>unrealistic maximum contention</em> benchmark, using <abbr title="Uses an atomic increment on a single shared counter.">atomic add</abbr> (<a href="https://www.felixcloutier.com/x86/xadd"><code>lock xadd</code></a> at the hardware level) is significantly better than <abbr title="Compare-and-swap: an atomic operation implemented on x86 and other CPUs">CAS</abbr>. The second would be that <code>std::mutex</code> doesn’t come out looking all that bad on Skylake. It is only slightly worse than the <abbr title="Compare-and-swap: an atomic operation implemented on x86 and other CPUs">CAS</abbr> approach at 2 cores and beats it at 3 and 4 cores. It is slower than the atomic increment approach, but less than three times as slow and seems to be scaling in a reasonable way.</p>

<p>All of these operations are belong to <em>level 2</em> in the hierarchy. The primary characteristic of level 2 is that they make a <em>contended access</em> to a shared variable. This means that at a minimum, the line containing the data needs to move out to the caching agent that manages coherency<sup id="fnref:l3"><a href="#fn:l3">7</a></sup>, and then back up to the core that will receive ownership next. That’s about 70 cycles minimum just for that operation<sup id="fnref:inter"><a href="#fn:inter">8</a></sup>.</p>

<p>Can it get slower? You bet it can. <em>Way</em> slower.</p>

<h3 id="level-3-system-calls">Level 3: System Calls</h3>

<p>The next level up (“up” is not good here…) is level 3. The key characteristic of implementations at this level is that they make a <em>system call on almost every operation</em>.</p>

<p>It is easy to write concurrency primitives that make a system call <em>unconditionally</em> (e.g,. a lock which always tries to wake waiters via a <code>futex(2)</code> call, even if there aren’t any), but we won’t look at those here. Rather we’ll take a look at a case where the fast path is written to avoid a system call, but the design or way it is used implies that such a call usually happens anyway.</p>

<p>Specifically, we are going to look at some <em>fair locks</em>. Fair locks allow threads into the critical section in the same order they began waiting. That is, when the critical section becomes available, the thread that has been waiting the longest is given the chance to take it.</p>

<p>Sounds like a good idea, right? Sometimes yes, but as we will see it can have significant performance implications.</p>

<p>On the menu are three different fair locks.</p>

<p>The first is a <a href="https://en.wikipedia.org/wiki/Ticket_lock">ticket lock</a> with a <code>sched_yield</code> in the spin loop. The idea of the yield is to give other threads which may hold the lock time to run. This <code>yield()</code> approach is publicly frowned upon by concurrency experts<sup id="fnref:notwhat"><a href="#fn:notwhat">9</a></sup>, who then sometimes go right ahead and use it anyway.</p>

<p>We will call it <abbr title="A ticket lock that calls sched_yield in a spin loop while waiting for its turn.">ticket yield</abbr> and it looks like this:</p>



<div><div><pre><code><span>/**
 * A ticket lock which uses sched_yield() while waiting
 * for the ticket to be served.
 */</span>
<span>class</span> <span>ticket_yield</span> <span>{</span>
    <span>std</span><span>::</span><span>atomic</span><span>&lt;</span><span>size_t</span><span>&gt;</span> <span>dispenser</span><span>{},</span> <span>serving</span><span>{};</span>

<span>public:</span>
    <span>void</span> <span>lock</span><span>()</span> <span>{</span>
        <span>auto</span> <span>ticket</span> <span>=</span> <span>dispenser</span><span>.</span><span>fetch_add</span><span>(</span><span>1</span><span>,</span> <span>std</span><span>::</span><span>memory_order_relaxed</span><span>);</span>

        <span>while</span> <span>(</span><span>ticket</span> <span>!=</span> <span>serving</span><span>.</span><span>load</span><span>(</span><span>std</span><span>::</span><span>memory_order_acquire</span><span>))</span>
            <span>sched_yield</span><span>();</span>
    <span>}</span>

    <span>void</span> <span>unlock</span><span>()</span> <span>{</span>
        <span>serving</span><span>.</span><span>store</span><span>(</span><span>serving</span><span>.</span><span>load</span><span>()</span> <span>+</span> <span>1</span><span>,</span> <span>std</span><span>::</span><span>memory_order_release</span><span>);</span>
    <span>}</span>
<span>};</span>
</code></pre></div></div>

<p>Let’s plot the performance results for this lock alongside the existing approaches:</p>



<p>This is level 3 visualized: it is an order of magnitude slower than the level 2 approaches. The slowdown comes from the <code>sched_yield</code> call: this is a system call and these are generally on the order of 100s of nanoseconds<sup id="fnref:spectre"><a href="#fn:spectre">10</a></sup>, and it shows in the results.</p>

<p>This lock <em>does</em> have a fast path where <code>sched_yield</code> isn’t called: if the lock is available, no spinning occurs and <code>sched_yield</code> is never called. However, the combination of being a <em>fair</em> lock and the high contention in this test means that a lock convoy quickly forms (we’ll describe this in more detail later) and so the spin loop is entered basically every time <code>lock()</code> is called.</p>

<p>So have we <em>now</em> fully plumbed the depths of slow concurrency constructs? Not in close. We are only now just about to cross the River Styx.</p>

<h4 id="revisiting-stdmutex">Revisiting std::mutex</h4>

<p>Before we proceed, let’s quickly revisit the <code>std::mutex</code> implementation discussed in level 2 in light of our definition of level 3 as requiring a system call. Doesn’t <code>std::mutex</code> <em>also</em> make system calls? If a thread tries to lock a <code>std::mutex</code> object which is already locked, we expect that thread to block using OS-provided primitives. So why isn’t it level 3 and slow like <abbr title="A ticket lock that calls sched_yield in a spin loop while waiting for its turn.">ticket yield</abbr>?</p>

<p>The primary reason is that it makes <em>few</em> system calls in practice. Through a combination of spinning and unfairness I measure only about 0.18 system calls per increment, with three threads on my Skylake box. So <em>most</em> increments happen without a system call. On the other hand, <abbr title="A ticket lock that calls sched_yield in a spin loop while waiting for its turn.">ticket yield</abbr> makes about 2.4 system calls per increment, more than an order of magnitude more, and so it suffers a corresponding decrease in performance.</p>

<p>That out of way, let’s get even slower.</p>

<h3 id="level-4-implied-context-switch">Level 4: Implied Context Switch</h3>

<p>The next level is when the implementation forces a significant number of concurrent operations to cause a <em>context switch</em>.</p>

<p>The yielding lock wasn’t resulting in many context switches, since we are not running more threads than there are cores, and so there usually is no other runnable process (except for the occasional background process). Therefore, the current thread stays on the CPU when we call <code>sched_yield</code>. Of course, this burns a lot of CPU.</p>

<p>As the experts recommend whenever one suggests <em>yielding</em> in a spin loop, let us try a <em>blocking lock</em> instead.</p>

<div><p><strong>Blocking Locks</strong></p><p>

A more resource friendly design, and one that will often perform better is a <em>blocking</em> lock.</p><p>Rather than busy waiting, these locks ask the OS to put the current thread to sleep until the lock becomes available. On Linux, the <a href="http://man7.org/linux/man-pages/man2/futex.2.html"><code>futex(3)</code></a> system call is the preferred way to accomplish this, while on Windows you have the <a href="https://docs.microsoft.com/en-us/windows/win32/api/synchapi/nf-synchapi-waitforsingleobject"><code>WaitFor*Object</code></a> API family. Above the OS interfaces, things like C++’s <code>std::condition_varibale</code> provide a general purpose mechanism to wait until an arbitrary condition is true.</p></div>

<p>Our first blocking lock is again a ticket-based design, except this time it uses a condition variable to block when it detects that it isn’t first in line to be served (i.e., that the lock was held by another thread). We’ll name it <abbr title="A ticket lock which blocks if it cannot immediately acquire the lock.">ticket blocking</abbr> and it looks like this:</p>

<div><div><pre><code><span>void</span> <span>blocking_ticket</span><span>::</span><span>lock</span><span>()</span> <span>{</span>
    <span>auto</span> <span>ticket</span> <span>=</span> <span>dispenser</span><span>.</span><span>fetch_add</span><span>(</span><span>1</span><span>,</span> <span>std</span><span>::</span><span>memory_order_relaxed</span><span>);</span>

    <span>if</span> <span>(</span><span>ticket</span> <span>==</span> <span>serving</span><span>.</span><span>load</span><span>(</span><span>std</span><span>::</span><span>memory_order_acquire</span><span>))</span>
        <span>return</span><span>;</span> <span>// uncontended case</span>

    <span>std</span><span>::</span><span>unique_lock</span><span>&lt;</span><span>std</span><span>::</span><span>mutex</span><span>&gt;</span> <span>lock</span><span>(</span><span>mutex</span><span>);</span>
    <span>while</span> <span>(</span><span>ticket</span> <span>!=</span> <span>serving</span><span>.</span><span>load</span><span>(</span><span>std</span><span>::</span><span>memory_order_acquire</span><span>))</span> <span>{</span>
        <span>cvar</span><span>.</span><span>wait</span><span>(</span><span>lock</span><span>);</span>
    <span>}</span>
<span>}</span>

<span>void</span> <span>blocking_ticket</span><span>::</span><span>unlock</span><span>()</span> <span>{</span>
    <span>std</span><span>::</span><span>unique_lock</span><span>&lt;</span><span>std</span><span>::</span><span>mutex</span><span>&gt;</span> <span>lock</span><span>(</span><span>mutex</span><span>);</span>
    <span>auto</span> <span>s</span> <span>=</span> <span>serving</span><span>.</span><span>load</span><span>(</span><span>std</span><span>::</span><span>memory_order_relaxed</span><span>)</span> <span>+</span> <span>1</span><span>;</span>
    <span>serving</span><span>.</span><span>store</span><span>(</span><span>s</span><span>,</span> <span>std</span><span>::</span><span>memory_order_release</span><span>);</span>
    <span>auto</span> <span>d</span> <span>=</span> <span>dispenser</span><span>.</span><span>load</span><span>(</span><span>std</span><span>::</span><span>memory_order_relaxed</span><span>);</span>
    <span>assert</span><span>(</span><span>s</span> <span>&lt;=</span> <span>d</span><span>);</span>
    <span>if</span> <span>(</span><span>s</span> <span>&lt;</span> <span>d</span><span>)</span> <span>{</span>
        <span>// wake all waiters</span>
        <span>cvar</span><span>.</span><span>notify_all</span><span>();</span>
    <span>}</span>
<span>}</span>
</code></pre></div></div>

<p>The main difference with the earlier lock is that when we don’t acquire the lock immediately (we don’t return at the location marked <code>// uncontended case</code>). Instead, we take the mutex associated with the condition variable and wait until notified. Every time we are notified we check if it is our turn.</p>

<p>Even without <abbr title="When a waiter on a condition variable is woken up even though no other thread notified it.">spurious wakeups</abbr> we might get woken many times, because this lock suffers from the <em>thundering herd</em> problem where every waiter is woken on <code>unlock()</code> even though only one will ultimately be able to get the lock.</p>

<p>We’ll try a second design too, that doesn’t suffer from thundering herd. This is a queued lock, where each lock waits its own private node in a queue of waiters, so only a single waiter (the new lock owner) is woken up in unlock. We will call it <abbr title="A blocking ticket lock where each waiter waits on a unique condition variable.">queued fifo</abbr> and if you’re interested in the implementation you can find it <a href="https://github.com/travisdowns/concurrency-hierarchy-bench/blob/9b8e0e0dfec7d38036d114038c6a9ed020b5b775/fairlocks.cpp#L61">here</a>.</p>

<p>Here’s how our new locks perform against the existing crowd:</p>



<p>You’re probably seeing the pattern now: performance is again a new level of terrible compared to the previous contenders. About an order of magnitude slower than the yielding approach, which was already slower than the earlier approaches, which are now just slivers a few pixels high on the plots. The queued version of the lock does slightly better at increasing thread counts (<em>especially</em> on Graviton [2]), as might be expected from the lack of the thundering herd effect, but is still very slow because the primary problem isn’t thundering herd, but rather a <a href="https://en.wikipedia.org/wiki/Lock_convoy"><em>lock convoy</em></a>.</p>

<div><p><strong>Lock Convoy</strong></p><p>

Unlike unfair locks, fair locks result can result in sustained convoys involving only a single lock, once the contention reaches a certain point<sup id="fnref:hyst"><a href="#fn:hyst">11</a></sup>.</p><p>

Consider what happens when two threads, <code>A</code> and <code>B</code>, try to acquire the lock repeatedly. Let’s say <code>A</code> gets ticket 1 and <code>B</code> ticket 2. So <code>A</code> gets to go first and <code>B</code> has to wait, and for these implementations that means blocking (we can say the thread is <em>parked</em> by the OS). Now, <code>A</code> unlocks the lock and sees <code>B</code> waiting and wakes it. <code>A</code> is still running and soon tries to get the lock again, receiving ticket 3, but it cannot acquire the lock immediately because the lock is <em>fair</em>: <code>A</code> can’t jump the queue and acquire the lock with ticket 3 before <code>B</code>, holding ticket 2, gets its chance to enter the lock.</p><p>

Of course, <code>B</code> is going to be a while: it needs to be woken by the scheduler and this takes a microsecond or two, at least. Now <code>B</code> wakes and gets the lock, and the same scenario repeats itself with the roles reversed. The upshot is that there is a full context switch for each acquisition of the lock.</p><p>

Unfair locks avoid this problem because they all queue jumping: in the scenario above, <code>A</code> (or any other thread) could re-acquire the lock after unlocked it, before <code>B</code> got its chance. So the use of the shared resource doesn’t grind to a halt while <code>B</code> wakes up.</p></div>

<p>So, are you tired of seeing mostly-white plots where the newly introduced algorithm relegates the rest of the pack to little chunks of color near the x-axis, yet?</p>

<p>I’ve just got one more left on the slow end of the scale. Unlike the other examples, I haven’t actually diagnosed something <em>this</em> bad in real life, but examples are out there.</p>

<h3 id="level-5-catastrophe">Level 5: Catastrophe</h3>

<p>Here’s a ticket lock which is identical to the <a href="#ys-lock">first ticket lock we saw</a>, except that the <code>sched_yield();</code> is replaced by <code>;</code>. That is, it busy waits instead of yielding (look <a href="https://github.com/travisdowns/concurrency-hierarchy-bench/blob/9b8e0e0dfec7d38036d114038c6a9ed020b5b775/fairlocks.cpp#L31">here</a> for the spin flavors which specialize on a shared ticket lock template). You could also replace this by an CPU-specific “relax” instruction like <a href="https://www.felixcloutier.com/x86/pause"><code>pause</code></a>, but it won’t change the outcome (see <a href="https://github.com/travisdowns/concurrency-hierarchy-bench/blob/9b8e0e0dfec7d38036d114038c6a9ed020b5b775/fairlocks.hpp#L26">here</a>). We call it <abbr title="A traditional spin-based ticket lock that does a hot spin while waiting for its ticket to be next.">ticket spin</abbr>, and here’s how it performs compared to the existing candidates:</p>



<p>What? That doesn’t look too bad at all. In fact, it is only slightly worse than the level 2 crew, the fastest we’ve seen so far<sup id="fnref:huh"><a href="#fn:huh">12</a></sup>.</p>

<p>The picture changes if we show the results for up to 6 threads, rather than just 4. Since I have 4 available cores<sup id="fnref:noht"><a href="#fn:noht">13</a></sup>, this means that not all the test threads will be ale to run at once:</p>



<p>Now it becomes clear why this level is called <em>catastrophic</em>. As soon as we oversubscribe the number of available cores, performance gets about <em>five hundred times worse</em>. We go from 100s of nanoseconds to 100s of microseconds. I don’t show more threads, but it only gets worse as you add more.</p>

<p>We are also about an order of magnitude slower than the best solution (<abbr title="A blocking ticket lock where each waiter waits on a unique condition variable.">queued fifo</abbr>) of the previous level, although it varies a lot by hardware: on Ice Lake the difference is more like <em>forty</em> times, while on Graviton this solution is actually slightly faster than <abbr title="A ticket lock which blocks if it cannot immediately acquire the lock.">ticket blocking</abbr> (also level 4) at 17 threads. Note also the huge error bars. This is the least consistent benchmark of the bunch and exhibits a lot of variance and the slowest and fastest runs might vary by a factor of 100.</p>

<h4 id="lock-convoy-on-steroids">Lock Convoy on Steroids</h4>

<p>So what happens here?</p>

<p>It’s similar to the lock convoy described above: all the threads queue on the lock and acquire it in a round-robin order due to the fair design. The difference is that threads don’t block when they can’t acquire the lock. This works out great when the cores are not oversubscribed, but falls off a cliff otherwise.</p>

<p>Imagine 5 threads, <code>T1</code>, <code>T2</code>, …, <code>T5</code>, where <code>T5</code> is the one not currently running. As soon as <code>T5</code> is the thread that needs the acquire the lock next (i.e., <code>T5</code>’s saved ticket value is equal to <code>dispensing</code>), nothing will happen because <code>T1</code> through <code>T4</code> are busily spinning away waiting for their turn. The OS scheduler sees no reason to interrupt them until their time slice expires. Time slices are usually measured in millseconds. Once one thread is preempted, say <code>T1</code>, <code>T5</code> will get the chance to run, but at most 4 total acquisitions can happen (<code>T5</code>, plus any of <code>T2</code>, <code>T3</code>, <code>T4</code>), before it’s <code>T1</code>’s turn. <code>T1</code> is waiting for their chance to run again, but since everyone is spinning this won’t occur until another time slice expires.</p>

<p>So the lock can only be acquired a few times (at most <code>$(nproc)</code> times), or as little as once<sup id="fnref:once"><a href="#fn:once">14</a></sup>, every time slice. Modern Linux using <a href="https://en.wikipedia.org/wiki/Completely_Fair_Scheduler">CFS</a> doesn’t have a fixed timeslice, but on my system, <code>sched_latency_ns</code> is 18,000,000 which means that we expect two threads competing for one core to get a typical timeslice of 9 ms. The measured numbers are roughly consistent with a timeslice of single-digit milliseconds.</p>

<p>If I was good at diagrams, there would be a diagram here.</p>

<p>Another way of thinking about this is that in this over-subscription scenario, the <abbr title="A traditional spin-based ticket lock that does a hot spin while waiting for its ticket to be next.">ticket spin</abbr> lock implies roughly the same number of context switches as the blocking ticket lock<sup id="fnref:perf"><a href="#fn:perf">15</a></sup>, but in the former case each context switch comes with a giant delay caused by the need to exhaust the timeslice, while in the blocking case we are only limited by how fast a context switch can occur.</p>

<p>Interestingly, although this benchmark uses 100% CPU on every core, the performance of the benchmark in the oversubscribed case doesn’t almost depend on your CPU speed! Performance is approximately the same if I throttle my CPU to 1 GHz, or enable turbo up to 3.5 GHz. All of other implementations scale almost proportionally with CPU frequency. The benchmark does scale strongly with adjustment to <code>sched_latency_ns</code> (and <code>sched_min_granularity_ns</code> if the former is set low enough): lower scheduling latency values gives proportionally better performance as the time slices shrink, helping to confirm our theory of how this works.</p>

<p>This behavior also explains the large amount of variance once the available cores are oversubscribed: by definition, not all threads will be running at once, so the test becomes very sensitive to exactly where the not-running threads took their context switch. At the beginning of the test, only 4 of 6 threads will be running, and the two will switched out, still waiting on the the <a href="https://github.com/travisdowns/concurrency-hierarchy-bench/blob/master/cyclic-barrier.hpp">barrier</a> that synchronizes the test start. Since the two switched out thread haven’t tried to get the lock yet, the four running threads will be able to quickly share the lock between themselves, since the six-thread convoy hasn’t been set up.</p>

<p>This runs up the “iteration count” (work done) during an initial period which varies randomly, until the first context switch lets the fifth thread join the competition and then the convoy gets set up<sup id="fnref:csdepend"><a href="#fn:csdepend">16</a></sup>. That’s when the catastrophe starts. This makes the results very noisy: for example, if you set a too-short time period for a trial, the <em>entire test</em> is composed of this initial phase and the results are artificially “good”.</p>

<p>We can probably invent something even worse, but that’s enough for now. Let’s move on to scenarios that are <em>faster</em> than the use of vanilla <abbr title="Uses an atomic increment on a single shared counter.">atomic add</abbr>.</p>

<h3 id="level-1-uncontended-atomics">Level 1: Uncontended Atomics</h3>

<p>Recall that we started at level 2: contended atomics. The name gives it away: the next faster level is when atomic operations are used but there is no contention, either by design or by luck. You might have noticed that so far we’ve only shown results for at least two threads. That’s because the single threaded case involves no contention, and so every implementation so far is level 1 if run on a single thread<sup id="fnref:notexx"><a href="#fn:notexx">17</a></sup>.</p>

<p>Here’s the results for all the implementations we’ve looked at so far, for a single thread:</p>



<p>The fastest implementations run in about 10 nanoseconds, which is 5x faster than the fastest solution for 2 or more threads. The <em>slowest</em> implementation (<abbr title="A blocking ticket lock where each waiter waits on a unique condition variable.">queued fifo</abbr>) for one thread ties the <em>fastest</em> implementation (<abbr title="Uses an atomic increment on a single shared counter.">atomic add</abbr>) at two threads, and beats it handily at three or four.</p>

<p>The number overlaid on each bar is the number of atomic operations<sup id="fnref:atomhow"><a href="#fn:atomhow">18</a></sup> each implementation makes per increment. It is obvious that the performance is almost directly proportional to the number of atomic instructions. On the other hand, performance does <em>not</em> have much of a relationship with the total number of instructions of any type, which vary a lot even between algorithms with the same performance as the following table shows:</p>

<table>
  <thead>
    <tr>
      <th>Algorithm</th>
      <th>Atomics</th>
      <th>Instructions</th>
      <th>Performance</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><abbr title="Uses a std::mutex and std::lock_guard to protect a plain integer counter.">mutex add</abbr></td>
      <td>2</td>
      <td>64</td>
      <td>~21 ns</td>
    </tr>
    <tr>
      <td><abbr title="Uses an atomic increment on a single shared counter.">atomic add</abbr></td>
      <td>1</td>
      <td>4</td>
      <td>~7 ns</td>
    </tr>
    <tr>
      <td><abbr title="Uses a CAS loop to increment a single shared counter.">cas add</abbr></td>
      <td>1</td>
      <td>7</td>
      <td>~12 ns</td>
    </tr>
    <tr>
      <td><abbr title="A ticket lock that calls sched_yield in a spin loop while waiting for its turn.">ticket yield</abbr></td>
      <td>1</td>
      <td>13</td>
      <td>~10 ns</td>
    </tr>
    <tr>
      <td><abbr title="A ticket lock which blocks if it cannot immediately acquire the lock.">ticket blocking</abbr></td>
      <td>3</td>
      <td>107</td>
      <td>~32 ns</td>
    </tr>
    <tr>
      <td><abbr title="A blocking ticket lock where each waiter waits on a unique condition variable.">queued fifo</abbr></td>
      <td>4</td>
      <td>167</td>
      <td>~45 ns</td>
    </tr>
    <tr>
      <td><abbr title="A traditional spin-based ticket lock that does a hot spin while waiting for its ticket to be next.">ticket spin</abbr></td>
      <td>1</td>
      <td>13</td>
      <td>~10 ns</td>
    </tr>
    <tr>
      <td><abbr title="A simple mutex from &quot;Futexes Are Tricky&quot;.">mutex3</abbr></td>
      <td>2</td>
      <td>17</td>
      <td>~20 ns</td>
    </tr>
  </tbody>
</table>

<p>In particular, note that <abbr title="Uses a std::mutex and std::lock_guard to protect a plain integer counter.">mutex add</abbr> has more than 9x the number of instructions compared to <abbr title="Uses a CAS loop to increment a single shared counter.">cas add</abbr> yet still runs at half the speed, in line with the 2:1 ratio of atomics. Similarly, <abbr title="A ticket lock that calls sched_yield in a spin loop while waiting for its turn.">ticket yield</abbr> and <abbr title="A traditional spin-based ticket lock that does a hot spin while waiting for its ticket to be next.">ticket spin</abbr> have slightly <em>better</em> performance than <abbr title="Uses a CAS loop to increment a single shared counter.">cas add</abbr> despite having about twice the number of instructions, in line with them all having a single atomic operation<sup id="fnref:casworse"><a href="#fn:casworse">19</a></sup>.</p>

<p>The last row in the table shows the performance of <abbr title="A simple mutex from &quot;Futexes Are Tricky&quot;.">mutex3</abbr>, an implementation we haven’t discussed. It is a basic mutex offering similar functionality to <code>std::mutex</code> and whose implementation is described in <a href="https://akkadia.org/drepper/futex.pdf">Futexes Are Tricky</a>. Because it doesn’t need to pass through two layers of abstraction<sup id="fnref:twolayer"><a href="#fn:twolayer">20</a></sup>, has only about one third the instruction count of <code>std::mutex</code>, yet performance is almost exactly the same, differing by less than 10%.</p>

<p>So the idea that you can almost ignore things that are in a lower cost tier seems to hold here. Don’t take this too far: if you design a lock with a single atomic operation but 1,000 other instructions, it is not going to be fast. There are also reasons to keep your instruction count low other than microbenchmark performance: smaller instruction cache footprint, less space occupied in various <abbr title="Out-of-order execution allows CPUs to execute instructions out of order with respect to the source.">out-of-order</abbr> execution buffers, more favorable inlining tradeoffs, etc.</p>

<p>Here it is important to note that the change in level of our various functions didn’t require a change in implementation. These are exactly the same few implementations we discussed in the slower levels. Instead, we simply changed (by fiat, i.e., adjusting the benchmark parameters) the contention level from “very high” to “zero”. So in this case the  level doesn’t depend only on the code, but also this external factor. Of course, just saying that we are going to get to level 1 by only running one thread is not very useful in real like: we often can’t simply ban multi-threaded operation.</p>

<p>So can we get to level 1 even under concurrent calls from multiple threads? For this particular problem, we can.</p>

<h4 id="adaptive-multi-counter">Adaptive Multi-Counter</h4>

<p>One option is to use multiple counters to represent the counter value. We try to organize it so that that threads running concurrently on different CPUs will increment different counters. Thus the <em>logical</em> counter value is split across all of these internal <em>physical</em> counters, and so a read of the logical counter value now needs to add together all the physical counter values.</p>

<p>Here’s an implementation:</p>

<div><div><pre><code><span>class</span> <span>cas_multi_counter</span> <span>{</span>
    <span>static</span> <span>constexpr</span> <span>size_t</span> <span>NUM_COUNTERS</span> <span>=</span> <span>64</span><span>;</span>

    <span>static</span> <span>thread_local</span> <span>size_t</span> <span>idx</span><span>;</span>
    <span>multi_holder</span> <span>array</span><span>[</span><span>NUM_COUNTERS</span><span>];</span>

<span>public:</span>

    <span>/** increment the logical counter value */</span>
    <span>uint64_t</span> <span>operator</span><span>++</span><span>(</span><span>int</span><span>)</span> <span>{</span>
        <span>while</span> <span>(</span><span>true</span><span>)</span> <span>{</span>
            <span>auto</span><span>&amp;</span> <span>counter</span> <span>=</span> <span>array</span><span>[</span><span>idx</span><span>].</span><span>counter</span><span>;</span>

            <span>auto</span> <span>cur</span> <span>=</span> <span>counter</span><span>.</span><span>load</span><span>();</span>
            <span>if</span> <span>(</span><span>counter</span><span>.</span><span>compare_exchange_strong</span><span>(</span><span>cur</span><span>,</span> <span>cur</span> <span>+</span> <span>1</span><span>))</span> <span>{</span>
                <span>return</span> <span>cur</span><span>;</span>
            <span>}</span>

            <span>// CAS failure indicates contention,</span>
            <span>// so try again at a different index</span>
            <span>idx</span> <span>=</span> <span>(</span><span>idx</span> <span>+</span> <span>1</span><span>)</span> <span>%</span> <span>NUM_COUNTERS</span><span>;</span>
        <span>}</span>
    <span>}</span>

    <span>uint64_t</span> <span>read</span><span>()</span> <span>{</span>
        <span>uint64_t</span> <span>sum</span> <span>=</span> <span>0</span><span>;</span>
        <span>for</span> <span>(</span><span>auto</span><span>&amp;</span> <span>h</span> <span>:</span> <span>array</span><span>)</span> <span>{</span>
            <span>sum</span> <span>+=</span> <span>h</span><span>.</span><span>counter</span><span>.</span><span>load</span><span>();</span>
        <span>}</span>
        <span>return</span> <span>sum</span><span>;</span>
    <span>}</span>
<span>};</span>
</code></pre></div></div>

<p>We’ll call this <abbr title="Uses a CAS on an adatively per-CPU counter.">cas multi</abbr>, and the approach is relatively straightforward.</p>

<p>There are 64 padded<sup id="fnref:padded"><a href="#fn:padded">21</a></sup> physical counters whose sum makes up the logical counter value. There is a thread-local <code>idx</code> value, initially zero for every thread, that points to the physical counter that each thread should increment. When <code>operator++</code> is called, we attempt to increment the counter pointed to by <code>idx</code> using <abbr title="Compare-and-swap: an atomic operation implemented on x86 and other CPUs">CAS</abbr>.</p>

<p>If this fails, however, we don’t simply retry. Failure indicates contention<sup id="fnref:notallfailure"><a href="#fn:notallfailure">22</a></sup> (this is the only way the <em>strong</em> variant of <code>compare_exchange</code> can fail), so we add one to <code>idx</code> to try another counter on the next attempt.</p>

<p>In a high-contention scenario like our benchmark, every CPU quickly ends up pointing to a different index value. If there is low contention, it is possible that only the first physical counter will be used.</p>

<p>Let’s compare this to the <code>atomic add</code> version we looked at above, which was the fastest of the level 2 approaches. Recall that it uses an <abbr title="Uses an atomic increment on a single shared counter.">atomic add</abbr> on a single counter.</p>



<p>For 1 active core, the results are the same as we saw earlier: the <abbr title="Compare-and-swap: an atomic operation implemented on x86 and other CPUs">CAS</abbr> approach performs the same as the <abbr title="Uses a CAS loop to increment a single shared counter.">cas add</abbr> algorithm<sup id="fnref:perfsame"><a href="#fn:perfsame">23</a></sup>, which is somewhat slower than <abbr title="Uses an atomic increment on a single shared counter.">atomic add</abbr>, due to the need for an additional load (i.e., the line with <code>counter.load()</code>) to set up the <abbr title="Compare-and-swap: an atomic operation implemented on x86 and other CPUs">CAS</abbr>.</p>

<p>For 2 to 4 cores, the situation changes dramatically. The multiple counter approach performs the <em>same</em> regardless of the number of active cores. That is, it exhibits perfect scaling with multiple cores – in contrast to the single-counter approach which scales poorly. At four cores, the relative speedup of the multi-counter approach is about 9x. On Amazon’s Graviton ARM processor the speedup approaches <em>eighty</em> times at 16 threads.</p>

<p>This improvement in increment performance comes at a cost, however:</p>

<ul>
  <li>64 counters ought to be enough for anyone, but they take 4096 (!!) bytes of memory to store what takes only 8 bytes in the <abbr title="Uses an atomic increment on a single shared counter.">atomic add</abbr> approach<sup id="fnref:eightbyte"><a href="#fn:eightbyte">24</a></sup>.</li>
  <li>The <code>read()</code> method is much slower: it needs to iterate over and add all 64 values, versus a single load for the earlier approaches.</li>
  <li>The implementation compiles to much larger code: 113 bytes versus 15 bytes for the single counter <abbr title="Compare-and-swap: an atomic operation implemented on x86 and other CPUs">CAS</abbr> approach or 7 bytes for the <abbr title="Uses an atomic increment on a single shared counter.">atomic add</abbr> approach.</li>
  <li>The concurrent behavior is considerably harder to reason about and document. For example, it is harder to explain the consistency condition provided by <code>read()</code> since it is no longer a single atomic read<sup id="fnref:read"><a href="#fn:read">25</a></sup>.</li>
  <li>There is a single thread-local <code>idx</code> variable. So while different <code>cas_multi_counter</code> instances are logically independent, the shared <code>idx</code> variable means that things that happen in one counter can affect the non-functional behavior of the others<sup id="fnref:sharedidx"><a href="#fn:sharedidx">26</a></sup>.</li>
</ul>

<p>Some of these downsides can be partly mitigated:</p>

<ul>
  <li>A much smaller number of counters would probably better for most all practical uses. We could also set the array size dynamically based on the detected number of logical CPUs since a larger array should not provide much of a performance increase. Better yet, we might make the size even more dynamic, based on contention: start with a single element and grow it only when contention is detected. This means that even on systems with many CPUs, the size will remain small if contention is never seen in practice. This has a runtime cost<sup id="fnref:rtcost"><a href="#fn:rtcost">27</a></sup>, however.</li>
  <li>We could optimize the <code>read()</code> method by stopping when we see a zero counter. I believe a careful analysis shows that the non-zero counter values for any instance of this class are all in a contiguous region starting from the beginning of the counter array<sup id="fnref:subtle"><a href="#fn:subtle">28</a></sup>.</li>
  <li>We could mitigate some of the code footprint by carefully carving the less hot”<sup id="fnref:lesshot"><a href="#fn:lesshot">29</a></sup> slow path out into a another function, and use our <a href="https://xania.org/201209/forcing-code-out-of-line-in-gcc">magic powers</a> to encourage the small fast path (the first <abbr title="Compare-and-swap: an atomic operation implemented on x86 and other CPUs">CAS</abbr>) to be inlined while the fallback remains not inlined.</li>
  <li>We could make the thread-local <code>idx</code> per instance specific to solve the “shared <code>idx</code> across all instances” problem. This does require some non-negligible amount of work to implement a dynamic TLS system which can create as many thread local keys as you want<sup id="fnref:dynamictls"><a href="#fn:dynamictls">30</a></sup>, and it is slower.</li>
</ul>

<p>So while we got a good looking chart, this solution doesn’t exactly dominate the simpler ones. You pay a price along several axes for the lack of contention and you shouldn’t blindly replace the simpler solutions with this one – it needs to be a carefully considered and use-case dependent decision.</p>

<p>Is it over yet? Can I close this browser tab and reclaim all that memory? Almost. Just one level to go.</p>

<h3 id="level-0-vanilla">Level 0: Vanilla</h3>

<p>The last, fastest level is no atomic instructions at all, without contention.</p>

<p>How can we increment a counter atomically while allowing it to be read from any thread? By ensuring there is only one writer for any given physical counter. If we keep a counter <em>per thread</em> and only allow the owning thread to write to it, there is no need for an atomic increment.</p>

<p>The obvious way to keep a per-thread counter is use thread-local storage. Something like this:</p>

<div><div><pre><code><span>/**
 * Keeps a counter per thread, readers need to sum
 * the counters from all active threads and add the
 * accumulated value from dead threads.
 */</span>
<span>class</span> <span>tls_counter</span> <span>{</span>
    <span>std</span><span>::</span><span>atomic</span><span>&lt;</span><span>uint64_t</span><span>&gt;</span> <span>counter</span><span>{</span><span>0</span><span>};</span>

    <span>/* protects all_counters and accumulator */</span>
    <span>static</span> <span>std</span><span>::</span><span>mutex</span> <span>lock</span><span>;</span>
    <span>/* list of all active counters */</span>
    <span>static</span> <span>std</span><span>::</span><span>vector</span><span>&lt;</span><span>tls_counter</span> <span>*&gt;</span> <span>all_counters</span><span>;</span>
    <span>/* accumulated value of counters from dead threads */</span>
    <span>static</span> <span>uint64_t</span> <span>accumulator</span><span>;</span>
    <span>/* per-thread tls_counter object */</span>
    <span>static</span> <span>thread_local</span> <span>tls_counter</span> <span>tls</span><span>;</span>

    <span>/** add ourself to the counter list */</span>
    <span>tls_counter</span><span>()</span> <span>{</span>
        <span>std</span><span>::</span><span>lock_guard</span><span>&lt;</span><span>std</span><span>::</span><span>mutex</span><span>&gt;</span> <span>g</span><span>(</span><span>lock</span><span>);</span>
        <span>all_counters</span><span>.</span><span>push_back</span><span>(</span><span>this</span><span>);</span>
    <span>}</span>

    <span>/**
     * destruction means the thread is going away, so
     * we stash the current value in the accumulator and
     * remove ourselves from the array
     */</span>
    <span>~</span><span>tls_counter</span><span>()</span> <span>{</span>
        <span>std</span><span>::</span><span>lock_guard</span><span>&lt;</span><span>std</span><span>::</span><span>mutex</span><span>&gt;</span> <span>g</span><span>(</span><span>lock</span><span>);</span>
        <span>accumulator</span> <span>+=</span> <span>counter</span><span>.</span><span>load</span><span>(</span><span>std</span><span>::</span><span>memory_order_relaxed</span><span>);</span>
        <span>all_counters</span><span>.</span><span>erase</span><span>(</span><span>std</span><span>::</span><span>remove</span><span>(</span><span>all_counters</span><span>.</span><span>begin</span><span>(),</span> <span>all_counters</span><span>.</span><span>end</span><span>(),</span> <span>this</span><span>),</span> <span>all_counters</span><span>.</span><span>end</span><span>());</span>
    <span>}</span>

    <span>void</span> <span>incr</span><span>()</span> <span>{</span>
        <span>auto</span> <span>cur</span> <span>=</span> <span>counter</span><span>.</span><span>load</span><span>(</span><span>std</span><span>::</span><span>memory_order_relaxed</span><span>);</span>
        <span>counter</span><span>.</span><span>store</span><span>(</span><span>cur</span> <span>+</span> <span>1</span><span>,</span> <span>std</span><span>::</span><span>memory_order_relaxed</span><span>);</span>
    <span>}</span>

<span>public:</span>

    <span>static</span> <span>uint64_t</span> <span>read</span><span>()</span> <span>{</span>
        <span>std</span><span>::</span><span>lock_guard</span><span>&lt;</span><span>std</span><span>::</span><span>mutex</span><span>&gt;</span> <span>g</span><span>(</span><span>lock</span><span>);</span>
        <span>uint64_t</span> <span>sum</span> <span>=</span> <span>0</span><span>,</span> <span>count</span> <span>=</span> <span>0</span><span>;</span>
        <span>for</span> <span>(</span><span>auto</span> <span>h</span> <span>:</span> <span>all_counters</span><span>)</span> <span>{</span>
            <span>sum</span> <span>+=</span> <span>h</span><span>-&gt;</span><span>counter</span><span>.</span><span>load</span><span>(</span><span>std</span><span>::</span><span>memory_order_relaxed</span><span>);</span>
            <span>count</span><span>++</span><span>;</span>
        <span>}</span>
        <span>return</span> <span>sum</span> <span>+</span> <span>accumulator</span><span>;</span>
    <span>}</span>

    <span>static</span> <span>void</span> <span>increment</span><span>()</span> <span>{</span>
        <span>tls</span><span>.</span><span>incr</span><span>();</span>
    <span>}</span>
<span>};</span>
</code></pre></div></div>

<p>The approach is the similar to the per-CPU counter, except that we keep one counter per thread, using <code>thread_local</code>. Unlike earlier implementations, you don’t create instances of this class: there is only one counter and you increment it by calling the static method <code>tls_counter::increment()</code>.</p>

<p>Let’s focus a moment on the actual increment inside the thread-local counter instance:</p>

<div><div><pre><code><span>void</span> <span>incr</span><span>()</span> <span>{</span>
    <span>auto</span> <span>cur</span> <span>=</span> <span>counter</span><span>.</span><span>load</span><span>(</span><span>std</span><span>::</span><span>memory_order_relaxed</span><span>);</span>
    <span>counter</span><span>.</span><span>store</span><span>(</span><span>cur</span> <span>+</span> <span>1</span><span>,</span> <span>std</span><span>::</span><span>memory_order_relaxed</span><span>);</span>
<span>}</span>
</code></pre></div></div>

<p>This is just a verbose way of saying “add 1 to this <code>std::atomic&lt;uint64_t&gt;</code> but it doesn’t have to be atomic”. We don’t need an atomic increment as there is only one writer<sup id="fnref:whyatomic"><a href="#fn:whyatomic">31</a></sup>. Using the <em>relaxed</em> memory order means that no barriers are inserted<sup id="fnref:barrier"><a href="#fn:barrier">32</a></sup>. We still need a way to read all the thread-local counters, and the rest of the code deals with that: there is a global vector of pointers to all the active <code>tls_counter</code> objects, and <code>read()</code> iterates over this. All access to this vector is protected by a <code>std::mutex</code>, since it will be accessed concurrently. When threads die, we remove their entry from the array, and add their final value to <code>tls_counter::accumulator</code> which is added to the sum of active counters in <code>read()</code>.</p>

<p>Whew.</p>

<p>So how does this <abbr title="Uses thread-local storage for a counter per thread.">tls add</abbr> implementation benchmark?</p>



<p>That’s two nanoseconds per increment, regardless of the number of active cores. This turns out to be exactly as fast as just incrementing a variable in memory with a single instruction like <code>inc [eax]</code> or <code>add [eax], 1</code>, so it’s somehow as fast as possible for any solution which ends up incrementing something in memory<sup id="fnref:whitelie"><a href="#fn:whitelie">33</a></sup>.</p>

<p>Let’s take a look at the number of atomics, total instructions and performance for the three implementations in the last plot, for four threads:</p>

<table>
  <thead>
    <tr>
      <th>Algorithm</th>
      <th>Atomics</th>
      <th>Instructions</th>
      <th>Performance</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><abbr title="Uses an atomic increment on a single shared counter.">atomic add</abbr></td>
      <td>1</td>
      <td>4</td>
      <td>~ 110 ns</td>
    </tr>
    <tr>
      <td><abbr title="Uses a CAS on an adatively per-CPU counter.">cas multi</abbr></td>
      <td>1</td>
      <td>7</td>
      <td>~ 12 ns</td>
    </tr>
    <tr>
      <td><abbr title="Uses thread-local storage for a counter per thread.">tls add</abbr></td>
      <td>0</td>
      <td>12</td>
      <td>~ 2 ns</td>
    </tr>
  </tbody>
</table>

<p>This is a clear indication that the difference in performance has very little to do with the number of instructions: the ranking by instruction count is exactly the reverse of the ranking by performance! <abbr title="Uses thread-local storage for a counter per thread.">tls add</abbr> has three times the number of instructions, yet is more than <em>fifty times</em> faster (so the <abbr title="Instructions per cycle: calculated over an interval by measuring the number of instructions executed and the duration in cycles.">IPC</abbr> varies by a factor of more than 150x).</p>

<p>As we saw at the last 1, this improvement in performance doesn’t come for free:</p>

<ul>
  <li>The total code size is considerably larger than the per-CPU approach, although most of it is related to creation of the initial object on each thread, and not on the hot path.</li>
  <li>We have one object per thread, instead of per CPU. For an application with many threads using the counter, this may mean the creation of many individual counters which use both more memory<sup id="fnref:tlsmem"><a href="#fn:tlsmem">34</a></sup> and result in a slower <code>read()</code> function.</li>
  <li>This implementation only supports <em>one</em> counter: the key methods in <code>tls_counter</code> are static. This boils down to the need for a <code>thread_local</code> object for the physical counter, which must be static by the rules of C++. A template parameter could be added to allow multiple counters based on dummy types used as tags, but this is still more awkward to use than instances of a class (and some platforms <a href="https://docs.microsoft.com/en-us/windows/win32/procthread/thread-local-storage">have limits</a> on the number of <code>thead_local</code> variables). This limitation could be removed in the same way as discussed earlier for the <abbr title="Uses a CAS on an adatively per-CPU counter.">cas multi</abbr> <code>idx</code> variable, but at a cost in performance and complexity.</li>
  <li>A lock was introduced to protect the array of all counters. Although the important increment operation is still lock-free, things like the <code>read()</code> call, the first counter access on a given thread and thread destruction all complete for the same lock. This could be eased with a read-write lock or a concurrent data structure, but at a cost as always.</li>
</ul>

<h2 id="the-table">The Table</h2>



<p>Let’s summarize all the levels in this table.</p>

<p>The <em>~Cost</em> column is a <em>very</em> approximate estimate of the cost of each “occurrence” of the expensive operation associated with the level. It should be taken as a very rough ballpark for current Intel and AMD hardware, but especially the later levels can vary a lot.</p>

<p>The <em>Perf Event</em> column lists a Linux <code>perf</code> even that you can use to count the number of times the operation associated with this level occurs, i.e., the thing that is slow. For example, in level 1, you count atomic operations using the <code>mem_inst_retired.lock_loads</code> counter, and if you get three counts per high level operation, you can expect roughly 3 x 10 ns = 30 ns cost. Of course, you don’t necessarily need perf in this case: you can inspect the assembly too.</p>

<p>The <em>Local</em> column records whether the behavior of this level is <em>core local</em>. If yes, it means that operations on different cores complete independently and don’t compete and so the performance scales with the number of cores. If not, there is contention or serialization, so the throughput of the entire system is often limited, regardless of how many cores are involved. For example, only one core at time perform an atomic operation on a cache line, so the throughput of the whole system is fixed and the throughput per core decreases as more cores become involved.</p>

<p>The <em>Key Characteristic</em> tries to get across the idea of the level in one bit-sized chunk.</p>

<table>
  <thead>
    <tr>
      <th>Level</th>
      <th>Name</th>
      <th>~Cost (ns)</th>
      <th>Perf Event</th>
      <th>Local</th>
      <th>Key Characteristic</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>Vanilla</td>
      <td>low</td>
      <td>depends</td>
      <td><strong>Yes</strong></td>
      <td>No atomic instructions or contended accesses at all</td>
    </tr>
    <tr>
      <td>1</td>
      <td>Uncontended Atomic</td>
      <td>10</td>
      <td><code>mem_inst_retired.</code> <code>lock_loads</code></td>
      <td><strong>Yes</strong></td>
      <td>Atomic instructions without contention</td>
    </tr>
    <tr>
      <td>2</td>
      <td>True Sharing</td>
      <td>40 - 400</td>
      <td><code>mem_load_l3_hit_retired.</code> <code>xsnp_hitm</code></td>
      <td><strong>No</strong></td>
      <td>Contented atomics or locks</td>
    </tr>
    <tr>
      <td>3</td>
      <td>Syscall</td>
      <td>1,000</td>
      <td><code>raw_syscalls:sys_enter</code></td>
      <td><strong>No</strong></td>
      <td>System call</td>
    </tr>
    <tr>
      <td>4</td>
      <td>Context Switch</td>
      <td>10,000</td>
      <td><code>context-switches</code></td>
      <td><strong>No</strong></td>
      <td>Forced context switch</td>
    </tr>
    <tr>
      <td>5</td>
      <td>Catastrophe</td>
      <td>huge</td>
      <td>depends</td>
      <td><strong>No</strong></td>
      <td>Stalls until quantum exhausted, or other sadness</td>
    </tr>
  </tbody>
</table>

<h2 id="so-what">So What?</h2>

<p>What’s the point of all this?</p>

<p>Primarily, I use the hierarchy as a simplification mechanism when thinking about concurrency and performance. As a first order approximation <em>you mostly only need to care about the operations related to the current level</em>. That is, if you are focusing on something which has contended atomic operations (level 2), you don’t need to worry too much about uncontended atomics or instruction counts: just focus on reducing contention. Similarly, if you are at level 1 (uncontended atomics) it is often worth using <em>more</em> instructions to reduce the number of atomics.</p>

<p>This guideline only goes so far: if you have to add 100 instructions to remove one atomic, it is probably not worth it.</p>

<p>Second, when optimizing a concurrent system I always try consider how I can get to a (numerically) lower level. Can I remove the last atomic? Can I avoid contention? Successfully moving to a lower level can often provide an order-of-magnitude boost to performance, so it should be attempted first, before finer-grained optimizations within the current level. Don’t spend forever optimizing your contended lock, if there’s some way to get rid of the contention entirely.</p>

<p>Of course, this is not always possible, or not possible without tradeoffs you are unwilling to make.</p>

<h3 id="getting-there">Getting There</h3>

<p>Here’s a quick look at some usual and unusual ways of achieving levels lower on the hierarchy.</p>

<h4 id="level-4">Level 4</h4>

<p>You probably don’t want to really be in level 4 but it’s certainly better than level 5. So, if you still have your job and your users haven’t all abandoned you, it’s usually pretty easy to get out of level 5. More than half the battle is just recognizing what’s going on and from there the solution is often clear. Many times, you’ve simply violated some rule lie “don’t use pure spinlocks in userspace” or “you built a spinlock by accident” or “so-and-so accidentally held that core lock during IO”. There’s never almost never any inherent reason you’d need to stay in level 5 and you can usually find an almost tradeoff-free fix.</p>

<p>A better approach than targeting level 4 is just to skip to level 2, since that’s usually not too difficult.</p>

<h4 id="level-3">Level 3</h4>

<p>Getting to level 3 just means solving the underlying reason for so many context switches. In the example used in this post, it means giving up fairness. Other approaches include not using threads for small work units, using smarter thread pools, not oversubscribing cores, and keeping locked regions short.</p>

<p>You don’t usually really want to be in level 3 though: just skip right to level 2.</p>

<h4 id="level-2">Level 2</h4>

<p>Level 3 isn’t a <em>terrible</em> place to be, but you’ll always have that gnawing in your stomach that you’re leaving a 10x speedup on the table. You just need to get rid of that system call or context switch, bringing you to level 2.</p>

<p>Most library provided concurrency primitives already avoid system calls on the happy path. E.g., pthreads mutex, <code>std::mutex</code>, Windows <code>CRITICAL_SECTION</code> will avoid a system call while acquiring and releasing an uncontended lock. There are some notable exceptions though: if are using a <a href="https://docs.microsoft.com/en-us/windows/win32/sync/mutex-objects">Win32 mutex object</a> or <a href="https://man7.org/linux/man-pages/man2/semop.2.html">System V semaphore</a> object, you are paying a for a system call on every operation. Double check if you can use an in-process alternative in this case.</p>

<p>System calls often creep in when home-grown synchronization solutions are used, e.g., using Windows events to build your own read-write lock or striped lock or whatever the flavor of the day is. You can often remove the call in the fast path by making a check in user-space to see if a system call is necessary. For example, rather than unconditionally unblocking any waiters when releasing some exclusive object, <em>check</em> to see if there are waiters<sup id="fnref:tricky"><a href="#fn:tricky">35</a></sup> in userspace and skip the system call if there are none.</p>

<p>If a lock is generally held for a short period, you can avoid unnecessary system calls and context switches with a hybrid lock that spins for an appropriate<sup id="fnref:spin"><a href="#fn:spin">36</a></sup> amount of time before blocking. This can trade tens of nanoseconds of spinning for hundreds or thousands of nanoseconds of system calls.</p>

<p>Ensure your use of threads is “right sized” as much as possible. A lot of unnecessary context switches occur when many more threads are running than there are CPUs, and this increases the chance of a lock being held during a context switch (and makes it worse when it does happen: it takes longer for the holding thread to run again as the scheduler probably cycles through all the other runnable threads first).</p>

<h4 id="level-1">Level 1</h4>

<p>A lot of code that does the work to get to level 2 actually ends up in level 1. Recall that the primary difference between level 1 and 2 is the lack of contention in level 1. So if your process naturally or by design has low contention, simply using existing off-the-shelf synchronization like <code>std::mutex</code> can get you to level 1.</p>

<p>I can’t give a step-by-step recipe for reducing contention, but here’s a laundry list of things to consider:</p>

<ul>
  <li>Keep your critical sections as short as possible. Ensure you do any heavy work that doesn’t directly involve a shared resource outside of the critical section. Sometimes this means making a copy of the shared data to work on it “outside” of the lock, which might increase the total amount of work done, but reduce contention.</li>
  <li>For things like atomic counters, try to batch your updates: e.g., if you update the counter multiple times during some operation, update a local on the stack rather than the global counter and only “upload” the entire value once at the end.</li>
  <li>Consider using structures that use fine-grained locks, striped locks or similar mechanisms that reduce contention by locking only portions of a container.</li>
  <li>Consider per-CPU structures, as in the examples above, or some approximation of them (e.g., hashing the current thread ID into an array of structures). This post used an atomic counter as a simple example, but it applies more generally to any case where the mutations can be done independently and aggregated later.</li>
</ul>

<p>For all of the advice above, when I say <em>consider doing X</em> I really mean <em>consider finding and using an existing off-the shelf component that does X</em>. Writing concurrent structures yourself should be considered a last resort – despite what you think, your use case is probably not all that unique.</p>

<p>Level 1 is where a lot of well written, straightforward and high-performing concurrent code lives. There is nothing wrong with this level – it is a happy place.</p>

<h4 id="level-0">Level 0</h4>

<p>It is not always easy or possible to remove the last atomic access from your fast paths, but if you just can’t live with the extra ~10 ns, here are some options:</p>

<ul>
  <li>The general approach of using thread local storage, as discussed above, can also be extended to structures more complicated than counters.</li>
  <li>Many lock-free structures offer atomic-free <em>read</em> paths, notably concurrent containers in garbage collected languages, such as <code>ConcurrentHashMap</code> in Java. Languages without garbage collection have fewer straightforward options, mostly because safe memory reclamation is a <a href="http://concurrencyfreaks.blogspot.com/2017/08/why-is-memory-reclamation-so-important.html">hard problem</a>, but there are still <a href="http://concurrencykit.org/">some</a> <a href="https://software.intel.com/content/www/us/en/develop/documentation/tbb-documentation/top/intel-threading-building-blocks-developer-guide/containers.html">good</a> <a href="https://github.com/facebook/folly/tree/master/folly/concurrency">options</a> out there.</li>
  <li>I find that <a href="https://liburcu.org/">RCU</a> is especially powerful and fairly general if you are using a garbage collected language, or can satisfy the requirements for an efficient reclamation method in a non-GC language.</li>
  <li>The <a href="https://en.wikipedia.org/wiki/Seqlock">seqlock</a><sup id="fnref:despite"><a href="#fn:despite">37</a></sup> is an under-rated and little known alternative to RCU without reclaim problems, although not as general. Concurrencykit has <a href="http://concurrencykit.org/doc/ck_sequence.html">an implementation</a>. It has an atomic-free read path for readers. Unfortunately, seqlocks don’t integrate cleanly with either the Java<sup id="fnref:stampedlock"><a href="#fn:stampedlock">38</a></sup> or <a href="http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2019/p1478r1.html">C++</a> memory models.</li>
  <li>It is also possible in some cases to do a per-CPU rather than a per-thread approach using only vanilla instructions, although the possibility of interruption at any point makes this tricky. <a href="https://www.efficios.com/blog/2019/02/08/linux-restartable-sequences/">Restartable sequences (rseq)</a> can help, and there are other tricks lurking out there.</li>
  <li>This is the last point, but it should be the first: you can probably often redesign your algorithm or application to avoid sharing data in the first place, or to share much less. For example, rather than constantly updating a shared collection with intermediate results, do as much private computation as possible before only merging the final results.</li>
</ul>

<h3 id="summary">Summary</h3>

<p>We looked at the six different levels that make up this concurrency cost hierarchy. The slow half (3, 4 and 5) are all basically performance bugs. You should be able to achieve level 2 or level 1 (if you naturally have low contention) for most designs fairly easily and those are probably what you should target by default. Level 1 in a contended scenario and level 0 are harder to achieve and often come with difficult tradeoffs, but the performance boost can be significant: often one or more orders of magnitude.</p>

<h3 id="thanks">Thanks</h3>

<p>Thanks to <a href="https://pvk.ca/">Paul Khuong</a> who showed me something that made me reconsider in what scenarios level 0 is achievable and typo fixes.</p>

<p>Thanks to <a href="https://twitter.com/never_released">@never_released</a> for help on a problem I had bringing up an EC2 bare-metal instance (tip: just wait).</p>

<p>Traffic light photo by <a href="https://unsplash.com/@harshaldesai">Harshal Desai</a> on <a href="https://unsplash.com/s/photos/traffic-light">Unsplash</a>.</p>

<h3 id="discussion-and-feedback">Discussion and Feedback</h3>

<p>You can leave a <a href="#comment-section">comment below</a>.</p>

<p>If you liked this post, check out the <a href="https://travisdowns.github.io/">homepage</a> for others you might enjoy.</p>

<hr>
<hr>




  </div><!-- travis override -->
  
    





  

  
</article>

      </div>
    </div></div></div>
    </div>
    <footer>
        <div>created by <a href="https://buildstarted.com">buildstarted</a> &copy; 2020 <a href="/about">about</a></div>
        <div>Share this page on social media: copy and paste this url https://linksfor.dev/</div>
        <div>If you prefer RSS: <a href="https://linksfor.dev/feed.xml">https://linksfor.dev/feed.xml</a></div>
        <div>Customer satisfaction guaranteed to be optional.</div>
    </footer>
    
    <script async defer>
        _dna = window._dna || {};
        _dna.siteId = "linksfor.devs";
        _dna.outlink = true;

        (function() {
            let dna = document.createElement('script');
            dna.type = 'text/javascript';
            dna.async = true;
            dna.src = '//dna.buildstarted.com/t.js';
            let s = document.getElementsByTagName('script')[0];
            s.parentNode.insertBefore(dna, s);
        })();
    </script>
    <noscript><img src="//dna.buildstarted.com/g?siteId=linksfor.devs"/></noscript>
</body>
</html>