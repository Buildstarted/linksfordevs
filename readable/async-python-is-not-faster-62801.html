<!DOCTYPE html>
<html lang="en">
<head>
    <title>
Async Python is not faster - linksfor.dev(s)    </title>
    <meta charset="utf-8">
    <link rel="alternate" type="application/rss+xml" title="Linksfor.dev(s) feed" href="https://linksfor.dev/feed.rss" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta name="google" value="notranslate">
    <link rel="stylesheet" type="text/css" href="/style.css" />
    <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon">
    <link rel="icon" href="/favicon.ico" type="image/x-icon">
    <meta property="og:title" content="Async Python is not faster - linksfor.dev(s)"/>
    <meta property="og:description" content="Async Python is slower than &quot;sync&quot; Python under a realistic benchmark. A bigger worry is that async frameworks go a bit wobbly under load."/>
    <meta property="og:type" content="website"/>
    <meta property="og:url" content="http://calpaterson.com/async-python-is-not-faster.html"/>

<meta property="og:site_name" content="linksfor.dev(s)" />
</head>
<body>
    <div class="devring" style="background: #222">
        <div style="text-align:center">Explore other dev related sites in this ring. If you would like to join this ring <a href="https://devring.club">click here</a>.</div>
        <div class="grid">
            <div style="display: grid; grid-template-columns: .5fr 1fr 1fr 1fr; text-align: center;">
                <span class="devring-title"><a href="https://devring.club/">devring.club</a></span>
                <a href="https://devring.club/sites/1/prev" class="devring-previous">Previous</a>
                <a href="https://devring.club/random" class="devring-random">Random</a>
                <a href="https://devring.club/sites/1/next" class="devring-next">Next</a>
            </div>
        </div>
    </div>
    <div class="grid">
        <h1>
<a href="/" style="color:inherit">linksfor.dev(s)</a>
        </h1>
        <title>linksfor.dev(s) - Async Python is not faster</title>
<div class="readable">
        <h1>Async Python is not faster</h1>
            <div>Reading time: 14-18 minutes</div>
        <div>Posted here: 12 Jun 2020</div>
        <p><a href="http://calpaterson.com/async-python-is-not-faster.html">http://calpaterson.com/async-python-is-not-faster.html</a></p>
        <hr/>
<div id="readability-page-1" class="page"><article>
        
        <p>June 2020</p>
        <p id="article-description">Async Python is slower than "sync" Python under a
        realistic benchmark. A bigger worry is that async frameworks go a bit wobbly
        under load.</p>
        <hr>
        <p>Most people understand that async Python has a higher level of concurrency.
        It would make some sense for that to imply higher performance for common tasks
        like serving dynamic web sites or web APIs.</p>
        <p>Sadly async is not go-faster-stripes for the Python interpreter.</p>
        <p>Under realistic conditions (see below) asynchronous web frameworks are
        slightly worse throughput (requests/second) and much worse latency
        variance.</p>
        <h2>Benchmark results</h2>
        <p>I tested a wide variety of different sync and async webserver
        configurations:</p>
        <table>
            <thead>
                <tr>
                    <th>Webserver</th>
                    <th>Framework</th>
                    <th>Workers</th>
                    <th>P50</th>
                    <th>P99</th>
                    <th>Throughput</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td>Gunicorn with meinheld</td>
                    <td>Falcon</td>
                    <td>16</td>
                    <td>17</td>
                    <td>31</td>
                    <td>5589</td>
                </tr>
                <tr>
                    <td>Gunicorn with meinheld</td>
                    <td>Bottle</td>
                    <td>16</td>
                    <td>17</td>
                    <td>32</td>
                    <td>5780</td>
                </tr>
                <tr>
                    <td>UWGGI over http</td>
                    <td>Bottle</td>
                    <td>16</td>
                    <td>18</td>
                    <td>32</td>
                    <td>5497</td>
                </tr>
                <tr>
                    <td>UWSGI over http</td>
                    <td>Flask</td>
                    <td>16</td>
                    <td>22</td>
                    <td>33</td>
                    <td>4431</td>
                </tr>
                <tr>
                    <td>Gunicorn with meinheld</td>
                    <td>Flask</td>
                    <td>16</td>
                    <td>21</td>
                    <td>35</td>
                    <td>4510</td>
                </tr>
                <tr>
                    <td>UWSGI over 'uwsgi'</td>
                    <td>Bottle</td>
                    <td>16</td>
                    <td>18</td>
                    <td>36</td>
                    <td>5281</td>
                </tr>
                <tr>
                    <td>UWSGI over http</td>
                    <td>Falcon</td>
                    <td>16</td>
                    <td>18</td>
                    <td>37</td>
                    <td>5415</td>
                </tr>
                <tr>
                    <td>Gunicorn</td>
                    <td>Flask</td>
                    <td>14</td>
                    <td>28</td>
                    <td>42</td>
                    <td>3473</td>
                </tr>
                <tr>
                    <td>Uvicorn</td>
                    <td>Starlette</td>
                    <td>5</td>
                    <td>16</td>
                    <td>75</td>
                    <td>4952</td>
                </tr>
                <tr>
                    <td>AIOHTTP</td>
                    <td>AIOHTTP</td>
                    <td>5</td>
                    <td>19</td>
                    <td>76</td>
                    <td>4501</td>
                </tr>
                <tr>
                    <td>Uvicorn</td>
                    <td>Sanic</td>
                    <td>5</td>
                    <td>17</td>
                    <td>85</td>
                    <td>4687</td>
                </tr>
                <tr>
                    <td>Gunicorn with gevent</td>
                    <td>Flask</td>
                    <td>12</td>
                    <td>24</td>
                    <td>136</td>
                    <td>3077</td>
                </tr>
                <tr>
                    <td>Daphne</td>
                    <td>Starlette</td>
                    <td>5</td>
                    <td>20</td>
                    <td>364</td>
                    <td>2678</td>
                </tr>
            </tbody>
        </table>
        <p>50th and 99th percentile response times are in milliseconds, throughput is
        in requests per second. The table is ordered by P99, which I think is perhaps
        the most important real world statistic.</p>
        <p>Note that:</p>
        <ol>
            <li>
                <strong>The best performers are sync frameworks</strong>
                <ul>
                    <li>but Flask has lower throughput than others</li>
                </ul>
            </li>
            <li><strong>The worst performers are all async frameworks</strong></li>
            <li><strong>Async frameworks have far worse latency variation</strong></li>
            <li>Uvloop-based options do better than the builtin asyncio loop
                <ul>
                    <li>so if you have no option but to use asyncio, use uvloop</li>
                </ul>
            </li>
        </ol>
        <h2>Are these benchmarks really representative?</h2>
        <p>I think so. I tried to make them as realistic as possible. Here's the
        architecture I used:</p>
        <p><img alt="async benchmark architecture" src="http://calpaterson.com/assets/async-benchmark-architecture.svg"></p>
        <p>I've tried to model a real world deployment as best I can. There is a
        reverse proxy, the python code (ie: the variable), and a database. I've also
        included an external database connection pooler as I think that is a pretty
        common feature of real deployments of web applications (at least, it is for
        postgresql).</p>
        <p>The application in question queries a row by random key and returns the
        value as JSON. <a href="https://github.com/calpaterson/python-web-perf">Full
        source code is available on github</a>.</p>
        <h3>Why the worker count varies</h3>
        <p>The rule I used for deciding on what the optimal number of worker processes
        was is simple: for each framework I started at a single worker and increased
        the worker count successively until performance got worse.</p>
        <p>The optimal number of workers varies between async and sync frameworks and
        the reasons are straightforward. Async frameworks, due to their IO concurrency,
        are able to saturate a single CPU with a single worker process.</p>
        <p>The same is not true of sync workers: when they do IO they will block until
        the IO is finished. Consequently they need to have enough workers to ensure
        that all CPU cores are always in full use when under load.</p>
        <p>For more information on this, see the <a href="https://docs.gunicorn.org/en/stable/design.html#how-many-workers">gunicorn
        documentation</a>:</p>
        <blockquote>
            <p>Generally we recommend (2 x $num_cores) + 1 as the number of workers to
            start off with. While not overly scientific, the formula is based on the
            assumption that for a given core, one worker will be reading or writing
            from the socket while the other worker is processing a request.</p>
        </blockquote>
        <h3>Machine spec</h3>
        <p>I ran the benchmark on Hetzner's CX31 machine type, which is basically a 4
        "vCPU"/8 GB RAM machine. It was run under Ubuntu 20.04. I ran the load
        generator on another (smaller) VM.</p>
        <h2>Why does async do worse?</h2>
        <h3>Throughput</h3>
        <p>On <strong><em>throughput</em></strong> (ie: requests/second) the primary
        factor is not async vs sync but how much Python code has been replaced with
        native code. Simply put, the more performance sensitive Python code you can
        replace the better you will do. This is Python performance tactic with a long
        history (see also: numpy).</p>
        <p>Meinheld and UWSGI (~5.3k requests/sec each) are large bodies of C code.
        Standard Gunicorn (~3.4k requests/sec) is pure Python.</p>
        <p>Uvicorn+Starlette (~4.9k requests/sec) replaces much more Python code than
        AIOHTTP's default server (~4.5k requests/sec) (though AIOHTTP was also
        installed with its optional "speedups").</p>
        <h3>Latency</h3>
        <p>On <strong><em>latency</em></strong> the problem is deeper. Under load,
        async does badly and latency starts to spike out to a much greater extent than
        under a traditional, sync, deployment.</p>
        <p>Why is this? In async Python, the multi-threading is
        <strong>co-operative</strong>, which simply means that threads are not
        interrupted by a central governor (such as the kernel) but instead have to
        voluntarily yield their execution time to others. In asyncio, the execution is
        yielded upon three language keywords: <code>await</code>, <code>async
        for</code> and <code>async with</code>.</p>
        <p>This means that execution time is not distributed "fairly" and one thread
        can inadvertently starve another of CPU time while it is working. This is why
        latency is more erratic.</p>
        <p>In contrast, traditional sync Python webservers like UWSGI use the
        <strong>pre-emptive</strong> multi-processing of the kernel scheduler, which
        works to ensure fairness by periodically swapping processes out from execution.
        This means that time is divided more fairly and that latency variance is
        lower.</p>
        <h2>Why do other benchmarks show different results?</h2>
        <p>The majority of other benchmarks (particularly those from async framework
        authors!) simply do not configure sync frameworks with enough workers. This
        means that those sync frameworks are effectively prevented from accessing most
        of the CPU time that is really available.</p>
        <p>Here is a sample benchmark from the Vibora project. (I didn't test this
        framework because it's one of the less popular ones.)</p>
        <p><img alt="Vibora's benchmarks claim it's five times faster than flask" src="http://calpaterson.com/assets/vibora-claims.png"></p>
        <p>Vibora claims 500% higher throughput than Flask. However when I reviewed
        their benchmark code I found that they are misconfiguring Flask to use one
        worker per CPU. When I correct that, I get the following numbers:</p>
        <table>
            <thead>
                <tr>
                    <th>Webserver</th>
                    <th>Throughput</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td>Flask</td>
                    <td>11925 req/s</td>
                </tr>
                <tr>
                    <td>Vibora</td>
                    <td>14148 req/s</td>
                </tr>
            </tbody>
        </table>
        <p>The throughput benefit of using Vibora over Flask is really <strong>just
        18%</strong>. Flask is one of the lower throughput sync frameworks I tested so
        I expect that a better sync setup would be much faster than Vibora, despite the
        impressive looking graph.</p>
        <p>Another problem is that many benchmarks de-prioritise latency results in
        favour of throughput results (Vibora's doesn't even mention it for example).
        However, while throughput can be improved by adding machines, latency under
        load doesn't get better when you do that.</p>
        <p>Increased throughput is really only useful <em>while latency is within an
        acceptable range</em>.</p>
        <h2>Further reasoning, supposition and anecdata</h2>
        <p>Although my benchmark is fairly realistic in terms of the things involved
        it's still much more homogenous than a real life workload - all requests do a
        database query and they all do the same thing with that query. Real
        applications typically have much more inherent variation: there will be some
        slow operations, some fast ones, some that do lots of IO and some that use lots
        of CPU. It seems reasonable to assume (and it's true in my experience) that
        latency variance is actually much higher in a real application.</p>
        <p>My hunch is that async applications' performance will be even more
        problematic in this case. Publicly available anecdotes are consistent with this
        idea:</p>
        <p><a href="https://twitter.com/mcfunley/status/1194713711337852928?lang=en">Dan McKinley
        wrote</a> about his experiences operating a Twisted-based system at Etsy. It
        seems that that system suffered from chronic latency variance:</p>
        <blockquote>
            <p>[The Twisted consultants] said that although Twisted was good at overall
            throughput, outlying requests could experience severe latency. Which was a
            problem for [Etsy's system], because the way the PHP frontend used it was
            hundreds/thousands of times per web request.</p>
        </blockquote>
        <p>Mike Bayer, the SQLAlchemy author, wrote <a href="https://techspot.zzzeek.org/2015/02/15/asynchronous-python-and-databases/">Asynchronous
        Python and Databases</a> several years ago in which he considers async from a
        slightly different perspective. He also benchmarks, and finds asyncio less
        efficient.</p>
        <p>"Rachel by the Bay" wrote an article called <a href="https://rachelbythebay.com/w/2020/03/07/costly/">"We have to talk about this
        Python, Gunicorn, Gevent thing"</a> in which she describes operations chaos
        arising from a gevent-based configuration. I've also had troubles (though not
        performance-related) with gevent in production.</p>
        <p>The other thing I should mention is that in the course of setting up these
        benchmarks <em>every single async implementation managed to fall over</em> in
        an annoying way.</p>
        <p>Uvicorn had its parent process terminate without terminating any of its
        children which meant that I then had to go pid hunting for the children who
        were still holding onto port 8001. At one point AIOHTTP raised an internal
        critical error to do with file descriptors but did not exit (and so would not
        be restarted by any process supervisor - <em>a cardinal sin!</em>). Daphne also
        ran into trouble locally but I forget exactly how.</p>
        <p>All of these errors were transient and easily resolved with SIGKILL. However
        the fact remains that I wouldn't want to be responsible for code based on these
        libraries in a production context. By contrast I didn't have any problems with
        Gunicorn or UWSGI - except that I really dislike that UWSGI doesn't exit if
        your app hasn't loaded correctly.</p>
        <h2>In conclusion</h2>
        <p>My recommendation: for performance purposes, just use normal, synchronous
        Python but use native code for as much as possible. For webservers, it's worth
        considering frameworks other than Flask if throughput is paramount but even
        Flask under UWSGI has latency characteristics as good as the best.</p>
        <p>Thanks to my friend <a href="https://tudormunteanu.com/">Tudor Munteanu</a>
        for double checking my numbers!</p>
        <h2>See also</h2>
        <p>Flask's original author has posted a couple of times about his concerns
        regarding asyncio, first posting <a href="https://lucumr.pocoo.org/2016/10/30/i-dont-understand-asyncio/">"I don't
        understand Python's Asyncio"</a> which actually gives a pretty good explanation
        of the technology and recently with <a href="https://lucumr.pocoo.org/2020/1/1/async-pressure/">"I'm not feeling the async
        pressure"</a> in which he says:</p>
        <blockquote>
            <p>async/await is great but it encourages writing stuff that will behave
            catastrophically when overloaded</p>
        </blockquote>
        <p><a href="https://journal.stuffwithstuff.com/2015/02/01/what-color-is-your-function/">What
        color is your function?</a> explains some of the reasons why it is more painful
        to have a language with sync and async at the same time.</p>
        <p>Function colouring is a big problem in Python and the community is now sadly
        bifurcated into people writing sync code and people writing async code - they
        can't share the same libraries. Worse yet, some async libraries are also
        incompatible with other async libraries so the async Python community is even
        further divided.</p>
        <p>Chris Wellons wrote <a href="https://nullprogram.com/blog/2020/05/24/">an
        article</a> recently which also touches on latency issues and some footguns in
        the asyncio standard library. This is the kind of problem that makes async
        programs much harder to get right unfortunately.</p>
        <p>Nathaniel J. Smith has a series of brill articles on async that I recommend
        to anyone trying to get to grips with it:</p>
        <ul>
            <li>
                <a href="https://vorpus.org/blog/notes-on-structured-concurrency-or-go-statement-considered-harmful/">
                Notes on structured concurrency</a>
            </li>
            <li>
                <a href="https://vorpus.org/blog/some-thoughts-on-asynchronous-api-design-in-a-post-asyncawait-world/">
                Some thoughts on asynchronous API design in a post-async/await
                world</a>
            </li>
            <li>
                <a href="https://vorpus.org/blog/control-c-handling-in-python-and-trio/">Control-C
                handling in Python and Trio</a>
            </li>
            <li>
                <a href="https://vorpus.org/blog/timeouts-and-cancellation-for-humans/">Timeouts
                and cancellation for humans</a>
            </li>
        </ul>
        <p>He contends that the asyncio library is misconceived. My worry is that if
        the big brains who debate PEPs can't get it right, what hope is there for mere
        mortals like myself?</p>
    </article></div></div>
    </div>
    <footer>
        <div>created by <a href="https://buildstarted.com">buildstarted</a> &copy; 2020 <a href="/about">about</a></div>
        <div>Share this page on social media: copy and paste this url https://linksfor.dev/</div>
        <div>If you prefer RSS: <a href="https://linksfor.dev/feed.xml">https://linksfor.dev/feed.xml</a></div>
        <div>Customer satisfaction guaranteed to be optional.</div>
    </footer>
    
    <script async defer>
        _dna = window._dna || {};
        _dna.siteId = "linksfor.devs";
        _dna.outlink = true;

        (function() {
            let dna = document.createElement('script');
            dna.type = 'text/javascript';
            dna.async = true;
            dna.src = '//dna.buildstarted.com/t.js';
            let s = document.getElementsByTagName('script')[0];
            s.parentNode.insertBefore(dna, s);
        })();
    </script>
    <noscript><img src="//dna.buildstarted.com/g?siteId=linksfor.devs"/></noscript>
</body>
</html>