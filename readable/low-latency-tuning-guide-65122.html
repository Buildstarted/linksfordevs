<!DOCTYPE html>
<html lang="en">
<head>
    <title>
Low latency tuning guide - linksfor.dev(s)    </title>
    <meta charset="utf-8">
    <link rel="alternate" type="application/rss+xml" title="Linksfor.dev(s) feed" href="https://linksfor.dev/feed.rss" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta name="google" value="notranslate">
    <link rel="stylesheet" type="text/css" href="/style.css" />
    <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon">
    <link rel="icon" href="/favicon.ico" type="image/x-icon">
    <meta property="og:title" content="Low latency tuning guide - linksfor.dev(s)"/>
    <meta property="article:author" content="Erik Rigtorp"/>
    <meta property="og:description" content="This guide describes how to tune your AMD64/x86_64 hardware and Linux system for running real-time or low latency workloads."/>
    <meta property="og:type" content="website"/>
    <meta property="og:url" content="https://rigtorp.se/low-latency-guide/"/>

<meta property="og:site_name" content="linksfor.dev(s)" />
</head>
<body>
    <div class="devring" style="background: #222">
        <div style="text-align:center">Explore other dev related sites in this ring. If you would like to join this ring <a href="https://devring.club">click here</a>.</div>
        <div class="grid">
            <div style="display: grid; grid-template-columns: .5fr 1fr 1fr 1fr; text-align: center;">
                <span class="devring-title"><a href="https://devring.club/">devring.club</a></span>
                <a href="https://devring.club/sites/1/prev" class="devring-previous">Previous</a>
                <a href="https://devring.club/random" class="devring-random">Random</a>
                <a href="https://devring.club/sites/1/next" class="devring-next">Next</a>
            </div>
        </div>
    </div>
    <div class="grid">
        <h1>
<a href="/" style="color:inherit">linksfor.dev(s)</a>
        </h1>
        <title>linksfor.dev(s) - Low latency tuning guide</title>
<div class="readable">
        <h1>Low latency tuning guide</h1>
            <div>by Erik Rigtorp</div>
            <div>Reading time: 14-17 minutes</div>
        <div>Posted here: 02 Aug 2020</div>
        <p><a href="https://rigtorp.se/low-latency-guide/">https://rigtorp.se/low-latency-guide/</a></p>
        <hr/>
<div id="readability-page-1" class="page"><div>
				<p>This guide describes how to tune your AMD64/x86_64 hardware and Linux system for
running real-time or low latency workloads. Example workloads where this type of
tuning would be appropriate:</p>
<ul>
<li>Line rate <a href="https://en.wikipedia.org/wiki/Packet_analyzer">packet capture</a></li>
<li>Line rate
<a href="https://en.wikipedia.org/wiki/Deep_packet_inspection">deep packet inspection (DPI)</a></li>
<li>Applications using kernel-bypass networking</li>
<li>Accurate benchmarking of
<a href="https://stackoverflow.com/questions/868568/what-do-the-terms-cpu-bound-and-i-o-bound-mean">CPU bound</a>
programs</li>
</ul>
<p>The term latency in this context refers to the time between receiving some event
and the time when the event was processed. For example:</p>
<ul>
<li>The time between a network packet was received by a NIC until an application
finished processing the packet.</li>
<li>The time between a request was submitted to a queue and the worker thread
finished processing the request.</li>
</ul>
<p>To achieve low latency this guide describes how to:</p>
<ul>
<li>Maximize per core performance by maximizing CPU frequency and disabling
power saving features.</li>
<li>Minimize jitter caused by interrupts, timers and other applications
interfering with your workload.</li>
</ul>
<p>You can measure the reduced system jitter using my tool
<a href="https://github.com/rigtorp/hiccups">hiccups</a>. In this example it shows how core
3 was isolated and experienced a maximum jitter of 18 us:</p>
<pre><code>$ hiccups | column -t -R 1,2,3,4,5,6
cpu  threshold_ns  hiccups  pct99_ns  pct999_ns    max_ns
  0           168    17110     83697    6590444  17010845
  1           168     9929    169555    5787333   9517076
  2           168    20728     73359    6008866  16008460
  3           168    28336      1354       4870     17869
</code></pre><h2 id="hardware-tuning">Hardware tuning</h2>
<h3 id="enable-performance-mode">Enable performance mode</h3>
<p>The systems UEFI or BIOS usually have a setting for energy profile that adjusts
available CPU power states, you should set this to “maximum performance” or
equivalent.</p>
<h3 id="disable-hyper-threading">Disable hyper-threading</h3>
<p><a href="https://en.wikipedia.org/wiki/Hyper-threading">Hyper-threading (HT)</a> or
<a href="https://en.wikipedia.org/wiki/Hyper-threading">Simultaneous multithreading (SMT)</a>
is a technology to maximize processor resource usage for workloads with low
<a href="https://en.wikipedia.org/wiki/Instructions_per_cycle">instructions per cycle (IPC)</a>.
Since HT/SMT increases contention on processor resources it’s recommended to
turn it off if you want to reduce jitter introduced by contention on processor
resources.</p>
<p>HT/SMT can be turned off in your system’s UEFI or BIOS firmware settings.</p>
<h3 id="enable-turbo-boost">Enable Turbo Boost</h3>
<p><a href="https://en.wikipedia.org/wiki/Intel_Turbo_Boost">Intel Turbo Boost</a> and
<a href="https://en.wikipedia.org/wiki/AMD_Turbo_Core">AMD Turbo Core</a> technologies
allows the processor to automatically overclock itself as long as it stays
within some power and thermal envelope. If you have good cooling (set fan speed
to max in BIOS), disable unused cores in BIOS or using the CPU hotplug
functionality it’s possible to run your application cores continuously at the
higher boost frequency.</p>
<p>Check if turbo boost is enabled:</p>
<pre><code>cat /sys/devices/system/cpu/intel_pstate/no_turbo
</code></pre><p>Output should be 0 if turbo boost is enabled.</p>
<p>Alternatively you can use <code>cpupower</code> to check the status of turbo boost:</p>
<pre><code>cpupower frequency-info
</code></pre><p>Use the <code>turbostat</code> tool to verify the clock frequency of each core. Note that
<code>turbostat</code> will cause scheduling jitter and should not be used during
production.</p>
<p>References:</p>
<ul>
<li><a href="https://www.kernel.org/doc/html/latest/admin-guide/pm/cpufreq.html#frequency-boost-support">https://www.kernel.org/doc/html/latest/admin-guide/pm/cpufreq.html#frequency-boost-support</a></li>
<li><a href="https://www.kernel.org/doc/html/latest/core-api/cpu_hotplug.html">https://www.kernel.org/doc/html/latest/core-api/cpu_hotplug.html</a></li>
</ul>
<h3 id="overclocking">Overclocking</h3>
<p>Consider overclocking your processors. Running your processor at higher
frequency will reduce jitter and latency. It’s not possible to overclock Intel
Xeon server processors, but you can overclock Intel’s consumer gaming processors
and AMD’s processors.</p>
<h2 id="kernel-tuning">Kernel tuning</h2>
<h3 id="use-the-performance-cpu-frequency-scaling-governor">Use the performance CPU frequency scaling governor</h3>
<p>Use the performance CPU frequency scaling governor to maximize core frequency.</p>
<p>Set all cores to use the performance governor:</p>
<pre><code># find /sys/devices/system/cpu -name scaling_governor -exec sh -c 'echo performance &gt; {}' ';'
</code></pre><p>This can also be done by using the <code>tuned</code> performance profile:</p>
<pre><code># tuned-adm profile latency-performance
</code></pre><p>Verify that the performance governor is used with <code>cpupower</code>:</p>
<pre><code>cpupower frequency-info
</code></pre><p>References:</p>
<ul>
<li><a href="https://www.kernel.org/doc/html/latest/admin-guide/pm/cpufreq.html">https://www.kernel.org/doc/html/latest/admin-guide/pm/cpufreq.html</a></li>
<li><a href="https://www.kernel.org/doc/html/latest/admin-guide/pm/intel_pstate.html">https://www.kernel.org/doc/html/latest/admin-guide/pm/intel_pstate.html</a></li>
<li><a href="https://github.com/redhat-performance/tuned">https://github.com/redhat-performance/tuned</a></li>
</ul>
<h3 id="isolate-cores">Isolate cores</h3>
<p>By default the kernel scheduler will load balance all threads across all
available cores. To stop system threads from interfering with your application
threads from you can use the kernel command line option <code>isolcpus</code>. It disables
scheduler load balancing for the isolated cores and causes threads to be
restricted to the non-isolated cores by default. Note that your critical
application threads needs to be specifically pinned to the isolated cores in
order to run there.</p>
<p>For example to isolate cores 1 through 7 add <code>isolcpus=1-7</code> to your kernel
command line.</p>
<p>When using isolcpus the kernel will still create several kernel threads on the
isolated cores. Some of these kernel threads can be moved to the non-isolated
cores.</p>
<p>Try to move all kernel threads to core 0:</p>
<pre><code># pgrep -P 2 | xargs -i taskset -p -c 0 {}
</code></pre><p>Alternatively use <code>tuna</code> move all kernel threads away from cores 1-7:</p>
<pre><code># tuna --cpus=1-7 --isolate
</code></pre><p>Verify by using the <code>tuna</code> command to show CPU affinities for all threads:</p>
<pre><code>tuna -P
</code></pre><p>Additionally kernel workqueues needs to be moved away from isolated cores. To
move all work queues to core 0 (cpumask 0x1):</p>
<pre><code># find /sys/devices/virtual/workqueue -name cpumask  -exec sh -c 'echo 1 &gt; {}' ';'
</code></pre><p>Verify by listing current workqueue affinities:</p>
<pre><code>find /sys/devices/virtual/workqueue -name cpumask -print -exec cat '{}' ';'
</code></pre><p>Finally verify if cores were successfully isolated by checking how many thread
context switches are occurring per core:</p>
<pre><code># perf stat -e 'sched:sched_switch' -a -A --timeout 10000
</code></pre><p>The isolated cores should show a very low context switch count.</p>
<p>There is a work in progress patch set to improve task isolation even further,
see
<a href="https://lwn.net/Articles/816298/">A full task-isolation mode for the kernel</a>.</p>
<p>References:</p>
<ul>
<li><a href="https://www.kernel.org/doc/html/latest/admin-guide/kernel-parameters.html?highlight=isolcpus">https://www.kernel.org/doc/html/latest/admin-guide/kernel-parameters.html?highlight=isolcpus</a></li>
<li><a href="https://www.kernel.org/doc/html/latest/admin-guide/kernel-per-CPU-kthreads.html">https://www.kernel.org/doc/html/latest/admin-guide/kernel-per-CPU-kthreads.html</a></li>
<li><a href="http://man7.org/linux/man-pages/man2/sched_setaffinity.2.html">http://man7.org/linux/man-pages/man2/sched_setaffinity.2.html</a></li>
<li><a href="https://rt.wiki.kernel.org/index.php/Tuna">https://rt.wiki.kernel.org/index.php/Tuna</a></li>
<li><a href="https://lwn.net/Articles/659490/">https://lwn.net/Articles/659490/</a></li>
<li><a href="https://lwn.net/Articles/816298/">https://lwn.net/Articles/816298/</a></li>
<li><a href="https://lwn.net/ml/linux-kernel/aed12dd15ea2981bc9554cfa8b5e273c1342c756.camel@marvell.com/">https://lwn.net/ml/linux-kernel/aed12dd15ea2981bc9554cfa8b5e273c1342c756.camel@marvell.com/</a></li>
</ul>
<h3 id="reducing-timer-tick-interrupts">Reducing timer tick interrupts</h3>
<p>The scheduler runs regularly on each core in order to switch between runnable
threads. This will introduce jitter for latency critical applications. If you
have isolated your application cores and are running a single application thread
per isolated core you can use the <a href="https://www.kernel.org/doc/html/latest/admin-guide/kernel-parameters.html?highlight=nohz_full"><code>nohz_full</code></a> kernel command line option
in order to suppress the timer interrupts.</p>
<p>For example to enable <code>nohz_full</code> on cores 1-7 add <code>nohz_full=1-7 rcu_nocbs=1-7</code>
to your kernel command line.</p>
<p>It’s important to note that the timer tick is only disabled on a core when there
is only a single runnable thread scheduled on that core. You can see the number
of runnable threads per core in <em>/proc/sched_debug</em>.</p>
<p>The virtual memory subsystem runs a per core statistics update task every 1
second by default. You can reduce this interval by setting <code>vm.stat_interval</code> to
a higher value, for example 120 seconds:</p>
<pre><code data-lang="console"># sysctl vm.stat_interval=120
</code></pre><p>Finally you can verify that the timer interrupt frequency is reduced by
inspecting <em>/proc/interrupts</em> or using <a href="https://perf.wiki.kernel.org/index.php/Main_Page"><em>perf</em></a> to monitor timer
interrupts:</p>
<pre><code data-lang="console"># perf stat -e 'irq_vectors:local_timer_entry' -a -A --timeout 30000

 Performance counter stats for 'system wide':

CPU0                   31,204      irq_vectors:local_timer_entry
CPU1                    3,771      irq_vectors:local_timer_entry
CPU2                        3      irq_vectors:local_timer_entry
CPU3                        4      irq_vectors:local_timer_entry

      30.001671482 seconds time elapsed
</code></pre><p>In the above example cores 2 and 3 has a reduced timer interrupt frequency.
Expect <code>isolcpus</code> + <code>nohz_full</code> cores to show a timer interrupt every other
second or so. Unfortunately the timer tick cannot be completely eliminated.</p>
<p>References:</p>
<ul>
<li>The Linux Kernel Documentation. “NO_HZ: Reducing Scheduling-Clock Ticks”.
<a href="https://www.kernel.org/doc/html/latest/timers/no_hz.html">https://www.kernel.org/doc/html/latest/timers/no_hz.html</a></li>
<li>Jonathan Corbet. “(Nearly) full tickless operation in 3.10 “.
<a href="https://lwn.net/Articles/549580/">https://lwn.net/Articles/549580/</a></li>
<li>Jeremy Eder. “nohz_full=godmode ?".
<a href="https://jeremyeder.com/2013/11/15/nohz_fullgodmode/">https://jeremyeder.com/2013/11/15/nohz_fullgodmode/</a></li>
</ul>
<h3 id="interrupt-affinity">Interrupt affinity</h3>
<p>Reduce jitter from interrupt processing by changing the CPU affinity of the
interrupts. This can easily be done by running
<a href="https://github.com/Irqbalance/irqbalance"><em>irqbalance</em></a>. By default
<em>irqbalance</em> will automatically isolate the cores specified by the kernel
command line parameter <code>isolcpus</code>. You can also specify cores to isolate using
the <code>IRQBALANCE_BANNED_CPUS</code> environment variable.</p>
<p>To isolate cores specified in <code>isolcpus</code>:</p>
<pre><code data-lang="console">irqbalance --foreground --oneshot
</code></pre><p>To isolate core 3 (hexadecimal bitmask 0x8):</p>
<pre><code data-lang="console">IRQBALANCE_BANNED_CPUS=8 irqbalance --foreground --oneshot
</code></pre><p>List CPU affinity for all IRQs:</p>
<pre><code data-lang="console">find /proc/irq/ -name smp_affinity_list -print -exec cat '{}' ';'
</code></pre><p>Finally verify that isolated cores are not receiving interrupts by monitoring
<em>/proc/interrupts</em>:</p>
<pre><code data-lang="console">watch cat /proc/interrupts
</code></pre><p>References:</p>
<ul>
<li>The Linux Kernel Documentation. “SMP IRQ affinity”.
<a href="https://www.kernel.org/doc/html/latest/core-api/irq/irq-affinity.html">https://www.kernel.org/doc/html/latest/core-api/irq/irq-affinity.html</a></li>
<li>irqbalance. <a href="https://github.com/Irqbalance/irqbalance">https://github.com/Irqbalance/irqbalance</a>,
<a href="https://linux.die.net/man/1/irqbalance">https://linux.die.net/man/1/irqbalance</a></li>
</ul>
<h3 id="disable-transparent-huge-pages">Disable transparent huge pages</h3>
<p><a href="https://www.kernel.org/doc/html/latest/admin-guide/mm/transhuge.html">Linux <em>transparent huge page</em> (THP) support</a> allows the kernel to
automatically promote regular memory pages into huge pages. Huge pages reduces
TLB pressure, but THP support introduces latency spikes when pages are promoted
into huge pages and when memory compaction is triggered.</p>
<p>Disable transparent huge page support by supplying the kernel command line
parameter <code>transparent_hugepage=never</code> or running the following command:</p>
<pre><code data-lang="console">echo never &gt; /sys/kernel/mm/transparent_hugepage/enabled
</code></pre><p>References:</p>
<ul>
<li>“Transparent Hugepage Support”.
<a href="https://www.kernel.org/doc/html/latest/admin-guide/mm/transhuge.html">https://www.kernel.org/doc/html/latest/admin-guide/mm/transhuge.html</a></li>
<li>“Transparent Hugepages: measuring the performance impact”.
<a href="https://alexandrnikitin.github.io/blog/transparent-hugepages-measuring-the-performance-impact/">https://alexandrnikitin.github.io/blog/transparent-hugepages-measuring-the-performance-impact/</a></li>
<li>Erik Rigtorp. “Latency implications of virtual memory”.
<a href="https://rigtorp.se/virtual-memory/">https://rigtorp.se/virtual-memory/</a></li>
<li>Jonathan Corbet. “Memory compaction”. <a href="https://lwn.net/Articles/368869/">https://lwn.net/Articles/368869/</a></li>
<li>Nitin Gupta. “Proactive compaction for the kernel”.
<a href="https://lwn.net/Articles/817905/">https://lwn.net/Articles/817905/</a></li>
<li>Nitin Gupta. “Proactive Compaction for the Linux kernel”.
<a href="https://nitingupta.dev/post/proactive-compaction/">https://nitingupta.dev/post/proactive-compaction/</a></li>
</ul>
<h3 id="disable-kernel-samepage-merging">Disable kernel samepage merging</h3>
<p><a href="https://www.kernel.org/doc/html/latest/admin-guide/mm/ksm.html">Linux kernel samepage merging (KSM)</a> is a feature that de-duplicates
memory pages that contains identical data. The merging process needs to lock the
page tables and issue TLB shootdowns, leading to unpredictable memory access
latencies. KSM only operates on memory pages that has been opted in to samepage
merging using <code>madvise(...MADV_MERGEABLE)</code>. If needed KSM can be disabled system
wide by running the following command:</p>
<pre><code data-lang="console">echo 0 &gt; /sys/kernel/mm/ksm/run
</code></pre><h3 id="disable-mitigations-for-cpu-vulnerabilities">Disable mitigations for CPU vulnerabilities</h3>
<p>This is application dependent, but consider disabling mitigations for CPU
vulnerabilities. The <a href="https://www.phoronix.com/scan.php?page=article&amp;item=intel-10900k-mitigations&amp;num=1">mitigations can have considerable performance
impact</a> on system performance. Add
<a href="https://www.kernel.org/doc/html/latest/admin-guide/kernel-parameters.html?highlight=mitigations"><code>mitigations=off</code></a> to your kernel command line to disable all
mitigations.</p>
<p>Also consider using older CPU microcode without the microcode mitigations for
CPU vulnerabilities.</p>
<h3 id="use-cache-partitioning">Use cache partitioning</h3>
<p>If your processor supports cache partitioning (Intel Cache Allocation
Technology) consider using it to allocate most of the last-level cache (LLC) to
your application.</p>
<p>References:</p>
<ul>
<li><a href="https://github.com/intel/intel-cmt-cat">https://github.com/intel/intel-cmt-cat</a></li>
<li><a href="https://danluu.com/intel-cat/">https://danluu.com/intel-cat/</a></li>
<li><a href="https://software.intel.com/en-us/articles/introduction-to-cache-allocation-technology">https://software.intel.com/en-us/articles/introduction-to-cache-allocation-technology</a></li>
</ul>
<h2 id="application-design-and-tuning">Application design and tuning</h2>
<h3 id="consider-numa-topology">Consider NUMA topology</h3>
<p>TODO</p>
<h3 id="use-huge-pages">Use huge pages</h3>
<p>The <em>translation lookaside buffer</em> (TLB) has a limited number of entries. If
your application tries to access a memory page that is missing in the TLB, it
causes a <em>TLB miss</em> requiring the MMU to walk the page table. The default page
size is 4096 bytes, by using huge pages of 2 MB or 1 GB you can reduce the
amount of TLB misses for the same amount of actively used RAM.</p>
<p>You can monitor TLB misses with the <code>perf</code> tool:</p>
<pre><code data-lang="console"># perf stat -e 'dTLB-loads,dTLB-load-misses,iTLB-loads,iTLB-load-misses' -a --timeout 10000

 Performance counter stats for 'system wide':

        10,525,620      dTLB-loads
         2,964,792      dTLB-load-misses          #   28.17% of all dTLB cache hits
         1,998,243      iTLB-loads
         1,068,635      iTLB-load-misses          #   53.48% of all iTLB cache hits

      10.002451754 seconds time elapsed
</code></pre><p>The above output shows the fraction of data loads (dTLB) and instruction loads
(iTLB) that miss. If you observe a large fraction of TLB misses, you should
consider using huge pages to reduce the number of TLB misses. There are
additional CPU performance counters you can use to measure TLB pressure, consult
your processor manual for a complete list of TLB related performance counters.</p>
<p>References:</p>
<ul>
<li>Wikipedia. “Translation lookaside buffer”.
<a href="https://en.wikipedia.org/wiki/Translation_lookaside_buffer">https://en.wikipedia.org/wiki/Translation_lookaside_buffer</a></li>
<li>Ulrich Drepper. “Memory part 3: Virtual Memory”.
<a href="https://lwn.net/Articles/253361/">https://lwn.net/Articles/253361/</a></li>
<li>“Transparent Hugepages: measuring the performance impact”.
<a href="https://alexandrnikitin.github.io/blog/transparent-hugepages-measuring-the-performance-impact/">https://alexandrnikitin.github.io/blog/transparent-hugepages-measuring-the-performance-impact/</a></li>
</ul>
<h3 id="tlb-shootdowns">TLB shootdowns</h3>
<p>Each process has a <em>page table</em>
<a href="https://lwn.net/Articles/253361/">mapping virtual address to physical address</a>.
When the page table changes such that memory is unmapped (<code>munmap</code>) or access to
memory is restricted (<code>mmap</code> changing <code>PROT_*</code> flags) the TLB needs to be
flushed on all cores currently running the application process. This is called a
<em>TLB shootdown</em> and is implemented as a inter-processor interrupt (IPI) that
will introduce jitter to your running application. In addition the subsequent
TLB misses introduces memory access latency. Other causes of TLB shootdowns are:
<a href="https://www.kernel.org/doc/html/latest/admin-guide/mm/transhuge.html">transparent huge pages (THP)</a>, <a href="https://lwn.net/Articles/368869/">memory compaction</a>, <a href="https://www.kernel.org/doc/html/latest/admin-guide/mm/ksm.html">kernel samepage
merging (KSM)</a>, <a href="https://www.kernel.org/doc/html/latest/vm/page_migration.html">page migration</a> and page cache writeback.</p>
<p>To avoid TLB shootdowns:</p>
<ul>
<li>Never release memory back to the kernel (<code>madvise(...MADV_FREE)</code>/<code>munmap</code>)</li>
<li>Disable <a href="https://www.kernel.org/doc/html/latest/admin-guide/mm/transhuge.html">THP</a></li>
<li>Disable <a href="https://www.kernel.org/doc/html/latest/vm/page_migration.html">NUMA balancing / page migration</a></li>
<li>Don’t create any file backed writable memory mappings (<code>mmap(...PROT_WRITE</code>)</li>
</ul>
<p>You can view the number of TLB shootdowns by core in <em>/proc/interrupts</em>.</p>
<p>Monitor number of TLB flushes:</p>
<pre><code data-lang="console"># perf stat -e 'tlb_flush.dtlb_thread' -a -A --timeout 10000
</code></pre><p>References:</p>
<ul>
<li>Stackverflow. “Who performs the TLB shootdown?".
<a href="https://stackoverflow.com/questions/50256740/who-performs-the-tlb-shootdown">https://stackoverflow.com/questions/50256740/who-performs-the-tlb-shootdown</a></li>
<li>Stackoverflow. “What is TLB shootdown?".
<a href="https://stackoverflow.com/questions/3748384/what-is-tlb-shootdown">https://stackoverflow.com/questions/3748384/what-is-tlb-shootdown</a></li>
<li>Wikipedia. “Inter-processor interrupt”.
<a href="https://en.wikipedia.org/wiki/Inter-processor_interrupt">https://en.wikipedia.org/wiki/Inter-processor_interrupt</a></li>
</ul>

			</div></div></div>
    </div>
    <footer>
        <div>created by <a href="https://buildstarted.com">buildstarted</a> &copy; 2020 <a href="/about">about</a></div>
        <div>Share this page on social media: copy and paste this url https://linksfor.dev/</div>
        <div>If you prefer RSS: <a href="https://linksfor.dev/feed.xml">https://linksfor.dev/feed.xml</a></div>
        <div>Customer satisfaction guaranteed to be optional.</div>
    </footer>
    
    <script async defer>
        _dna = window._dna || {};
        _dna.siteId = "linksfor.devs";
        _dna.outlink = true;

        (function() {
            let dna = document.createElement('script');
            dna.type = 'text/javascript';
            dna.async = true;
            dna.src = '//dna.buildstarted.com/t.js';
            let s = document.getElementsByTagName('script')[0];
            s.parentNode.insertBefore(dna, s);
        })();
    </script>
    <noscript><img src="//dna.buildstarted.com/g?siteId=linksfor.devs"/></noscript>
</body>
</html>