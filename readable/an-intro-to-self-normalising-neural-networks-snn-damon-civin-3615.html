<!DOCTYPE html>
<html lang="en">
<head>
    <title>
An intro to self-normalising neural networks (SNN) - linksfor.dev(s)    </title>
    <meta charset="utf-8">
    <link rel="alternate" type="application/rss+xml" title="Linksfor.dev(s) feed" href="https://linksfor.dev/feed.rss" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta name="google" value="notranslate">
    <link rel="stylesheet" type="text/css" href="/style.css" />
    <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon">
    <link rel="icon" href="/favicon.ico" type="image/x-icon">
    <meta property="og:title" content="An intro to self-normalising neural networks (SNN) - linksfor.dev(s)"/>
    <meta property="article:author" content="Damon Civin"/>
    <meta property="og:description" content="making vanilla nets cool again"/>
    <meta property="og:type" content="website"/>
    <meta property="og:url" content="https://medium.com/@damoncivin/self-normalising-neural-networks-snn-2a972c1d421"/>

<meta property="og:site_name" content="linksfor.dev(s)" />
</head>
<body>
    <div class="devring" style="background: #222">
        <div style="text-align:center">Explore other dev related sites in this ring. If you would like to join this ring <a href="https://devring.club">click here</a>.</div>
        <div class="grid">
            <div style="display: grid; grid-template-columns: .5fr 1fr 1fr 1fr; text-align: center;">
                <span class="devring-title"><a href="https://devring.club/">devring.club</a></span>
                <a href="https://devring.club/sites/1/prev" class="devring-previous">Previous</a>
                <a href="https://devring.club/random" class="devring-random">Random</a>
                <a href="https://devring.club/sites/1/next" class="devring-next">Next</a>
            </div>
        </div>
    </div>
    <div class="grid">
        <h1>
<a href="/" style="color:inherit">linksfor.dev(s)</a>
        </h1>
        <title>linksfor.dev(s) - An intro to self-normalising neural networks (SNN)</title>
<div class="readable">
        <h1>An intro to self-normalising neural networks (SNN)</h1>
            <div>by Damon Civin</div>
            <div>Reading time: 4-5 minutes</div>
        <div>Posted here: 27 Feb 2019</div>
        <p><a href="https://medium.com/@damoncivin/self-normalising-neural-networks-snn-2a972c1d421">https://medium.com/@damoncivin/self-normalising-neural-networks-snn-2a972c1d421</a></p>
        <hr/>
<div id="readability-page-1" class="page"><section><div><div><h2 id="3ad8" data-selectable-paragraph=""><strong>Normalization background</strong></h2><p id="80c8" data-selectable-paragraph="">Normalization most often means transforming inputs to zero-mean and unit variance. This is often done as a pre-processing step. It speeds up learning and improves accuracy. Why?</p><ul><li id="e62e" data-selectable-paragraph=""><mark>Normalization makes the values of different features comparable</mark></li><li id="3dec" data-selectable-paragraph="">During training, the weights and parameters adjust those values</li><li id="2cbe" data-selectable-paragraph="">This can mess up the scaling again, despite the pre-processing, which can cause the gradients to get out of control. This hurts learning. So normalisation needs to be applied during training</li></ul><p id="00c1" data-selectable-paragraph="">Many other normalization methods exist: batch normalization, layer normalization, weight normalization etc, but SGD and dropout perturb these kinds of normalisation (and they can be tricky to code), leading to high variance in training error. CNNs and RNNs get around this by sharing weights (though RNNs are still subject to exploding/vanishing gradients). The effect gets worse with depth, so deep vanilla networks tend to suck.</p><h2 id="3966" data-selectable-paragraph=""><strong>Self-normalisation map</strong></h2><p id="42d8" data-selectable-paragraph="">The key idea is to prove that there exists a fixed point for the mapping from the mean and variance of activations from one layer to the next.</p><figure><div><div><div><p><img src="https://miro.medium.com/max/60/1*300DhIitAA2L6gVJfT9OFw.png?q=20" width="412" height="86" role="presentation"></p><p><img width="412" height="86" role="presentation" src="https://miro.medium.com/max/412/1*300DhIitAA2L6gVJfT9OFw.png"></p></div></div></div><figcaption data-selectable-paragraph="">The mapping from the mean and variance of activations from one layer to the next</figcaption></figure><p id="5013" data-selectable-paragraph="">The mathematical tool for this is the <a href="https://en.wikipedia.org/wiki/Banach_fixed-point_theorem" target="_blank" rel="noopener nofollow">Banach fixed point theorem</a>. Using the theorem requires proving that there is a domain of <em>Ω</em> of<em> μ, ν</em> values for which the mapping <em>g</em> is a contraction and never maps to values outside of Ω. This latter part is usually the painful thing to prove. The authors have a great 90 page appendix, featuring a computationally assisted proof of this. Heroes.</p><p id="2df6" data-selectable-paragraph="">The authors introduce a new activation function and a new kind of dropout to make this fixed point mechanism work for them.</p><h2 id="4849" data-selectable-paragraph="">The SELU</h2><p id="64c5" data-selectable-paragraph="">The SELU activation function is defined as</p><figure><div><div><div><p><img src="https://miro.medium.com/max/60/1*RvV9tJXF0gvqeiA1BG91AA.png?q=20" width="508" height="114" role="presentation"></p><p><img width="508" height="114" role="presentation"></p></div></div></div><figcaption data-selectable-paragraph="">The new Scaled Exponential Linear Unit (SELU) activation function (see the paper for the parameters α and λ)</figcaption></figure><p id="4d14" data-selectable-paragraph="">Here α and λ are solved for in the equations resulting from find a fixed point <em>μ, ν = g(μ, ν). </em>The SELU looks like this:</p><figure><div><div><div><p><img src="https://miro.medium.com/max/60/1*WyQS-lnoemRA3_FpRL7r5w.png?q=20" width="479" height="329" role="presentation"></p><p><img width="479" height="329" role="presentation"></p></div></div></div><figcaption data-selectable-paragraph="">The SELU activation function</figcaption></figure><h2 id="527f" data-selectable-paragraph=""><strong>α-dropout</strong></h2><p id="3c76" data-selectable-paragraph="">Normal dropout (randomly setting weights to 0 with some probability) would ruin the desired mean and variance, so this needs to be amended. The mean is preserved by scaling the activations. Preservation of the variance is achieved by applying an affine transformation. This affine transformation exploits the fact dropout works well for RELUs because 0 is in the low variance region for that activation. For SELU, the low variance limit is SELU(x)=-λα=:α’ as x tends to -∞. So α-dropout randomly sets inputs to α’ instead of 0. This transformation adds 2 more parameters that are solved for the specific desired fixed point of <em>(μ, ν)=(0,1).</em></p><h2 id="a643" data-selectable-paragraph="">Key results</h2><p id="61e3" data-selectable-paragraph="">Constructing neural nets this way ensures that the distribution of neuron activations remains stable. That is, the mean and variance of the data through each layer remains near 0 and 1 respectively.</p><p id="08d8" data-selectable-paragraph=""><strong>Theorem 1</strong> — Under some conditions on the weights, the map <em>g</em> has a stable and attracting fixed point, meaning that SNNs really are self-normalising.</p><p id="42c1" data-selectable-paragraph="">The intuition here is that high variance in one layer is mapped to low variance in the next layer and vice versa. This works because the SELU decreases the variance for negative inputs and increases the variance for positive inputs. The decrease effect is stronger for very negative inputs, and the increase effect is stronger for near-zero values.</p><p id="f298" data-selectable-paragraph=""><strong>Theorem 2 — </strong>the mapping of variance is bounded from above so gradients cannot explode</p><p id="d397" data-selectable-paragraph=""><strong>Theorem 3—</strong>the mapping of variance is bounded from below so gradients cannot vanish</p><p id="18a9" data-selectable-paragraph="">Lo and behold, it works, just look how smooth this training curve is! Check out the code below for more empirical evidence that things work out nicely.</p><figure><div><div><div><div><p><img src="https://miro.medium.com/max/60/1*41LHXEasUHhENehdWVc23g.png?q=20" width="1582" height="852" role="presentation"></p><p><img width="1582" height="852" role="presentation"></p></div></div></div></div><figcaption data-selectable-paragraph="">Image from Klambauer et al, <a href="https://arxiv.org/abs/1706.02515" target="_blank" rel="noopener nofollow">https://arxiv.org/abs/1706.02515</a> [license <code><a href="http://arxiv.org/licenses/nonexclusive-distrib/1.0/" target="_blank" rel="noopener nofollow">http://arxiv.org/licenses/nonexclusive-distrib/1.0/</a></code>]</figcaption></figure><h2 id="947d" data-selectable-paragraph="">Outlook</h2><p id="15fc" data-selectable-paragraph="">Vanilla nets are cool again. Deeper networks competitive with more complex architectures can be trained using SNN.</p><p id="e069" data-selectable-paragraph="">There is a <a href="https://github.com/bioinf-jku/SNNs" target="_blank" rel="noopener nofollow">TensorFlow</a> implementation of SELU, so expect to see loads of them coming up.</p><figure><div></div><figcaption>Empirical results</figcaption></figure><p id="53a9" data-selectable-paragraph="">* Klambauer et al, <a href="https://arxiv.org/abs/1706.02515" target="_blank" rel="noopener nofollow"><em>Self-Normalizing Neural Networks</em></a><br>* <a href="https://news.ycombinator.com/item?id=14527686" target="_blank" rel="noopener nofollow">Hacker news discussion</a><br>* Code below adapted from user CaseOfTuesday on <a href="https://www.reddit.com/r/MachineLearning/comments/6g5tg1/r_selfnormalizing_neural_networks_improved_elu/" target="_blank" rel="noopener nofollow">reddit</a></p></div></div></section></div></div>
    </div>
    <footer>
        <div>created by <a href="https://buildstarted.com">buildstarted</a> &copy; 2020 <a href="/about">about</a></div>
        <div>Share this page on social media: copy and paste this url https://linksfor.dev/</div>
        <div>If you prefer RSS: <a href="https://linksfor.dev/feed.xml">https://linksfor.dev/feed.xml</a></div>
        <div>Customer satisfaction guaranteed to be optional.</div>
    </footer>
    
    <script async defer>
        _dna = window._dna || {};
        _dna.siteId = "linksfor.devs";
        _dna.outlink = true;

        (function() {
            let dna = document.createElement('script');
            dna.type = 'text/javascript';
            dna.async = true;
            dna.src = '//dna.buildstarted.com/t.js';
            let s = document.getElementsByTagName('script')[0];
            s.parentNode.insertBefore(dna, s);
        })();
    </script>
    <noscript><img src="//dna.buildstarted.com/g?siteId=linksfor.devs"/></noscript>
</body>
</html>