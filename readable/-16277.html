<!DOCTYPE html>
<html lang="en">
<head>
    <title>
Frogger AI Explains Its Decisions - linksfor.dev(s)    </title>
    <meta charset="utf-8">
    <link rel="alternate" type="application/rss+xml" title="Linksfor.dev(s) feed" href="https://linksfor.dev/feed.rss" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta name="google" value="notranslate">
    <link rel="stylesheet" type="text/css" href="/style.css" />
    <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon">
    <link rel="icon" href="/favicon.ico" type="image/x-icon">
    <meta property="og:title" content="Frogger AI Explains Its Decisions - linksfor.dev(s)"/>
    <meta property="og:description" content="...using automated, plausible lies: After training the AI system to play the popular arcade game Frogger, and documenting how human players explained the decisions they made while playing the game, the team of researchers developed the agent to generate language in real time to explain the motivations behind its actions. [...] When it comes to neural networks -- a kind of AI architecture made"/>
    <meta property="og:type" content="website"/>
    <meta property="og:url" content="https://www.jwz.org/blog/2019/04/frogger-ai-explains-its-decisions/"/>

<meta property="og:site_name" content="linksfor.dev(s)" />
</head>
<body>
    <div class="grid">
        <h1>
                <span style="cursor: default" title="linksfor.dev(s) has been running for 1 year! :partypopper:">ðŸŽ‰</span>
<a href="/" style="color:inherit">linksfor.dev(s)</a>
        </h1>
        <title>linksfor.dev(s) - Frogger AI Explains Its Decisions</title>
<div class="readable">
        <h1>Frogger AI Explains Its Decisions</h1>
        <p>
Reading time: 3-4 minutes        </p>
        <p><a href="https://www.jwz.org/blog/2019/04/frogger-ai-explains-its-decisions/">https://www.jwz.org/blog/2019/04/frogger-ai-explains-its-decisions/</a></p>
        <hr/>
<div id="readability-page-1" class="page"><div>
<p><a href="https://motherboard.vice.com/en_us/article/j5wezy/scientists-created-a-frogger-playing-ai-that-explains-its-decisions"><img src="https://www.jwz.org/images/scaled/768/2019/1554992329116-study_screenshot.jpg" data-size="1200x675" srcset="https://www.jwz.org/images/2019/1554992329116-study_screenshot.jpg 1200w, https://www.jwz.org/images/scaled/768/2019/1554992329116-study_screenshot.jpg 768w, https://www.jwz.org/images/scaled/640/2019/1554992329116-study_screenshot.jpg 640w, https://www.jwz.org/images/scaled/360/2019/1554992329116-study_screenshot.jpg 360w" sizes="(max-width: 660px) 50vw, 36em">...using automated, plausible lies:</a></p><blockquote>After training the AI system to play the popular arcade game <i> Frogger</i>, and documenting how human players explained the decisions they made while playing the game, the team of researchers developed the agent to generate language in real time to explain the motivations behind its actions. [...]<p>When it comes to neural networks -- a kind of AI architecture made up of potentially thousands of computational nodes -- their decisions can be inscrutable, even to the engineers that design them. Such systems are capable of analyzing large swaths of data and identifying patterns and connections, and even if the math that governs them is understandable to humans, their aggregate decisions often aren't.</p><p>Upol Ehsan, lead researcher and PhD student in the School of Interactive Computing at Georgia Tech along with Riedl and their colleagues at Georgia Tech, Cornell University, and University of Kentucky, think that if average users can receive easy-to-understand explanations for how an AI software makes decisions, it will build trust in the technology to make sound decisions in a variety of applications.</p><p>"If I'm sitting in a self-driving car and it makes a weird decision, like change lanes unexpectedly, I would feel more comfortable getting in that car if I could ask it questions about why it was doing what it was doing," Riedl said.</p></blockquote><p>This is absolutely horrifying. The idea here is: the self-driving car has done something inscrutable. What if there was a chatbot that was good at looking at what just happened, and <i>making up a plausible explanation for it after the fact?</i></p><p>(Side note: that's probably how human consciousness actually works. Your "mind" is the post-facto explainer for what the meat-zombie automaton just happened to do. Sweet dreams.)</p><p>But this research is just mechanized "truthiness"! It's not actually explaining what happened inside the neural net, which it still treats as a black box. It's solving the problem of: "If the self-driving murderbox decided to cross 4 lanes and slam on the brakes, let's dig around in our memory and imagination, find a scenario where a <i>human</i> might have done that same thing, and proffer that as the explanation."</p><p>Even though the AI and the human might have done those for completely different reasons, that's the explanation that will sound most plausible!</p><p>The human's decisions were based on an unbroken straight line chain of 85 million years of "I am the primate who did not get eaten", layered with decades of fine motor control trailing. Whereas the AI based its decisions on whatever ad hoc junk a bunch of low-paid contractors dumped into the training set over a period maybe as long as 18 months.</p><p>So they're building an AI whose goal is to lie to you -- to build your confidence in decisions made by AIs.</p><p>We are so completely doomed.</p><blockquote> <table> <tbody><tr> <td nowrap=""><b>Maxwell &nbsp;<br>Smart:</b></td> <td>Because, at this very minute, 25 CONTROL agents are converging on this building. Would you believe it? 25 CONTROL agents!</td> </tr> <tr> <td><b>Savage:</b></td> <td>I find that hard to beleive.</td> </tr> <tr> <td><b>Max:</b></td> <td>Would you believe 2 squad cars and a motorcycle cop?</td> </tr> <tr> <td><b>Savage:</b></td> <td>I don't think so.</td> </tr> <tr> <td><b>Max:</b></td> <td>How about a vicious street cleaner and a toothless police dog?</td> </tr> </tbody></table></blockquote><p><a href="https://www.jwz.org/blog/2018/03/paperclip-optimizers-exploit-glitches-in-the-matrix/">Previously</a>, <a href="https://www.jwz.org/blog/2018/05/more-trolleys-more-problems/">previously</a>, <a href="https://www.jwz.org/blog/2018/06/mario-klingemanns-music-videos/">previously</a>, <a href="https://www.jwz.org/blog/2017/08/today-in-cv-dazzle-news-making-self-driving-cars-read-stop-signs-as-speed-limit-signs/">previously</a>, <a href="https://www.jwz.org/blog/2017/03/using-ritual-magic-to-trap-self-driving-cars/">previously</a>. </p></div></div></div>
    </div>
    <footer>
        <div>created by buildstarted &copy; 2020 <a href="/about">about</a></div>
        <div>Share this page on social media: copy and paste this url https://linksfor.dev/</div>
        <div>If you prefer RSS: <a href="https://linksfor.dev/feed.xml">https://linksfor.dev/feed.xml</a></div>
    </footer>
    
    <script async defer>
        _dna = window._dna || {};
        _dna.siteId = "linksfor.devs";
        _dna.outlink = true;

        (function () {
            let dna = document.createElement('script');
            dna.type = 'text/javascript';
            dna.async = true;
            dna.src = '//dna.buildstarted.com/t.js';
            let s = document.getElementsByTagName('script')[0];
            s.parentNode.insertBefore(dna, s);
        })();
    </script>
    <noscript><img src="//dna.buildstarted.com/g?siteId=linksfor.devs"/></noscript>
</body>
</html>