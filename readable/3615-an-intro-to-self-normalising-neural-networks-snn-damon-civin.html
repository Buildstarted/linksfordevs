<!DOCTYPE html>
<html lang="en">
<head>
    <title>
An intro to self-normalising neural networks (SNN) - Damon Civin -
linksfor.dev(s)
    </title>
    <link rel="alternate" type="application/rss+xml" title="Linksfor.dev(s) feed" href="https://linksfor.dev/feed.rss" />
    <meta charset="utf-8">
    <meta name="Description" content="A curated source of links that devs might find interesting. Updated around the clock." />
    <meta name="google" value="notranslate">
    <link rel="stylesheet" type="text/css" href="/style.css" />
    <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon">
    <link rel="icon" href="/favicon.ico" type="image/x-icon">
</head>
<body>
    <div class="grid">
        
<div class="readable">
    <div id="readOverlay" class="style-ebook"><div id="readInner" class="margin-medium size-medium"><h1>An intro to self-normalising neural networks (SNN) - Damon Civin</h1><div><div class="ac ae af ag ah cz aj ak"><h1 id="8082" class="gt gu dc bk bj gv de hu dg hv gy hw ha hx hc hy he">Key ideas</h1><h2 id="3ad8" class="hz gu dc bk bj gv ia ib ic id ie if ig ih ii ij ik"><strong class="az">Normalization background</strong></h2><p id="80c8" class="gd ge dc bk gf b dw hf dy hg gi hh gk hi gm hj go cu">Normalization most often means transforming inputs to zero-mean and unit variance. This is often done as a pre-processing step. It speeds up learning and improves accuracy. Why?</p><ul class=""><li id="e62e" class="gd ge dc bk gf b dw gg dy gh gi gj gk gl gm gn go il im in">Normalization makes the values of different features comparable</li><li id="3dec" class="gd ge dc bk gf b dw io dy ip gi iq gk ir gm is go il im in">During training, the weights and parameters adjust those values</li><li id="2cbe" class="gd ge dc bk gf b dw io dy ip gi iq gk ir gm is go il im in">This can mess up the scaling again, despite the pre-processing, which can cause the gradients to get out of control. This hurts learning. So normalisation needs to be applied during training</li></ul><p id="00c1" class="gd ge dc bk gf b dw gg dy gh gi gj gk gl gm gn go cu">Many other normalization methods exist: batch normalization, layer normalization, weight normalization etc, but SGD and dropout perturb these kinds of normalisation (and they can be tricky to code), leading to high variance in training error. CNNs and RNNs get around this by sharing weights (though RNNs are still subject to exploding/vanishing gradients). The effect gets worse with depth, so deep vanilla networks tend to suck.</p><h2 id="3966" class="hz gu dc bk bj gv ia ib ic id ie if ig ih ii ij ik"><strong class="az">Self-normalisation map</strong></h2><p id="42d8" class="gd ge dc bk gf b dw hf dy hg gi hh gk hi gm hj go cu">The key idea is to prove that there exists a fixed point for the mapping from the mean and variance of activations from one layer to the next.</p><figure class="iu iv iw ix iy iz cl cm paragraph-image"><figcaption class="bo ex jn jo ho cn cl cm jp jq bj dv">The mapping from the mean and variance of activations from one layer to the next</figcaption></figure><p id="5013" class="gd ge dc bk gf b dw gg dy gh gi gj gk gl gm gn go cu">The mathematical tool for this is the <a href="https://en.wikipedia.org/wiki/Banach_fixed-point_theorem" class="at cg gp gq gr gs" target="_blank" rel="noopener nofollow">Banach fixed point theorem</a>. Using the theorem requires proving that there is a domain of <em class="jr">Ω</em> of<em class="jr"> μ, ν</em> values for which the mapping <em class="jr">g</em> is a contraction and never maps to values outside of Ω. This latter part is usually the painful thing to prove. The authors have a great 90 page appendix, featuring a computationally assisted proof of this. Heroes.</p><p id="2df6" class="gd ge dc bk gf b dw gg dy gh gi gj gk gl gm gn go cu">The authors introduce a new activation function and a new kind of dropout to make this fixed point mechanism work for them.</p><h2 id="4849" class="hz gu dc bk bj gv ia ib ic id ie if ig ih ii ij ik">The SELU</h2><p id="64c5" class="gd ge dc bk gf b dw hf dy hg gi hh gk hi gm hj go cu">The SELU activation function is defined as</p><figure class="iu iv iw ix iy iz cl cm paragraph-image"><figcaption class="bo ex jn jo ho cn cl cm jp jq bj dv">The new Scaled Exponential Linear Unit (SELU) activation function (see the paper for the parameters α and λ)</figcaption></figure><p id="4d14" class="gd ge dc bk gf b dw gg dy gh gi gj gk gl gm gn go cu">Here α and λ are solved for in the equations resulting from find a fixed point <em class="jr">μ, ν = g(μ, ν). </em>The SELU looks like this:</p><figure class="iu iv iw ix iy iz cl cm paragraph-image"><figcaption class="bo ex jn jo ho cn cl cm jp jq bj dv">The SELU activation function</figcaption></figure><h2 id="527f" class="hz gu dc bk bj gv ia ib ic id ie if ig ih ii ij ik"><strong class="az">α-dropout</strong></h2><p id="3c76" class="gd ge dc bk gf b dw hf dy hg gi hh gk hi gm hj go cu">Normal dropout (randomly setting weights to 0 with some probability) would ruin the desired mean and variance, so this needs to be amended. The mean is preserved by scaling the activations. Preservation of the variance is achieved by applying an affine transformation. This affine transformation exploits the fact dropout works well for RELUs because 0 is in the low variance region for that activation. For SELU, the low variance limit is SELU(x)=-λα=:α’ as x tends to -∞. So α-dropout randomly sets inputs to α’ instead of 0. This transformation adds 2 more parameters that are solved for the specific desired fixed point of <em class="jr">(μ, ν)=(0,1).</em></p><h2 id="a643" class="hz gu dc bk bj gv ia ib ic id ie if ig ih ii ij ik">Key results</h2><p id="61e3" class="gd ge dc bk gf b dw hf dy hg gi hh gk hi gm hj go cu">Constructing neural nets this way ensures that the distribution of neuron activations remains stable. That is, the mean and variance of the data through each layer remains near 0 and 1 respectively.</p><p id="08d8" class="gd ge dc bk gf b dw gg dy gh gi gj gk gl gm gn go cu"><strong class="gf jw">Theorem 1</strong> — Under some conditions on the weights, the map <em class="jr">g</em> has a stable and attracting fixed point, meaning that SNNs really are self-normalising.</p><p id="42c1" class="gd ge dc bk gf b dw gg dy gh gi gj gk gl gm gn go cu">The intuition here is that high variance in one layer is mapped to low variance in the next layer and vice versa. This works because the SELU decreases the variance for negative inputs and increases the variance for positive inputs. The decrease effect is stronger for very negative inputs, and the increase effect is stronger for near-zero values.</p><p id="f298" class="gd ge dc bk gf b dw gg dy gh gi gj gk gl gm gn go cu"><strong class="gf jw">Theorem 2 — </strong>the mapping of variance is bounded from above so gradients cannot explode</p><p id="d397" class="gd ge dc bk gf b dw gg dy gh gi gj gk gl gm gn go cu"><strong class="gf jw">Theorem 3—</strong>the mapping of variance is bounded from below so gradients cannot vanish</p><p id="18a9" class="gd ge dc bk gf b dw gg dy gh gi gj gk gl gm gn go cu">Lo and behold, it works, just look how smooth this training curve is! Check out the code below for more empirical evidence that things work out nicely.</p><figure class="iu iv iw ix iy iz cl cm paragraph-image"><figcaption class="bo ex jn jo ho cn cl cm jp jq bj dv">Image from Klambauer et al, <a href="https://arxiv.org/abs/1706.02515" class="at cg gp gq gr gs" target="_blank" rel="noopener nofollow">https://arxiv.org/abs/1706.02515</a> [license <code class="jh kc kd ke kf b"><a href="http://arxiv.org/licenses/nonexclusive-distrib/1.0/" class="at cg gp gq gr gs" target="_blank" rel="noopener nofollow">http://arxiv.org/licenses/nonexclusive-distrib/1.0/</a></code>]</figcaption></figure><h2 id="947d" class="hz gu dc bk bj gv ia ib ic id ie if ig ih ii ij ik">Outlook</h2><p id="15fc" class="gd ge dc bk gf b dw hf dy hg gi hh gk hi gm hj go cu">Vanilla nets are cool again. Deeper networks competitive with more complex architectures can be trained using SNN.</p><p id="e069" class="gd ge dc bk gf b dw gg dy gh gi gj gk gl gm gn go cu">There is a <a href="https://github.com/bioinf-jku/SNNs" class="at cg gp gq gr gs" target="_blank" rel="noopener nofollow">TensorFlow</a> implementation of SELU, so expect to see loads of them coming up.</p><h1 id="b9f8" class="gt gu dc bk bj gv de gw dg gx gy gz ha hb hc hd he">Code</h1><figure class="iu iv iw ix iy iz"><figcaption class="bo ex jn jo ho cn cl cm jp jq bj dv">Empirical results</figcaption></figure><h1 id="c65b" class="gt gu dc bk bj gv de gw dg gx gy gz ha hb hc hd he">References</h1><p id="53a9" class="gd ge dc bk gf b dw hf dy hg gi hh gk hi gm hj go cu">* Klambauer et al, <a href="https://arxiv.org/abs/1706.02515" class="at cg gp gq gr gs" target="_blank" rel="noopener nofollow"><em class="jr">Self-Normalizing Neural Networks</em></a><br>* <a href="https://news.ycombinator.com/item?id=14527686" class="at cg gp gq gr gs" target="_blank" rel="noopener nofollow">Hacker news discussion</a><br>* Code below adapted from user CaseOfTuesday on <a href="https://www.reddit.com/r/MachineLearning/comments/6g5tg1/r_selfnormalizing_neural_networks_improved_elu/" class="at cg gp gq gr gs" target="_blank" rel="noopener nofollow">reddit</a></p></div></div></div></div>
</div>
    </div>
    <footer>
        <div>created by buildstarted &copy; 2020 <a href="/about">about</a></div>
        <div>Share this page on social media: copy and paste this url https://linksfor.dev/</div>
        <div>If you prefer RSS: <a href="https://linksfor.dev/feed.xml">https://linksfor.dev/feed.xml</a></div>
    </footer>
    
    <script>
        (function () {
            var COLLECT_URL = "https://dna.buildstarted.com/t";
            var SITE_ID = "linksfor.devs";
            var GLOBAL_VAR_NAME = "__DNA__";

            window[GLOBAL_VAR_NAME] = {};

            window[GLOBAL_VAR_NAME].sendPageView = function () {
                var path = location.pathname;
                var referrer = document.referrer;

                var url = COLLECT_URL + "?siteid=" + SITE_ID + "&p=" + encodeURIComponent(path) + "&r=" + encodeURIComponent(referrer);

                try { fetch(url, { method: "GET" }); } catch (e) { }
            };

            window[GLOBAL_VAR_NAME].sendPageView();
        })();
    </script>
</body>
</html>