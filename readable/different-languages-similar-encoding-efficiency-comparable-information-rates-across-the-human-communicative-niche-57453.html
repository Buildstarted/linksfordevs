<!DOCTYPE html>
<html lang="en">
<head>
    <title>
Different languages, similar encoding efficiency: Comparable information rates across the human communicative niche - linksfor.dev(s)    </title>
    <meta charset="utf-8">
    <link rel="alternate" type="application/rss+xml" title="Linksfor.dev(s) feed" href="https://linksfor.dev/feed.rss" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta name="google" value="notranslate">
    <link rel="stylesheet" type="text/css" href="/style.min.css" />
    <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon">
    <link rel="icon" href="/favicon.ico" type="image/x-icon">
    <meta property="og:title" content="Different languages, similar encoding efficiency: Comparable information rates across the human communicative niche - linksfor.dev(s)"/>
    <meta property="article:author" content="&#x21B5;* These authors contributed equally to this work."/>
    <meta property="og:description" content="Language is universal, but it has few indisputably universal characteristics, with cross-linguistic variation being the norm. For example, languages differ greatly in the number of syllables they allow, resulting in large variation in the Shannon information per syllable. Nevertheless, all natural languages allow their speakers to efficiently encode and transmit information. We show here, using quantitative methods on a large cross-linguistic corpus of 17 languages, that the coupling between language-level (information per syllable) and speaker-level (speech rate) properties results in languages encoding similar information rates (~39 bits/s) despite wide differences in each property individually: Languages are more similar in information rates than in Shannon information or speech rate. These findings highlight the intimate feedback loops between languages&#x2019; structural properties and their speakers&#x2019; neurocognition and biology under communicative pressures. Thus, language is the product of a multiscale communicative niche construction process at the intersection of biology, environment, and culture."/>
    <meta property="og:type" content="website"/>
    <meta property="og:url" content="https://advances.sciencemag.org/content/5/9/eaaw2594"/>

<meta property="og:site_name" content="linksfor.dev(s)" />
</head>
<body>
    <div class="devring" style="background: #222">
        <div style="text-align:center">Explore other dev related sites in this ring. If you would like to join this ring <a href="https://devring.club">click here</a>.</div>
        <div class="grid">
            <div style="display: grid; grid-template-columns: .5fr 1fr 1fr 1fr; text-align: center;">
                <span class="devring-title"><a href="https://devring.club/">devring.club</a></span>
                <a href="https://devring.club/sites/1/prev" class="devring-previous">Previous</a>
                <a href="https://devring.club/random" class="devring-random">Random</a>
                <a href="https://devring.club/sites/1/next" class="devring-next">Next</a>
            </div>
        </div>
    </div>
    <div class="grid">
        <h1 style="margin: unset">
<a href="/" style="color:inherit">linksfor.dev(s)</a>
        </h1>
        <title>linksfor.dev(s) - Different languages, similar encoding efficiency: Comparable information rates across the human communicative niche</title>
<div class="readable">
        <h1>Different languages, similar encoding efficiency: Comparable information rates across the human communicative niche</h1>
            <div>by &#x21B5;* These authors contributed equally to this work.</div>
            <div>Reading time: 45-57 minutes</div>
        <div>Posted here: 10 Feb 2020</div>
        <p><a href="https://advances.sciencemag.org/content/5/9/eaaw2594">https://advances.sciencemag.org/content/5/9/eaaw2594</a></p>
        <hr/>
<div id="readability-page-1" class="page"><div><div id="abstract-2"><h2>Abstract</h2><p id="p-4">Language is universal, but it has few indisputably universal characteristics, with cross-linguistic variation being the norm. For example, languages differ greatly in the number of syllables they allow, resulting in large variation in the Shannon information per syllable. Nevertheless, all natural languages allow their speakers to efficiently encode and transmit information. We show here, using quantitative methods on a large cross-linguistic corpus of 17 languages, that the coupling between language-level (information per syllable) and speaker-level (speech rate) properties results in languages encoding similar information rates (~39 bits/s) despite wide differences in each property individually: Languages are more similar in information rates than in Shannon information or speech rate. These findings highlight the intimate feedback loops between languages’ structural properties and their speakers’ neurocognition and biology under communicative pressures. Thus, language is the product of a multiscale communicative niche construction process at the intersection of biology, environment, and culture.</p></div><div id="sec-1"><h2>INTRODUCTION</h2><p id="p-5">Language is universally used by all human groups, but it hardly displays undisputable universal characteristics, with a few possible exceptions related to pragmatic and communicative constraints (<a id="xref-ref-1-1" href="#ref-1"><em>1</em></a>, <a id="xref-ref-2-1" href="#ref-2"><em>2</em></a>). This ubiquity comes with very high levels of variation across the 7000 or so languages (<a id="xref-ref-3-1" href="#ref-3"><em>3</em></a>). For example, linguistic differences between Japanese and English lead to a ratio of 1:11 in their number of distinct syllables. These differences in repertoire size result in large variation in the amount of information they encode per syllable according to Shannon’s theory of communication. Despite those differences, Japanese and English endow their respective speakers with linguistic systems that fulfill equally well one of the most important roles of spoken communication, namely, information transmission. We show here that the interplay between language-specific structural properties (as reflected by the amount of information per syllable) and speaker-level language processing and production [as reflected by speech rate (SR)] leads languages to gravitate around an information rate (IR) of about 39 bits/s. This finding, based on quantitative methods applied to a large cross-linguistic corpus of 17 languages, highlights the intimate feedback loops between languages and their speakers due to communicative pressures. We suggest that this phenomenon is rooted in the human neurocognitive capacity, probably present in our lineage for a long time (<a id="xref-ref-4-1" href="#ref-4"><em>4</em></a>), and that human language can be analyzed as the product of a multiscale communicative and cultural niche construction process involving biology, environment, and culture (<a id="xref-ref-5-1" href="#ref-5"><em>5</em></a>).</p><p id="p-6">Each human language provides its speakers with a communication system that fulfills their needs for transmitting information to their peers. The Uniform Information Density hypothesis (<a id="xref-ref-6-1" href="#ref-6"><em>6</em></a>) and similar approaches [e.g., (<a id="xref-ref-7-1" href="#ref-7"><em>7</em></a>) and (<a id="xref-ref-8-1" href="#ref-8"><em>8</em></a>)] suggested that speakers distribute information along the speech signal following a smooth distribution rather than high-amplitude fluctuations. Compatible with Shannon’s theory, this optimization process guarantees the robust information transmission at a rate close to the channel capacity. We adopt here a quite different perspective, where we compare, across very different languages, the average rates at which information is emitted. This approach enables us to estimate the channel capacity and to assess whether the large differences observed among languages in terms of encoding result in analog differences in channel capacity or, conversely, whether there exist compensating strategies that go beyond the local adaptation operating during speech production. Therefore, we investigate the interaction between information encoding and average SR and, more specifically, whether the variation among languages in IR is regulated by communicative constraints. Thus, does too low an IR hinder communicative efficiency? And, at the other extreme, does pushing it too high incur too heavy physiological and cognitive costs? While a negative correlation between average SR and the informativeness of linguistic constituents has been demonstrated in a small multilanguage corpus (<a id="xref-ref-9-1" href="#ref-9"><em>9</em></a>), the distribution of IRs across human languages is almost totally unknown despite its crucial importance for understanding human spoken communication. While our data here come only from speech production (information encoding), our results, nevertheless, implicitly address also speech perception (information retrieval) and processing, as they are all intimately coupled and coevolve during language acquisition, use, and change (<a id="xref-ref-10-1" href="#ref-10"><em>10</em></a>).</p><p id="p-7">We have chosen to focus here on the syllable as the information-encoding unit for both linguistic and cognitive reasons. On the linguistic side, despite a long-lasting debate in phonology about whether the syllable is a universal unit in the world’s languages (<a id="xref-ref-11-1" href="#ref-11"><em>11</em></a>) being a cornerstone of this controversy, analyzing the encoding of information in terms of syllables does offer several advantages over other levels of linguistic description (such as phonemes and morphemes). First, syllables are much less prone than phonemes to complete deletion in casual speech, allowing more robust estimates of SR (<a id="xref-ref-12-1" href="#ref-12"><em>12</em></a>) [readers can also refer to (<a id="xref-ref-9-2" href="#ref-9"><em>9</em></a>) for a more detailed discussion on this matter]. Moreover, we chose the syllable over meaning-bearing units (morphemes or words), as the latter levels rely more on a language-specific linguistic analysis, on top of various methodological difficulties affecting their robust counting in a cross-linguistic framework (see text S1 and fig. S1 for a discussion on the relation between meaning and information encoding). On the neurocognitive side, the past decade has witnessed an abundance of models and studies that underpin the pivotal role of the syllabic time scale for speech comprehension, especially through the entrainment of cortical oscillations by the speech signal [see (<a id="xref-ref-13-1" href="#ref-13"><em>13</em></a>–<a id="xref-ref-16-1" href="#ref-16"><em>16</em></a>), among others]. These findings led to a view where “the sensitivity to syllable rate [is] arguably the most fundamental property of speech perception and production” (<a id="xref-ref-16-2" href="#ref-16"><em>16</em></a>), a view particularly relevant to our study here.</p><p id="p-8">We studied a sample of 17 languages from 9 language families spread across Europe and Asia, showing a remarkable diversity in terms of linguistic and typological features at all levels, from phonetics and phonology to morphology and syntax and to semantics and pragmatics (see table S1). Focusing on their phonetics and phonology, these languages vary in their number of phonemes (from 25 in Japanese and Spanish to more than 40 in English and Thai), the number of distinct syllables (from a few hundred in Japanese to almost 7000 in English), tonal complexity (from none to six contrastive tones), and various other phonological phenomena (e.g., vowel harmony is present in Finnish, Hungarian, Korean, and Turkish). Thanks to its size and diversity, this sample is adequate to reveal robust trends reflecting phenomena that can potentially be extrapolated to human language in general.</p><p id="p-9">We collected recordings of 170 native adult speakers of the aforementioned 17 languages, each reading at their normal rate a standardized set of 15 semantically similar texts across the languages (for a total amount of approximately 240,000 syllables). Speakers became familiar with the texts, by reading them several times before being recorded, so that they understand the described situation and minimize reading errors (see Materials and Methods below for more details). For each recording, we extracted the duration [in seconds, excluding pauses longer than 150 ms, i.e., longer than typical phonemic silences (<a id="xref-ref-17-1" href="#ref-17"><em>17</em></a>)] and the total number of syllables (NS) of the text’s “canonical” pronunciation. This term refers to the standard pronunciation found in dictionaries and lexical databases (<a id="xref-ref-12-2" href="#ref-12"><em>12</em></a>). For instance, the word “probably” in English will be transcribed as [pɹɑ.bə.bli] and accounted for three syllables, even if some speakers adopted a pronunciation variant such as [pɹɑ.bli]. By adopting this convention, we considered the NS encoded in the signal and potentially retrieved from it. We computed the ratio between the NS and duration, which will be denoted as SR here (rather than the more precise but also less transparent “canonical articulation rate”). Using read speech (as opposed to spontaneous or conversational speech), we constrained the speakers in terms of lexical and syntactic strategies, and we encouraged them to adopt a clear speech pronunciation. Moreover (and very important here), we emphasized the cross-language comparability of the information encoded and retrievable (i.e., the canonical syllables), rather than the various reductions potentially performed by the speakers. Indeed “[s]peakers can produce utterances with more or less articulatory detail or even completely omit certain words, while still conveying the same message” (<a id="xref-ref-18-1" href="#ref-18"><em>18</em></a>). Last, for German, the canonical and realized SRs in a “normal, clearly spoken style” (which is somewhat similar to read style) have been shown to exhibit virtually no difference, even at the phonemic level, according to (<a id="xref-ref-19-1" href="#ref-19"><em>19</em></a>), providing yet another argument for considering <em>SR</em> as relevant here (unfortunately, because of the lack of cross-linguistic robustness of the automatic estimation of realized SR, we could not check this in our cross-linguistic database; see text S2 and fig. S2).</p><p id="p-10">In parallel, from independently available written corpora in these languages, we estimated each language’s information density (<em>ID</em>) as the syllable conditional entropy to take word-internal syllable-bigram dependencies into account. We then computed the average IR by multiplying the ID by the SR for each text read by each speaker in our dataset. Individual SR varies in a ratio of more than one to two, with the slowest speaker hovering around 4.3 syllables/s and the fastest one reaching 9.1 syllables/s on average. <em>ID</em>, computed at the language level, varies in a more limited but still substantial way (from 4.8 bits per syllable for Basque to 8.0 bits per syllable for Vietnamese).</p></div><div id="sec-2"><h2>RESULTS</h2><p id="p-11">As a preliminary analysis, we checked whether our definition of <em>ID</em> provides a relevant measure of linguistic ID, using the syntagmatic density of information ratio (SDIR), defined in (<a id="xref-ref-9-3" href="#ref-9"><em>9</em></a>), as a control. <em>SDIR</em> quantifies the relative informational density of language <em>L</em> compared to a reference language, based on the semantic information expressed in the context of a limited oral corpus (see Materials and Methods below for more details). It thus provides the ground truth on the semantic information conveyed by the sentences in the spoken corpus. Following (<a id="xref-ref-9-4" href="#ref-9"><em>9</em></a>), we used Vietnamese as a reference, such that a language <em>L</em> with a ratio bigger than one (or, respectively, less than one) is denser (respectively, less dense) than Vietnamese in terms of semantic information. By contrast, being estimated from a very large written lexical database, <em>ID</em> subsumes an overall syllable usage disregarding any semantic consideration. The preliminary analysis nevertheless shows that the two information quantification approaches are connected; we obtain, for our data, a very high correlation between <em>ID</em> and <em>SDIR</em> (Pearson’s <em>r</em> = 0.91, <em>P</em> = 3.4 × 10<sup>−7</sup> and Spearman’s ρ = 0.80, <em>P</em> = 0.00011), which suggests that, despite differences in material (heterogeneous and written corpus versus parallel and spoken corpus) and nature (an entropy measured on a large lexicon versus a normalized ratio derived from small texts), our <em>ID</em> is a good estimate of the average amount of information per syllable.</p><p id="p-12">We next attempted to model the <em>SR</em> and <em>IR</em> distributions using linear mixed-effects regression, but we observed heteroscedasticity of the residuals in both cases. Therefore, we decided to use generalized additive models for location, scale, and shape (GAMLSS) (<a id="xref-ref-20-1" href="#ref-20"><em>20</em></a>, <a id="xref-ref-21-1" href="#ref-21"><em>21</em></a>), as they allowed us to model both the mean (μ) and SD (σ) of Gaussian distributions, considering sex as fixed effect and text, language, and speaker as random effects (with a log link function for σ). This resulted in a better fit to the data [as judged by the Akaike information criterion, with AIC (<em>SR</em>, fixed σ) − AIC (<em>SR</em>, modeled σ) = 171.2 and AIC (<em>IR</em>, fixed σ) − AIC (<em>IR</em>, modeled σ) = 167.5], a distribution of residuals very close to normality, and a reduced heteroscedasticity to the point where no additional corrections were necessary.</p><p id="p-13">We found that <em>IR</em> is centered on a mean of 39.15 bits/s with an SD of 5.10 bits/s, while <em>SR</em> is centered on a mean of 6.63 syllables/s, with an SD of 1.15 syllables/s (see <a id="xref-fig-1-1" href="#F1">Fig. 1</a>). The fixed effect of sex is significant for both <em>SR</em> and <em>IR</em>, with females having significantly lower means (<em>SR</em>, −0.17; <em>IR</em>, −1.01) and SDs (<em>SR</em>, −0.06; <em>IR</em>, −0.06); this finding extends previous observations on English (<a id="xref-ref-22-1" href="#ref-22"><em>22</em></a>) to a larger set of languages from different families and geographic areas. In addition, most of the variation in the random effects for both mean and SD is by language [SR, σ<sub>b</sub>(μ) = 0.87 and σ<sub>b</sub>(σ) = 0.15; <em>IR</em>, σ<sub>b</sub>(μ) = 3.10 and σ<sub>b</sub>(σ) = 0.16] and speaker (<em>SR</em>, σ<sub>b</sub>(μ) = 0.57 and σ<sub>b</sub>(σ) = 0.18; <em>IR</em>, σ<sub>b</sub>(μ) = 3.39 and σ<sub>b</sub>(σ) = 0.19]. The model suggests that the relative impact of the two random factors differs, language having the largest impact on <em>SR</em>, while speaker has the largest on <em>IR</em>. In other words, while <em>SR</em> is mainly clustered by language and relatively less so by speaker, the influence of this language-level clustering is reduced for <em>IR</em> [family has a very small effect beyond language, with <em>SR</em> σ<sub>b</sub>(μ) = 0.000019 and σ<sub>b</sub>(σ) = 0.00019 for SR and <em>IR</em> σ<sub>b</sub>(μ) = 0.00023 and σ<sub>b</sub>(σ) = 0.00011 for IR]. The style differences among the 15 texts have a much smaller effect, as revealed by the small variation associated with text for both <em>SR</em> and <em>IR</em> [<em>SR</em>, σ<sub>b</sub>(μ) = 0.11 and σ<sub>b</sub>(σ) = 0.0005; <em>IR</em>, σ<sub>b</sub>(μ) = 0.66 and σ<sub>b</sub>(σ) = 0.00028]. We included the speaker’s age (and its interactions with the other factors) in the models, and we found that while, on the one hand, its effects are as expected (i.e., a negative impact on <em>SR</em>), on the other, its inclusion does not improve the model fit [according to the AIC and BIC (Bayesian information criterion)]. Therefore, we adopt the simpler models without age for the main analysis reported here but the models including it are available in the analysis report file S1.</p><figure id="F1">
  <div>
    <p><a href="https://advances.sciencemag.org/content/advances/5/9/eaaw2594/F1.large.jpg?width=800&amp;height=600&amp;carousel=1" title="SR and IR. The distribution of SR (in syllables per second) (left) and IR (in bits per second) (right) within the languages in our database (colored areas; colors represent the language families) and across them (black areas at the top) using a Gaussian kernel density estimate. The black vertical lines spanning the whole plot represent the means (solid lines) ± 1 SD (dashed lines). The short black vertical lines represent the actual data points." rel="gallery-fragment-images-1376276825" data-figure-caption="<div class=&quot;highwire-markup&quot;><span class=&quot;fig-label&quot;>Fig. 1</span> <span class=&quot;caption-title&quot;>SR and IR.</span><p id=&quot;p-14&quot; class=&quot;first-child&quot;>The distribution of SR (in syllables per second) (left) and IR (in bits per second) (right) within the languages in our database (colored areas; colors represent the language families) and across them (black areas at the top) using a Gaussian kernel density estimate. The black vertical lines spanning the whole plot represent the means (solid lines) ± 1 SD (dashed lines). The short black vertical lines represent the actual data points.</p><div class=&quot;sb-div caption-clear&quot;/></div>" data-icon-position="" data-hide-link-title="0"><span><img aria-describedby="F1-caption" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="https://advances.sciencemag.org/content/advances/5/9/eaaw2594/F1.medium.gif"></span></a></p>        
      </div>
    <figcaption id="F1-caption">
    <span>Fig. 1</span> <span>SR and IR.</span><p id="p-14">The distribution of SR (in syllables per second) (left) and IR (in bits per second) (right) within the languages in our database (colored areas; colors represent the language families) and across them (black areas at the top) using a Gaussian kernel density estimate. The black vertical lines spanning the whole plot represent the means (solid lines) ± 1 SD (dashed lines). The short black vertical lines represent the actual data points.</p>  </figcaption>
  </figure><p id="p-15">To explore the relationship between <em>SR</em> and <em>ID</em>, we included <em>ID</em> as a fixed effect in the GAMLSS modeling of <em>SR</em> (here, we dropped language as a random effect, since there is, by definition, a single <em>ID</em> value per language, but we did include family): We found a significant negative effect of <em>ID</em> not only on the mean of <em>SR</em> (β = −0.89, <em>P</em> &lt; 2.2 × 10<sup>−16</sup>) but also on its SD (β = −0.09, <em>P</em> = 6.4 × 10<sup>−7</sup>). This negative relationship (see <a id="xref-fig-2-1" href="#F2">Fig. 2</a>)—between two parameters derived from independent written and oral corpora—indicates that there is a trade-off between <em>SR</em> and <em>ID</em>, the languages with lower IDs being spoken faster, as also illustrated by “classic” correlation estimates (Pearson’s <em>r</em> = −0.71 and Spearman’s ρ = −0.70, in both cases with <em>P</em> &lt; 2.2 × 10<sup>−16</sup>).</p><figure id="F2">
  <div>
    <p><a href="https://advances.sciencemag.org/content/advances/5/9/eaaw2594/F2.large.jpg?width=800&amp;height=600&amp;carousel=1" title="Relationship between SR and ID across languages Colors represent the language families, and individual languages are identified by the labels on top (to avoid overlapping labels, short black lines might show their actual position). While there is only one value of ID per language, there are as many values of SR per language as texts read by individual speakers. The straight yellow line represents the linear regression [with 95% confidence interval (CI)], and the black curve represents the locally estimated scatterplot smoothing regression (with 95% CI) of SR on ID." rel="gallery-fragment-images-1376276825" data-figure-caption="<div class=&quot;highwire-markup&quot;><span class=&quot;fig-label&quot;>Fig. 2</span> <span class=&quot;caption-title&quot;>Relationship between SR and ID across languages</span><p id=&quot;p-16&quot; class=&quot;first-child&quot;>Colors represent the language families, and individual languages are identified by the labels on top (to avoid overlapping labels, short black lines might show their actual position). While there is only one value of <em>ID</em> per language, there are as many values of SR per language as texts read by individual speakers. The straight yellow line represents the linear regression [with 95% confidence interval (CI)], and the black curve represents the locally estimated scatterplot smoothing regression (with 95% CI) of SR on <em>ID</em>.</p><div class=&quot;sb-div caption-clear&quot;/></div>" data-icon-position="" data-hide-link-title="0"><span><img aria-describedby="F2-caption" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="https://advances.sciencemag.org/content/advances/5/9/eaaw2594/F2.medium.gif"></span></a></p>        
      </div>
    <figcaption id="F2-caption">
    <span>Fig. 2</span> <span>Relationship between SR and ID across languages</span><p id="p-16">Colors represent the language families, and individual languages are identified by the labels on top (to avoid overlapping labels, short black lines might show their actual position). While there is only one value of <em>ID</em> per language, there are as many values of SR per language as texts read by individual speakers. The straight yellow line represents the linear regression [with 95% confidence interval (CI)], and the black curve represents the locally estimated scatterplot smoothing regression (with 95% CI) of SR on <em>ID</em>.</p>  </figcaption>
  </figure><p id="p-17">The visual inspection of the distributions of <em>SR</em> and <em>IR</em> (<a id="xref-fig-1-2" href="#F1">Fig. 1</a>, black areas) suggests that <em>IR</em> and <em>SR</em> differ in terms of the compactness of their overall distribution and that languages are more similar in terms of <em>IR</em> than <em>SR</em>. To assess this difference, we computed several pairwise divergence metrics between languages (Kolmogorov-Smirnov, Kullback-Leibler, Jensen-Shannon, Hellinger, and chi-square divergences; <a id="xref-fig-3-1" href="#F3">Fig. 3</a> and analysis report file S1) to quantify their dispersion in the distribution of number of syllables per text (<em>NS</em>), <em>SR</em>, and <em>IR</em>. <em>NS</em> is considered here as a proxy for information dilution, since the texts are semantically similar across the languages. Using randomization paired <em>t</em> tests (with 1000 permutations), we found that, for all measures, languages are significantly more similar to each other in <em>IR</em> than in <em>NS</em> and <em>SR</em> (all randomization <em>P</em> &lt; 10<sup>−4</sup>). Last, <em>IR</em>s are less dispersed around their mean than <em>SR</em>, as shown by their coefficients of variation (17.3% for <em>SR</em> versus 13.0% for <em>IR</em>) and also by several unimodality tests with permutation (see analysis report file S1).</p><figure id="F3">
  <div>
    <p><a href="https://advances.sciencemag.org/content/advances/5/9/eaaw2594/F3.large.jpg?width=800&amp;height=600&amp;carousel=1" title="Pairwise divergence between languages. The distribution of the Jensen-Shannon divergence between pairs of languages for the NS, SR, and IR, also showing the significant differences using a randomization paired t test (1000 permutations). The IR-SR and IR-NS P values are " rel="gallery-fragment-images-1376276825" data-figure-caption="<div class=&quot;highwire-markup&quot;><span class=&quot;fig-label&quot;>Fig. 3</span> <span class=&quot;caption-title&quot;>Pairwise divergence between languages.</span><p id=&quot;p-18&quot; class=&quot;first-child&quot;>The distribution of the Jensen-Shannon divergence between pairs of languages for the NS, SR, and IR, also showing the significant differences using a randomization paired <em>t</em> test (1000 permutations). The <em>IR</em>-<em>SR</em> and <em>IR</em>-<em>NS</em> <em>P</em> values are <10<sup>−4</sup>, while the <em>NS</em>-<em>SR</em> <em>P</em> value is 0.30. All other divergence measures produce essentially identical results. n.s., not significant. ****<em>P</em> ≤ 0.0001.</p><div class=&quot;sb-div caption-clear&quot;/></div>" data-icon-position="" data-hide-link-title="0"><span><img aria-describedby="F3-caption" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="https://advances.sciencemag.org/content/advances/5/9/eaaw2594/F3.medium.gif"></span></a></p>        
      </div>
    <figcaption id="F3-caption">
    <span>Fig. 3</span> <span>Pairwise divergence between languages.</span><p id="p-18">The distribution of the Jensen-Shannon divergence between pairs of languages for the NS, SR, and IR, also showing the significant differences using a randomization paired <em>t</em> test (1000 permutations). The <em>IR</em>-<em>SR</em> and <em>IR</em>-<em>NS</em> <em>P</em> values are &lt;10<sup>−4</sup>, while the <em>NS</em>-<em>SR</em> <em>P</em> value is 0.30. All other divergence measures produce essentially identical results. n.s., not significant. ****<em>P</em> ≤ 0.0001.</p>  </figcaption>
  </figure></div><div id="sec-3"><h2>DISCUSSION</h2><p id="p-19">In this study, we investigated the relationship between ID (estimated from written data) and SR (computed from parallel spoken data) across 17 languages. By recording read parallel data rather than more casual or spontaneous speech, we deliberately increased the comparability across languages and speakers and constrained the degrees of freedom available to each speaker [in line with (<a id="xref-ref-23-1" href="#ref-23"><em>23</em></a>)]. Having the same texts read by all speakers controls for one of the main effects reported in (<a id="xref-ref-24-1" href="#ref-24"><em>24</em></a>), namely the fact that, for a given language, “fast speakers are likely to produce less informative content.” Therefore, while this corpus is not appropriate for studying pragmatic and cognitive planning, it does allow robust results in what concerns the differences across languages and speakers given a controlled linguistic content. In addition, it does not require any preliminary data curation that could raise methodological concerns and potentially induce biases.</p><p id="p-20">We argue that our results, based on a controlled linguistic material and consisting of read speech, do reflect actual phenomena found in more natural settings. As such, we found that the effects of text in the <em>SR</em> and <em>IR</em> models are much smaller than those of speaker and language, despite the stylistic differences among the 15 texts (some corresponding to phone information request scenarios, e.g., P0, while others are narratives, such as P8). This aspect rules out the existence of text-related systematic bias across languages and suggests that when talking at a normal rate, each speaker’s average SR is quite robust to variation in linguistic content (lexical frequency, syntactic structure, phrase length, etc.). This is still fully compatible with the local changes in SR that have been extensively demonstrated, in a few languages at least [see (<a id="xref-ref-18-2" href="#ref-18"><em>18</em></a>, <a id="xref-ref-24-2" href="#ref-24"><em>24</em></a>) among many others]. The limited effect of text also suggests that the results reported here should hold for interactions involving styles similar to the normal, clearly spoken style in Koreman’s terminology (<a id="xref-ref-19-2" href="#ref-19"><em>19</em></a>). In other words, as long as the communication situation requires a correct decoding of the linguistic information encoded by the speaker, we suggest that the trade-off presented above will be observed. We can expect that its strength would gradually decrease along a continuum ranging from very carefully pronounced content to very informal interactions where understanding is heavily reliant on contextual and pragmatic factors rather than on the linguistic information itself.</p><p id="p-21">Together, our findings show that while there is wide interspeaker variation in speech and IRs, this variation is also structured by language. This means that an individual’s speech behavior is not entirely due to individual characteristics but is further constrained by the language being spoken. The effect of sex we found here is analogous to its effect in other phenomena, such as, for example, body height or the fundamental voice frequency. While both have universally constrained ranges in humans (<a id="xref-ref-25-1" href="#ref-25"><em>25</em></a>, <a id="xref-ref-26-1" href="#ref-26"><em>26</em></a>) and result from complex interactions between genetics and environment (<a id="xref-ref-27-1" href="#ref-27"><em>27</em></a>, <a id="xref-ref-28-1" href="#ref-28"><em>28</em></a>), they differ between languages/groups (<a id="xref-ref-26-2" href="#ref-26"><em>26</em></a>, <a id="xref-ref-29-1" href="#ref-29"><em>29</em></a>) and sexes (<a id="xref-ref-25-2" href="#ref-25"><em>25</em></a>, <a id="xref-ref-26-3" href="#ref-26"><em>26</em></a>). Of relevance here, the statistically significant difference between males and females does not preclude universal tendencies or between-group patterns of variation (<a id="xref-ref-30-1" href="#ref-30"><em>30</em></a>).</p><p id="p-22">However, languages seem to stably inhabit an optimal range of IRs, away from the extremes that can still be available to individual speakers. Languages achieve this balance through a trade-off between ID and SR, resulting in a narrower distribution of IRs compared to SRs. In the introduction, we rhetorically asked whether too low or too high an IR would impede communicative and/or cognitive efficiency. Our results here suggest that the answer to both questions is positive and that human communication seems to avoid two extreme sociolinguistic profiles: on the one hand, high <em>ID</em> languages spoken fast by their speakers (“high-fast”), and, on the other, low <em>ID</em> languages spoken slowly by their speakers (“low-slow”). Both speakers and listeners have an interest in avoiding high-fast languages: For the speaker, production comes at higher costs both in terms of articulation (more complex and infrequent, less routinized syllables) and planning (since they are less predictable from the context), while for the listener, the resulting speech flow may exceed channel capacity or at least be challenging in terms of lexical access and syntactic parsing. Avoidance of high-fast languages may thus result from a convergence of production- and perception-oriented pressures, with similar factors being suggested in (<a id="xref-ref-24-3" href="#ref-24"><em>24</em></a>) to explain that in American English corpora of conversational speech, fast speakers produced less informative content (both in terms of content words and syntactic structure). On the other hand, low-slow languages, if they existed, would present a twofold challenge: First, in terms of general communicative efficiency, they would lead to longer turns in interaction [in human interactions, turn duration is 2 s on average (<a id="xref-ref-1-2" href="#ref-1"><em>1</em></a>)]. A second—and probably related—factor is that they would require their speakers to keep longer chunks in working memory, for a given informational content. One can thus hypothesize that speakers from this language would swiftly accelerate their articulation rate to compensate for their language’s low <em>ID</em>.</p><p id="p-23">This study provides the most extensive estimation of spoken IR to date, whether in terms of numbers of speakers, languages, or language families. Such an IR centered on 39 bits/s (with an SD of about 5 bits/s) is certainly compatible with the rare estimates available for English, Mandarin Chinese, and Spanish (<a id="xref-ref-31-1" href="#ref-31"><em>31</em></a>, <a id="xref-ref-32-1" href="#ref-32"><em>32</em></a>). The most notable result is that languages are much closer in terms of IR than in SR. Despite the across-language dispersion observed for ID and SR, their regulatory interaction seems to give rise to a universal attractor. This result is far from trivial, especially considering that the substantial freedom speakers have to depart from their average SR without any apparent effort (<a id="xref-ref-33-1" href="#ref-33"><em>33</em></a>). Despite this essential capacity enabling each speaker to adapt to specific situations of communication, a convergence is observed, and the deviation from a flat distribution shown here could be explained by a soft constraint toward an average IR of around 39 bits/s.</p><p id="p-24">Metaphorically, our data suggest that languages tend to inhabit a valley of possible IRs with gradual slopes, which allow some speakers to occupy peripheral positions farther away from the attractor in both directions. We suggest that this valley in a fitness landscape illustrates the concept of “good-enough” control proposed as an alternative to optimal control for biological systems (<a id="xref-ref-34-1" href="#ref-34"><em>34</em></a>) and that its existence is due to functional and cognitive factors. Several recent proposals highlight that the neural capacity to track speech dynamics through cortical oscillations is crucial for speech processing and understanding, especially in the so-called θ range (<a id="xref-ref-13-2" href="#ref-13"><em>13</em></a>), for which an optimal speech-brain alignment would be essential for syllabic sampling (<a id="xref-ref-14-1" href="#ref-14"><em>14</em></a>). This line of research led not only to an estimation of an auditory channel capacity of about 9 syllables/s for American English participants (<a id="xref-ref-13-3" href="#ref-13"><em>13</em></a>) but also to a much more restricted “optimal” range around 4.5 syllables/s (<a id="xref-ref-16-3" href="#ref-16"><em>16</em></a>). This notion of optimality is still to be refined, and it is not yet established whether the auditory channel capacity is a matter of information or of acoustic duration: These experiments manipulate SR within a given language (generally English), and cross-linguistic assessments will be necessary to estimate whether the boundaries of the θ range depend on each individual’s mother tongue and to revisit the notion of optimal rate. More specifically, we show here that between-language differences in IR are much smaller than those in SRs. Consequently, given that all humans are fully cognitively equipped regardless of their mother language, IR provides a better candidate than SR for investigating invariance in cognitive capacity. This line of investigation can shed new light on the long-lasting difficulty faced in attempting to detect temporal regularities and predictability in the acoustic signal. The apparent discrepancy observed between the arguable existence of linguistic rhythmic classes and the lack of temporal regularities is beyond the scope of this paper, but interested readers can refer to (<a id="xref-ref-35-1" href="#ref-35"><em>35</em></a>) for a recent cross-linguistic approach that found limited evidence for temporal predictability.</p><p id="p-25">We suggest that this cross-linguistic tendency stems from the interaction between social and neurocognitive pressures that define an optimal range for IR, around which the complex adaptive system (consisting of each language and its speakers) hovers. While ID is mainly a property derived from the language itself (its grammar, lexicon, and long-term usage), SR reflects individual speakers’ behavior instantiating language-level norms and constraints generated in their own biocognitive apparatus. The interplay of this long-term collective property with this individual short-term behavior, we propose, leads all individuals to continuously monitor (consciously or not) and adapt their SR to the specific linguistic and communicative context. A prediction is that when a community implements linguistic changes that may cause the IR to drift away from the optimal range, compensatory mechanisms that affect SR (e.g., coarticulation) may bring the average IR back toward optimal regions.</p><p id="p-26">Speakers are obviously not limited to manipulating the phonological level of their languages to achieve an efficient communication, and an obvious extension of this study will be to take grammatical information into account. This can be achieved by considering a longer context when estimating the syllable ID (thus, going beyond the previous syllable within the same word) or by moving to units longer than the syllable (such as words). Doing this will, however, force one to depart from the “mere encoding” considered here and get closer to higher levels of language-specific strategies, generating new challenges in terms of (semantic and pragmatic) information quantification [see (<a id="xref-ref-36-1" href="#ref-36"><em>36</em></a>) for a discussion from the angle of overt and hidden linguistic complexity], challenges that require different methods and data and that we leave for future research.</p><p id="p-27">To conclude from a broad evolutionary perspective, we thus see human language as inhabiting a biocultural niche spanning two scales. At a local scale, each system consisting of a given language and its speakers represents one instantiation of a cultural niche construction process (<a id="xref-ref-5-2" href="#ref-5"><em>5</em></a>, <a id="xref-ref-37-1" href="#ref-37"><em>37</em></a>) in a specific context involving the ecological (<a id="xref-ref-38-1" href="#ref-38"><em>38</em></a>), biological (<a id="xref-ref-39-1" href="#ref-39"><em>39</em></a>), social (<a id="xref-ref-40-1" href="#ref-40"><em>40</em></a>), and cultural (<a id="xref-ref-41-1" href="#ref-41"><em>41</em></a>) environments. At a global scale, all of these language speakers’ local systems are subjected to universal communicative pressures characterizing the human-specific communication niche and consequently fulfilling universal functions of communication essential for the human species.</p></div><div id="sec-4"><h2>MATERIALS AND METHODS</h2><div id="sec-5"><h3>Data and code availability</h3><p id="p-28">The data are contained in two <kbd>TAB</kbd>-separated <kbd>CSV</kbd> files, the <kbd>R</kbd> code for the analysis and plotting is contained in an <kbd>RMarkdown</kbd> script, and all the results and plots (obtained by compiling this <kbd>RMarkdown</kbd> script) are in an <kbd>HTML</kbd> analysis report; all these files are freely available under an open-source license in the Supplementary Materials and in the GitHub repository <a href="https://github.com/keruiduo/SupplMatInfoRate">https://github.com/keruiduo/SupplMatInfoRate</a>.</p></div><div id="sec-6"><h3>Data</h3><p id="p-29">We analyzed here the data from 17 languages from 9 language families, listed below as family [language name (ISO 639-3 code)]: Austroasiatic [Vietnamese (VIE)], Basque [Basque (EUS)], Indo-European [Catalan (CAT), German (DEU), English (ENG), French (FRA), Italian (ITA), Spanish (SPA), and Serbian (SRP)], Japanese [Japanese (JPN)], Korean [Korean (KOR)], Sino-Tibetan [Mandarin Chinese (CMN) and Yue Chinese/Cantonese (YUE)], Tai-Kadai [Thai (THA)], Turkic [Turkish (TUR)], and Uralic [Finnish (FIN) and Hungarian (HUN)]. For each language, we collected existing or new oral and text corpora and performed analyses considering syllable as the reference unit. Syllable is both regarded as “a unit in the organization of the sounds of an utterance” (<a id="xref-ref-42-1" href="#ref-42"><em>42</em></a>) and as a salient unit for cognitive processing by psycholinguists and neuroscientists. We opted for the syllabic level to focus on the direct mapping between semantic information and speech signal (seen as a sequence of syllabic “bricks”), bypassing the mental lexicon, which is highly dependent on the language morphological characteristics. The linguistic implications go beyond the scope of this paper and are not further discussed.</p><p id="p-30">The oral corpus (see text S3 for details) is initially derived from the MULTEXT (Multilingual Text Tools and Corpora, ID: ELRA-S0060) parallel corpus (<a id="xref-ref-43-1" href="#ref-43"><em>43</em></a>). For British English, German, and Italian, we considered 15 short texts from this corpus, all composed of three to five semantically connected sentences carefully translated by a native speaker from the British English original. For the other 14 languages, two of the authors (C.C. or Y.O.) supervised the translation and recording of new datasets by native speakers of the target language, preferably of a specific variety whenever possible (e.g., Mandarin spoken in Beijing, Serbian in Belgrade, and Korean in Seoul). No strict control of age or other sociolinguistic variables was imposed, but speakers (170 in total, 85 females) were mainly students or members of academic institutions. This data collection complied with ethical regulations at the Université de Lyon, and given its nature, it did not require a formal approval by an Ethics Committee. After providing informed consent, speakers were asked to read each text first silently once and then aloud at least two times, allowing familiarization and reducing reading errors. The <kbd>ROCme!</kbd> software (<a id="xref-ref-44-1" href="#ref-44"><em>44</em></a>) was used for presentation of the experiment instructions and texts, as well as for recordings. The texts were presented one by one on the screen in random order, one sentence at a time following a self-paced reading paradigm, with the second or the third aloud recording being analyzed here. Thus, in total, we have 2265 data points (i.e., each data point consists of a text <em>t</em> read by a speaker <em>s</em> in language <em>L</em>; some speakers did not read all texts). For each such data point, we measured the total speech duration (<em>D</em>; in seconds) and the total NS of the text’s canonical transcription (denoted NS). Pauses longer than 150 ms were identified and discarded through visual inspection of the waveforms and spectrograms. We computed the SR for text <em>t</em> in language <em>L</em> read by speaker <em>s</em> (<span id="inline-formula-1"><span><span><span id="MJXp-Span-1"><span id="MJXp-Span-2"><span id="MJXp-Span-3"><span id="MJXp-Span-4">S</span><span id="MJXp-Span-5"><span id="MJXp-Span-6">R</span><span><span><span><span><span id="MJXp-Span-11">L</span></span></span></span><span><span><span><span id="MJXp-Span-7"><span id="MJXp-Span-8">t</span><span id="MJXp-Span-9">,</span><span id="MJXp-Span-10">s</span></span></span></span></span></span></span></span></span></span></span><span id="MathJax-Element-1-Frame" tabindex="0"><nobr><span id="MathJax-Span-1"><span><span><span id="MathJax-Span-2"><span id="MathJax-Span-3" definitionurl="" encoding=""><span id="MathJax-Span-4"><span id="MathJax-Span-5">S</span><span id="MathJax-Span-6"><span><span><span id="MathJax-Span-7">R</span><span></span></span><span><span id="MathJax-Span-8">L</span><span></span></span><span><span id="MathJax-Span-9"><span id="MathJax-Span-10">t</span><span id="MathJax-Span-11">,</span><span id="MathJax-Span-12">s</span></span><span></span></span></span></span></span></span></span></span></span></span></nobr></span></span></span>) as<span id="disp-formula-1"><span><span><span id="MJXp-Span-12"><span id="MJXp-Span-13"><span id="MJXp-Span-14"><span id="MJXp-Span-15"><span id="MJXp-Span-16">SR</span><span><span><span><span><span id="MJXp-Span-21">L</span></span></span></span><span><span><span><span id="MJXp-Span-17"><span id="MJXp-Span-18">t</span><span id="MJXp-Span-19">,</span><span id="MJXp-Span-20">s</span></span></span></span></span></span></span><span id="MJXp-Span-22">=</span><span id="MJXp-Span-23"><span><span id="MJXp-Span-24"><span id="MJXp-Span-25">NS</span><span><span><span><span><span id="MJXp-Span-27">L</span></span></span></span><span><span><span><span id="MJXp-Span-26">t</span></span></span></span></span></span></span><span><span><span><span></span></span><span><span><span id="MJXp-Span-28"><span id="MJXp-Span-29">D</span><span><span><span><span><span id="MJXp-Span-34">L</span></span></span></span><span><span><span><span id="MJXp-Span-30"><span id="MJXp-Span-31">t</span><span id="MJXp-Span-32">,</span><span id="MJXp-Span-33">s</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span><p><span id="MathJax-Element-2-Frame" tabindex="0"><nobr><span id="MathJax-Span-13"><span><span><span id="MathJax-Span-14"><span id="MathJax-Span-15" definitionurl="" encoding=""><span id="MathJax-Span-16"><span id="MathJax-Span-17"><span><span><span id="MathJax-Span-18">SR</span><span></span></span><span><span id="MathJax-Span-19">L</span><span></span></span><span><span id="MathJax-Span-20"><span id="MathJax-Span-21">t</span><span id="MathJax-Span-22">,</span><span id="MathJax-Span-23">s</span></span><span></span></span></span></span><span id="MathJax-Span-24">=</span><span id="MathJax-Span-25"><span><span><span id="MathJax-Span-26"><span><span><span id="MathJax-Span-27">NS</span><span></span></span><span><span id="MathJax-Span-28">L</span><span></span></span><span><span id="MathJax-Span-29">t</span><span></span></span></span></span><span></span></span><span><span id="MathJax-Span-30"><span><span><span id="MathJax-Span-31">D</span><span></span></span><span><span id="MathJax-Span-32">L</span><span></span></span><span><span id="MathJax-Span-33"><span id="MathJax-Span-34">t</span><span id="MathJax-Span-35">,</span><span id="MathJax-Span-36">s</span></span><span></span></span></span></span><span></span></span><span><span></span><span></span></span></span></span></span></span></span></span></span></span></nobr></span></p></span><span>(1)</span></span>and the syntagmatic density of information ratio [SDIR<em><sup>L</sup></em>; defined in (<a id="xref-ref-9-5" href="#ref-9"><em>9</em></a>)] for language <em>L</em> as<span id="disp-formula-2"><span><span><span id="MJXp-Span-35"><span id="MJXp-Span-36"><span id="MJXp-Span-37"><span id="MJXp-Span-38"><span id="MJXp-Span-39">SDIR</span><span id="MJXp-Span-40">L</span></span><span id="MJXp-Span-41">=</span><span id="MJXp-Span-42"><span><span id="MJXp-Span-43">1</span></span><span><span><span><span></span></span><span><span><span id="MJXp-Span-44"><span id="MJXp-Span-45">N</span><span id="MJXp-Span-46">T</span></span></span></span></span></span></span><span id="MJXp-Span-47"><span id="MJXp-Span-48"><span><span><span><span id="MJXp-Span-54"><span id="MJXp-Span-55">N</span><span id="MJXp-Span-56">T</span></span></span><span><span id="MJXp-Span-49"><span>∑</span></span></span></span></span><span><span id="MJXp-Span-50"><span id="MJXp-Span-51">t</span><span id="MJXp-Span-52">=</span><span id="MJXp-Span-53">1</span></span></span></span></span><span id="MJXp-Span-57"><span><span id="MJXp-Span-58"><span id="MJXp-Span-59">NS</span><span><span><span><span><span id="MJXp-Span-61">VIE</span></span></span></span><span><span><span><span id="MJXp-Span-60">t</span></span></span></span></span></span></span><span><span><span><span></span></span><span><span><span id="MJXp-Span-62"><span id="MJXp-Span-63">NS</span><span><span><span><span><span id="MJXp-Span-65">L</span></span></span></span><span><span><span><span id="MJXp-Span-64">t</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span><p><span id="MathJax-Element-3-Frame" tabindex="0"><nobr><span id="MathJax-Span-37"><span><span><span id="MathJax-Span-38"><span id="MathJax-Span-39" definitionurl="" encoding=""><span id="MathJax-Span-40"><span id="MathJax-Span-41"><span><span><span id="MathJax-Span-42">SDIR</span><span></span></span><span><span id="MathJax-Span-43">L</span><span></span></span></span></span><span id="MathJax-Span-44">=</span><span id="MathJax-Span-45"><span><span><span id="MathJax-Span-46">1</span><span></span></span><span><span id="MathJax-Span-47"><span><span><span id="MathJax-Span-48">N<span></span></span><span></span></span><span><span id="MathJax-Span-49">T<span></span></span><span></span></span></span></span><span></span></span><span><span></span><span></span></span></span></span><span id="MathJax-Span-50"><span id="MathJax-Span-51"><span id="MathJax-Span-52"><span><span><span id="MathJax-Span-53">∑</span><span></span></span><span><span id="MathJax-Span-54"><span id="MathJax-Span-55">t</span><span id="MathJax-Span-56">=</span><span id="MathJax-Span-57">1</span></span><span></span></span><span><span id="MathJax-Span-58"><span><span><span id="MathJax-Span-59">N<span></span></span><span></span></span><span><span id="MathJax-Span-60">T<span></span></span><span></span></span></span></span><span></span></span></span></span></span></span><span id="MathJax-Span-61"><span><span><span id="MathJax-Span-62"><span><span><span id="MathJax-Span-63">NS</span><span></span></span><span><span id="MathJax-Span-64">VIE</span><span></span></span><span><span id="MathJax-Span-65">t</span><span></span></span></span></span><span></span></span><span><span id="MathJax-Span-66"><span><span><span id="MathJax-Span-67">NS</span><span></span></span><span><span id="MathJax-Span-68">L</span><span></span></span><span><span id="MathJax-Span-69">t</span><span></span></span></span></span><span></span></span><span><span></span><span></span></span></span></span></span></span></span></span></span></span></nobr></span></p></span><span>(2)</span></span>with <em>N<sub>T</sub></em> = 15 (number of distinct texts in the oral corpus). It can be seen that, by definition, an <em>SDIR<sup>L</sup></em> &gt;1 represents a language <em>L</em> denser than Vietnamese in terms of semantic information (since it requires less syllables than Vietnamese to encode a similar semantic content), while an <em>SDIR<sup>L</sup></em> &lt;1 represents a language <em>L</em> less dense than Vietnamese.</p><p id="p-31">The text corpora were acquired from various sources containing large amounts of written text (see table S2 for details). After an initial data curation, each corpus was phonetically transcribed and automatically syllabified using a rule-based program written by one of the authors (Y.O.), except (i) when syllabification was already provided with the dataset (for English, French, German, and Vietnamese for the multisyllabic words) and (ii) when the corpus was syllabified by an automatic grapheme-to-phoneme converter (for Catalan, Spanish, and Thai). In addition, no syllabification was required for Sino-Tibetan languages (Cantonese and Mandarin Chinese) since each ideogram corresponds to a single syllable. When applicable, syllables bearing specific tone or accent were considered as distinct in the inventory. For more information on the data and its processing, see (<a id="xref-ref-45-1" href="#ref-45"><em>45</em></a>).</p><p id="p-32">For each language <em>L</em>, we computed information-theoretical metrics derived from Shannon’s seminal theory to estimate the average amount of information transmitted per syllable. More precisely, we estimated the first- and second-order entropies of the syllable distribution. The first-order entropy is the standard Shannon entropy (ShE)<span id="disp-formula-3"><span><span><span id="MJXp-Span-66"><span id="MJXp-Span-67"><span id="MJXp-Span-68"><span id="MJXp-Span-69">ShE</span><span id="MJXp-Span-70">=</span><span id="MJXp-Span-71">−</span><span id="MJXp-Span-72"><span id="MJXp-Span-73"><span><span id="MJXp-Span-74"><span>∑</span></span></span><span><span id="MJXp-Span-75">x</span></span></span></span><span id="MJXp-Span-76">p</span><span id="MJXp-Span-77">(</span><span id="MJXp-Span-78">x</span><span id="MJXp-Span-79">)</span><span id="MJXp-Span-80">.</span><span id="MJXp-Span-81"><span id="MJXp-Span-82">log</span><span id="MJXp-Span-83">2</span></span><span id="MJXp-Span-84">(</span><span id="MJXp-Span-85">p</span><span id="MJXp-Span-86">(</span><span id="MJXp-Span-87">x</span><span id="MJXp-Span-88">)</span><span id="MJXp-Span-89">)</span></span></span></span></span><p><span id="MathJax-Element-4-Frame" tabindex="0"><nobr><span id="MathJax-Span-70"><span><span><span id="MathJax-Span-71"><span id="MathJax-Span-72" definitionurl="" encoding=""><span id="MathJax-Span-73"><span id="MathJax-Span-74">ShE</span><span id="MathJax-Span-75">=</span><span id="MathJax-Span-76">−</span><span id="MathJax-Span-77"><span id="MathJax-Span-78"><span id="MathJax-Span-79"><span><span><span id="MathJax-Span-80">∑</span><span></span></span><span><span id="MathJax-Span-81">x</span><span></span></span></span></span></span></span><span id="MathJax-Span-82">p</span><span id="MathJax-Span-83">(</span><span id="MathJax-Span-84">x</span><span id="MathJax-Span-85">)</span><span id="MathJax-Span-86">.</span><span id="MathJax-Span-87"><span><span><span id="MathJax-Span-88">log</span><span></span></span><span><span id="MathJax-Span-89">2</span><span></span></span></span></span><span id="MathJax-Span-90">(</span><span id="MathJax-Span-91">p</span><span id="MathJax-Span-92">(</span><span id="MathJax-Span-93">x</span><span id="MathJax-Span-94">)</span><span id="MathJax-Span-95">)</span></span></span></span></span></span></span></nobr></span></p></span><span>(3)</span></span>where <em>p</em>(<em>x</em>) is the maximum likelihood estimates of the syllable unigram probabilities observed in the corpus.</p><p id="p-33">The second-order entropy is the main information index used here, and it is thus denoted by <em>ID</em>. It refers to conditional entropy where the context in which each syllable occurs is taken into account. We characterized this context as the identity of the previous syllable or a null marker for syllables occurring word initially (thus, no bigrams span across word boundaries)<span id="disp-formula-4"><span><span><span id="MJXp-Span-90"><span id="MJXp-Span-91"><span id="MJXp-Span-92"><span id="MJXp-Span-93">ID</span><span id="MJXp-Span-94">=</span><span id="MJXp-Span-95">−</span><span id="MJXp-Span-96"><span id="MJXp-Span-97"><span><span id="MJXp-Span-98"><span>∑</span></span></span><span><span id="MJXp-Span-99"><span id="MJXp-Span-100">x</span><span id="MJXp-Span-101">,</span><span id="MJXp-Span-102">y</span></span></span></span></span><span id="MJXp-Span-103">p</span><span id="MJXp-Span-104">(</span><span id="MJXp-Span-105">x</span><span id="MJXp-Span-106">,</span><span id="MJXp-Span-107">y</span><span id="MJXp-Span-108">)</span><span id="MJXp-Span-109">.</span><span id="MJXp-Span-110"><span id="MJXp-Span-111">log</span><span id="MJXp-Span-112">2</span></span><span id="MJXp-Span-113"><span>(</span></span><span id="MJXp-Span-114"><span><span id="MJXp-Span-115"><span id="MJXp-Span-116">p</span><span id="MJXp-Span-117">(</span><span id="MJXp-Span-118">x</span><span id="MJXp-Span-119">,</span><span id="MJXp-Span-120">y</span><span id="MJXp-Span-121">)</span></span></span><span><span><span><span></span></span><span><span><span id="MJXp-Span-122"><span id="MJXp-Span-123">p</span><span id="MJXp-Span-124">(</span><span id="MJXp-Span-125">x</span><span id="MJXp-Span-126">)</span></span></span></span></span></span></span><span id="MJXp-Span-127"><span>)</span></span></span></span></span></span></span><span>(4)</span></span>where <em>p</em>(<em>x</em>, <em>y</em>) is the maximum likelihood estimates of the syllable bigram probabilities observed in the corpus.</p><p id="p-34">The numerical difference between the first- and second-order entropies differs among languages, being larger for languages where across-syllable binding is tighter because of morphology. Given that, for several languages, the text corpora only provide word frequencies (and not raw texts), we considered within-word context only. In future studies, a broader and across-word context could be considered so that we can refine the entropy estimations, but the very strong correlation observed between <em>ID</em> and the syntagmatic density of semantic information previously used in (<a id="xref-ref-9-6" href="#ref-9"><em>9</em></a>) suggests that the second-order entropy is a relevant proxy of ID.</p><p id="p-35">Last, for each data point (i.e., one text read by one speaker), we computed the Shannon IR (ShIR) and conditional IR (for short, IR) as<span id="disp-formula-5"><span><span><span id="MJXp-Span-128"><span id="MJXp-Span-129"><span id="MJXp-Span-130"><span id="MJXp-Span-131">ShIR</span><span id="MJXp-Span-132">=</span><span id="MJXp-Span-133">ShE</span><span id="MJXp-Span-134">⋅</span><span id="MJXp-Span-135">SR</span></span></span></span></span></span><span>(5)</span></span><span id="disp-formula-6"><span><span><span id="MJXp-Span-136"><span id="MJXp-Span-137"><span id="MJXp-Span-138"><span id="MJXp-Span-139">IR</span><span id="MJXp-Span-140">=</span><span id="MJXp-Span-141">ID</span><span id="MJXp-Span-142">⋅</span><span id="MJXp-Span-143">SR</span></span></span></span></span></span><span>(6)</span></span></p><p id="p-36">ShE and ShIR offer approximations of upper boundaries for each language since they do not take any context into account, despite the large amount of redundancy and dependency induced by morphology in human languages. For this reason, we reported here only the results from conditional entropy <em>ID</em> and conditional IR, since they provide much better estimations of the actual information transmitted during speech communication.</p></div><div id="sec-7"><h3>Statistical analysis</h3><p id="p-37">We describe below the various statistical analyses we performed, each under a dedicated subheading.</p></div><div id="sec-8"><h3>Intraclass correlations for text, language, and speaker</h3><p id="p-38">A priori, we expected that productions (respectively) of the same text, in the same language, or by the same speaker are not independent, which means that we should model them as random effects (<a id="xref-ref-46-1" href="#ref-46"><em>46</em></a>). As a preliminary analysis, we first estimated how much of the variation is explained by each of these factors (i.e., how similar their productions are) using linear mixed models (LMMs; as implemented by <kbd>R</kbd>’s <kbd>lmer()</kbd> function in the <kbd>lme4</kbd> package) to compute the linear intraclass correlations for the random effects text (representing the 15 short texts that were read by the speakers), language (representing the 17 languages) embedded within family (there are 9 language families, each language belonging to a unique family), and speaker (representing the 170 speakers), separately for the dependent variables NS, SR, and IR. For SR and IR, we used the AIC and the BIC to select the best fitting LMM, which, in both cases, included sex as a fixed effect and all three random effects; using <kbd>R</kbd>’s <kbd>lmer</kbd> notation:</p><p id="p-39"><kbd>lmer(SR ~ 1 + Sex + (1 | Text) + (1 | Family/Language) + (1 | Speaker))</kbd></p><p id="p-40"><kbd>lmer(IR ~ 1 + Sex + (1 | Text) + (1 | Family/Language) + (1 | Speaker))</kbd></p></div><div id="sec-9"><h3>Generalized additive models for location, scale, and shape</h3><p id="p-41">While the fit obtained using LMMs is relatively good, the inspection of the diagnostic plots revealed slightly but potentially relevant deviations from the assumptions of this class of models (<a id="xref-ref-46-2" href="#ref-46"><em>46</em></a>), prompting us to use GAMLSS (<a id="xref-ref-20-2" href="#ref-20"><em>20</em></a>), as implemented by <kbd>R</kbd>’s <kbd>gamlss()</kbd> function in package <kbd>gamlss</kbd>. This class of models is much more flexible than LMMs, allowing us to model not only the mean (location) but also the variance (scale) and the shape of the distribution, and, while relatively recent, it has already been successfully applied to problems in the language sciences (<a id="xref-ref-21-2" href="#ref-21"><em>21</em></a>).</p><p id="p-42">More precisely, to preserve simplicity and interpretability, we compared Gaussian distributions with fixed versus modeled SD σ (both model the mean μ). To select among alternative models, we used AIC. For both SR and IR, modeling SD (with all three random effects and sex as fixed effect) results in a better fit to the data. For these models, the link function of the mean is the identity, but for the SD, it is the natural logarithm to ensure that the predicted values are always positive. In <kbd>gamlss</kbd> notation, the models are as follows:</p><p id="p-43"><kbd>gamlss(formula = SR ~ 1 + Sex + random(Text) + random(Language) + random(Family) + random(Speaker), sigma.formula = ~1 + Sex + random(Text) + random(Language) + random(Family) + random(Speaker), family = NO)</kbd></p><p id="p-44"><kbd>gamlss(formula = IR ~ 1 + Sex + random(Text) + random(Language) + random(Family) + random(Speaker), sigma.formula = ~1 + Sex + random(Text) + random(Family) + random(Language) + random(Speaker), family = NO)</kbd></p><p id="p-45">We also modeled the relationship between <em>SR</em> and <em>ID</em> using <kbd>gamlss</kbd> as follows (here, language is meaningless as a random effect as there is only one <em>ID</em> value per language, but family is still potentially meaningful):</p><p id="p-46"><kbd>gamlss(formula = SR ~ 1 + ID + Sex + random(Text) + random(Speaker)</kbd> + <kbd>random(Family), sigma.formula = ~1 + ID + Sex + random(Text) + random(Speaker) + random(Family), family = NO).</kbd></p></div><div id="sec-10"><h3>Unimodality</h3><p id="p-47">Although <em>SR</em> and <em>IR</em> distributions are composite, the differences induced by the underlying groups (sex, language, text, and speaker) are compatible with overall unimodal distributions (<a id="xref-ref-30-2" href="#ref-30"><em>30</em></a>). To judge this unimodality, we also used two quantitative approaches, in addition to the visual inspection of the histograms. In the first approach, we fitted Gaussian mixtures to the actual distributions, assessing between one and five Gaussian components, using the <kbd>gamlssMX()</kbd> function in the <kbd>gamlss.mx</kbd> package [for example, for <em>SR</em> with one component: <kbd>gamlssMX(formula = SR ~ 1, family = NO, K = 1)</kbd>], and we used AIC to select the best fitting mixtures. However, despite being simple, modeling our distributions with a mixture of Gaussians might not sufficiently capture how “unimodal” these distributions are because the model might need more than one component to fit, for example, a leptokurtic distribution. Mixtures of distributions other than Gaussian could be considered here, but we lack relevant arguments for choosing one distribution over another.</p><p id="p-48">The second approach uses three unimodality tests: the Silverman test, the dip test, and the bimodality coefficient (BC). The Silverman test (<a id="xref-ref-47-1" href="#ref-47"><em>47</em></a>) tests the null hypothesis that an underlying density has at most <em>k</em> modes; its null hypothesis is that the underlying density has at most <em>k</em> modes (H0: number of modes ≤ <em>k</em>), and the result is the bootstrapped <em>P</em> value for rejecting a unimodal distribution (our implementation is based on the code available at <a href="https://www.mathematik.uni-marburg.de/~stochastik/R_packages">www.mathematik.uni-marburg.de/~stochastik/R_packages</a>). The dip test computes Hartigans’ dip statistic D (<a id="xref-ref-48-1" href="#ref-48"><em>48</em></a>) and the associated (interpolated) <em>P</em> value for rejecting a unimodal distribution (as implemented in the <kbd>R</kbd> package <kbd>diptest</kbd>). The BC is based on an empirical relationship between bimodality and the third and fourth statistical moments of a distribution (skewness and kurtosis) and is proportional to the division of squared skewness by uncorrected kurtosis. The underlying logic is that a bimodal distribution will have very low kurtosis, an asymmetric character, or both, all of which increase BC, with values exceeding 0.555 (the value representing a uniform distribution) suggesting bimodality. We implemented it as <kbd>BC = (s2 + 1)/(k + 3 · ((n − 1)2 / ((n − 2) · (n − 3))))</kbd>, following (<a id="xref-ref-49-1" href="#ref-49"><em>49</em></a>). Unfortunately, these tests tend to disagree (see analysis report file S1), and the problem of unimodality testing is far from settled (<a id="xref-ref-49-2" href="#ref-49"><em>49</em></a>). For all SR and IR and for each of these tests, we performed four randomization procedures to obtain an estimate of the “specialness” of the observed unimodality estimate: permutation model 1 (PM1), randomly permute the <em>SR</em> values freely among speakers, texts, and languages; PM2, randomly permute the <em>ID</em> values among languages; PM3, randomly permute the speaker average <em>SR</em> values among speakers (irrespective of language); and PM4, randomly permute the language average <em>SR</em> values among languages. Each of these procedures results in a distribution of expected test (or <em>P</em>) values that can be compared with the observed estimates actually obtained.</p></div><div id="sec-11"><h3>Pairwise distances between languages</h3><p id="p-49"><em>NS</em>, <em>SR</em>, and <em>IR</em> each have a distribution of values within a given language; thus, differences in the distribution of any of these three variables can be computed for any language pair. For each of the three variables, we computed all the possible 17 · (17 − 1)/2 = 136 differences between the pairs of languages, and we compared these distributions using paired permutation <em>t</em> tests (with 1000 permutations). We used five methods for computing these differences [all implemented by <kbd>R</kbd>’s function <kbd>distance()</kbd> in package <kbd>philentropy</kbd>]: Hellinger distance, Jensen-Shannon divergence, Kolmogorov-Smirnov distance, Kullback-Leibler divergence, and chi-square divergence, and we found that all strongly agree.</p></div></div><div id="app-1"><h2>SUPPLEMENTARY MATERIALS</h2><div id="DC1"><p id="p-50">Supplementary material for this article is available at <a href="http://advances.sciencemag.org/cgi/content/full/5/9/eaaw2594/DC1">http://advances.sciencemag.org/cgi/content/full/5/9/eaaw2594/DC1</a></p><p id="p-51">Text S1. Relationship between semantic and encoding levels of information.</p><p id="p-52">Text S2. Relationship between canonical SR and automatically estimated SR.</p><p id="p-53">Text S3. Multilingual parallel corpus.</p><p id="p-54">Fig. S1. Two different encoding strategies that convey the same semantic information.</p><p id="p-55">Fig. S2. Automatic versus canonical SRs per language.</p><p id="p-56">Table S1. The 17 languages used in this study.</p><p id="p-57">Table S2. The written corpora.</p><p id="p-58">Data file S1. Raw data (as a <kbd>TAB</kbd>-separated CSV file) used for the analyses.</p><p id="p-59">Data file S2. Raw data (as a <kbd>TAB</kbd>-separated CSV file) used for the analyses.</p><p id="p-60">Analysis report file S1. Full analysis report and details about the data in <kbd>HTML</kbd> format.</p><p id="p-61">Analysis script file S1. The <kbd>RMarkdown</kbd> script continuing all the R code needed to reproduce the results and plots reported here.</p><p id="p-62">References (<a id="xref-ref-50-1" href="#ref-50"><em>50</em></a>–<a id="xref-ref-59-1" href="#ref-59"><em>59</em></a>)</p></div></div><p id="p-2">This is an open-access article distributed under the terms of the <a href="http://creativecommons.org/licenses/by-nc/4.0/" rel="license">Creative Commons Attribution-NonCommercial license</a>, which permits use, distribution, and reproduction in any medium, so long as the resultant use is <strong>not</strong> for commercial advantage and provided the original work is properly cited.</p><div id="ref-list-1"><h2>REFERENCES AND NOTES</h2><ol><li><a href="#xref-ref-1-1" title="View reference 1 in text" id="ref-1">↵</a></li><li><a href="#xref-ref-2-1" title="View reference 2 in text" id="ref-2">↵</a></li><li><a href="#xref-ref-3-1" title="View reference 3 in text" id="ref-3">↵</a></li><li><a href="#xref-ref-4-1" title="View reference 4 in text" id="ref-4">↵</a></li><li><a href="#xref-ref-5-1" title="View reference 5 in text" id="ref-5">↵</a><div id="cit-5.9.eaaw2594.5"><p>J. Odling-Smee, K. N. Laland, Cultural niche construction: Evolution’s cradle of language, in <em>The Prehistory of Language</em> (Oxford Univ. Press, 2009), pp. 99–121.</p></div></li><li><a href="#xref-ref-6-1" title="View reference 6 in text" id="ref-6">↵</a></li><li><a href="#xref-ref-7-1" title="View reference 7 in text" id="ref-7">↵</a></li><li><a href="#xref-ref-8-1" title="View reference 8 in text" id="ref-8">↵</a><div id="cit-5.9.eaaw2594.8"><p>A. Fenk, G. Fenk-Oczlon, Menzerath’s law and the constant flow of linguistic information, in <em>Contributions to Quantitative Linguistics: Proceedings of the First International Conference on Quantitative Linguistics, QUALICO, Trier, 1991</em>, R. Köhler, B. B. Rieger, Eds. (Springer, 1993), pp. 11–31.</p></div></li><li><a href="#xref-ref-9-1" title="View reference 9 in text" id="ref-9">↵</a></li><li><a href="#xref-ref-10-1" title="View reference 10 in text" id="ref-10">↵</a></li><li><a href="#xref-ref-11-1" title="View reference 11 in text" id="ref-11">↵</a></li><li><a href="#xref-ref-12-1" title="View reference 12 in text" id="ref-12">↵</a></li><li><a href="#xref-ref-13-1" title="View reference 13 in text" id="ref-13">↵</a></li><li><a href="#xref-ref-14-1" title="View reference 14 in text" id="ref-14">↵</a></li><li></li><li><a href="#xref-ref-16-1" title="View reference 16 in text" id="ref-16">↵</a></li><li><a href="#xref-ref-17-1" title="View reference 17 in text" id="ref-17">↵</a></li><li><a href="#xref-ref-18-1" title="View reference 18 in text" id="ref-18">↵</a><div id="cit-5.9.eaaw2594.18"><p>T. F. Jaeger, E. Buz, Signal reduction and linguistic encoding, in <em>The Handbook of Psycholinguistics</em>, E. M. Fernández, H. S. Cairns, Eds. (John Wiley &amp; Sons Ltd., 2017), pp. 38–81.</p></div></li><li><a href="#xref-ref-19-1" title="View reference 19 in text" id="ref-19">↵</a></li><li><a href="#xref-ref-20-1" title="View reference 20 in text" id="ref-20">↵</a><div id="cit-5.9.eaaw2594.20"><p>M. D. Stasinopoulos, R. A. Rigby, G. Z. Heller, V. Voudouris, F. De Bastiani, <em>Flexible Regression and Smoothing: Using GAMLSS in R</em> (CRC Press, 2017).</p></div></li><li><a href="#xref-ref-21-1" title="View reference 21 in text" id="ref-21">↵</a></li><li><a href="#xref-ref-22-1" title="View reference 22 in text" id="ref-22">↵</a></li><li><a href="#xref-ref-23-1" title="View reference 23 in text" id="ref-23">↵</a></li><li><a href="#xref-ref-24-1" title="View reference 24 in text" id="ref-24">↵</a></li><li><a href="#xref-ref-25-1" title="View reference 25 in text" id="ref-25">↵</a></li><li><a href="#xref-ref-26-1" title="View reference 26 in text" id="ref-26">↵</a></li><li><a href="#xref-ref-27-1" title="View reference 27 in text" id="ref-27">↵</a></li><li><a href="#xref-ref-28-1" title="View reference 28 in text" id="ref-28">↵</a></li><li><a href="#xref-ref-29-1" title="View reference 29 in text" id="ref-29">↵</a></li><li><a href="#xref-ref-30-1" title="View reference 30 in text" id="ref-30">↵</a></li><li><a href="#xref-ref-31-1" title="View reference 31 in text" id="ref-31">↵</a></li><li><a href="#xref-ref-32-1" title="View reference 32 in text" id="ref-32">↵</a><div id="cit-5.9.eaaw2594.32"><p>J. Villasenor, Y. Han, D. Wen, E. Gonzales, J. Chen, J. Wen, The information rate of modern speech and its implications for language evolution, in <em>Evolution Of Language, The - Proceedings Of The 9th International Conference (Evolang9)</em>, T. Scott-Phillips, M. Tamariz, E. A. Cartmill, J. R. Hurford, Eds. (World Scientific, 2012), pp. 376–383.</p></div></li><li><a href="#xref-ref-33-1" title="View reference 33 in text" id="ref-33">↵</a></li><li><a href="#xref-ref-34-1" title="View reference 34 in text" id="ref-34">↵</a></li><li><a href="#xref-ref-35-1" title="View reference 35 in text" id="ref-35">↵</a></li><li><a href="#xref-ref-36-1" title="View reference 36 in text" id="ref-36">↵</a></li><li><a href="#xref-ref-37-1" title="View reference 37 in text" id="ref-37">↵</a></li><li><a href="#xref-ref-38-1" title="View reference 38 in text" id="ref-38">↵</a></li><li><a href="#xref-ref-39-1" title="View reference 39 in text" id="ref-39">↵</a></li><li><a href="#xref-ref-40-1" title="View reference 40 in text" id="ref-40">↵</a></li><li><a href="#xref-ref-41-1" title="View reference 41 in text" id="ref-41">↵</a><div id="cit-5.9.eaaw2594.41"><p>D. Dediu, M. Cysouw, S. C. Levinson, A. Baronchelli, M. H. Christiansen, W. A. Croft, N. J. Evans, S. Garrod, R. D. Gray, A. Kandler, E. Leiven, Cultural evolution of language, in <em>Cultural Evolution: Society, Technology, Language, and Religion</em>, P. J. Richerson, M. H. Christiansen, Eds. (MIT Press, 2013), vol. 12, pp. 303–332.</p></div></li><li><a href="#xref-ref-42-1" title="View reference 42 in text" id="ref-42">↵</a></li><li><a href="#xref-ref-43-1" title="View reference 43 in text" id="ref-43">↵</a><div id="cit-5.9.eaaw2594.43"><p>E. Campione, J. Véronis, A statistical study of pitch target points in five languages, in <em>Fifth International Conference on Spoken Language Processing</em> (International Speech Communicaton Association, 1998), pp. 3163–3166.</p></div></li><li><a href="#xref-ref-44-1" title="View reference 44 in text" id="ref-44">↵</a><div id="cit-5.9.eaaw2594.44"><p>E. Ferragne, S. Flavier, C. Fressard, in <em>Proceedings of 14th Interspeech Conference</em> (International Speech Communicaton Association, 2013), pp. 1864–1865.</p></div></li><li><a href="#xref-ref-45-1" title="View reference 45 in text" id="ref-45">↵</a><div id="cit-5.9.eaaw2594.45"><p>Y. M. Oh, thesis, Université de Lyon, France (2015).</p></div></li><li><a href="#xref-ref-46-1" title="View reference 46 in text" id="ref-46">↵</a><div id="cit-5.9.eaaw2594.46"><p>A. Gelman, J. Hill, <em>Data Analysis Using Regression and Multilevel/Hierarchical Models</em> (Cambridge Univ. Press, ed. 1, 2006).</p></div></li><li><a href="#xref-ref-47-1" title="View reference 47 in text" id="ref-47">↵</a></li><li><a href="#xref-ref-48-1" title="View reference 48 in text" id="ref-48">↵</a></li><li><a href="#xref-ref-49-1" title="View reference 49 in text" id="ref-49">↵</a></li><li><a href="#xref-ref-50-1" title="View reference 50 in text" id="ref-50">↵</a></li><li></li><li><div id="cit-5.9.eaaw2594.52"><p>K. Maekawa, H. Kikuchi, Corpus-based analysis of vowel devoicing in spontaneous Japanese: an interim report, in <em>Voicing in Japanese</em>, J. van de Weijer, K. Nanjo, T. Nishihara, Eds. (De Gruyter Mouton, 2005), pp. 205–228.</p></div></li><li><div id="cit-5.9.eaaw2594.53"><p>I. Maddieson, S. Flavier, E. Marsico, C. Coupé, F. Pellegrino, LAPSyd: Lyon-Albuquerque phonological systems database, in <em>Proceedings of 14th Interspeech Conference</em> (International Speech Communicaton Association, 2013), pp. 3022–3026.</p></div></li><li></li><li></li><li></li><li><div id="cit-5.9.eaaw2594.57"><p>V. Lyding, E. Stemle, C. Borghetti, M. Brunello, S. Castagnoli, F. Dell’Orletta, H. Dittmann, A. Lenci, V. Pirrelli, The PAISÁ corpus of Italian web texts, in <em>Proceedings of the 9th Web as Corpus Workshop (WaC-9)</em> (Association for Computational Linguistics, 2014), pp. 36–43.</p></div></li><li></li><li><a href="#xref-ref-59-1" title="View reference 59 in text" id="ref-59">↵</a><div id="cit-5.9.eaaw2594.59"><p>V.-B. Le, D.-D. Tran, E. Castelli, L. Besacier, J.-F. Serignat, Spoken and written language resources for Vietnamese, in <em>Proceedings of LREC4</em> (European Language Resources Association, 2004), pp. 599–602.</p></div></li></ol></div><p><strong>Acknowledgments: </strong>We thank D. E. Blasi for suggestions and feedback on the statistical analysis and on previous versions of this paper. We also thank E. Castelli for help with collecting the Vietnamese data. <strong>Funding:</strong> D.D. was funded by a European Institutes for Advanced Study (EURIAS) Fellowship 2017–2018 and by an IDEXLYON (16-IDEX-0005) Fellowship grant (2018–2021). C.C., Y.M.O., and F.P. were funded by LABEX ASLAN (ANR-10-LABX-0081) of Université de Lyon within the French program Investissements d’Avenir program (ANR-11-IDEX-0007) operated by the National Research Agency (ANR). <strong>Author contributions:</strong> C.C., Y.O., and F.P. jointly conceived the study. C.C. and Y.O. collected the data. C.C., Y.O., D.D., and F.P. analyzed the data, produced and discussed the results, and wrote the manuscript. <strong>Competing interests:</strong> The authors declare that they have no competing interests. <strong>Data and materials availability:</strong> All data needed to evaluate the conclusions in the paper are present in the paper and/or the Supplementary Materials. Additional data related to this paper may be requested from the authors. The primary data are also available in the dedicated GitHub repository <a href="https://github.com/keruiduo/SupplMatInfoRate">https://github.com/keruiduo/SupplMatInfoRate</a>.</p><ul><li id="copyright-statement-1">Copyright © 2019 The Authors, some rights reserved; exclusive licensee American Association for the Advancement of Science. No claim to original U.S. Government Works. Distributed under a Creative Commons Attribution NonCommercial License 4.0 (CC BY-NC).</li></ul></div></div></div>
    </div>
    <footer>
        <div>created by <a href="https://buildstarted.com">buildstarted</a> &copy; 2020 <a href="/about">about</a></div>
        <div>Share this page on social media: copy and paste this url https://linksfor.dev/</div>
        <div>If you prefer RSS: <a href="https://linksfor.dev/feed.xml">https://linksfor.dev/feed.xml</a></div>
        <div>Customer satisfaction guaranteed to be optional.</div>
    </footer>
    
    <script async defer>
        _dna = window._dna || {};
        _dna.siteId = "linksfor.devs";
        _dna.outlink = true;

        (function() {
            let dna = document.createElement('script');
            dna.type = 'text/javascript';
            dna.async = true;
            dna.src = '//dna.buildstarted.com/t.js';
            let s = document.getElementsByTagName('script')[0];
            s.parentNode.insertBefore(dna, s);
        })();
    </script>
    <noscript><img src="//dna.buildstarted.com/g?siteId=linksfor.devs"/></noscript>
</body>
</html>