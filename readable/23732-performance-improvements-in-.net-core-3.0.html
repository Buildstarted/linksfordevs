<!DOCTYPE html>
<html lang="en">
<head>
    <title>
Performance Improvements in .NET Core 3.0 -
linksfor.dev(s)
    </title>
    <link rel="alternate" type="application/rss+xml" title="Linksfor.dev(s) feed" href="https://linksfor.dev/feed.rss" />
    <meta charset="utf-8">
    <meta name="Description" content="A curated source of links that devs might find interesting. Updated around the clock." />
    <meta name="google" value="notranslate">
    <link rel="stylesheet" type="text/css" href="/style.css" />
    <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon">
    <link rel="icon" href="/favicon.ico" type="image/x-icon">
</head>
<body>
    <div class="grid">
        
<div class="readable">
    <div id="readOverlay" class="style-ebook"><div id="readInner" class="margin-medium size-medium"><h1>Performance Improvements in .NET Core 3.0</h1><div><div class="entry-content col-12 sharepostcontent"><div class="row justify-content-center"><div class="col-md-4"><div><img src="https://secure.gravatar.com/avatar/49e67eaea0533a547f3489aa03707bbb?s=58&amp;d=mm&amp;r=g" width="58" height="58" alt="Avatar" class="avatar avatar-58 wp-user-avatar wp-user-avatar-58 photo avatar-default"></div></div></div><p>May 15th, 2019</p><p>Back when we were getting ready to ship .NET Core 2.0, I wrote a&nbsp;<a href="https://blogs.msdn.microsoft.com/dotnet/2017/06/07/performance-improvements-in-net-core/">blog post</a>&nbsp;exploring some of the many performance improvements that had gone into it. I enjoyed putting it together so much and received such a positive response to the post that I did it&nbsp;<a href="https://devblogs.microsoft.com/dotnet/performance-improvements-in-net-core-2-1">again for .NET Core 2.1</a>, a version for which performance was also a significant focus. With <a href="https://www.microsoft.com/en-us/build">//build</a> last week and <a href="https://dotnet.microsoft.com/download/dotnet-core/3.0">.NET Core 3.0</a>‘s release now on the horizon, I’m thrilled to have an opportunity to do it again.</p><p>.NET Core 3.0 has a ton to offer, from Windows Forms and WPF, to single-file executables, to async enumerables, to platform intrinsics, to HTTP/2, to fast JSON reading and writing, to assembly unloadability, to enhanced cryptography, and on and on and on… there is a wealth of new functionality to get excited about. For me, however, performance is the primary feature that makes me excited to go to work in the morning, and there’s a staggering amount of performance goodness in .NET Core 3.0.</p><p>In this post, we’ll take a tour through some of the many improvements, big and small, that have gone into the .NET Core runtime and core libraries in order to make your apps and services leaner and faster.</p><h3>Setup</h3><p><a href="http://github.com/dotnet/benchmarkdotnet">Benchmark.NET</a>&nbsp;has become the preeminent tool for doing benchmarking of .NET libraries, and so as I did in my 2.1 post, I’ll use Benchmark.NET to demonstrate the improvements. Throughout the post, I’ll include the individual snippets of benchmarks that highlight the particular improvement being discussed. To be able to execute those benchmarks, you can use the following setup:</p><ol><li>Ensure you have&nbsp;<a href="https://dotnet.microsoft.com/download/dotnet-core/3.0" rel="nofollow">.NET Core 3.0</a>&nbsp;installed, as well as&nbsp;<a href="https://dotnet.microsoft.com/download/dotnet-core/2.1" rel="nofollow">.NET Core 2.1</a>&nbsp;for comparison purposes.</li><li>Create a directory named&nbsp;<code>BlogPostBenchmarks</code>.</li><li>In that directory, run&nbsp;<code>dotnet new console</code>.</li><li>Replace the contents of BlogPostBenchmarks.csproj with the following:<pre class="tab-convert:true lang:xhtml decode:true " title="BlogPostBenchmarks.csproj">&lt;Project Sdk="Microsoft.NET.Sdk"&gt;

  &lt;PropertyGroup&gt;
    &lt;OutputType&gt;Exe&lt;/OutputType&gt;
    &lt;AllowUnsafeBlocks&gt;true&lt;/AllowUnsafeBlocks&gt;
    &lt;TargetFrameworks&gt;netcoreapp2.1;netcoreapp3.0&lt;/TargetFrameworks&gt;
  &lt;/PropertyGroup&gt;

  &lt;ItemGroup&gt;
    &lt;PackageReference Include="BenchmarkDotNet" Version="0.11.5" /&gt;
    &lt;PackageReference Include="System.Drawing.Common" Version="4.5.0" /&gt;
    &lt;PackageReference Include="System.IO.Pipelines" Version="4.5.0" /&gt;
    &lt;PackageReference Include="System.Threading.Channels" Version="4.5.0" /&gt;
  &lt;/ItemGroup&gt;

&lt;/Project&gt;</pre></li><li>Replace the contents of Program.cs with the following:<pre class="wrap:false lang:c# decode:true">using BenchmarkDotNet.Attributes;
using BenchmarkDotNet.Configs;
using BenchmarkDotNet.Jobs;
using BenchmarkDotNet.Running;
using BenchmarkDotNet.Toolchains.CsProj;
using Microsoft.Win32.SafeHandles;
using System;
using System.Buffers;
using System.Collections;
using System.Collections.Concurrent;
using System.Collections.Generic;
using System.Collections.Immutable;
using System.Diagnostics;
using System.Drawing;
using System.Drawing.Drawing2D;
using System.Globalization;
using System.IO;
using System.IO.Compression;
using System.IO.Pipelines;
using System.Linq;
using System.Net;
using System.Net.Http;
using System.Net.NetworkInformation;
using System.Net.Security;
using System.Net.Sockets;
using System.Runtime.CompilerServices;
using System.Runtime.InteropServices;
using System.Security.Authentication;
using System.Security.Cryptography.X509Certificates;
using System.Text;
using System.Text.RegularExpressions;
using System.Threading;
using System.Threading.Channels;
using System.Threading.Tasks;
using System.Xml;

[MemoryDiagnoser]
public class Program
{
    static void Main(string[] args) =&gt; BenchmarkSwitcher.FromTypes(new[] { typeof(Program) }).Run(args);

    // ... paste benchmark code here
}</pre></li></ol><p>To execute a particular benchmark, unless otherwise noted, copy and paste the relevant code to replace the&nbsp;<code>// ...</code>above, and execute&nbsp;<code>dotnet run -c Release -f netcoreapp2.1 --runtimes netcoreapp2.1 netcoreapp3.0 --filter "*Program*"</code>. This will compile and run the tests in release builds, on both .NET Core 2.1 and .NET Core 3.0, and print out the results for comparison in a table.</p><h3>Caveats</h3><p>A few caveats before we get started:</p><ol><li>Any discussion involving microbenchmark results deserves a caveat that measurements can and do vary from machine to machine. I’ve tried to pick stable examples to share (and have run these tests on multiple machines in multiple configurations to help validate that), but don’t be too surprised if your numbers differ from the ones I’ve shown; hopefully, however, the magnitude of the improvements demonstrated carries through. All of the shown results are from a nightly Preview 6 build for .NET Core 3.0. Here’s my configuration, as summarized by Benchmark.NET, on my Windows configuration and on my Linux configuration:<pre class="wrap:false lang:default decode:true">BenchmarkDotNet=v0.11.5, OS=Windows 10.0.17763.437 (1809/October2018Update/Redstone5)
Intel Core i7-7660U CPU 2.50GHz (Kaby Lake), 1 CPU, 4 logical and 2 physical cores
.NET Core SDK=3.0.100-preview6-011854
  [Host]     : .NET Core 2.1.9 (CoreCLR 4.6.27414.06, CoreFX 4.6.27415.01), 64bit RyuJIT
  Job-RODBZD : .NET Core 2.1.9 (CoreCLR 4.6.27414.06, CoreFX 4.6.27415.01), 64bit RyuJIT
  Job-TVOWAH : .NET Core 3.0.0-preview6-27712-03 (CoreCLR 3.0.19.26071, CoreFX 4.700.19.26005), 64bit RyuJIT

BenchmarkDotNet=v0.11.5, OS=ubuntu 18.04
Intel Xeon CPU E5-2673 v4 2.30GHz, 1 CPU, 4 logical and 2 physical cores
.NET Core SDK=3.0.100-preview6-011877
  [Host]     : .NET Core 2.1.10 (CoreCLR 4.6.27514.02, CoreFX 4.6.27514.02), 64bit RyuJIT
  Job-SSHMNT : .NET Core 2.1.10 (CoreCLR 4.6.27514.02, CoreFX 4.6.27514.02), 64bit RyuJIT
  Job-CHXNFO : .NET Core 3.0.0-preview6-27713-12 (CoreCLR 3.0.19.26071, CoreFX 4.700.19.26307), 64bit RyuJIT</pre></li><li>Unless otherwise mentioned, benchmarks were executed on Windows. In many cases, performance is equivalent between Windows and Unix, but in others, there can be non-trivial discrepancies between them, in particular in places where .NET relies on OS functionality, and the OS itself has different performance characteristics.</li><li>I mentioned posts on .NET Core 2.0 and .NET Core 2.1, but I didn’t mention .NET Core 2.2. .NET Core 2.2 was primarily focused on ASP.NET, and while there were terrific performance improvements at the ASP.NET layer in 2.2, the release was primarily focused on servicing for the runtime and core libraries, with most improvements post-2.1 skipping 2.2 and going into 3.0.</li></ol><p>With that out of the way, let’s have some fun.</p><h3>Span and Friends</h3><p>One of the more notable features introduced in .NET Core 2.1 was&nbsp;<code>Span&lt;T&gt;</code>, along with its friends&nbsp;<code>ReadOnlySpan&lt;T&gt;</code>,&nbsp;<code>Memory&lt;T&gt;</code>, and&nbsp;<code>ReadOnlyMemory&lt;T&gt;</code>. The introduction of these new types came with hundreds of new methods for interacting with them, some on new types and some with overloaded functionality on existing types, as well as optimizations in the just-in-time compiler (JIT) for making working with them very efficient. The release also included some internal usage of&nbsp;<code>Span&lt;T&gt;</code>&nbsp;to make existing operations leaner and faster while still enjoying maintainable and safe code. In .NET Core 3.0, much additional work has gone into further improving all such aspects of these types: making the runtime better at generating code for them, increasing the use of them internally to help improve many other operations, and improving the various library utilities that interact with them to make consumption of these operations faster.</p><p>To work with a span, one first needs to get a span, and several PRs have made doing so faster. In particular, passing around a&nbsp;<code>Memory&lt;T&gt;</code>&nbsp;and then getting a&nbsp;<code>Span&lt;T&gt;</code>&nbsp;from it is a very common way of creating a span; this is, for example, how the various&nbsp;<code>Stream.WriteAsync</code>&nbsp;and&nbsp;<code>ReadAsync</code>&nbsp;methods work, accepting a&nbsp;<code>{ReadOnly}Memory&lt;T&gt;</code>&nbsp;(so that it can be stored on the heap) and then accessing its&nbsp;<code>Span</code>&nbsp;property once the actual bytes need to be read or written. PR&nbsp;<a href="https://github.com/dotnet/coreclr/pull/20771">dotnet/coreclr#20771</a>&nbsp;improved this by removing an argument validation branch (both for&nbsp;<code>{ReadOnly}Memory&lt;T&gt;.Span</code>&nbsp;and for&nbsp;<code>{ReadOnly}Span&lt;T&gt;.Slice</code>), and while removing a branch is a small thing, in span-heavy code (such as when doing formatting and parsing), small things done over and over and over again add up. More impactful, PR&nbsp;<a href="https://github.com/dotnet/coreclr/pull/20386">dotnet/coreclr#20386</a>&nbsp;plays tricks at the runtime level to safely eliminate some of the runtime checked casting and bit masking logic that had been used to enable&nbsp;<code>{ReadOnly}Memory&lt;T&gt;</code>&nbsp;to wrap various types, like&nbsp;<code>string</code>,&nbsp;<code>T[]</code>, and&nbsp;<code>MemoryManager&lt;T&gt;</code>, providing a seamless veneer over all of them. The net result of these PRs is a nice speed-up when fishing a&nbsp;<code>Span&lt;T&gt;</code>&nbsp;out of a&nbsp;<code>Memory&lt;T&gt;</code>, which in turn improves all other operations that do so.</p><pre class="lang:c# decode:true">private ReadOnlyMemory&lt;byte&gt; _mem = new byte[1];

[Benchmark]
public ReadOnlySpan&lt;byte&gt; GetSpan() =&gt; _mem.Span;</pre><table border="1"><thead><tr><th>Method</th><th>Toolchain</th><th align="right">Mean</th><th align="right">Error</th><th align="right">StdDev</th><th align="right">Ratio</th></tr></thead><tbody><tr><td>GetSpan</td><td>netcoreapp2.1</td><td align="right">3.873 ns</td><td align="right">0.0927 ns</td><td align="right">0.0822 ns</td><td align="right">1.00</td></tr><tr><td>GetSpan</td><td>netcoreapp3.0</td><td align="right">1.843 ns</td><td align="right">0.0401 ns</td><td align="right">0.0375 ns</td><td align="right">0.48</td></tr></tbody></table><p>Of course, once you get a span, you want to use it, and there are a myriad of ways to use one, many of which have also been optimized further in .NET Core 3.0.</p><p>For example, just as with arrays, to pass the data from a span to native code via a P/Invoke, the data needs to be pinned (unless it’s already immovable, such as if the span were created to wrap some natively allocated memory not on the GC heap or if it were created for some data on the stack). To pin a span, the easiest way is to simply rely on the C# language’s support added in C# 7.3 that supports a pattern-based way to use any type with the&nbsp;<code>fixed</code>&nbsp;keyword. All a type need do is expose a&nbsp;<code>GetPinnableReference</code>&nbsp;method (or extension method) that returns a&nbsp;<code>ref T</code>&nbsp;to the data stored in that instance, and that type can be used with&nbsp;<code>fixed</code>.&nbsp;<code>{ReadOnly}Span&lt;T&gt;</code>&nbsp;does exactly this. However, even though&nbsp;<code>{ReadOnly}Span&lt;T&gt;.GetPinnableReference</code>&nbsp;generally gets inlined, a call it makes internally to&nbsp;<code>Unsafe.AsRef</code>&nbsp;was getting blocked from inlining; PR&nbsp;<a href="https://github.com/dotnet/coreclr/pull/18274">dotnet/coreclr#18274</a>&nbsp;fixed this, enabling the whole operation to be inlined. Further, the aforementioned code was actually tweaked in PR&nbsp;<a href="https://github.com/dotnet/coreclr/pull/20428">dotnet/coreclr#20428</a>&nbsp;to eliminate one branch on the hot path. Both of these combine to result in a measurable boost when pinning a span:</p><pre class="lang:c# decode:true">private readonly byte[] _bytes = new byte[10_000];

[Benchmark(OperationsPerInvoke = 10_000)]
public unsafe int PinSpan()
{
    Span&lt;byte&gt; s = _bytes;
    int total = 0;

    for (int i = 0; i &lt; s.Length; i++)
        fixed (byte* p = s) // equivalent to `fixed (byte* p = &amp;s.GetPinnableReference())`
            total += *p;

    return total;
}</pre><table border="1"><thead><tr><th>Method</th><th>Toolchain</th><th align="right">Mean</th><th align="right">Error</th><th align="right">StdDev</th><th align="right">Ratio</th><th align="right">RatioSD</th></tr></thead><tbody><tr><td>PinSpan</td><td>netcoreapp2.1</td><td align="right">0.7930 ns</td><td align="right">0.0177 ns</td><td align="right">0.0189 ns</td><td align="right">1.00</td><td align="right">0.00</td></tr><tr><td>PinSpan</td><td>netcoreapp3.0</td><td align="right">0.6496 ns</td><td align="right">0.0109 ns</td><td align="right">0.0102 ns</td><td align="right">0.82</td><td align="right">0.03</td></tr></tbody></table><p>It’s worth noting, as well, that if you’re interested in these kinds of micro-optimizations, you might also want to avoid using the default pinning at all, at least on super hot paths. The&nbsp;<code>{ReadOnly}Span&lt;T&gt;.GetPinnableReference</code>&nbsp;method was designed to behave just like pinning of arrays and strings, where null or empty inputs result in a null pointer. This behavior requires an additional check to be performed to see whether the length of the span is zero:</p><pre class="wrap:false lang:c# decode:true">// https://github.com/dotnet/coreclr/blob/52aff202cd382c233d903d432da06deffaa21868/src/System.Private.CoreLib/shared/System/Span.Fast.cs#L168-L174

[EditorBrowsable(EditorBrowsableState.Never)]
public unsafe ref T GetPinnableReference()
{
    // Ensure that the native code has just one forward branch that is predicted-not-taken.
    ref T ret = ref Unsafe.AsRef&lt;T&gt;(null);
    if (_length != 0) ret = ref _pointer.Value;
    return ref ret;
}</pre><p>If in your code by construction you know that the span will not be empty, you can choose to instead use&nbsp;<code>MemoryMarshal.GetReference</code>, which performs the same operation but without the length check:</p><pre class="wrap:false lang:c# decode:true">// https://github.com/dotnet/coreclr/blob/52aff202cd382c233d903d432da06deffaa21868/src/System.Private.CoreLib/shared/System/Runtime/InteropServices/MemoryMarshal.Fast.cs#L79

public static ref T GetReference&lt;T&gt;(Span&lt;T&gt; span) =&gt; ref span._pointer.Value;</pre><p>Again, while a single check adds minor overhead, when executed over and over and over, that can add up:</p><pre class="lang:c# decode:true ">private readonly byte[] _bytes = new byte[10_000];

[Benchmark(OperationsPerInvoke = 10_000, Baseline = true)]
public unsafe int PinSpan()
{
    Span&lt;byte&gt; s = _bytes;
    int total = 0;

    for (int i = 0; i &lt; s.Length; i++)
        fixed (byte* p = s) // equivalent to `fixed (byte* p = &amp;s.GetPinnableReference())`
            total += *p;

    return total;
}

[Benchmark(OperationsPerInvoke = 10_000)]
public unsafe int PinSpanExplicit()
{
    Span&lt;byte&gt; s = _bytes;
    int total = 0;

    for (int i = 0; i &lt; s.Length; i++)
        fixed (byte* p = &amp;MemoryMarshal.GetReference(s))
            total += *p;

    return total;
}</pre><table border="1"><thead><tr><th>Method</th><th align="right">Mean</th><th align="right">Error</th><th align="right">StdDev</th><th align="right">Ratio</th><th align="right">RatioSD</th></tr></thead><tbody><tr><td>PinSpan</td><td align="right">0.6524 ns</td><td align="right">0.0129 ns</td><td align="right">0.0159 ns</td><td align="right">1.00</td><td align="right">0.00</td></tr><tr><td>PinSpanExplicit</td><td align="right">0.5200 ns</td><td align="right">0.0111 ns</td><td align="right">0.0140 ns</td><td align="right">0.80</td><td align="right">0.03</td></tr></tbody></table><p>Of course, there are many other (and generally preferred) ways to operate over a span’s data than to use&nbsp;<code>fixed</code>. For example, it’s a bit surprising that until&nbsp;<code>Span&lt;T&gt;</code>&nbsp;came along, .NET didn’t have a built-in equivalent of&nbsp;<code>memcmp</code>, but nevertheless,&nbsp;<code>Span&lt;T&gt;</code>‘s&nbsp;<code>SequenceEqual</code>&nbsp;and&nbsp;<code>SequenceCompareTo</code>&nbsp;methods have become go-to methods for comparing in-memory data in .NET. In .NET Core 2.1, both&nbsp;<code>SequenceEqual</code>&nbsp;and&nbsp;<code>SequenceCompareTo</code>&nbsp;were optimized to utilize&nbsp;<code>System.Numerics.Vector</code>&nbsp;for vectorization, but the nature of&nbsp;<code>SequenceEqual</code>&nbsp;made it more amenable to best take advantage. In PR&nbsp;<a href="https://github.com/dotnet/coreclr/pull/22127">dotnet/coreclr#22127</a>, @benaadams updated&nbsp;<code>SequenceCompareTo</code>&nbsp;to take advantage of the new hardware instrinsics APIs available in .NET Core 3.0 to specifically target AVX2 and SSE2, resulting in significant improvements when comparing both small and large spans. (For more information on hardware intrinsics in .NET Core 3.0, see&nbsp;<a href="https://github.com/dotnet/designs/blob/master/accepted/platform-intrinsics.md">platform-intrinsics.md</a>&nbsp;and&nbsp;<a href="https://devblogs.microsoft.com/dotnet/using-net-hardware-intrinsics-api-to-accelerate-machine-learning-scenarios/" rel="nofollow">using-net-hardware-intrinsics-api-to-accelerate-machine-learning-scenarios</a>.)</p><pre class="lang:c# decode:true">private byte[] _orig, _same, _differFirst, _differLast;

[Params(16, 256)]
public int Length { get; set; }

[GlobalSetup]
public void Setup()
{
    _orig = Enumerable.Range(0, Length).Select(i =&gt; (byte)i).ToArray();
    _same = (byte[])_orig.Clone();

    _differFirst = (byte[])_orig.Clone();
    _differFirst[0] = (byte)(_orig[0] + 1);

    _differLast = (byte[])_orig.Clone();
    _differLast[_differLast.Length - 1] = (byte)(_orig[_orig.Length - 1] + 1);
}

[Benchmark]
public int CompareSame() =&gt; _orig.AsSpan().SequenceCompareTo(_same);

[Benchmark]
public int CompareDifferFirst() =&gt; _orig.AsSpan().SequenceCompareTo(_differFirst);

[Benchmark]
public int CompareDifferLast() =&gt; _orig.AsSpan().SequenceCompareTo(_differLast);</pre><table border="1"><thead><tr><th>Method</th><th>Toolchain</th><th>Length</th><th align="right">Mean</th><th align="right">Error</th><th align="right">StdDev</th><th align="right">Ratio</th></tr></thead><tbody><tr><td>CompareSame</td><td>netcoreapp2.1</td><td>16</td><td align="right">16.955 ns</td><td align="right">0.2009 ns</td><td align="right">0.1781 ns</td><td align="right">1.00</td></tr><tr><td>CompareSame</td><td>netcoreapp3.0</td><td>16</td><td align="right">4.757 ns</td><td align="right">0.0938 ns</td><td align="right">0.0732 ns</td><td align="right">0.28</td></tr><tr><td></td><td></td><td></td><td align="right"></td><td align="right"></td><td align="right"></td><td align="right"></td></tr><tr><td>CompareDifferFirst</td><td>netcoreapp2.1</td><td>16</td><td align="right">11.874 ns</td><td align="right">0.1240 ns</td><td align="right">0.1100 ns</td><td align="right">1.00</td></tr><tr><td>CompareDifferFirst</td><td>netcoreapp3.0</td><td>16</td><td align="right">5.174 ns</td><td align="right">0.0543 ns</td><td align="right">0.0508 ns</td><td align="right">0.44</td></tr><tr><td></td><td></td><td></td><td align="right"></td><td align="right"></td><td align="right"></td><td align="right"></td></tr><tr><td>CompareDifferLast</td><td>netcoreapp2.1</td><td>16</td><td align="right">16.644 ns</td><td align="right">0.2146 ns</td><td align="right">0.2007 ns</td><td align="right">1.00</td></tr><tr><td>CompareDifferLast</td><td>netcoreapp3.0</td><td>16</td><td align="right">5.373 ns</td><td align="right">0.0479 ns</td><td align="right">0.0448 ns</td><td align="right">0.32</td></tr><tr><td></td><td></td><td></td><td align="right"></td><td align="right"></td><td align="right"></td><td align="right"></td></tr><tr><td>CompareSame</td><td>netcoreapp2.1</td><td>256</td><td align="right">43.740 ns</td><td align="right">0.8226 ns</td><td align="right">0.7292 ns</td><td align="right">1.00</td></tr><tr><td>CompareSame</td><td>netcoreapp3.0</td><td>256</td><td align="right">11.055 ns</td><td align="right">0.1625 ns</td><td align="right">0.1441 ns</td><td align="right">0.25</td></tr><tr><td></td><td></td><td></td><td align="right"></td><td align="right"></td><td align="right"></td><td align="right"></td></tr><tr><td>CompareDifferFirst</td><td>netcoreapp2.1</td><td>256</td><td align="right">12.144 ns</td><td align="right">0.0849 ns</td><td align="right">0.0752 ns</td><td align="right">1.00</td></tr><tr><td>CompareDifferFirst</td><td>netcoreapp3.0</td><td>256</td><td align="right">6.663 ns</td><td align="right">0.1044 ns</td><td align="right">0.0977 ns</td><td align="right">0.55</td></tr><tr><td></td><td></td><td></td><td align="right"></td><td align="right"></td><td align="right"></td><td align="right"></td></tr><tr><td>CompareDifferLast</td><td>netcoreapp2.1</td><td>256</td><td align="right">39.697 ns</td><td align="right">0.9291 ns</td><td align="right">2.6054 ns</td><td align="right">1.00</td></tr><tr><td>CompareDifferLast</td><td>netcoreapp3.0</td><td>256</td><td align="right">11.242 ns</td><td align="right">0.2218 ns</td><td align="right">0.1732 ns</td><td align="right">0.32</td></tr></tbody></table><p>As background, “vectorization” is an approach to parallelization that performs multiple operations as part of individual instructions on a single core. Some optimizing compilers can perform automatic vectorization, whereby the compiler analyzes loops to determine whether it can generate functionally equivalent code that would utilize such instructions to run faster. The .NET JIT compiler does not currently perform auto-vectorization, but it is possible to manually vectorize loops, and the options for doing so have significantly improved in .NET Core 3.0. Just as a simple example of what vectorization can look like, imagine having an array of bytes and wanting to search it for the first non-zero byte, returning the position of that byte. The simple solution is to just iterate through all of the bytes:</p><pre class="lang:c# decode:true">private byte[] _buffer = new byte[10_000].Concat(new byte[] { 42 }).ToArray();

[Benchmark(Baseline = true)]
public int LoopBytes()
{
    byte[] buffer = _buffer;
    for (int i = 0; i &lt; buffer.Length; i++)
    {
        if (buffer[i] != 0)
            return i;
    }
    return -1;
}</pre><p>That of course works functionally, and for very small arrays it’s fine. But for larger arrays, we end up doing significantly more work than is actually necessary. Consider instead in a 64-bit process re-interpreting the array of bytes as an array of longs, which&nbsp;<code>Span&lt;T&gt;</code>&nbsp;nicely supports. We then effectively compare 8 bytes at a time rather than 1 byte at a time, at the expense of added code complexity: once we find a non-zero long, we then need to look at each byte it contains to determine the position of the first non-zero one (though there are ways to improve that, too). Similarly, the array’s length may not evenly divide by 8, so we need to be able to handle the overflow.</p><pre class="lang:c# decode:true">[Benchmark]
public int LoopLongs()
{
    byte[] buffer = _buffer;
    int remainingStart = 0;

    if (IntPtr.Size == sizeof(long))
    {
        Span&lt;long&gt; longBuffer = MemoryMarshal.Cast&lt;byte, long&gt;(buffer);
        remainingStart = longBuffer.Length * sizeof(long);

        for (int i = 0; i &lt; longBuffer.Length; i++)
        {
            if (longBuffer[i] != 0)
            {
                remainingStart = i * sizeof(long);
                break;
            }
        }
    }

    for (int i = remainingStart; i &lt; buffer.Length; i++)
    {
        if (buffer[i] != 0)
            return i;
    }

    return -1;
}</pre><p>For longer arrays, this yields really nice wins:</p><table border="1"><thead><tr><th>Method</th><th align="right">Mean</th><th align="right">Error</th><th align="right">StdDev</th><th align="right">Ratio</th></tr></thead><tbody><tr><td>LoopBytes</td><td align="right">5,462.3 ns</td><td align="right">107.093 ns</td><td align="right">105.180 ns</td><td align="right">1.00</td></tr><tr><td>LoopLongs</td><td align="right">568.6 ns</td><td align="right">6.895 ns</td><td align="right">5.758 ns</td><td align="right">0.10</td></tr></tbody></table><p>I’ve glossed over some details here, but it should convey the core idea. .NET includes additional mechanisms for vectorizing as well. In particular, the aforementioned&nbsp;<code>System.Numerics.Vector</code>&nbsp;type allows for a developer to write code using&nbsp;<code>Vector</code>&nbsp;and then have the JIT compiler translate that into the best instructions available on the current platform.</p><pre class="tab-convert:true lang:c# decode:true ">[Benchmark]
public int LoopVectors()
{
    byte[] buffer = _buffer;
    int remainingStart = 0;

	if (Vector.IsHardwareAccelerated)
	{
		while (remainingStart &lt;= buffer.Length - Vector&lt;byte&gt;.Count)
		{
			var vector = new Vector&lt;byte&gt;(buffer, remainingStart);
			if (!Vector.EqualsAll(vector, default))
			{
				break;
			}
			remainingStart += Vector&lt;byte&gt;.Count;
		}
	}

    for (int i = remainingStart; i &lt; buffer.Length; i++)
    {
        if (buffer[i] != 0)
            return i;
    }

    return -1;
}</pre><table border="1"><thead><tr><th>Method</th><th align="right">Mean</th><th align="right">Error</th><th align="right">StdDev</th><th align="right">Ratio</th></tr></thead><tbody><tr><td>LoopBytes</td><td align="right">5,462.3 ns</td><td align="right">107.093 ns</td><td align="right">105.180 ns</td><td align="right">1.00</td></tr><tr><td>LoopLongs</td><td align="right">568.6 ns</td><td align="right">6.895 ns</td><td align="right">5.758 ns</td><td align="right">0.10</td></tr><tr><td>LoopVectors</td><td align="right">306.0 ns</td><td align="right">4.502 ns</td><td align="right">4.211 ns</td><td align="right">0.06</td></tr></tbody></table><p>Further, .NET Core 3.0 includes new hardware intrinsics that allow a properly-motivated developer to eke out the best possible performance on supporting hardware, utilizing extensions like AVX or SSE that can compare well more than 8 bytes at a time. Many of the improvements in .NET Core 3.0 come from utilizing these techniques.</p><p>Back to examples, copying spans has also improved, thanks to PRs&nbsp;<a href="https://github.com/dotnet/coreclr/pull/18006">dotnet/coreclr#18006</a>&nbsp;from @benaadams and&nbsp;<a href="https://github.com/dotnet/coreclr/pull/17889">dotnet/coreclr#17889</a>, in particular for relatively small spans…</p><pre class="lang:c# decode:true">private byte[] _from = new byte[] { 1, 2, 3, 4 };
private byte[] _to = new byte[4];

[Benchmark]
public void CopySpan() =&gt; _from.AsSpan().CopyTo(_to);</pre><table border="1"><thead><tr><th>Method</th><th>Toolchain</th><th align="right">Mean</th><th align="right">Error</th><th align="right">StdDev</th><th align="right">Ratio</th></tr></thead><tbody><tr><td>CopySpan</td><td>netcoreapp2.1</td><td align="right">10.913 ns</td><td align="right">0.1960 ns</td><td align="right">0.1737 ns</td><td align="right">1.00</td></tr><tr><td>CopySpan</td><td>netcoreapp3.0</td><td align="right">3.568 ns</td><td align="right">0.0528 ns</td><td align="right">0.0494 ns</td><td align="right">0.33</td></tr></tbody></table><p>Searching is one of the most commonly performed operations in any program, and searches with spans are generally performed with&nbsp;<code>IndexOf</code>&nbsp;and its variants (e.g.&nbsp;<code>IndexOfAny</code>&nbsp;and&nbsp;<code>Contains</code>) In PR&nbsp;<a href="https://github.com/dotnet/coreclr/pull/20738">dotnet/coreclr#20738</a>, @benaadams again utilized vectorization, this time to improve the performance of&nbsp;<code>IndexOfAny</code>&nbsp;when operating over bytes, a particularly common case in many networking-related scenarios (e.g. parsing bytes off the wire as part of an HTTP stack). You can see the effects of this in the following microbenchmark:</p><pre class="wrap:false lang:c# decode:true">private byte[] _arr = Encoding.UTF8.GetBytes("This is a test to see improvements to IndexOfAny.  How'd they work?");
[Benchmark] public int IndexOf() =&gt; new Span&lt;byte&gt;(_arr).IndexOfAny((byte)'.', (byte)'?');</pre><table border="1"><thead><tr><th>Method</th><th>Toolchain</th><th align="right">Mean</th><th align="right">Error</th><th align="right">StdDev</th><th align="right">Ratio</th></tr></thead><tbody><tr><td>IndexOf</td><td>netcoreapp2.1</td><td align="right">12.828 ns</td><td align="right">0.1805 ns</td><td align="right">0.1600 ns</td><td align="right">1.00</td></tr><tr><td>IndexOf</td><td>netcoreapp3.0</td><td align="right">4.504 ns</td><td align="right">0.0968 ns</td><td align="right">0.0858 ns</td><td align="right">0.35</td></tr></tbody></table><p>I love these kinds of improvements, because they’re low-enough in the stack that they end up having multiplicative effects across so much code. The above change only affected&nbsp;<code>byte</code>, but subsequent PRs were submitted to cover&nbsp;<code>char</code>&nbsp;as well, and then PR&nbsp;<a href="https://github.com/dotnet/coreclr/pull/20855">dotnet/coreclr#20855</a>&nbsp;made a nice change that brought these same changes to other primitives of the same sizes. For example, we can recast the previous benchmark to use sbyte instead of byte, and as of that PR, a similar improvement applies:</p><pre class="wrap:false lang:c# decode:true">private sbyte[] _arr = Encoding.UTF8.GetBytes("This is a test to see improvements to IndexOfAny.  How'd they work?").Select(b =&gt; (sbyte)b).ToArray();

[Benchmark]
public int IndexOf() =&gt; new Span&lt;sbyte&gt;(_arr).IndexOfAny((sbyte)'.', (sbyte)'?');</pre><table border="1"><thead><tr><th>Method</th><th>Toolchain</th><th align="right">Mean</th><th align="right">Error</th><th align="right">StdDev</th><th align="right">Ratio</th></tr></thead><tbody><tr><td>IndexOf</td><td>netcoreapp2.1</td><td align="right">24.636 ns</td><td align="right">0.2292 ns</td><td align="right">0.2144 ns</td><td align="right">1.00</td></tr><tr><td>IndexOf</td><td>netcoreapp3.0</td><td align="right">9.795 ns</td><td align="right">0.1419 ns</td><td align="right">0.1258 ns</td><td align="right">0.40</td></tr></tbody></table><p>As another example, consider PR&nbsp;<a href="https://github.com/dotnet/coreclr/pull/20275">dotnet/coreclr#20275</a>. That change similarly utilized vectorization to improve the performance of To{Upper/Lower}{Invariant}.</p><pre class="lang:c# decode:true">private string _src = "This is a source string that needs to be capitalized.";
private char[] _dst = new char[1024];
[Benchmark] public int ToUpperInvariant() =&gt; _src.AsSpan().ToUpperInvariant(_dst);</pre><table border="1"><thead><tr><th>Method</th><th>Toolchain</th><th align="right">Mean</th><th align="right">Error</th><th align="right">StdDev</th><th align="right">Ratio</th></tr></thead><tbody><tr><td>ToUpperInvariant</td><td>netcoreapp2.1</td><td align="right">64.36 ns</td><td align="right">0.8099 ns</td><td align="right">0.6763 ns</td><td align="right">1.00</td></tr><tr><td>ToUpperInvariant</td><td>netcoreapp3.0</td><td align="right">26.48 ns</td><td align="right">0.2411 ns</td><td align="right">0.2137 ns</td><td align="right">0.41</td></tr></tbody></table><p>PR&nbsp;<a href="https://github.com/dotnet/coreclr/pull/19959">dotnet/coreclr#19959</a>&nbsp;optimizes the Trim{Start/End} helpers on&nbsp;<code>ReadOnlySpan&lt;char&gt;</code>, another very commonly-applied method, with equally exciting results (it’s hard to see with the white space in the results, but the results in the table go in order of the arguments in the Params attribute):</p><pre class="lang:default decode:true">[Params("", " abcdefg ", "abcdefg")]
public string Data;

[Benchmark]
public ReadOnlySpan&lt;char&gt; Trim() =&gt; Data.AsSpan().Trim();</pre><table border="1"><thead><tr><th>Method</th><th>Toolchain</th><th>Data</th><th align="right">Mean</th><th align="right">Error</th><th align="right">StdDev</th><th align="right">Ratio</th></tr></thead><tbody><tr><td>Trim</td><td>netcoreapp2.1</td><td></td><td align="right">12.999 ns</td><td align="right">0.1913 ns</td><td align="right">0.1789 ns</td><td align="right">1.00</td></tr><tr><td>Trim</td><td>netcoreapp3.0</td><td></td><td align="right">3.078 ns</td><td align="right">0.0349 ns</td><td align="right">0.0326 ns</td><td align="right">0.24</td></tr><tr><td></td><td></td><td></td><td align="right"></td><td align="right"></td><td align="right"></td><td align="right"></td></tr><tr><td>Trim</td><td>netcoreapp2.1</td><td>abcdefg</td><td align="right">17.618 ns</td><td align="right">0.3534 ns</td><td align="right">0.2951 ns</td><td align="right">1.00</td></tr><tr><td>Trim</td><td>netcoreapp3.0</td><td>abcdefg</td><td align="right">7.927 ns</td><td align="right">0.0934 ns</td><td align="right">0.0828 ns</td><td align="right">0.45</td></tr><tr><td></td><td></td><td></td><td align="right"></td><td align="right"></td><td align="right"></td><td align="right"></td></tr><tr><td>Trim</td><td>netcoreapp2.1</td><td>abcdefg</td><td align="right">15.522 ns</td><td align="right">0.2200 ns</td><td align="right">0.1951 ns</td><td align="right">1.00</td></tr><tr><td>Trim</td><td>netcoreapp3.0</td><td>abcdefg</td><td align="right">5.227 ns</td><td align="right">0.0750 ns</td><td align="right">0.0665 ns</td><td align="right">0.34</td></tr></tbody></table><p>Sometimes optimizations are just about being smarter about code management. PR&nbsp;<a href="https://github.com/dotnet/coreclr/pull/17890">dotnet/coreclr#17890</a>&nbsp;removed an unnecessary layer of functions that were on many globalization-related code paths, and just removing those extra unnecessary method invocations results in measurable speed-ups when working with small spans, e.g.</p><div class="highlight highlight-source-cs"><pre class="wrap:false lang:default decode:true">[Benchmark]
public bool EndsWith() =&gt; "Hello world".AsSpan().EndsWith("world", StringComparison.OrdinalIgnoreCase);</pre></div><table border="1"><thead><tr><th>Method</th><th>Toolchain</th><th align="right">Mean</th><th align="right">Error</th><th align="right">StdDev</th><th align="right">Ratio</th></tr></thead><tbody><tr><td>EndsWith</td><td>netcoreapp2.1</td><td align="right">37.80 ns</td><td align="right">0.3290 ns</td><td align="right">0.2917 ns</td><td align="right">1.00</td></tr><tr><td>EndsWith</td><td>netcoreapp3.0</td><td align="right">12.26 ns</td><td align="right">0.1479 ns</td><td align="right">0.1384 ns</td><td align="right">0.32</td></tr></tbody></table><p>Of course, one of the great things about span is that it is a reusable building-block that enables many higher-level operations. That includes operations on both arrays and strings…</p><h3>Arrays and Strings</h3><p>As a theme that’s emerged within .NET Core, wherever possible, new performance-focused functionality should not only be exposed for public use but also be used internally; after all, given the depth and breadth of functionality within .NET Core, if some performance-focused feature doesn’t meet the needs of .NET Core itself, there’s a reasonable chance it also won’t meet the public need. As such, internal usage of new features is a key benchmark as to whether the design is adequate, and in the process of evaluating such criteria, many additional code paths benefit, and these improvements have a multiplicative effect.</p><p>This isn’t just about new APIs. Many of the language features introduced in C# 7.2, 7.3, and 8.0 are influenced by the needs of .NET Core itself and have been used to improve things that we couldn’t reasonably improve before (other than dropping down to unsafe code, which we try to avoid when possible). For example, PR&nbsp;<a href="https://github.com/dotnet/coreclr/pull/17891">dotnet/coreclr#17891</a>&nbsp;speeds up Array.Reverse by taking advantage of the C# 7.2 ref locals feature and the 7.3 ref local reassignment feature. Using the new feature allows for the code to be expressed in a way that lets the JIT generate better code for the inner loop, and in turn results in a measurable speed-up:</p><pre class="lang:default decode:true">private int[] _arr = Enumerable.Range(0, 256).ToArray();

[Benchmark]
public void Reverse() =&gt; Array.Reverse(_arr);</pre><table border="1"><thead><tr><th>Method</th><th>Toolchain</th><th align="right">Mean</th><th align="right">Error</th><th align="right">StdDev</th><th align="right">Ratio</th><th align="right">RatioSD</th></tr></thead><tbody><tr><td>Reverse</td><td>netcoreapp2.1</td><td align="right">105.06 ns</td><td align="right">2.488 ns</td><td align="right">7.337 ns</td><td align="right">1.00</td><td align="right">0.00</td></tr><tr><td>Reverse</td><td>netcoreapp3.0</td><td align="right">74.12 ns</td><td align="right">1.494 ns</td><td align="right">2.536 ns</td><td align="right">0.66</td><td align="right">0.02</td></tr></tbody></table><p>Another example for arrays, the&nbsp;<code>Clear</code>&nbsp;method improved in PR&nbsp;<a href="https://github.com/dotnet/coreclr/pull/24302">dotnet/coreclr#24302</a>, which works around an alignment issue that results in the underlying memset used to implement the operation being up to 2x slower. The change manually clears up to a few bytes one by one, such that the pointer we then hand off to memset is properly aligned. If you got “lucky” previously and the array happened to be aligned, performance was fine, but if it wasn’t aligned, there was a non-trivial performance hit incurred. This benchmark simulates the unlucky case:</p><div class="highlight highlight-source-cs"><pre class="lang:default decode:true">[GlobalSetup]
public void Setup()
{
    while (true)
    {
        var buffer = new byte[8192];
        GCHandle handle = GCHandle.Alloc(buffer, GCHandleType.Pinned);
        if (((long)handle.AddrOfPinnedObject()) % 32 != 0)
        {
            _handle = handle;
            _buffer = buffer;
            return;
        }
        handle.Free();
    }
}

[GlobalCleanup]
public void Cleanup() =&gt; _handle.Free();

private GCHandle _handle;
private byte[] _buffer;

[Benchmark] public void Clear() =&gt; Array.Clear(_buffer, 0, _buffer.Length);</pre></div><table border="1"><thead><tr><th>Method</th><th>Toolchain</th><th align="right">Mean</th><th align="right">Error</th><th align="right">StdDev</th><th align="right">Ratio</th></tr></thead><tbody><tr><td>Clear</td><td>netcoreapp2.1</td><td align="right">121.59 ns</td><td align="right">0.8349 ns</td><td align="right">0.6519 ns</td><td align="right">1.00</td></tr><tr><td>Clear</td><td>netcoreapp3.0</td><td align="right">87.91 ns</td><td align="right">1.7768 ns</td><td align="right">1.6620 ns</td><td align="right">0.73</td></tr></tbody></table><p>That said, many of the improvements are in fact based on new APIs. Span is a great example of this. It was introduced in .NET Core 2.1, and the initial push was to get it to be usable and expose sufficient surface area to allow it to be used meaningfully. But at the same time, we started utilizing it internally in order to both vet the design and benefit from the improvements it enables. Some of this was done in .NET Core 2.1, but the effort continues in .NET Core 3.0. Arrays and strings are both prime candidates for such optimizations.</p><p>For example, many of the same vectorization optimizations applied to spans are similarly applied to arrays. PR&nbsp;<a href="https://github.com/dotnet/coreclr/pull/21116">dotnet/coreclr#21116</a>&nbsp;from @benaadams optimized&nbsp;<code>Array.{Last}IndexOf</code>&nbsp;for both&nbsp;<code>byte</code>s and&nbsp;<code>char</code>s, utilizing the same internal helpers that were written to enable spans, and to similar effect:</p><div class="highlight highlight-source-cs"><pre class="lang:default decode:true">private char[] _arr = "This is a test to see improvements to IndexOf.  How'd they work?".ToCharArray();

[Benchmark]
public int IndexOf() =&gt; Array.IndexOf(_arr, '.');</pre></div><table border="1"><thead><tr><th>Method</th><th>Toolchain</th><th align="right">Mean</th><th align="right">Error</th><th align="right">StdDev</th><th align="right">Ratio</th><th align="right">RatioSD</th></tr></thead><tbody><tr><td>IndexOf</td><td>netcoreapp2.1</td><td align="right">34.976 ns</td><td align="right">0.6352 ns</td><td align="right">0.5631 ns</td><td align="right">1.00</td><td align="right">0.00</td></tr><tr><td>IndexOf</td><td>netcoreapp3.0</td><td align="right">9.471 ns</td><td align="right">0.6638 ns</td><td align="right">1.1091 ns</td><td align="right">0.29</td><td align="right">0.04</td></tr></tbody></table><p>And as with spans, thanks to PR&nbsp;<a href="https://github.com/dotnet/coreclr/pull/24293">dotnet/coreclr#24293</a>&nbsp;from @dschinde, these&nbsp;<code>IndexOf</code>optimizations also now apply to other primitives of the same size.</p><div class="highlight highlight-source-cs"><pre class="lang:default decode:true">private short[] _arr = "This is a test to see improvements to IndexOf.  How'd they work?".Select(c =&gt; (short)c).ToArray();

[Benchmark]
public int IndexOf() =&gt; Array.IndexOf(_arr, (short)'.');</pre></div><table border="1"><thead><tr><th>Method</th><th>Toolchain</th><th align="right">Mean</th><th align="right">Error</th><th align="right">StdDev</th><th align="right">Ratio</th></tr></thead><tbody><tr><td>IndexOf</td><td>netcoreapp2.1</td><td align="right">34.181 ns</td><td align="right">0.6626 ns</td><td align="right">0.6508 ns</td><td align="right">1.00</td></tr><tr><td>IndexOf</td><td>netcoreapp3.0</td><td align="right">9.600 ns</td><td align="right">0.1913 ns</td><td align="right">0.1598 ns</td><td align="right">0.28</td></tr></tbody></table><p>Vectorization optimizations have been applied to strings, too. You can see the effect of PR&nbsp;<a href="https://github.com/dotnet/coreclr/pull/21076">dotnet/coreclr#21076</a>&nbsp;from @benaadams in this microbenchmark:</p><div class="highlight highlight-source-cs"><pre class="wrap:false lang:default decode:true">[Benchmark]
public int IndexOf() =&gt; "Let's see how fast we can find the period towards the end of this string.  Pretty fast?".IndexOf('.', StringComparison.Ordinal);</pre></div><table border="1"><thead><tr><th>Method</th><th>Toolchain</th><th align="right">Mean</th><th align="right">Error</th><th align="right">StdDev</th><th align="right">Ratio</th><th align="right">Gen 0</th><th align="right">Gen 1</th><th align="right">Gen 2</th><th align="right">Allocated</th></tr></thead><tbody><tr><td>IndexOf</td><td>netcoreapp2.1</td><td align="right">75.14 ns</td><td align="right">1.5285 ns</td><td align="right">1.6355 ns</td><td align="right">1.00</td><td align="right">0.0151</td><td align="right">–</td><td align="right">–</td><td align="right">32 B</td></tr><tr><td>IndexOf</td><td>netcoreapp3.0</td><td align="right">11.70 ns</td><td align="right">0.2382 ns</td><td align="right">0.2111 ns</td><td align="right">0.16</td><td align="right">–</td><td align="right">–</td><td align="right">–</td><td align="right">–</td></tr></tbody></table><p>Also note in the above that the .NET Core 2.1 operation allocates (due to converting the search character into a string), whereas the .NET Core 3.0 implementation does not. That’s thanks to PR&nbsp;<a href="https://github.com/dotnet/coreclr/pull/19788">dotnet/coreclr#19788</a> from @benaadams.</p><p>There are of course pieces of functionality that are more unique to strings (albeit also applicable to new functionality exposed on spans), such as hash code computation with various string comparison methods. For example, PR&nbsp;<a href="https://github.com/dotnet/coreclr/pull/20309/">dotnet/coreclr#20309/</a>&nbsp;improved the performance of&nbsp;<code>String.GetHashCode</code>&nbsp;when performing&nbsp;<code>OrdinalIgnoreCase</code>&nbsp;operations, which along with&nbsp;<code>Ordinal</code>&nbsp;(the default) represent the two most common modes.</p><div class="highlight highlight-source-cs"><pre class="lang:default decode:true">[Benchmark]
public int GetHashCodeIgnoreCase() =&gt; "Some string".GetHashCode(StringComparison.OrdinalIgnoreCase);</pre></div><table border="1"><thead><tr><th>Method</th><th>Toolchain</th><th align="right">Mean</th><th align="right">Error</th><th align="right">StdDev</th><th align="right">Ratio</th></tr></thead><tbody><tr><td>GetHashCodeIgnoreCase</td><td>netcoreapp2.1</td><td align="right">47.70 ns</td><td align="right">0.5751 ns</td><td align="right">0.5380 ns</td><td align="right">1.00</td></tr><tr><td>GetHashCodeIgnoreCase</td><td>netcoreapp3.0</td><td align="right">14.28 ns</td><td align="right">0.1462 ns</td><td align="right">0.1296 ns</td><td align="right">0.30</td></tr></tbody></table><p><code>OrdinalsIgnoreCase</code>&nbsp;has been improved for other uses as well. For example, PR&nbsp;<a href="https://github.com/dotnet/coreclr/pull/20734">dotnet/coreclr#20734</a>&nbsp;improved&nbsp;<code>String.Equals</code>&nbsp;when using&nbsp;<code>StringComparer.OrdinalIgnoreCase</code>by both vectorizing (checking two chars at a time instead of one) and removing branches from an inner loop:</p><div class="highlight highlight-source-cs"><pre class="lang:default decode:true">[Benchmark]
public bool EqualsIC() =&gt; "Some string".Equals("sOME sTrinG", StringComparison.OrdinalIgnoreCase);</pre></div><table border="1"><thead><tr><th>Method</th><th>Toolchain</th><th align="right">Mean</th><th align="right">Error</th><th align="right">StdDev</th><th align="right">Ratio</th></tr></thead><tbody><tr><td>EqualsIC</td><td>netcoreapp2.1</td><td align="right">24.036 ns</td><td align="right">0.3819 ns</td><td align="right">0.3572 ns</td><td align="right">1.00</td></tr><tr><td>EqualsIC</td><td>netcoreapp3.0</td><td align="right">9.165 ns</td><td align="right">0.0589 ns</td><td align="right">0.0551 ns</td><td align="right">0.38</td></tr></tbody></table><p>The previous cases are examples of functionality in&nbsp;<code>String</code>‘s implementation, but there are lots of ancillary string-related functionality that have seen improvements as well. For example, various operations on&nbsp;<code>Char</code>&nbsp;have been improved, such as&nbsp;<code>Char.GetUnicodeCategory</code>&nbsp;via PRs&nbsp;<a href="https://github.com/dotnet/coreclr/pull/20983">dotnet/coreclr#20983</a>&nbsp;and&nbsp;<a href="https://github.com/dotnet/coreclr/pull/20864">dotnet/coreclr#20864</a>:</p><div class="highlight highlight-source-cs"><pre class="lang:default decode:true">[Params('.', 'a', '\x05D0')]
public char Char { get; set; }

[Benchmark]
public UnicodeCategory GetCategory() =&gt; char.GetUnicodeCategory(Char);</pre></div><table border="1"><thead><tr><th>Method</th><th>Toolchain</th><th>Char</th><th align="right">Mean</th><th align="right">Error</th><th align="right">StdDev</th><th align="right">Ratio</th><th align="right">RatioSD</th></tr></thead><tbody><tr><td>GetCategory</td><td>netcoreapp2.1</td><td>.</td><td align="right">1.8001 ns</td><td align="right">0.0160 ns</td><td align="right">0.0142 ns</td><td align="right">1.00</td><td align="right">0.00</td></tr><tr><td>GetCategory</td><td>netcoreapp3.0</td><td>.</td><td align="right">0.4925 ns</td><td align="right">0.0141 ns</td><td align="right">0.0132 ns</td><td align="right">0.27</td><td align="right">0.01</td></tr><tr><td></td><td></td><td></td><td align="right"></td><td align="right"></td><td align="right"></td><td align="right"></td><td align="right"></td></tr><tr><td>GetCategory</td><td>netcoreapp2.1</td><td>a</td><td align="right">1.7925 ns</td><td align="right">0.0144 ns</td><td align="right">0.0127 ns</td><td align="right">1.00</td><td align="right">0.00</td></tr><tr><td>GetCategory</td><td>netcoreapp3.0</td><td>a</td><td align="right">0.4957 ns</td><td align="right">0.0117 ns</td><td align="right">0.0091 ns</td><td align="right">0.28</td><td align="right">0.01</td></tr><tr><td></td><td></td><td></td><td align="right"></td><td align="right"></td><td align="right"></td><td align="right"></td><td align="right"></td></tr><tr><td>GetCategory</td><td>netcoreapp2.1</td><td>?</td><td align="right">3.7836 ns</td><td align="right">0.0493 ns</td><td align="right">0.0461 ns</td><td align="right">1.00</td><td align="right">0.00</td></tr><tr><td>GetCategory</td><td>netcoreapp3.0</td><td>?</td><td align="right">2.7531 ns</td><td align="right">0.0757 ns</td><td align="right">0.0633 ns</td><td align="right">0.73</td><td align="right">0.02</td></tr></tbody></table><p>Those PRs also highlight another case of benefiting from a language improvement. As of C# 7.3, the C# compiler is able to optimize properties of the form:</p><div class="highlight highlight-source-cs"><pre class="lang:default decode:true">static ReadOnlySpan&lt;byte&gt; s_byteData =&gt; new byte[] { … /* constant bytes */ }</pre><p><span>Rather than emitting this exactly as written, which would allocate a new byte array on each call, the compiler takes advantage of the facts that a) the bytes backing the array are all constant and b) it’s being returned as a read-only span, which means the consumer is unable to mutate the data using safe code. As such, with PR&nbsp;</span><a href="https://github.com/dotnet/roslyn/pull/24621">dotnet/roslyn#24621</a><span>, the C# compiler instead emits this by writing the bytes as a binary blob in metadata, and the property then simply creates a span that points directly to that data, making it very fast to access the data, more so even than if this property returned a static byte[].</span></p></div><div class="highlight highlight-source-cs"><pre class="wrap:false lang:default decode:true">// Run with: dotnet run -c Release -f netcoreapp2.1 --filter *Program* --runtimes netcoreapp3.0

private static byte[] ArrayProp { get; } = new byte[] { 1, 2, 3 };

[Benchmark(Baseline = true)]
public ReadOnlySpan&lt;byte&gt; GetArrayProp() =&gt; ArrayProp;

private static ReadOnlySpan&lt;byte&gt; SpanProp =&gt; new byte[] { 1, 2, 3 };

[Benchmark]
public ReadOnlySpan&lt;byte&gt; GetSpanProp() =&gt; SpanProp;</pre></div><table border="1"><thead><tr><th>Method</th><th align="right">Mean</th><th align="right">Error</th><th align="right">StdDev</th><th align="right">Median</th><th align="right">Ratio</th></tr></thead><tbody><tr><td>GetArrayProp</td><td align="right">1.3362 ns</td><td align="right">0.0498 ns</td><td align="right">0.0416 ns</td><td align="right">1.3366 ns</td><td align="right">1.000</td></tr><tr><td>GetSpanProp</td><td align="right">0.0125 ns</td><td align="right">0.0132 ns</td><td align="right">0.0110 ns</td><td align="right">0.0080 ns</td><td align="right">0.009</td></tr></tbody></table><p>Another string-related area that’s gotten some attention is&nbsp;<code>StringBuilder</code>&nbsp;(not necessarily improvements to&nbsp;<code>StringBuilder</code>&nbsp;itself, although it has received some of those, for example a new overload in PR&nbsp;<a href="https://github.com/dotnet/coreclr/pull/20773">dotnet/coreclr#20773</a>&nbsp;from @Wraith2 that helps avoid accidentally boxing and creating a string from a&nbsp;<code>ReadOnlyMemory&lt;char&gt;</code>&nbsp;appended to the builder). Rather, in many situations&nbsp;<code>StringBuilder</code>s have been used for convenience but added cost, and with just a little work (and in some cases the new&nbsp;<code>String.Create</code>&nbsp;method introduced in .NET Core 2.1), we can eliminate that overhead, in both CPU usage and allocation. Here a few examples…</p><div class="highlight highlight-source-cs"><pre class="lang:default decode:true">[Benchmark]
public IPHostEntry GetHostEntry() =&gt; Dns.GetHostEntry("34.206.253.53");</pre></div><table border="1"><thead><tr><th>Method</th><th>Toolchain</th><th align="right">Mean</th><th align="right">Error</th><th align="right">StdDev</th><th align="right">Median</th><th align="right">Ratio</th><th align="right">RatioSD</th><th align="right">Gen 0</th><th align="right">Gen 1</th><th align="right">Gen 2</th><th align="right">Allocated</th></tr></thead><tbody><tr><td>GetHostEntry</td><td>netcoreapp2.1</td><td align="right">532.7 us</td><td align="right">16.59 us</td><td align="right">46.79 us</td><td align="right">526.8 us</td><td align="right">1.00</td><td align="right">0.00</td><td align="right">1.9531</td><td align="right">–</td><td align="right">–</td><td align="right">4888 B</td></tr><tr><td>GetHostEntry</td><td>netcoreapp3.0</td><td align="right">527.7 us</td><td align="right">12.85 us</td><td align="right">37.06 us</td><td align="right">542.8 us</td><td align="right">1.00</td><td align="right">0.11</td><td align="right">–</td><td align="right">–</td><td align="right">–</td><td align="right">616 B</td></tr></tbody></table><pre class="lang:default decode:true">private static CultureInfo CreateCulture()
{
    var c = new CultureInfo("he-IL");
    c.DateTimeFormat.Calendar = new HebrewCalendar();
    return c;
}

private CultureInfo _hebrewIsrael = CreateCulture();

[Benchmark]
public string FormatHebrew() =&gt; new DateTime(2018, 11, 20).ToString(_hebrewIsrael);</pre><table border="1"><thead><tr><th>Method</th><th>Toolchain</th><th align="right">Mean</th><th align="right">Error</th><th align="right">StdDev</th><th align="right">Ratio</th><th align="right">RatioSD</th><th align="right">Gen 0</th><th align="right">Gen 1</th><th align="right">Gen 2</th><th align="right">Allocated</th></tr></thead><tbody><tr><td>FormatHebrew</td><td>netcoreapp2.1</td><td align="right">626.0 ns</td><td align="right">7.917 ns</td><td align="right">7.405 ns</td><td align="right">1.00</td><td align="right">0.00</td><td align="right">0.2890</td><td align="right">–</td><td align="right">–</td><td align="right">608 B</td></tr><tr><td>FormatHebrew</td><td>netcoreapp3.0</td><td align="right">570.6 ns</td><td align="right">10.504 ns</td><td align="right">9.825 ns</td><td align="right">0.91</td><td align="right">0.02</td><td align="right">0.1554</td><td align="right">–</td><td align="right">–</td><td align="right">328 B</td></tr></tbody></table><pre class="wrap:false lang:default decode:true">private readonly PhysicalAddress _short = new PhysicalAddress(new byte[1] { 42 });
private readonly PhysicalAddress _long = new PhysicalAddress(Enumerable.Range(0, 256).Select(i =&gt; (byte)i).ToArray());

[Benchmark]
public void PAShort() =&gt; _short.ToString();

[Benchmark]
public void PALong() =&gt; _long.ToString();</pre><table border="1"><thead><tr><th>Method</th><th>Toolchain</th><th align="right">Mean</th><th align="right">Error</th><th align="right">StdDev</th><th align="right">Ratio</th><th align="right">RatioSD</th><th align="right">Gen 0</th><th align="right">Gen 1</th><th align="right">Gen 2</th><th align="right">Allocated</th></tr></thead><tbody><tr><td>PAShort</td><td>netcoreapp2.1</td><td align="right">33.68 ns</td><td align="right">1.0378 ns</td><td align="right">2.9271 ns</td><td align="right">1.00</td><td align="right">0.00</td><td align="right">0.0648</td><td align="right">–</td><td align="right">–</td><td align="right">136 B</td></tr><tr><td>PAShort</td><td>netcoreapp3.0</td><td align="right">17.12 ns</td><td align="right">0.4240 ns</td><td align="right">0.7313 ns</td><td align="right">0.55</td><td align="right">0.04</td><td align="right">0.0153</td><td align="right">–</td><td align="right">–</td><td align="right">32 B</td></tr><tr><td></td><td></td><td align="right"></td><td align="right"></td><td align="right"></td><td align="right"></td><td align="right"></td><td align="right"></td><td align="right"></td><td align="right"></td><td align="right"></td></tr><tr><td>PALong</td><td>netcoreapp2.1</td><td align="right">2,761.80 ns</td><td align="right">50.1515 ns</td><td align="right">46.9117 ns</td><td align="right">1.00</td><td align="right">0.00</td><td align="right">1.1940</td><td align="right">–</td><td align="right">–</td><td align="right">2512 B</td></tr><tr><td>PALong</td><td>netcoreapp3.0</td><td align="right">787.78 ns</td><td align="right">27.4673 ns</td><td align="right">80.1234 ns</td><td align="right">0.31</td><td align="right">0.01</td><td align="right">0.5007</td><td align="right">–</td><td align="right">–</td><td align="right">1048 B</td></tr></tbody></table><pre class="wrap:false lang:default decode:true">private X509Certificate2 _cert = GetCert();

private static X509Certificate2 GetCert()
{
    using (var client = new Socket(AddressFamily.InterNetwork, SocketType.Stream, ProtocolType.Tcp))
    {
        client.Connect("microsoft.com", 443);
        using (var ssl = new SslStream(new NetworkStream(client)))
        {
            ssl.AuthenticateAsClient("microsoft.com", null, SslProtocols.None, false);
            return new X509Certificate2(ssl.RemoteCertificate);
        }
    }
}

[Benchmark]
public string CertProp() =&gt; _cert.Thumbprint;</pre><table border="1"><thead><tr><th>Method</th><th>Toolchain</th><th align="right">Mean</th><th align="right">Error</th><th align="right">StdDev</th><th align="right">Median</th><th align="right">Ratio</th><th align="right">RatioSD</th><th align="right">Gen 0</th><th align="right">Gen 1</th><th align="right">Gen 2</th><th align="right">Allocated</th></tr></thead><tbody><tr><td>CertProp</td><td>netcoreapp2.1</td><td align="right">209.30 ns</td><td align="right">4.464 ns</td><td align="right">10.435 ns</td><td align="right">204.35 ns</td><td align="right">1.00</td><td align="right">0.00</td><td align="right">0.1256</td><td align="right">–</td><td align="right">–</td><td align="right">264 B</td></tr><tr><td>CertProp</td><td>netcoreapp3.0</td><td align="right">95.82 ns</td><td align="right">1.822 ns</td><td align="right">1.704 ns</td><td align="right">96.43 ns</td><td align="right">0.45</td><td align="right">0.02</td><td align="right">0.0497</td><td align="right">–</td><td align="right">–</td><td align="right">104 B</td></tr></tbody></table><p>and so on. These PRs demonstrate that good gains can be had simply by making small tweaks that make existing code paths cheaper, and that expands well beyond&nbsp;<code>StringBuilder</code>. There are lots of places within .NET Core, for example, where&nbsp;<code>String.Substring</code>&nbsp;is used, and many of those cases can be replaced with use of&nbsp;<code>AsSpan</code>&nbsp;and&nbsp;<code>Slice</code>, for example as was done in PR&nbsp;<a href="https://github.com/dotnet/corefx/pull/29402">dotnet/corefx#29402</a>&nbsp;by @juliushardt, or PRs&nbsp;<a href="https://github.com/dotnet/coreclr/pull/17916">dotnet/coreclr#17916</a>&nbsp;and&nbsp;<a href="https://github.com/dotnet/corefx/pull/29539">dotnet/corefx#29539</a>, or as was done in PRs&nbsp;<a href="https://github.com/dotnet/corefx/pull/29227">dotnet/corefx#29227</a>&nbsp;and&nbsp;<a href="https://github.com/dotnet/corefx/pull/29721">dotnet/corefx#29721</a>&nbsp;to remove string allocations from FileSystemWatcher, delaying the creation of such strings until only when it was known they were absolutely necessary.</p><div class="highlight highlight-source-cs"><pre class="wrap:false lang:default decode:true">[Benchmark]
public void HtmlDecode() =&gt; WebUtility.HtmlDecode("&amp;#x6C34;&amp;#x6C34;&amp;#x6C34;&amp;#x6C34;&amp;#x6C34;&amp;#x6C34;&amp;#x6C34;");</pre></div><table border="1"><thead><tr><th>Method</th><th>Toolchain</th><th align="right">Mean</th><th align="right">Error</th><th align="right">StdDev</th><th align="right">Ratio</th><th align="right">Gen 0</th><th align="right">Gen 1</th><th align="right">Gen 2</th><th align="right">Allocated</th></tr></thead><tbody><tr><td>HtmlDecode</td><td>netcoreapp2.1</td><td align="right">638.2 ns</td><td align="right">8.474 ns</td><td align="right">7.077 ns</td><td align="right">1.00</td><td align="right">0.1516</td><td align="right">–</td><td align="right">–</td><td align="right">320 B</td></tr><tr><td>HtmlDecode</td><td>netcoreapp3.0</td><td align="right">153.7 ns</td><td align="right">2.776 ns</td><td align="right">2.461 ns</td><td align="right">0.24</td><td align="right">0.0191</td><td align="right">–</td><td align="right">–</td><td align="right">40 B</td></tr></tbody></table><p>Another example of using new APIs to improve existing functionality is with&nbsp;<code>String.Concat</code>. .NET Core 3.0 has several new&nbsp;<code>String.Concat</code>&nbsp;overloads, ones that accept&nbsp;<code>ReadOnlySpan&lt;char&gt;</code>&nbsp;instead of&nbsp;<code>string</code>. These make it easy to avoid allocations/copies of substrings in cases where concatenating pieces of other strings: instead of using&nbsp;<code>String.Concat</code>&nbsp;with&nbsp;<code>String.Substring</code>, it’s used instead with&nbsp;<code>String.AsSpan(...)</code>&nbsp;or&nbsp;<code>Slice</code>. In fact, the PRs&nbsp;<a href="https://github.com/dotnet/coreclr/pull/21766">dotnet/coreclr#21766</a>&nbsp;and&nbsp;<a href="https://github.com/dotnet/corefx/pull/34451">dotnet/corefx#34451</a>&nbsp;that implemented, exposed, and added tests for these new overloads also added tens of call sites to the new overloads across .NET Core. Here’s an example of the impact one of those has, improving the performance of accessing&nbsp;<code>Uri.DnsSafeHost</code>:</p><div class="highlight highlight-source-cs"><pre class="lang:default decode:true">[Benchmark]
public string DnsSafeHost() =&gt; new Uri("http://[fe80::3]%1").DnsSafeHost;</pre></div><table border="1"><thead><tr><th>Method</th><th>Toolchain</th><th align="right">Mean</th><th align="right">Error</th><th align="right">StdDev</th><th align="right">Ratio</th><th align="right">RatioSD</th><th align="right">Gen 0</th><th align="right">Gen 1</th><th align="right">Gen 2</th><th align="right">Allocated</th></tr></thead><tbody><tr><td>DnsSafeHost</td><td>netcoreapp2.1</td><td align="right">733.7 ns</td><td align="right">14.448 ns</td><td align="right">17.20 ns</td><td align="right">1.00</td><td align="right">0.00</td><td align="right">0.2012</td><td align="right">–</td><td align="right">–</td><td align="right">424 B</td></tr><tr><td>DnsSafeHost</td><td>netcoreapp3.0</td><td align="right">450.1 ns</td><td align="right">9.013 ns</td><td align="right">18.41 ns</td><td align="right">0.63</td><td align="right">0.02</td><td align="right">0.1059</td><td align="right">–</td><td align="right">–</td><td align="right">224 B</td></tr></tbody></table><p>Another example, using&nbsp;<code>Path.ChangeExtension</code>&nbsp;to change from one non-null extension to another:</p><div class="highlight highlight-source-cs"><pre class="lang:default decode:true">[Benchmark]
public string ChangeExtension() =&gt; Path.ChangeExtension("filename.txt", ".dat");</pre></div><table border="1"><thead><tr><th>Method</th><th>Toolchain</th><th align="right">Mean</th><th align="right">Error</th><th align="right">StdDev</th><th align="right">Ratio</th><th align="right">Gen 0</th><th align="right">Gen 1</th><th align="right">Gen 2</th><th align="right">Allocated</th></tr></thead><tbody><tr><td>ChangeExtension</td><td>netcoreapp2.1</td><td align="right">30.57 ns</td><td align="right">0.7124 ns</td><td align="right">0.6664 ns</td><td align="right">1.00</td><td align="right">0.0495</td><td align="right">–</td><td align="right">–</td><td align="right">104 B</td></tr><tr><td>ChangeExtension</td><td>netcoreapp3.0</td><td align="right">24.54 ns</td><td align="right">0.3398 ns</td><td align="right">0.2838 ns</td><td align="right">0.80</td><td align="right">0.0229</td><td align="right">–</td><td align="right">–</td><td align="right">48 B</td></tr></tbody></table><p>Finally, a very closely related area is that of encoding. A bunch of improvements were made in .NET Core 3.0 around&nbsp;<code>Encoding</code>, both in general and for specific encodings, such as PR&nbsp;<a href="https://github.com/dotnet/coreclr/pull/18263">dotnet/coreclr#18263</a>&nbsp;that allowed an existing corner-case optimization to be applied for&nbsp;<code>Encoding.Unicode.GetString</code>&nbsp;in many more cases, or&nbsp;<a href="https://github.com/dotnet/coreclr/pull/18487">dotnet/coreclr#18487</a>&nbsp;that removed a bunch of unnecessary virtual indirections from various encoding implementations, or PR&nbsp;<a href="https://github.com/dotnet/coreclr/pull/20768">dotnet/coreclr#20768</a>&nbsp;that improved the performance of&nbsp;<code>Encoding.Preamble</code>&nbsp;by taking advantage of the same metadata-blob span support discussed earlier, or PRs&nbsp;<a href="https://github.com/dotnet/coreclr/pull/21948">dotnet/coreclr#21948</a>&nbsp;and&nbsp;<a href="https://github.com/dotnet/coreclr/pull/23098">dotnet/coreclr#23098</a>&nbsp;that overhauled and streamlined the implementions of&nbsp;<code>UTF8Encoding</code>&nbsp;and&nbsp;<code>AsciiEncoding</code>.</p><pre class="wrap:false lang:default decode:true">private byte[] _data = Encoding.ASCII.GetBytes("This is a test of ASCII encoding. It's faster now.");

[Benchmark]
public string ASCII() =&gt; Encoding.ASCII.GetString(_data);</pre><table border="1"><thead><tr><th>Method</th><th>Toolchain</th><th align="right">Mean</th><th align="right">Error</th><th align="right">StdDev</th><th align="right">Ratio</th><th align="right">Gen 0</th><th align="right">Gen 1</th><th align="right">Gen 2</th><th align="right">Allocated</th></tr></thead><tbody><tr><td>ASCII</td><td>netcoreapp2.1</td><td align="right">66.92 ns</td><td align="right">0.8942 ns</td><td align="right">0.8364 ns</td><td align="right">1.00</td><td align="right">0.0609</td><td align="right">–</td><td align="right">–</td><td align="right">128 B</td></tr><tr><td>ASCII</td><td>netcoreapp3.0</td><td align="right">28.04 ns</td><td align="right">0.6325 ns</td><td align="right">0.9467 ns</td><td align="right">0.42</td><td align="right">0.0612</td><td align="right">–</td><td align="right">–</td><td align="right">128 B</td></tr></tbody></table><p>These examples all served to highlight improvements made in and around strings. That’s all well and good, but where the improvements related to strings really start to shine is when looking at improvements around formatting and parsing.</p><h3>Parsing/Formatting</h3><p>Parsing and formatting are the lifeblood of any modern web app or service: take data off the wire, parse it, manipulate it, format it back out. As such, in .NET Core 2.1 along with bringing up&nbsp;<code>Span&lt;T&gt;</code>, we invested in the formatting and parsing of primitives, from&nbsp;<code>Int32</code>&nbsp;to&nbsp;<code>DateTime</code>. Many of those changes can be read about in my previous blog posts, but one of the key factors in enabling those performance improvements was in moving a lot of native code to managed. That may be counter-intuitive, in that it’s “common knowledge” that C code is faster than C# code. However, in addition to the gap between them narrowing, having (mostly) safe C# code has made the code base easier to experiment in, so whereas we may have been skittish about tweaking the native implementations, the community-at-large has dived head first into optimizing these implementations wherever possible. That effort continues in full force in .NET Core 3.0, with some very nice rewards reaped.</p><p>Let’s start with core integer primitives. PR&nbsp;<a href="https://github.com/dotnet/coreclr/pull/18897">dotnet/coreclr#18897</a>&nbsp;added a variety of special paths for the parsing of&nbsp;<code>Integer</code>-style signed values (e.g.&nbsp;<code>Int32</code>&nbsp;and&nbsp;<code>Int64</code>), PR&nbsp;<a href="https://github.com/dotnet/coreclr/pull/18930">dotnet/coreclr#18930</a> added similar support for unsigned (e.g.&nbsp;<code>UInt32</code>&nbsp;and&nbsp;<code>UInt64</code>), and PR&nbsp;<a href="https://github.com/dotnet/coreclr/pull/18952">dotnet/coreclr#18952</a>&nbsp;did a similar pass for hex. On top of those, PR&nbsp;<a href="https://github.com/dotnet/coreclr/pull/21365">dotnet/coreclr#21365</a>&nbsp;layered in additional optimizations, for example utilizing those changes for primitives like&nbsp;<code>byte</code>, skipping unnecessary layers of functions, streamlining some calls to improve inlining, and further reducing branching. The net impact here are some significant improvements to the performance of parsing integer primitive types in this release.</p><div class="highlight highlight-source-cs"><pre class="lang:default decode:true">[Benchmark]
public int ParseInt32Dec() =&gt; int.Parse("12345678");

[Benchmark]
public int ParseInt32Hex() =&gt; int.Parse("BC614E", NumberStyles.HexNumber);</pre></div><table border="1"><thead><tr><th>Method</th><th>Toolchain</th><th align="right">Mean</th><th align="right">Error</th><th align="right">StdDev</th><th align="right">Ratio</th></tr></thead><tbody><tr><td>ParseInt32Dec</td><td>netcoreapp2.1</td><td align="right">77.30 ns</td><td align="right">0.8710 ns</td><td align="right">0.8147 ns</td><td align="right">1.00</td></tr><tr><td>ParseInt32Dec</td><td>netcoreapp3.0</td><td align="right">16.08 ns</td><td align="right">0.2168 ns</td><td align="right">0.2028 ns</td><td align="right">0.21</td></tr><tr><td></td><td></td><td align="right"></td><td align="right"></td><td align="right"></td><td align="right"></td></tr><tr><td>ParseInt32Hex</td><td>netcoreapp2.1</td><td align="right">69.01 ns</td><td align="right">1.0024 ns</td><td align="right">0.9377 ns</td><td align="right">1.00</td></tr><tr><td>ParseInt32Hex</td><td>netcoreapp3.0</td><td align="right">17.39 ns</td><td align="right">0.1123 ns</td><td align="right">0.0995 ns</td><td align="right">0.25</td></tr></tbody></table><p>Formatting of such types was also improved, even though it had already been improved significantly between .NET Core 2.0 and .NET Core 2.1. PR&nbsp;<a href="https://github.com/dotnet/coreclr/pull/19551">dotnet/coreclr#19551</a>&nbsp;tweaked the structure of the code to avoid needing to access the current culture number formatting data if it wouldn’t be needed (e.g. when formatting a value as hex, there’s no customization based on current culture), and PR&nbsp;<a href="https://github.com/dotnet/coreclr/pull/18935">dotnet/coreclr#18935</a>&nbsp;improved decimal formatting performance, in large part by optimizing how data is passed around (or not passed at all).</p><div class="highlight highlight-source-cs"><pre class="lang:default decode:true">[Benchmark]
public string DecimalToString() =&gt; 12345.6789m.ToString();</pre></div><table border="1"><thead><tr><th>Method</th><th>Toolchain</th><th align="right">Mean</th><th align="right">Error</th><th align="right">StdDev</th><th align="right">Ratio</th><th align="right">Gen 0</th><th align="right">Gen 1</th><th align="right">Gen 2</th><th align="right">Allocated</th></tr></thead><tbody><tr><td>DecimalToString</td><td>netcoreapp2.1</td><td align="right">88.79 ns</td><td align="right">1.4034 ns</td><td align="right">1.3127 ns</td><td align="right">1.00</td><td align="right">0.0228</td><td align="right">–</td><td align="right">–</td><td align="right">48 B</td></tr><tr><td>DecimalToString</td><td>netcoreapp3.0</td><td align="right">76.62 ns</td><td align="right">0.5957 ns</td><td align="right">0.5572 ns</td><td align="right">0.86</td><td align="right">0.0228</td><td align="right">–</td><td align="right">–</td><td align="right">48 B</td></tr></tbody></table><p>In fact,&nbsp;<code>System.Decimal</code>&nbsp;itself has been overhauled in .NET Core 3.0, as of PR&nbsp;<a href="https://github.com/dotnet/coreclr/pull/18948">dotnet/coreclr#18948</a> now with an entirely managed implementation, and with additional performance work in PRs like&nbsp;<a href="https://github.com/dotnet/coreclr/pull/20305">dotnet/coreclr#20305</a>.</p><div class="highlight highlight-source-cs"><pre class="lang:default decode:true">private decimal _a = 67891.2345m;
private decimal _b = 12345.6789m;

[Benchmark]
public decimal Add() =&gt; _a + _b;

[Benchmark]
public decimal Subtract() =&gt; _a - _b;

[Benchmark]
public decimal Multiply() =&gt; _a * _b;

[Benchmark]
public decimal Divide() =&gt; _a / _b;

[Benchmark]
public decimal Mod() =&gt; _a % _b;

[Benchmark]
public decimal Floor() =&gt; decimal.Floor(_a);

[Benchmark]
public decimal Round() =&gt; decimal.Round(_a);</pre></div><table border="1"><thead><tr><th>Method</th><th>Toolchain</th><th align="right">Mean</th><th align="right">Error</th><th align="right">StdDev</th><th align="right">Median</th><th align="right">Ratio</th><th align="right">RatioSD</th></tr></thead><tbody><tr><td>Add</td><td>netcoreapp2.1</td><td align="right">12.021 ns</td><td align="right">0.6813 ns</td><td align="right">2.0088 ns</td><td align="right">11.507 ns</td><td align="right">1.00</td><td align="right">0.00</td></tr><tr><td>Add</td><td>netcoreapp3.0</td><td align="right">8.300 ns</td><td align="right">0.0553 ns</td><td align="right">0.0518 ns</td><td align="right">8.312 ns</td><td align="right">0.87</td><td align="right">0.04</td></tr><tr><td></td><td></td><td align="right"></td><td align="right"></td><td align="right"></td><td align="right"></td><td align="right"></td><td align="right"></td></tr><tr><td>Subtract</td><td>netcoreapp2.1</td><td align="right">13.026 ns</td><td align="right">0.2599 ns</td><td align="right">0.2431 ns</td><td align="right">13.046 ns</td><td align="right">1.00</td><td align="right">0.00</td></tr><tr><td>Subtract</td><td>netcoreapp3.0</td><td align="right">8.613 ns</td><td align="right">0.2024 ns</td><td align="right">0.2770 ns</td><td align="right">8.488 ns</td><td align="right">0.66</td><td align="right">0.03</td></tr><tr><td></td><td></td><td align="right"></td><td align="right"></td><td align="right"></td><td align="right"></td><td align="right"></td><td align="right"></td></tr><tr><td>Multiply</td><td>netcoreapp2.1</td><td align="right">19.215 ns</td><td align="right">0.2813 ns</td><td align="right">0.2631 ns</td><td align="right">19.229 ns</td><td align="right">1.00</td><td align="right">0.00</td></tr><tr><td>Multiply</td><td>netcoreapp3.0</td><td align="right">7.182 ns</td><td align="right">0.1795 ns</td><td align="right">0.2457 ns</td><td align="right">7.131 ns</td><td align="right">0.38</td><td align="right">0.01</td></tr><tr><td></td><td></td><td align="right"></td><td align="right"></td><td align="right"></td><td align="right"></td><td align="right"></td><td align="right"></td></tr><tr><td>Divide</td><td>netcoreapp2.1</td><td align="right">196.827 ns</td><td align="right">4.3572 ns</td><td align="right">4.6621 ns</td><td align="right">194.721 ns</td><td align="right">1.00</td><td align="right">0.00</td></tr><tr><td>Divide</td><td>netcoreapp3.0</td><td align="right">75.456 ns</td><td align="right">1.5301 ns</td><td align="right">1.7007 ns</td><td align="right">75.089 ns</td><td align="right">0.38</td><td align="right">0.01</td></tr><tr><td></td><td></td><td align="right"></td><td align="right"></td><td align="right"></td><td align="right"></td><td align="right"></td><td align="right"></td></tr><tr><td>Mod</td><td>netcoreapp2.1</td><td align="right">464.968 ns</td><td align="right">7.0295 ns</td><td align="right">6.5754 ns</td><td align="right">466.825 ns</td><td align="right">1.00</td><td align="right">0.00</td></tr><tr><td>Mod</td><td>netcoreapp3.0</td><td align="right">13.756 ns</td><td align="right">0.2476 ns</td><td align="right">0.2316 ns</td><td align="right">13.729 ns</td><td align="right">0.03</td><td align="right">0.00</td></tr><tr><td></td><td></td><td align="right"></td><td align="right"></td><td align="right"></td><td align="right"></td><td align="right"></td><td align="right"></td></tr><tr><td>Floor</td><td>netcoreapp2.1</td><td align="right">33.593 ns</td><td align="right">0.8348 ns</td><td align="right">2.2710 ns</td><td align="right">32.734 ns</td><td align="right">1.00</td><td align="right">0.00</td></tr><tr><td>Floor</td><td>netcoreapp3.0</td><td align="right">12.109 ns</td><td align="right">0.1325 ns</td><td align="right">0.1239 ns</td><td align="right">12.085 ns</td><td align="right">0.33</td><td align="right">0.02</td></tr><tr><td></td><td></td><td align="right"></td><td align="right"></td><td align="right"></td><td align="right"></td><td align="right"></td><td align="right"></td></tr><tr><td>Round</td><td>netcoreapp2.1</td><td align="right">32.181 ns</td><td align="right">0.5660 ns</td><td align="right">0.5294 ns</td><td align="right">32.018 ns</td><td align="right">1.00</td><td align="right">0.00</td></tr><tr><td>Round</td><td>netcoreapp3.0</td><td align="right">12.798 ns</td><td align="right">0.1572 ns</td><td align="right">0.1394 ns</td><td align="right">12.808 ns</td><td align="right">0.40</td><td align="right">0.01</td></tr></tbody></table><p>Back to formatting and parsing, there are even some new formatting special-cases that might look silly at first, but that represent optimizations targeting real-world cases. In some sizeable web applications, we found that a large number of strings on the managed heap were simple integral values like “0” and “1”. And since the fastest code is code you don’t need to execute at all, why bother allocating and formatting these small numbers over and over when we can instead just cache and reuse the results (effectively our own string interning pool)? That’s what PR&nbsp;<a href="https://github.com/dotnet/coreclr/pull/18383">dotnet/coreclr#18383</a>&nbsp;does, creating a small, specialized cache of the strings for “0” through “9”, and any time we now find ourselves formatting a single-digit integer primitive, we instead just grab the relevant string from this cache.</p><div class="highlight highlight-source-cs"><pre class="lang:default decode:true">private int _digit = 4;

[Benchmark]
public string SingleDigitToString() =&gt; _digit.ToString();</pre></div><table border="1"><thead><tr><th>Method</th><th>Toolchain</th><th align="right">Mean</th><th align="right">Error</th><th align="right">StdDev</th><th align="right">Ratio</th><th align="right">Gen 0</th><th align="right">Gen 1</th><th align="right">Gen 2</th><th align="right">Allocated</th></tr></thead><tbody><tr><td>SingleDigitToString</td><td>netcoreapp2.1</td><td align="right">17.72 ns</td><td align="right">0.3273 ns</td><td align="right">0.3061 ns</td><td align="right">1.00</td><td align="right">0.0152</td><td align="right">–</td><td align="right">–</td><td align="right">32 B</td></tr><tr><td>SingleDigitToString</td><td>netcoreapp3.0</td><td align="right">11.57 ns</td><td align="right">0.1750 ns</td><td align="right">0.1551 ns</td><td align="right">0.65</td><td align="right">–</td><td align="right">–</td><td align="right">–</td><td align="right">–</td></tr></tbody></table><p>Enums have also seen sizable parsing and formatting improvements in .NET Core 3.0. PR&nbsp;<a href="https://github.com/dotnet/coreclr/pull/21214">dotnet/coreclr#21214</a>&nbsp;improved the handling of&nbsp;<code>Enum.Parse</code>&nbsp;and&nbsp;<code>Enum.TryParse</code>, for both the generic and non-generic variants. PR&nbsp;<a href="https://github.com/dotnet/coreclr/pull/21254">dotnet/coreclr#21254</a>&nbsp;improved the performance of&nbsp;<code>ToString</code>&nbsp;when dealing with&nbsp;<code>[Flags]</code>&nbsp;enums, and PR&nbsp;<a href="https://github.com/dotnet/coreclr/pull/21284">dotnet/coreclr#21284</a>&nbsp;further improved other ToString cases. The net effect of these changes is a sizeable improvement in&nbsp;<code>Enum</code>-related performance:</p><div class="highlight highlight-source-cs"><pre class="lang:default decode:true">[Benchmark]
public DayOfWeek EnumParse() =&gt; Enum.Parse&lt;DayOfWeek&gt;("Thursday");

[Benchmark]
public string EnumToString() =&gt; NumberStyles.Integer.ToString();</pre></div><table border="1"><thead><tr><th>Method</th><th>Toolchain</th><th align="right">Mean</th><th align="right">Error</th><th align="right">StdDev</th><th align="right">Ratio</th><th align="right">Gen 0</th><th align="right">Gen 1</th><th align="right">Gen 2</th><th align="right">Allocated</th></tr></thead><tbody><tr><td>EnumParse</td><td>netcoreapp2.1</td><td align="right">154.42 ns</td><td align="right">1.6917 ns</td><td align="right">1.5824 ns</td><td align="right">1.00</td><td align="right">0.0114</td><td align="right">–</td><td align="right">–</td><td align="right">24 B</td></tr><tr><td>EnumParse</td><td>netcoreapp3.0</td><td align="right">62.92 ns</td><td align="right">1.2239 ns</td><td align="right">1.1448 ns</td><td align="right">0.41</td><td align="right">–</td><td align="right">–</td><td align="right">–</td><td align="right">–</td></tr><tr><td></td><td></td><td align="right"></td><td align="right"></td><td align="right"></td><td align="right"></td><td align="right"></td><td align="right"></td><td align="right"></td><td align="right"></td></tr><tr><td>EnumToString</td><td>netcoreapp2.1</td><td align="right">85.81 ns</td><td align="right">1.6458 ns</td><td align="right">1.3743 ns</td><td align="right">1.00</td><td align="right">0.0305</td><td align="right">–</td><td align="right">–</td><td align="right">64 B</td></tr><tr><td>EnumToString</td><td>netcoreapp3.0</td><td align="right">27.89 ns</td><td align="right">0.6076 ns</td><td align="right">0.7901 ns</td><td align="right">0.32</td><td align="right">0.0114</td><td align="right">0.0001</td><td align="right">–</td><td align="right">24 B</td></tr></tbody></table><p>In .NET Core 2.1,&nbsp;<code>DateTime.TryFormat</code>&nbsp;and&nbsp;<code>ToString</code>&nbsp;were optimized for the commonly-used “o” and “r” formats; in .NET Core 3.0, the parsing equivalents get a similar treatment. PR&nbsp;<a href="https://github.com/dotnet/coreclr/pull/18800">dotnet/coreclr#18800</a>&nbsp;significantly improves the performance of parsing&nbsp;<code>DateTime{Offset}</code>s formatted with the Roundtrip “o” format, and PR&nbsp;<a href="https://github.com/dotnet/coreclr/pull/18771">dotnet/coreclr#18771</a>&nbsp;does the same for the RFC1123 “r” format. For any serialization formats heavy in&nbsp;<code>DateTime</code>s, these improvements can make a substantial impact:</p><div class="highlight highlight-source-cs"><pre class="lang:default decode:true">private string _r = DateTime.Now.ToString("r");
private string _o = DateTime.Now.ToString("o");

[Benchmark]
public DateTime ParseR() =&gt; DateTime.ParseExact(_r, "r", null);

[Benchmark]
public DateTime ParseO() =&gt; DateTime.ParseExact(_o, "o", null);</pre></div><table border="1"><thead><tr><th>Method</th><th>Toolchain</th><th align="right">Mean</th><th align="right">Error</th><th align="right">StdDev</th><th align="right">Median</th><th align="right">Ratio</th><th align="right">Gen 0</th><th align="right">Gen 1</th><th align="right">Gen 2</th><th align="right">Allocated</th></tr></thead><tbody><tr><td>ParseR</td><td>netcoreapp2.1</td><td align="right">2,254.6 ns</td><td align="right">44.340 ns</td><td align="right">45.534 ns</td><td align="right">2,263.2 ns</td><td align="right">1.00</td><td align="right">0.0420</td><td align="right">–</td><td align="right">–</td><td align="right">96 B</td></tr><tr><td>ParseR</td><td>netcoreapp3.0</td><td align="right">113.7 ns</td><td align="right">3.440 ns</td><td align="right">9.926 ns</td><td align="right">112.6 ns</td><td align="right">0.06</td><td align="right">–</td><td align="right">–</td><td align="right">–</td><td align="right">–</td></tr><tr><td></td><td></td><td align="right"></td><td align="right"></td><td align="right"></td><td align="right"></td><td align="right"></td><td align="right"></td><td align="right"></td><td align="right"></td><td align="right"></td></tr><tr><td>ParseO</td><td>netcoreapp2.1</td><td align="right">1,337.1 ns</td><td align="right">26.542 ns</td><td align="right">68.987 ns</td><td align="right">1,363.8 ns</td><td align="right">1.00</td><td align="right">0.0744</td><td align="right">–</td><td align="right">–</td><td align="right">160 B</td></tr><tr><td>ParseO</td><td>netcoreapp3.0</td><td align="right">354.9 ns</td><td align="right">4.801 ns</td><td align="right">3.748 ns</td><td align="right">354.9 ns</td><td align="right">0.30</td><td align="right">–</td><td align="right">–</td><td align="right">–</td><td align="right">–</td></tr></tbody></table><p>Tying back to the&nbsp;<code>StringBuilder</code>&nbsp;discussion from earlier, default&nbsp;<code>DateTime</code>&nbsp;formatting was also improved by PR&nbsp;<a href="https://github.com/dotnet/coreclr/pull/22111">dotnet/coreclr#22111</a>, tweaking how&nbsp;<code>DateTime</code>&nbsp;internally interacts with a&nbsp;<code>StringBuilder</code>&nbsp;that’s used to build up the resulting state.</p><div class="highlight highlight-source-cs"><pre class="lang:default decode:true">private DateTime _now = DateTime.Now;

[Benchmark]
public string DateTimeToString() =&gt; _now.ToString();</pre></div><table border="1"><thead><tr><th>Method</th><th>Toolchain</th><th align="right">Mean</th><th align="right">Error</th><th align="right">StdDev</th><th align="right">Ratio</th><th align="right">RatioSD</th><th align="right">Gen 0</th><th align="right">Gen 1</th><th align="right">Gen 2</th><th align="right">Allocated</th></tr></thead><tbody><tr><td>DateTimeToString</td><td>netcoreapp2.1</td><td align="right">337.8 ns</td><td align="right">6.560 ns</td><td align="right">5.815 ns</td><td align="right">1.00</td><td align="right">0.00</td><td align="right">0.0834</td><td align="right">–</td><td align="right">–</td><td align="right">176 B</td></tr><tr><td>DateTimeToString</td><td>netcoreapp3.0</td><td align="right">269.4 ns</td><td align="right">5.274 ns</td><td align="right">5.416 ns</td><td align="right">0.80</td><td align="right">0.02</td><td align="right">0.0300</td><td align="right">–</td><td align="right">–</td><td align="right">64 B</td></tr></tbody></table><p><code>TimeSpan</code>&nbsp;formatting was also significantly improved, via PR&nbsp;<a href="https://github.com/dotnet/coreclr/pull/18990">dotnet/coreclr#18990</a>:</p><div class="highlight highlight-source-cs"><pre class="lang:default decode:true">private TimeSpan _ts = new TimeSpan(3, 10, 2, 34, 567);

[Benchmark]
public string TimeSpanToString() =&gt; _ts.ToString();</pre></div><table border="1"><thead><tr><th>Method</th><th>Toolchain</th><th align="right">Mean</th><th align="right">Error</th><th align="right">StdDev</th><th align="right">Ratio</th><th align="right">Gen 0</th><th align="right">Gen 1</th><th align="right">Gen 2</th><th align="right">Allocated</th></tr></thead><tbody><tr><td>TimeSpanToString</td><td>netcoreapp2.1</td><td align="right">151.11 ns</td><td align="right">2.0037 ns</td><td align="right">1.874 ns</td><td align="right">1.00</td><td align="right">0.0303</td><td align="right">–</td><td align="right">–</td><td align="right">64 B</td></tr><tr><td>TimeSpanToString</td><td>netcoreapp3.0</td><td align="right">34.73 ns</td><td align="right">0.7680 ns</td><td align="right">1.304 ns</td><td align="right">0.23</td><td align="right">0.0305</td><td align="right">–</td><td align="right">–</td><td align="right">64 B</td></tr></tbody></table><p><code>Guid</code>&nbsp;parsing also got in the perf-optimization game, with PR&nbsp;<a href="https://github.com/dotnet/coreclr/pull/20183">dotnet/coreclr#20183</a>&nbsp;improved parsing performance of&nbsp;<code>Guid</code>, primarily by avoiding overhead in helper routines, as well as by avoiding some searches used to determine which parsing routines to employ.</p><div class="highlight highlight-source-cs"><pre class="lang:default decode:true">private string _guid = Guid.NewGuid().ToString("D");

[Benchmark]
public Guid ParseGuid() =&gt; Guid.ParseExact(_guid, "D");</pre></div><table border="1"><thead><tr><th>Method</th><th>Toolchain</th><th align="right">Mean</th><th align="right">Error</th><th align="right">StdDev</th><th align="right">Median</th><th align="right">Ratio</th></tr></thead><tbody><tr><td>ParseGuid</td><td>netcoreapp2.1</td><td align="right">287.5 ns</td><td align="right">11.606 ns</td><td align="right">28.688 ns</td><td align="right">277.2 ns</td><td align="right">1.00</td></tr><tr><td>ParseGuid</td><td>netcoreapp3.0</td><td align="right">111.7 ns</td><td align="right">2.199 ns</td><td align="right">2.057 ns</td><td align="right">112.4 ns</td><td align="right">0.33</td></tr></tbody></table><p>Related, PR&nbsp;<a href="https://github.com/dotnet/coreclr/pull/21336">dotnet/coreclr#21336</a>&nbsp;again takes advantage of vectorization to improve&nbsp;<code>Guid</code>‘s construction and formatting to and from byte arrays and spans:</p><div class="highlight highlight-source-cs"><pre class="lang:default decode:true">private Guid _guid = Guid.NewGuid();
private byte[] _buffer = new byte[16];

[Benchmark]
public void GuidToFromBytes()
{
    _guid.TryWriteBytes(_buffer);
    _guid = new Guid(_buffer);
}</pre></div><table border="1"><thead><tr><th>Method</th><th>Toolchain</th><th align="right">Mean</th><th align="right">Error</th><th align="right">StdDev</th><th align="right">Ratio</th></tr></thead><tbody><tr><td>GuidToFromBytes</td><td>netcoreapp2.1</td><td align="right">16.623 ns</td><td align="right">0.2917 ns</td><td align="right">0.2586 ns</td><td align="right">1.00</td></tr><tr><td>GuidToFromBytes</td><td>netcoreapp3.0</td><td align="right">5.701 ns</td><td align="right">0.1047 ns</td><td align="right">0.0980 ns</td><td align="right">0.34</td></tr></tbody></table><h3>Regular Expressions</h3><p>Often related to parsing is the area of regular expressions. A bit of work was done on&nbsp;<code>System.Text.RegularExpressions</code>&nbsp;in .NET Core 3.0. PR&nbsp;<a href="https://github.com/dotnet/corefx/pull/30474">dotnet/corefx#30474</a>&nbsp;replaced some usage of an internal&nbsp;<code>StringBuilder</code>&nbsp;cache with a&nbsp;<code>ref struct</code>-based builder that takes advantage of stack-allocated space and pooled buffers. And PR&nbsp;<a href="https://github.com/dotnet/corefx/pull/30632">dotnet/corefx#30632</a>&nbsp;continued the effort by taking further advantage of spans. But the biggest improvement came in PR&nbsp;<a href="https://github.com/dotnet/corefx/pull/32899">dotnet/corefx#32899</a>&nbsp;from @Alois-xx, which tweaks the code generated for a&nbsp;<code>RegexOptions.Compiled</code>&nbsp;<code>Regex</code>&nbsp;to avoid gratuitous thread-local accesses to look up the current culture. This is particularly impactful when also using&nbsp;<code>RegexOptions.IgnoreCase</code>. To see the impact, I found a complicated&nbsp;<code>Regex</code>&nbsp;that used both&nbsp;<code>Compiled</code>&nbsp;and&nbsp;<code>IgnoreCase</code>, and put it into a benchmark:</p><div class="highlight highlight-source-cs"><pre class="wrap:false lang:default decode:true">// Pattern and options copied from https://github.com/microsoft/referencesource/blob/aaca53b025f41ab638466b1efe569df314f689ea/System.ComponentModel.DataAnnotations/DataAnnotations/EmailAddressAttribute.cs#L54-L55
private Regex _regex = new Regex(
    @"^((([a-z]|\d|[!#\$%&amp;'\*\+\-\/=\?\^_`{\|}~]|[\u00A0-\uD7FF\uF900-\uFDCF\uFDF0-\uFFEF])+(\.([a-z]|\d|[!#\$%&amp;'\*\+\-\/=\?\^_`{\|}~]|[\u00A0-\uD7FF\uF900-\uFDCF\uFDF0-\uFFEF])+)*)|((\x22)((((\x20|\x09)*(\x0d\x0a))?(\x20|\x09)+)?(([\x01-\x08\x0b\x0c\x0e-\x1f\x7f]|\x21|[\x23-\x5b]|[\x5d-\x7e]|[\u00A0-\uD7FF\uF900-\uFDCF\uFDF0-\uFFEF])|(\\([\x01-\x09\x0b\x0c\x0d-\x7f]|[\u00A0-\uD7FF\uF900-\uFDCF\uFDF0-\uFFEF]))))*(((\x20|\x09)*(\x0d\x0a))?(\x20|\x09)+)?(\x22)))@((([a-z]|\d|[\u00A0-\uD7FF\uF900-\uFDCF\uFDF0-\uFFEF])|(([a-z]|\d|[\u00A0-\uD7FF\uF900-\uFDCF\uFDF0-\uFFEF])([a-z]|\d|-|\.|_|~|[\u00A0-\uD7FF\uF900-\uFDCF\uFDF0-\uFFEF])*([a-z]|\d|[\u00A0-\uD7FF\uF900-\uFDCF\uFDF0-\uFFEF])))\.)+(([a-z]|[\u00A0-\uD7FF\uF900-\uFDCF\uFDF0-\uFFEF])|(([a-z]|[\u00A0-\uD7FF\uF900-\uFDCF\uFDF0-\uFFEF])([a-z]|\d|-|\.|_|~|[\u00A0-\uD7FF\uF900-\uFDCF\uFDF0-\uFFEF])*([a-z]|[\u00A0-\uD7FF\uF900-\uFDCF\uFDF0-\uFFEF])))\.?$",
    RegexOptions.Compiled | RegexOptions.IgnoreCase | RegexOptions.ExplicitCapture);

[Benchmark]
public bool RegexCompiled() =&gt; _regex.IsMatch("someAddress@someCompany.com");</pre></div><table border="1"><thead><tr><th>Method</th><th>Toolchain</th><th align="right">Mean</th><th align="right">Error</th><th align="right">StdDev</th><th align="right">Ratio</th><th align="right">RatioSD</th></tr></thead><tbody><tr><td>RegexCompiled</td><td>netcoreapp2.1</td><td align="right">1.946 us</td><td align="right">0.0406 us</td><td align="right">0.0883 us</td><td align="right">1.00</td><td align="right">0.00</td></tr><tr><td>RegexCompiled</td><td>netcoreapp3.0</td><td align="right">1.209 us</td><td align="right">0.0432 us</td><td align="right">0.1254 us</td><td align="right">0.64</td><td align="right">0.08</td></tr></tbody></table><h3>Threading</h3><p>Threading is one of those things that’s ever-present and yet most apps and libraries don’t need to explicitly interact with most of the time. That makes it an area ripe for runtime performance improvements to drive down overhead as much as possible, so that user code just gets faster. Previous releases of .NET Core saw a lot of investment in this area, and .NET Core 3.0 continues the trend. This is another area where new APIs have been exposed and then also used in .NET Core itself for further gain.</p><p>For example, historically the only work item types that could be queued to the&nbsp;<code>ThreadPool</code>&nbsp;were ones implemented in the runtime, namely those created by&nbsp;<code>ThreadPool.QueueUserWorkItem</code>&nbsp;and friends, by&nbsp;<code>Task</code>, by&nbsp;<code>Timer</code>, and other such core types. But in .NET Core 3.0, the&nbsp;<code>ThreadPool</code>&nbsp;has an&nbsp;<code>UnsafeQueueUserWorkItem</code>&nbsp;overload that accepts the newly public&nbsp;<code>IThreadPoolWorkItem</code>&nbsp;interface. This interface is very simple, with a single method that just&nbsp;<code>Execute</code>s work, and that means that any object that implements this interface can be queued directly to the thread pool. This is advanced; most code is just fine using the existing work item types. But this additional option affords a lot of flexibility, in particular in being able to implement the interface on a reusable object that can be queued over and over again to the pool. This is now used in a bunch of additional places in .NET Core 3.0.</p><p>One such place is in&nbsp;<code>System.Threading.Channels</code>. The&nbsp;<code>Channels</code>&nbsp;library introduced in .NET Core 2.1 already had a fairly low allocation profile, but there were still times it would allocate. For example, one of the options when creating a channel is whether continuations created by the library should run synchronously or asynchronously as part of a task completing (e.g. when a&nbsp;<code>TryWrite</code>&nbsp;call on a channel wakes up a corresponding&nbsp;<code>ReadAsync</code>, whether the continuation from that&nbsp;<code>ReadAsync</code>&nbsp;invoked synchronously or queued by the&nbsp;<code>TryWrite</code>&nbsp;call). The default is that continuations are never invoked synchronously, but that also then requires allocating an object as part of queueing the continuation to the thread pool. With PR&nbsp;<a href="https://github.com/dotnet/corefx/pull/33080">dotnet/corefx#33080</a>, the reusable&nbsp;<code>IValueTaskSource</code>&nbsp;implementation that already backs the&nbsp;<code>ValueTask</code>s returned from&nbsp;<code>ReadAsync</code>&nbsp;calls also implements&nbsp;<code>IThreadPoolWorkItem</code>&nbsp;and can thus itself be queued, avoiding that allocation. This can have a measurable impact on throughput.</p><div class="highlight highlight-source-cs"><pre class="wrap:false lang:default decode:true">// Run with: dotnet run -c Release -f netcoreapp2.1 --filter *Program*

private sealed class Config : ManualConfig // also add [Config(typeof(Config))] to the Program class
{
    public Config()
    {
        Add(Job.MediumRun.With(CsProjCoreToolchain.NetCoreApp21).WithNuGet("System.Threading.Channels", "4.5.0").WithId("4.5.0"));
        Add(Job.MediumRun.With(CsProjCoreToolchain.NetCoreApp30).WithNuGet("System.Threading.Channels", "4.6.0-preview5.19224.8").WithId("4.6.0-preview5.19224.8"));
    }
}

private Channel&lt;int&gt; _channel1 = Channel.CreateUnbounded&lt;int&gt;();
private Channel&lt;int&gt; _channel2 = Channel.CreateUnbounded&lt;int&gt;();

[GlobalSetup]
public void Setup()
{
    Task.Run(async () =&gt;
    {
        var reader = _channel1.Reader;
        var writer = _channel2.Writer;
        while (true)
        {
            writer.TryWrite(await reader.ReadAsync());
        }
    });
}

[Benchmark]
public async Task PingPong()
{
    var writer = _channel1.Writer;
    var reader = _channel2.Reader;
    for (int i = 0; i &lt; 10_000; i++)
    {
        writer.TryWrite(i);
        await reader.ReadAsync();
    }
}</pre></div><table border="1"><thead><tr><th>Method</th><th>Job</th><th>NuGetReferences</th><th>Toolchain</th><th align="right">Mean</th><th align="right">Error</th><th align="right">StdDev</th><th align="right">Gen 0</th><th align="right">Gen 1</th><th align="right">Gen 2</th></tr></thead><tbody><tr><td>PingPong</td><td>4.5.0</td><td>System.Threading.Channels 4.5.0</td><td>.NET Core 2.1</td><td align="right">22.44 ms</td><td align="right">0.3246 ms</td><td align="right">0.4757 ms</td><td align="right">593.7500</td><td align="right">–</td><td align="right">–</td></tr><tr><td>PingPong</td><td>4.6.0-preview5.19224.8</td><td>System.Threading.Channels 4.6.0-preview5.19224.8</td><td>.NET Core 3.0</td><td align="right">16.81 ms</td><td align="right">0.4246 ms</td><td align="right">0.6356 ms</td><td align="right">31.2500</td><td align="right">–</td><td align="right">–</td></tr></tbody></table><p><code>IThreadPoolWorkItem</code>&nbsp;is now also utilized in other places, like in&nbsp;<code>ConcurrentExclusiveSchedulerPair</code>&nbsp;(a little known but useful type that provides an exclusive scheduler that limits execution to only one task at a time, a concurrent scheduler that limits a user-defined number of tasks to run at a time, and that coordinate with each other so that no concurrent tasks may run while an exclusive task is running, ala a reader-writer lock), which now implements&nbsp;<code>IThreadPoolWorkItem</code>&nbsp;on an internally reusable work item object such that it also can avoid allocations when queueing its own processors. It’s also used in ASP.NET Core, and is one of the reasons key ASP.NET benchmarks are ammortized to 0 allocations per request. But by far the most impactful new implementer is in the async/await infrastructure.</p><p>In .NET Core 2.1, the runtime’s support for async/await was overhauled, drastically reducing the overheads involved in async methods. Previously when an async method awaited for the first time an awaitable that wasn’t yet complete, the struct-based state machine for the async method would be boxed (literally a runtime box) to the heap. With .NET Core 2.1, we changed that to instead use a generic object that stores the struct as a field on it. This has a myriad of benefits, but one of these benefits is that it now enables us to implement additional interfaces on that object, such as implementing&nbsp;<code>IThreadPoolWorkItem</code>. PR&nbsp;<a href="https://github.com/dotnet/coreclr/pull/20159">dotnet/coreclr#20159</a>&nbsp;does exactly that, and it enables another large swath of scenarios to have further reduced allocations, in particular situations where&nbsp;<code>TaskCreationOptions.RunContinuationsAsynchronously</code>&nbsp;was used with a&nbsp;<code>TaskCompletionSource&lt;T&gt;</code>. This can be seen in a benchmark like the following.</p><div class="highlight highlight-source-cs"><pre class="wrap:false lang:default decode:true">// Run with: dotnet run -c Release -f netcoreapp2.1 --filter *Program*

private sealed class Config : ManualConfig // also add [Config(typeof(Config))] to the Program class
{
    public Config()
    {
        Add(Job.MediumRun.With(CsProjCoreToolchain.NetCoreApp21).WithNuGet("System.Threading.Channels", "4.5.0").WithId("4.5.0"));
        Add(Job.MediumRun.With(CsProjCoreToolchain.NetCoreApp30).WithNuGet("System.Threading.Channels", "4.6.0-preview5.19224.8").WithId("4.6.0-preview5.19224.8"));
    }
}

private Channel&lt;TaskCompletionSource&lt;bool&gt;&gt; _channel = Channel.CreateUnbounded&lt;TaskCompletionSource&lt;bool&gt;&gt;();

[GlobalSetup]
public void Setup()
{
    Task.Run(async () =&gt;
    {
        var reader = _channel.Reader;
        while (true) (await reader.ReadAsync()).TrySetResult(true);
    });
}

[Benchmark]
public async Task AsyncAllocs()
{
    var writer = _channel.Writer;
    for (int i = 0; i &lt; 1_000_000; i++)
    {
        var tcs = new TaskCompletionSource&lt;bool&gt;(TaskCreationOptions.RunContinuationsAsynchronously);
        writer.TryWrite(tcs);
        await tcs.Task;
    }
}</pre></div><table border="1"><thead><tr><th>Method</th><th>Job</th><th>NuGetReferences</th><th>Toolchain</th><th align="right">Mean</th><th align="right">Error</th><th align="right">StdDev</th><th align="right">Gen 0</th><th align="right">Gen 1</th><th align="right">Gen 2</th></tr></thead><tbody><tr><td>AsyncAllocs</td><td>4.5.0</td><td>System.Threading.Channels 4.5.0</td><td>.NET Core 2.1</td><td align="right">2.396 s</td><td align="right">0.0486 s</td><td align="right">0.0728 s</td><td align="right">96000.0000</td><td align="right">–</td><td align="right">–</td></tr><tr><td>AsyncAllocs</td><td>4.6.0-preview5.19224.8</td><td>System.Threading.Channels 4.6.0-preview5.19224.8</td><td>.NET Core 3.0</td><td align="right">1.512 s</td><td align="right">0.0256 s</td><td align="right">0.0359 s</td><td align="right">49000.0000</td><td align="right">–</td><td align="right">–</td></tr></tbody></table><p>That change allowed subsequent optimizations, such as PR&nbsp;<a href="https://github.com/dotnet/coreclr/pull/20186">dotnet/coreclr#20186</a>&nbsp;using it to make&nbsp;<code>await Task.Yield();</code>&nbsp;allocation-free:</p><div class="highlight highlight-source-cs"><pre class="lang:default decode:true">[Benchmark]
public async Task Yield()
{
    for (int i = 0; i &lt; 1_000_000; i++)
    {
        await Task.Yield();
    }
}</pre></div><table border="1"><thead><tr><th>Method</th><th>Toolchain</th><th align="right">Mean</th><th align="right">Error</th><th align="right">StdDev</th><th align="right">Ratio</th><th align="right">RatioSD</th><th align="right">Gen 0</th></tr></thead><tbody><tr><td>Yield</td><td>netcoreapp2.1</td><td align="right">581.3 ms</td><td align="right">11.615 ms</td><td align="right">30.39 ms</td><td align="right">1.00</td><td align="right">0.00</td><td align="right">19000.0000</td></tr><tr><td>Yield</td><td>netcoreapp3.0</td><td align="right">464.4 ms</td><td align="right">9.087 ms</td><td align="right">10.46 ms</td><td align="right">0.81</td><td align="right">0.06</td><td align="right">–</td></tr></tbody></table><p>It’s even utilized further in&nbsp;<code>Task</code>&nbsp;itself. There’s an interesting race condition that has to be handled in awaitables: what happens if the awaited operation completes after the call to&nbsp;<code>IsCompleted</code>&nbsp;but before the call to&nbsp;<code>OnCompleted</code>? As a reminder, the code:</p><p>compiles down to code along the lines of:</p><div class="highlight highlight-source-cs"><pre class="lang:default decode:true">var $awaiter = something.GetAwaiter();
if (!$awaiter.IsCompleted)
{
    _state = 42;
    AwaitOnCompleted(ref $awaiter);
    return;
}
Label42:
$awaiter.GetResult();</pre></div><p>Once we go down the path of&nbsp;<code>IsCompleted</code>&nbsp;having returned&nbsp;<code>false</code>, we’re going to call&nbsp;<code>AwaitOnCompleted</code>&nbsp;and return. If the operation has completed by the time we call&nbsp;<code>AwaitOnCompleted</code>, we don’t want to synchronously invoke the continuation that re-enters this state machine, as we’ll be doing so further down the stack, and if that happened repeatedly, we’d “stack dive” and could end up overflowing the stack. Instead, we’re forced to queue the continuation. This case isn’t the common case, but it happens more often than you might expect, as it simply requires an operation that completes asynchronously very quickly (various networking operations often fall into this category). As of PR&nbsp;<a href="https://github.com/dotnet/coreclr/pull/22373">dotnet/coreclr#22373</a>, the runtime now takes advantage of the async state machine box object implementing&nbsp;<code>IThreadPoolWorkItem</code>&nbsp;to avoid the allocations in this case as well!</p><p>In addition to&nbsp;<code>IThreadPoolWorkItem</code>&nbsp;being used with async/await to allow the async implementation to queue work items to the thread pool in a more allocation-friendly manner just as any other code can, changes were also made that give the&nbsp;<code>ThreadPool</code>&nbsp;1st-hand knowledge of the state machine box in order to help it optimize additional cases. PR&nbsp;<a href="https://github.com/dotnet/coreclr/pull/21159">dotnet/coreclr#21159</a>&nbsp;from @benaadams teaches the&nbsp;<code>ThreadPool</code>&nbsp;to re-route some&nbsp;<code>UnsafeQueueUserWorkItem(Action&lt;object&gt;, object, bool)</code>&nbsp;calls to instead use&nbsp;<code>UnsafeQueueUserWorkItem(IAsyncStateMachineBox, bool)</code>&nbsp;under the covers, so that higher-level libraries can get these allocation benefits without having to be aware of the box machinery.</p><p>Another async-related area that’s seen measurable improvements are&nbsp;<code>Timer</code>s. In .NET Core 2.1, some important improvements were made to&nbsp;<code>System.Threading.Timers</code>&nbsp;to help improve throughput and minimize contention for a common case where timers aren’t firing, but instead are quickly created and destroyed. And while those changes help a bit with the case when timers do actually fire, they didn’t help with the majority costs and sources of contention in that case, which is that potentially a lot of work (proportional to the number of timers registered) was done while holding locks. .NET Core 3.0 makes some big improvements here. PR&nbsp;<a href="https://github.com/dotnet/coreclr/pull/20302">dotnet/coreclr#20302</a>&nbsp;partitions the internal list of registered timers into two lists: one with timers that will soon fire and one with timers that won’t fire for a while. In most workloads that have a lot of registered timers, the majority of timers fall into the latter bucket at any given point in time, and this partitioning scheme enables the runtime to only consider the small bucket when firing timers most of the time. In doing so, it can significantly reduce the costs involved in firing timers, and as a result, also significantly reduce contention on the lock held while manipulating those lists. One customer who tried out these changes after having experienced issues due to tons of active timers had this to say about the impact:</p><blockquote><p><span>“We got the change in production yesterday and the results are amazing, with 99% reduction in lock contention. We have also measured 4-5% CPU gains, and more importantly 0.15% improvement in reliability for our service (which is huge!).”</span></p></blockquote><p>The nature of the scenario makes it a little difficult to see the impact in a Benchmark.NET benchmark, so we’ll do something a little different. Rather than measuring the thing that was actually changed, we’ll measure something else that’s indirectly impacted. In particular, these changes didn’t directly impact the performance of creating and destroying timers; in fact, one of the goals was to avoid doing so (in particular to avoid harming that important path). But by reducing the costs of firing timers, we reduce how long locks are held, which then also reduces the contention that the creating/destroying of timers faces. So, our benchmark creates a bunch of timers, ranging in when and how often they fire, and then we time how long it takes to create and destroy a bunch of additional timers.</p><div class="highlight highlight-source-cs"><pre class="lang:default decode:true">private Timer[] _timers;

[GlobalSetup]
public void Setup()
{
    _timers = new Timer[1_000_000];
    for (int i = 0; i &lt; _timers.Length; i++)
    {
        _timers[i] = new Timer(_ =&gt; { }, null, i, i);
    }
    Thread.Sleep(1000);
}

[Benchmark]
public void CreateDestroy()
{
    for (int i = 0; i &lt; 1_000; i++)
    {
        new Timer(_ =&gt; { }, 0, 100, 100).Dispose();
    }
}</pre></div><table border="1"><thead><tr><th>Method</th><th>Toolchain</th><th align="right">Mean</th><th align="right">Error</th><th align="right">StdDev</th><th align="right">Median</th><th align="right">Ratio</th><th align="right">RatioSD</th><th align="right">Gen 0</th></tr></thead><tbody><tr><td>CreateDestroy</td><td>netcoreapp2.1</td><td align="right">289.1 us</td><td align="right">7.131 us</td><td align="right">20.687 us</td><td align="right">282.8 us</td><td align="right">1.00</td><td align="right">0.00</td><td align="right">80.0781</td></tr><tr><td>CreateDestroy</td><td>netcoreapp3.0</td><td align="right">199.5 us</td><td align="right">3.983 us</td><td align="right">5.584 us</td><td align="right">199.2 us</td><td align="right">0.71</td><td align="right">0.04</td><td align="right">80.3223</td></tr></tbody></table><p><code>Timer</code>&nbsp;improvements have also taken other forms. For example, PR&nbsp;<a href="https://github.com/dotnet/coreclr/pull/22233">dotnet/coreclr#22233</a>&nbsp;from @benaadams shrinks the allocation involved in&nbsp;<code>Task.Delay</code>&nbsp;when used without a&nbsp;<code>CancellationToken</code>&nbsp;by 24 bytes, and PR&nbsp;<a href="https://github.com/dotnet/coreclr/pull/20509">dotnet/coreclr#20509</a>&nbsp;reduces the timer-related allocations involved in creating timed&nbsp;<code>CancellationTokenSource</code>s, which also has a nice effect on throughput:</p><div class="highlight highlight-source-cs"><pre class="lang:default decode:true">[Benchmark]
public void CTSTimer()
{
    using (var cts = new CancellationTokenSource())
        cts.CancelAfter(1_000_000);
}</pre></div><table border="1"><thead><tr><th>Method</th><th>Toolchain</th><th align="right">Mean</th><th align="right">Error</th><th align="right">StdDev</th><th align="right">Median</th><th align="right">Ratio</th><th align="right">RatioSD</th><th align="right">Gen 0</th><th align="right">Gen 1</th><th align="right">Gen 2</th><th align="right">Allocated</th></tr></thead><tbody><tr><td>CTSTimer</td><td>netcoreapp2.1</td><td align="right">231.3 ns</td><td align="right">6.293 ns</td><td align="right">16.018 ns</td><td align="right">224.8 ns</td><td align="right">1.00</td><td align="right">0.00</td><td align="right">0.0987</td><td align="right">–</td><td align="right">–</td><td align="right">208 B</td></tr><tr><td>CTSTimer</td><td>netcoreapp3.0</td><td align="right">115.3 ns</td><td align="right">1.769 ns</td><td align="right">1.655 ns</td><td align="right">115.0 ns</td><td align="right">0.46</td><td align="right">0.04</td><td align="right">0.0764</td><td align="right">–</td><td align="right">–</td><td align="right">160 B</td></tr></tbody></table><p>There are other even lower-level improvements that have gone into the release. For example, PR&nbsp;<a href="https://github.com/dotnet/coreclr/pull/21328">dotnet/coreclr#21328</a>&nbsp;from @benaadams improved&nbsp;<code>Thread.CurrentThread</code>&nbsp;by changing the implementation to store the relevant&nbsp;<code>Thread</code>&nbsp;in a&nbsp;<code>[ThreadStatic]</code>&nbsp;field rather than forcing&nbsp;<code>CurrentThread</code>&nbsp;to make an&nbsp;<code>InternalCall</code>&nbsp;into the native portions of the runtime.</p><div class="highlight highlight-source-cs"><pre class="lang:default decode:true">[Benchmark]
public Thread CurrentThread() =&gt; Thread.CurrentThread;</pre></div><table border="1"><thead><tr><th>Method</th><th>Toolchain</th><th align="right">Mean</th><th align="right">Error</th><th align="right">StdDev</th><th align="right">Ratio</th><th align="right">RatioSD</th></tr></thead><tbody><tr><td>CurrentThread</td><td>netcoreapp2.1</td><td align="right">6.101 ns</td><td align="right">0.2587 ns</td><td align="right">0.7547 ns</td><td align="right">1.00</td><td align="right">0.00</td></tr><tr><td>CurrentThread</td><td>netcoreapp3.0</td><td align="right">2.822 ns</td><td align="right">0.0439 ns</td><td align="right">0.0389 ns</td><td align="right">0.45</td><td align="right">0.04</td></tr></tbody></table><p>As other examples, PR&nbsp;<a href="https://github.com/dotnet/coreclr/pull/23747">dotnet/coreclr#23747</a>&nbsp;taught the runtime to better respect Docker –cpu limits, PRs&nbsp;<a href="https://github.com/dotnet/coreclr/pull/21722">dotnet/coreclr#21722</a>&nbsp;and&nbsp;<a href="https://github.com/dotnet/coreclr/pull/21586">dotnet/coreclr#21586</a>&nbsp;improved spinning behavior when contention was encountered across a variety of synchronization sites, PR&nbsp;<a href="https://github.com/dotnet/coreclr/pull/22686">dotnet/coreclr#22686</a>&nbsp;improved performance of&nbsp;<code>SemaphoreSlim</code>&nbsp;when consumers of an instance were mixing both synchronous&nbsp;<code>Wait</code>s and asynchronous&nbsp;<code>WaitAsync</code>s, and PR&nbsp;<a href="https://github.com/dotnet/coreclr/pull/18098">dotnet/coreclr#18098</a>&nbsp;from @Quogu special-cased&nbsp;<code>CancellationTokenSource</code>&nbsp;created with a timeout of 0 to avoid&nbsp;<code>Timer</code>-related costs.</p><h3>Collections</h3><p>Moving on from threading, let’s explore some of the performance improvements that have gone into collections. Collections are so commonly used in pretty much every program that they’ve received a lot of performance-focused attention in previous .NET Core releases. Even so, there continues to be areas for improvement. Here are some example such improvements in .NET Core 3.0.</p><p><code>ConcurrentDictionary&lt;TKey, TValue&gt;</code>&nbsp;has an&nbsp;<code>IsEmpty</code>&nbsp;property that states whether the dictionary is empty at that moment-in-time. In previous releases, it took all of the dictionary’s locks in order to get a proper moment-in-time answer. But as it turns out, those locks only need to be held if we think the collection might be empty: if we see anything in any of the dictionary’s internals buckets, the locks aren’t needed, as we’d stop looking at additional buckets anyway the moment we found one bucket to contain anything. Thus, PR&nbsp;<a href="https://github.com/dotnet/corefx/pull/30098">dotnet/corefx#30098</a>&nbsp;from @drewnoakes added a fast path that first checks each bucket without the locks, in order to optimize for the common case where the dictionary isn’t empty (the impact on the case where the dictionary is empty is minimal).</p><div class="highlight highlight-source-cs"><pre class="lang:default decode:true">private ConcurrentDictionary&lt;int, int&gt; _cd;

[GlobalSetup]
public void Setup()
{
    _cd = new ConcurrentDictionary&lt;int, int&gt;();
    _cd.TryAdd(1, 1);
}

[Benchmark] public bool IsEmpty() =&gt; _cd.IsEmpty;</pre></div><table border="1"><thead><tr><th>Method</th><th>Toolchain</th><th align="right">Mean</th><th align="right">Error</th><th align="right">StdDev</th><th align="right">Ratio</th></tr></thead><tbody><tr><td>IsEmpty</td><td>netcoreapp2.1</td><td align="right">73.675 ns</td><td align="right">0.3934 ns</td><td align="right">0.3285 ns</td><td align="right">1.00</td></tr><tr><td>IsEmpty</td><td>netcoreapp3.0</td><td align="right">3.160 ns</td><td align="right">0.0402 ns</td><td align="right">0.0356 ns</td><td align="right">0.04</td></tr></tbody></table><p><code>ConcurrentDictionary</code>&nbsp;wasn’t the only concurrent collection to get some attention. An improvement came to&nbsp;<code>ConcurrentQueue&lt;T&gt;</code>&nbsp;in&nbsp;<a href="https://github.com/dotnet/coreclr/pull/18035">dotnet/coreclr#18035</a>, and it’s an interesting example in how performance optimization often is a trade-off between scenarios. In .NET Core 2.0, we overhauled the&nbsp;<code>ConcurrentQueue</code>&nbsp;implementation in a way that significantly improved throughput while also significantly reducing memory allocations, turning the&nbsp;<code>ConcurrentQueue</code>&nbsp;into a linked list of circular arrays. However, the change involved a concession: because of the producer/consumer nature of the arrays, if any operation needed to observe data in-place in a segment (rather than dequeueing it), the segment that was observed would be “frozen” for any further enqueues… this was to avoid problems where, for example, one thread was enumerating the contents of the segment while another thread was enqueueing and dequeueing. When there were multiple segments in the queue, accessing&nbsp;<code>Count</code>&nbsp;ended up being treated as an observation, but that meant that simply accessing the&nbsp;<code>ConcurrentQueue</code>‘s&nbsp;<code>Count</code>&nbsp;would render all of the multiple segments in the queue dead for further enqueues. The theory at the time was that such a trade-off was fine, because no one should be accessing the&nbsp;<code>Count</code>&nbsp;of the queue frequently enough for this to matter. That theory was wrong, and several customers reported significant slowdowns in their workloads because they were accessing the&nbsp;<code>Count</code>&nbsp;on every enqueue or dequeue. While the right solution is in general to avoid doing that, we wanted to fix this, and as it turns out, the fix was relatively straightforward, such that we could have our performance cake and eat it, too. The results are very obvious in the following benchmark.</p><div class="highlight highlight-source-cs"><pre class="lang:default decode:true">private ConcurrentQueue&lt;int&gt; _cq;

[GlobalSetup]
public void Setup()
{
    _cq = new ConcurrentQueue&lt;int&gt;();
    for (int i = 0; i &lt; 100; i++)
    {
        _cq.Enqueue(i);
    }
}

[Benchmark]
public void EnqueueCountDequeue()
{
    _cq.Enqueue(42);
    _ = _cq.Count;
    _cq.TryDequeue(out _);
}</pre></div><table border="1"><thead><tr><th>Method</th><th>Toolchain</th><th align="right">Mean</th><th align="right">Error</th><th align="right">StdDev</th><th align="right">Ratio</th><th align="right">Gen 0</th><th align="right">Gen 1</th><th align="right">Gen 2</th><th align="right">Allocated</th></tr></thead><tbody><tr><td>EnqueueCountDequeue</td><td>netcoreapp2.1</td><td align="right">708.48 ns</td><td align="right">23.8638 ns</td><td align="right">21.1546 ns</td><td align="right">1.00</td><td align="right">0.1669</td><td align="right">0.0830</td><td align="right">0.0010</td><td align="right">704 B</td></tr><tr><td>EnqueueCountDequeue</td><td>netcoreapp3.0</td><td align="right">22.79 ns</td><td align="right">0.4471 ns</td><td align="right">0.4182 ns</td><td align="right">0.03</td><td align="right">–</td><td align="right">–</td><td align="right">–</td><td align="right">–</td></tr></tbody></table><p><code>ImmutableDictionary&lt;TKey, TValue&gt;</code>&nbsp;also got some attention. A customer reported that they’d compared&nbsp;<code>ImmutableDictionary&lt;TKey, TValue&gt;</code>&nbsp;and&nbsp;<code>Dictionary&lt;TKey, TValue&gt;</code>&nbsp;and found the former to be measurably slower for lookups. This is to be expected, as the types use very different data structures, with&nbsp;<code>ImmutableDictionary</code>&nbsp;optimized for being able to inexpensively create a copy of the dictionary with a mutation, something that’s quite expensive to do with&nbsp;<code>Dictionary</code>; the trade-off is that it ends up being slower for lookups. Still, it caused us to take a look at the costs involved in&nbsp;<code>ImmutableDictionary</code>&nbsp;lookups, and PR&nbsp;<a href="https://github.com/dotnet/corefx/pull/35759">dotnet/corefx#35759</a>&nbsp;includes several tweaks to improve it, changing a recursive call to be non-recursive and inlinable and avoiding some unnecessary struct wrapping. While this doesn’t make&nbsp;<code>ImmutableDictionary</code>&nbsp;and&nbsp;<code>Dictionary</code>&nbsp;lookups equivalent, it does improve&nbsp;<code>ImmutableDictionary</code>&nbsp;measurably, especially when it contains just a few elements.</p><div class="highlight highlight-source-cs"><pre class="lang:default decode:true">private ImmutableDictionary&lt;int, int&gt; _hundredInts;

[GlobalSetup]
public void Setup()
{
    _hundredInts = ImmutableDictionary.Create&lt;int, int&gt;();
    for (int i = 0; i &lt; 100; i++)
    {
        _hundredInts = _hundredInts.Add(i, i);
    }
}

[Benchmark]
public int Lookup()
{
    int count = 0;
    {
        for (int i = 0; i &lt; 100; i++)
        {
            for (int j = 0; j &lt; 100; j++)
            {
                if (_hundredInts.TryGetValue(j, out _))
                {
                    count++;
                }
            }
        }
    }
    return count;
}</pre></div><table border="1"><thead><tr><th>Method</th><th>Toolchain</th><th align="right">Mean</th><th align="right">Error</th><th align="right">StdDev</th><th align="right">Median</th><th align="right">Ratio</th><th align="right">RatioSD</th></tr></thead><tbody><tr><td>Lookup</td><td>netcoreapp2.1</td><td align="right">303.9 us</td><td align="right">7.271 us</td><td align="right">15.016 us</td><td align="right">297.8 us</td><td align="right">1.00</td><td align="right">0.00</td></tr><tr><td>Lookup</td><td>netcoreapp3.0</td><td align="right">174.5 us</td><td align="right">3.360 us</td><td align="right">2.806 us</td><td align="right">174.5 us</td><td align="right">0.57</td><td align="right">0.03</td></tr></tbody></table><p>Another collection that’s seen measurable improvements in .NET Core 3.0 is&nbsp;<code>BitArray</code>. Lots of operations, including construction, were optimized in PR&nbsp;<a href="https://github.com/dotnet/corefx/pull/33367">dotnet/corefx#33367</a>.</p><div class="highlight highlight-source-cs"><pre class="lang:default decode:true">private byte[] _bytes = Enumerable.Range(0, 100).Select(i =&gt; (byte)i).ToArray();

[Benchmark]
public BitArray BitArrayCtor() =&gt; new BitArray(_bytes);</pre></div><table border="1"><thead><tr><th>Method</th><th>Toolchain</th><th align="right">Mean</th><th align="right">Error</th><th align="right">StdDev</th><th align="right">Median</th><th align="right">Ratio</th><th align="right">RatioSD</th></tr></thead><tbody><tr><td>BitArrayCtor</td><td>netcoreapp2.1</td><td align="right">82.28 ns</td><td align="right">2.601 ns</td><td align="right">7.546 ns</td><td align="right">77.89 ns</td><td align="right">1.00</td><td align="right">0.00</td></tr><tr><td>BitArrayCtor</td><td>netcoreapp3.0</td><td align="right">46.87 ns</td><td align="right">2.738 ns</td><td align="right">8.030 ns</td><td align="right">44.63 ns</td><td align="right">0.57</td><td align="right">0.10</td></tr></tbody></table><p>Core operations like&nbsp;<code>Set</code>&nbsp;and&nbsp;<code>Get</code>&nbsp;were further improved in PR&nbsp;<a href="https://github.com/dotnet/corefx/pull/35364">dotnet/corefx#35364</a>&nbsp;from @omariom by streamlining the relevant methods and making them inlineable</p><div class="highlight highlight-source-cs"><pre class="wrap:false lang:default decode:true">private BitArray _ba = new BitArray(Enumerable.Range(0, 1000).Select(i =&gt; i % 2 == 0).ToArray());

[Benchmark]
public void GetSet()
{
    BitArray ba = _ba;
    for (int i = 0; i &lt; 1000; i++)
    {
        ba.Set(i, !ba.Get(i));
    }
}</pre></div><table border="1"><thead><tr><th>Method</th><th>Toolchain</th><th align="right">Mean</th><th align="right">Error</th><th align="right">StdDev</th><th align="right">Ratio</th></tr></thead><tbody><tr><td>GetSet</td><td>netcoreapp2.1</td><td align="right">6.497 us</td><td align="right">0.0854 us</td><td align="right">0.0713 us</td><td align="right">1.00</td></tr><tr><td>GetSet</td><td>netcoreapp3.0</td><td align="right">2.049 us</td><td align="right">0.0233 us</td><td align="right">0.0218 us</td><td align="right">0.32</td></tr></tbody></table><p>while other operations like&nbsp;<code>Or</code>,&nbsp;<code>And</code>, and&nbsp;<code>Xor</code>&nbsp;were vectorized in PR&nbsp;<a href="https://github.com/dotnet/corefx/pull/33781">dotnet/corefx#33781</a>. This benchmark highlights some of the wins.</p><div class="highlight highlight-source-cs"><pre class="lang:default decode:true">private BitArray _ba1 = new BitArray(Enumerable.Range(0, 1000).Select(i =&gt; i % 2 == 0).ToArray());
private BitArray _ba2 = new BitArray(Enumerable.Range(0, 1000).Select(i =&gt; i % 2 == 1).ToArray());

[Benchmark]
public void Xor() =&gt; _ba1.Xor(_ba2);</pre></div><table border="1"><thead><tr><th>Method</th><th>Toolchain</th><th align="right">Mean</th><th align="right">Error</th><th align="right">StdDev</th><th align="right">Ratio</th></tr></thead><tbody><tr><td>Xor</td><td>netcoreapp2.1</td><td align="right">28.57 ns</td><td align="right">0.4086 ns</td><td align="right">0.3822 ns</td><td align="right">1.00</td></tr><tr><td>Xor</td><td>netcoreapp3.0</td><td align="right">10.92 ns</td><td align="right">0.0924 ns</td><td align="right">0.0772 ns</td><td align="right">0.38</td></tr></tbody></table><p>Another example:&nbsp;<code>SortedSet&lt;T&gt;</code>. PR&nbsp;<a href="https://github.com/dotnet/corefx/pull/30921">dotnet/corefx#30921</a>&nbsp;from @acerbusace tweaks how&nbsp;<code>GetViewBetween</code>&nbsp;changes how counts of the overall set and subset are managed, resulting in a nice performance boost.</p><div class="highlight highlight-source-cs"><pre class="lang:default decode:true">private SortedSet&lt;int&gt; _set = new SortedSet&lt;int&gt;(Enumerable.Range(0, 1000));

[Benchmark]
public int EnumerateViewBetween()
{
    int count = 0;
    foreach (int item in _set.GetViewBetween(100, 200)) count++;
    return count;
}</pre></div><table border="1"><thead><tr><th>Method</th><th>Toolchain</th><th align="right">Mean</th><th align="right">Error</th><th align="right">StdDev</th><th align="right">Ratio</th><th align="right">Gen 0</th><th align="right">Gen 1</th><th align="right">Gen 2</th><th align="right">Allocated</th></tr></thead><tbody><tr><td>EnumerateViewBetween</td><td>netcoreapp2.1</td><td align="right">5.117 us</td><td align="right">0.0590 us</td><td align="right">0.0552 us</td><td align="right">1.00</td><td align="right">0.2518</td><td align="right">–</td><td align="right">–</td><td align="right">544 B</td></tr><tr><td>EnumerateViewBetween</td><td>netcoreapp3.0</td><td align="right">2.510 us</td><td align="right">0.0307 us</td><td align="right">0.0287 us</td><td align="right">0.49</td><td align="right">0.1373</td><td align="right">–</td><td align="right">–</td><td align="right">288 B</td></tr></tbody></table><p>Comparers have also seen some nice improvements in .NET Core 3.0. For example, PR&nbsp;<a href="https://github.com/dotnet/coreclr/pull/21604">dotnet/coreclr#21604</a>&nbsp;overhauled how comparers for enums are implemented in the runtime, borrowing the approach used in CoreRT. It’s often the case that performance optimizations involve adding code; this is one of those fortuitous cases where the better approach is not only faster, it’s also simpler and smaller.</p><div class="highlight highlight-source-cs"><pre class="lang:default decode:true">private enum ExampleEnum : byte { A, B }

[Benchmark]
public void CompareEnums()
{
    var comparer = Comparer&lt;ExampleEnum&gt;.Default;
    for (int i = 0; i &lt; 100_000_000; i++)
    {
        comparer.Compare(ExampleEnum.A, ExampleEnum.B);
    }
}</pre></div><table border="1"><thead><tr><th>Method</th><th>Toolchain</th><th align="right">Mean</th><th align="right">Error</th><th align="right">StdDev</th><th align="right">Ratio</th><th align="right">RatioSD</th></tr></thead><tbody><tr><td>CompareEnums</td><td>netcoreapp2.1</td><td align="right">239.5 ms</td><td align="right">10.130 ms</td><td align="right">10.403 ms</td><td align="right">1.00</td><td align="right">0.00</td></tr><tr><td>CompareEnums</td><td>netcoreapp3.0</td><td align="right">131.7 ms</td><td align="right">2.479 ms</td><td align="right">2.319 ms</td><td align="right">0.55</td><td align="right">0.03</td></tr></tbody></table><h3>Networking</h3><p>From the Kestrel web server running on&nbsp;<code>System.Net.Sockets</code>&nbsp;and&nbsp;<code>System.Net.Security</code>&nbsp;to applications accessing web services via&nbsp;<code>HttpClient</code>,&nbsp;<code>System.Net</code>&nbsp;now more than ever is critical path for many applications. It received a lot of attention in .NET Core 2.1, and continues to in .NET Core 3.0.</p><p>Let’s start with&nbsp;<code>HttpClient</code>. One improvement made in PR&nbsp;<a href="https://github.com/dotnet/corefx/pull/32820">dotnet/corefx#32820</a>&nbsp;was around how buffering is handled, and in particular better respecting larger buffer size requests made as part of copying the response data when a content length was provided by the server. On a fast connection and with a large response body (such as the 10MB in this example), this can make a sizeable difference in throughput due to reduced syscalls to transfer data.</p><div class="highlight highlight-source-cs"><pre class="wrap:false lang:default decode:true">private HttpClient _client = new HttpClient();
private Socket _listener;
private Uri _uri;

[GlobalSetup]
public void Setup()
{
    _listener = new Socket(AddressFamily.InterNetwork, SocketType.Stream, ProtocolType.Tcp);
    _listener.Bind(new IPEndPoint(IPAddress.Loopback, 0));
    _listener.Listen(int.MaxValue);
    var ep = (IPEndPoint)_listener.LocalEndPoint;
    _uri = new Uri($"http://{ep.Address}:{ep.Port}");

    Task.Run(async () =&gt;
    {
        while (true)
        {
            Socket s = await _listener.AcceptAsync();
            var ignored = Task.Run(async () =&gt;
            {
                ReadOnlyMemory&lt;byte&gt; headers = Encoding.ASCII.GetBytes("HTTP/1.1 200 OK\r\nContent-Length: 10485760\r\n\r\n");
                ReadOnlyMemory&lt;byte&gt; data = new byte[10*1024*1024]; // 10485760

                using (var serverStream = new NetworkStream(s, true))
                using (var reader = new StreamReader(serverStream))
                {
                    while (true)
                    {
                        while (!string.IsNullOrEmpty(await reader.ReadLineAsync())) ;
                        await s.SendAsync(headers, SocketFlags.None);
                        await s.SendAsync(data, SocketFlags.None);
                    }
                }
            });
        }
    });
}

[Benchmark]
public async Task HttpDownload()
{
    using (HttpResponseMessage r = await _client.GetAsync(_uri, HttpCompletionOption.ResponseHeadersRead))
    using (Stream s = await r.Content.ReadAsStreamAsync())
    {
        await s.CopyToAsync(Stream.Null);
    }
}</pre></div><table border="1"><thead><tr><th>Method</th><th>Toolchain</th><th align="right">Mean</th><th align="right">Error</th><th align="right">StdDev</th><th align="right">Ratio</th><th align="right">RatioSD</th></tr></thead><tbody><tr><td>HttpDownload</td><td>netcoreapp2.1</td><td align="right">8.792 ms</td><td align="right">0.1833 ms</td><td align="right">0.3397 ms</td><td align="right">1.00</td><td align="right">0.00</td></tr><tr><td>HttpDownload</td><td>netcoreapp3.0</td><td align="right">4.615 ms</td><td align="right">0.0356 ms</td><td align="right">0.0278 ms</td><td align="right">0.52</td><td align="right">0.02</td></tr></tbody></table><p>Now consider&nbsp;<code>SslStream</code>. Previous releases saw work done to make reads and writes on&nbsp;<code>SslStream</code>&nbsp;much more efficient, but additional work was done in .NET Core 3.0 as part of PRs&nbsp;<a href="https://github.com/dotnet/corefx/pull/35091">dotnet/corefx#35091</a>&nbsp;and&nbsp;<a href="https://github.com/dotnet/corefx/pull/35209">dotnet/corefx#35209</a>&nbsp;(and&nbsp;<a href="https://github.com/dotnet/corefx/pull/35367">dotnet/corefx#35367</a>&nbsp;on Unix) to make initiating the connection more efficient, in particular in terms of allocations.</p><div class="highlight highlight-source-cs"><pre class="wrap:false lang:default decode:true">private NetworkStream _client;
private NetworkStream _server;

private static X509Certificate2 s_cert = GetServerCertificate();

private static X509Certificate2 GetServerCertificate()
{
    var certCollection = new X509Certificate2Collection();
    byte[] testCertBytes = Convert.FromBase64String(@"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");
    certCollection.Import(testCertBytes, "testcertificate", X509KeyStorageFlags.DefaultKeySet);
    return certCollection.Cast&lt;X509Certificate2&gt;().First(c =&gt; c.HasPrivateKey);
}

[GlobalSetup]
public void Setup()
{
    using (var listener = new Socket(AddressFamily.InterNetwork, SocketType.Stream, ProtocolType.Tcp))
    {
        listener.Bind(new IPEndPoint(IPAddress.Loopback, 0));
        listener.Listen(1);

        var client = new Socket(AddressFamily.InterNetwork, SocketType.Stream, ProtocolType.Tcp);
        client.Connect(listener.LocalEndPoint);
        Socket server = listener.Accept();

        _client = new NetworkStream(client);
        _server = new NetworkStream(server);
    }
}

[Benchmark]
public void SslConnect()
{
    using (var sslClient = new SslStream(_client, true, delegate { return true; }))
    using (var sslServer = new SslStream(_server, true, delegate { return true; }))
    {
        Task t = sslServer.AuthenticateAsServerAsync(s_cert, false, SslProtocols.None, false);
        sslClient.AuthenticateAsClient("localhost", null, SslProtocols.None, false);
        t.Wait();
    }
}</pre></div><table border="1"><thead><tr><th>Method</th><th>Toolchain</th><th align="right">Mean</th><th align="right">Error</th><th align="right">StdDev</th><th align="right">Ratio</th><th align="right">RatioSD</th><th align="right">Gen 0</th><th align="right">Gen 1</th><th align="right">Gen 2</th><th align="right">Allocated</th></tr></thead><tbody><tr><td>SslConnect</td><td>netcoreapp2.1</td><td align="right">1,151.7 us</td><td align="right">34.85 us</td><td align="right">102.76 us</td><td align="right">1.00</td><td align="right">0.00</td><td align="right">5.8594</td><td align="right">–</td><td align="right">–</td><td align="right">9.82 KB</td></tr><tr><td>SslConnect</td><td>netcoreapp3.0</td><td align="right">915.5 us</td><td align="right">17.73 us</td><td align="right">26.54 us</td><td align="right">0.80</td><td align="right">0.08</td><td align="right">1.9531</td><td align="right">–</td><td align="right">–</td><td align="right">4.13 KB</td></tr></tbody></table><p>In&nbsp;<code>System.Net.Sockets</code>&nbsp;there’s another example of taking advantage of the&nbsp;<code>IThreadPoolWorkItem</code>&nbsp;interface discussed earlier. On Windows for asynchronous operations, we utilize “overlapped I/O”, utilizing threads from the I/O thread pool to execute continuations from socket operations; Windows queues I/O completion packets that these I/O pool threads then process, including invoking the continuations. On Unix, however, the mechanism is very different. There’s no concept of “overlapped I/O” on Unix, and instead asynchrony in&nbsp;<code>System.Net.Sockets</code>&nbsp;is achieved by using&nbsp;<code>epoll</code>&nbsp;(or&nbsp;<code>kqueues</code>&nbsp;on macOS), with all of the sockets in the system registered with an&nbsp;<code>epoll</code>&nbsp;file descriptor, and then one thread monitoring that&nbsp;<code>epoll</code>&nbsp;for changes. Any time an asynchronous operation completes for a socket, the&nbsp;<code>epoll</code>&nbsp;is signaled and the thread blocking on it wakes up to process it. If that thread were to run the socket continuation action then and there, it would end up potentially running unbounded work that could stall every other socket’s handling indefinitely, and in the extreme case, deadlock. Instead, this thread queues a work item back to the thread pool and then immediately goes back to processing any other socket work. Prior to .NET Core 3.0, that queueing involved an allocation, which meant that every asynchronously completing socket operation on Unix involved at least one allocation. As of PR&nbsp;<a href="https://github.com/dotnet/corefx/pull/32919">dotnet/corefx#32919</a>, that number drops to zero, as a cached object already being used (and reused) to represent asynchronous operations was changed to also implement&nbsp;<code>IThreadPoolWorkItem</code>&nbsp;and be queueable directly to the thread pool.</p><p>Other areas of&nbsp;<code>System.Net</code>&nbsp;have benefited from the efforts already alluded to previously, as well. For example,&nbsp;<code>Dns.GetHostName</code>&nbsp;used to use&nbsp;<code>StringBuilder</code>&nbsp;in its marshaling, but as of PR&nbsp;<a href="https://github.com/dotnet/corefx/pull/29594">dotnet/corefx#29594</a> it no longer does.</p><div class="highlight highlight-source-cs"><pre class="lang:default decode:true">[Benchmark]
public string GetHostName() =&gt; Dns.GetHostName();</pre></div><table border="1"><thead><tr><th>Method</th><th>Toolchain</th><th align="right">Mean</th><th align="right">Error</th><th align="right">StdDev</th><th align="right">Ratio</th><th align="right">RatioSD</th><th align="right">Gen 0</th><th align="right">Gen 1</th><th align="right">Gen 2</th><th align="right">Allocated</th></tr></thead><tbody><tr><td>GetHostName</td><td>netcoreapp2.1</td><td align="right">85.77 us</td><td align="right">1.656 us</td><td align="right">1.5489 us</td><td align="right">1.00</td><td align="right">0.00</td><td align="right">0.4883</td><td align="right">–</td><td align="right">–</td><td align="right">1176 B</td></tr><tr><td>GetHostName</td><td>netcoreapp3.0</td><td align="right">81.42 us</td><td align="right">1.016 us</td><td align="right">0.9503 us</td><td align="right">0.95</td><td align="right">0.02</td><td align="right">–</td><td align="right">–</td><td align="right">–</td><td align="right">48 B</td></tr></tbody></table><p>And&nbsp;<code>IPAddress.HostToNetworkOrder/NetworkToHostOrder</code>&nbsp;have benefiting indirectly from the intrinsics push that was mentioned previously. In .NET Core 2.1,&nbsp;<code>BinaryPrimitives.ReverseEndianness</code>&nbsp;was added with an optimized software implementation, and these<code>&nbsp;IPAddress</code>&nbsp;methods were rewritten as simple wrappers for&nbsp;<code>ReverseEndianness</code>. Now in .NET Core 3.0, PR&nbsp;<a href="https://github.com/dotnet/coreclr/pull/18398">dotnet/coreclr#18398</a>&nbsp;turned&nbsp;<code>ReverseEndianness</code>&nbsp;into a JIT intrinsic for which the JIT can emit a very efficient&nbsp;<code>BSWAP</code>&nbsp;instruction, with the resulting throughput improvements accruing to&nbsp;<code>IPAddress</code>&nbsp;as well.</p><div class="highlight highlight-source-cs"><pre class="lang:default decode:true">private long _value = 1234567890123456789;

[Benchmark]
public long HostToNetworkOrder() =&gt; IPAddress.HostToNetworkOrder(_value);</pre></div><table border="1"><thead><tr><th>Method</th><th>Toolchain</th><th align="right">Mean</th><th align="right">Error</th><th align="right">StdDev</th><th align="right">Median</th><th align="right">Ratio</th><th align="right">RatioSD</th></tr></thead><tbody><tr><td>HostToNetworkOrder</td><td>netcoreapp2.1</td><td align="right">0.4986 ns</td><td align="right">0.0398 ns</td><td align="right">0.0408 ns</td><td align="right">0.4758 ns</td><td align="right">1.000</td><td align="right">0.00</td></tr><tr><td>HostToNetworkOrder</td><td>netcoreapp3.0</td><td align="right">0.0043 ns</td><td align="right">0.0090 ns</td><td align="right">0.0076 ns</td><td align="right">0.0000 ns</td><td align="right">0.009</td><td align="right">0.02</td></tr></tbody></table><h3></h3><h3>System.IO</h3><p>Often going hand in hand with networking is compression, which has also seen some improvements in .NET Core 3.0. Most notably is that a key dependency was updated. On Unix,&nbsp;<code>System.IO.Compression</code>&nbsp;just uses the zlib library available on the machine, as it’s a standard part of most any distro/version. On Windows, however, zlib is generally nowhere to be found, and so it’s built and shipped as part of .NET Core on Windows. Rather than shipping the standard zlib, .NET Core includes a version modified by Intel with additional performance improvements not yet merged upstream. In .NET Core 3.0, we’ve sync’d to the latest available version of ZLib-Intel, version 1.2.11. This brings some very measurable performance improvements, in particular around decompression.</p><p>There have also been compression-related improvements that take advantage of previous improvements elsewhere in .NET Core. For example, the synchronous&nbsp;<code>Stream.CopyTo</code>&nbsp;was originally non-virtual, but as gains were found by overriding the asynchronous&nbsp;<code>CopyToAsync</code>&nbsp;and specializing its implementation for particular concrete stream types,&nbsp;<code>CopyTo</code>&nbsp;was made virtual to enjoy similar improvements. PR&nbsp;<a href="https://github.com/dotnet/corefx/pull/29751">dotnet/corefx#29751</a>&nbsp;capitalized on this to override&nbsp;<code>CopyTo</code>&nbsp;on&nbsp;<code>DeflateStream</code>, employing similar optimizations in the synchronous implementation as were employed in the asynchronous implementation, essentially entailing minimizing the interop costs with zlib.</p><div class="highlight highlight-source-cs"><pre class="wrap:false lang:default decode:true">private byte[] _compressed;

[GlobalSetup]
public void Setup()
{
    var ms = new MemoryStream();
    using (var ds = new DeflateStream(ms, CompressionLevel.Fastest))
    {
        ds.Write(Enumerable.Range(0, 1_000_000).Select(i =&gt; (byte)i).ToArray(), 0, 1_000_000);
    }
    _compressed = ms.ToArray();
}

[Benchmark]
public void DeflateDecompress()
{
    using (var ds = new DeflateStream(new MemoryStream(_compressed), CompressionMode.Decompress))
    {
        ds.CopyTo(Stream.Null);
    }
}</pre></div><table border="1"><thead><tr><th>Method</th><th>Toolchain</th><th align="right">Mean</th><th align="right">Error</th><th align="right">StdDev</th><th align="right">Ratio</th></tr></thead><tbody><tr><td>DeflateDecompress</td><td>netcoreapp2.1</td><td align="right">310.6 us</td><td align="right">1.960 us</td><td align="right">1.6367 us</td><td align="right">1.00</td></tr><tr><td>DeflateDecompress</td><td>netcoreapp3.0</td><td align="right">144.9 us</td><td align="right">1.050 us</td><td align="right">0.9819 us</td><td align="right">0.47</td></tr></tbody></table><p>Improvements were also made to&nbsp;<code>BrotliStream</code>&nbsp;(which as of .NET Core 3.0 is also used by&nbsp;<code>HttpClient</code>&nbsp;to automatically decompress Brotli-encoded content). Previously every new&nbsp;<code>BrotliStream</code>&nbsp;would also allocate a large buffer, but as of PR&nbsp;<a href="https://github.com/dotnet/corefx/pull/35492">dotnet/corefx#35492</a>, that buffer is pooled, as it is with&nbsp;<code>DeflateStream</code>&nbsp;(additionally,&nbsp;<code>BrotliStream</code>&nbsp;now as of PR&nbsp;<a href="https://github.com/dotnet/corefx/pull/30135">dotnet/corefx#30135</a>&nbsp;overrides&nbsp;<code>ReadByte</code>&nbsp;and&nbsp;<code>WriteByte</code>&nbsp;to avoid allocations in the base implementation).</p><div class="highlight highlight-source-cs"><pre class="lang:default decode:true">[Benchmark]
public void BrotliWrite()
{
    using (var bs = new BrotliStream(Stream.Null, CompressionLevel.Fastest))
    {
        for (int i = 0; i &lt; 1_000; i++)
        {
            bs.WriteByte((byte)i);
        }
    }
}</pre></div><table border="1"><thead><tr><th>Method</th><th>Toolchain</th><th align="right">Mean</th><th align="right">Error</th><th align="right">StdDev</th><th align="right">Ratio</th><th align="right">Gen 0</th><th align="right">Gen 1</th><th align="right">Gen 2</th><th align="right">Allocated</th></tr></thead><tbody><tr><td>BrotliWrite</td><td>netcoreapp2.1</td><td align="right">743.2 us</td><td align="right">10.056 us</td><td align="right">9.406 us</td><td align="right">1.00</td><td align="right">44.9219</td><td align="right">–</td><td align="right">–</td><td align="right">97680 B</td></tr><tr><td>BrotliWrite</td><td>netcoreapp3.0</td><td align="right">575.5 us</td><td align="right">9.181 us</td><td align="right">8.588 us</td><td align="right">0.77</td><td align="right">–</td><td align="right">–</td><td align="right">–</td><td align="right">136 B</td></tr></tbody></table><p>Moving on from compression, it’s worth highlighting that formatting applies in more situations than just formatting individual primitives.&nbsp;<code>TextWriter</code>, for example, has multiple methods for writing with format strings, e.g.&nbsp;<code>public override void Write(string format, object arg0, arg1)</code>. PR&nbsp;<a href="https://github.com/dotnet/coreclr/pull/19235">dotnet/coreclr#19235</a>&nbsp;improved on that for StreamWriter by providing specialized overrides that take a more efficient path that reduces allocation:</p><div class="highlight highlight-source-cs"><pre class="lang:default decode:true">private StreamWriter _writer = new StreamWriter(Stream.Null);

[Benchmark]
public void StreamWriterFormat() =&gt; _writer.Write("Writing out a value: {0}", 42);</pre></div><table border="1"><thead><tr><th>Method</th><th>Toolchain</th><th align="right">Mean</th><th align="right">Error</th><th align="right">StdDev</th><th align="right">Ratio</th><th align="right">Gen 0</th><th align="right">Gen 1</th><th align="right">Gen 2</th><th align="right">Allocated</th></tr></thead><tbody><tr><td>StreamWriterFormat</td><td>netcoreapp2.1</td><td align="right">207.4 ns</td><td align="right">2.103 ns</td><td align="right">1.864 ns</td><td align="right">1.00</td><td align="right">0.0455</td><td align="right">–</td><td align="right">–</td><td align="right">96 B</td></tr><tr><td>StreamWriterFormat</td><td>netcoreapp3.0</td><td align="right">170.2 ns</td><td align="right">1.800 ns</td><td align="right">1.595 ns</td><td align="right">0.82</td><td align="right">0.0114</td><td align="right">–</td><td align="right">–</td><td align="right">24 B</td></tr></tbody></table><p>As another example, PR&nbsp;<a href="https://github.com/dotnet/coreclr/pull/22102">dotnet/coreclr#22102</a>&nbsp;from @TomerWeisberg improved the parsing performance of various primitive types on&nbsp;<code>BinaryReader</code>&nbsp;by special-casing the common situation where the&nbsp;<code>BinaryReader</code> wraps a&nbsp;<code>MemoryStream</code>.</p><p>Or consider PR&nbsp;<a href="https://github.com/dotnet/corefx/pull/30667">dotnet/corefx#30667</a>&nbsp;from @MarcoRossignoli, who added overrides to&nbsp;<code>StringWriter</code>&nbsp;for the&nbsp;<code>Write{Line}{Async}</code>&nbsp;methods that take a&nbsp;<code>StringBuilder</code>&nbsp;argument.&nbsp;<code>StringWriter</code>&nbsp;is just a wrapper around a&nbsp;<code>StringBuilder</code>, and&nbsp;<code>StringBuilder</code>&nbsp;knows how to append another&nbsp;<code>StringBuilder</code>&nbsp;to it, so these overrides on&nbsp;<code>StringWriter</code>&nbsp;can feed them right through.</p><div class="highlight highlight-source-cs"><pre class="lang:default decode:true">private StringBuilder _underlying;
private StringWriter _writer;
private StringBuilder _sb;

[GlobalSetup]
public void Setup()
{
    _underlying = new StringBuilder();
    _writer = new StringWriter(_underlying);
    _sb = new StringBuilder("This is a test. This is only a test.");
}

[Benchmark]
public void Write()
{
    _underlying.Clear();
    _writer.Write(_sb);
}</pre></div><table border="1"><thead><tr><th>Method</th><th>Toolchain</th><th align="right">Mean</th><th align="right">Error</th><th align="right">StdDev</th><th align="right">Ratio</th><th align="right">Gen 0</th><th align="right">Gen 1</th><th align="right">Gen 2</th><th align="right">Allocated</th></tr></thead><tbody><tr><td>Write</td><td>netcoreapp2.1</td><td align="right">30.15 ns</td><td align="right">0.6065 ns</td><td align="right">0.5673 ns</td><td align="right">1.00</td><td align="right">0.0495</td><td align="right">–</td><td align="right">–</td><td align="right">104 B</td></tr><tr><td>Write</td><td>netcoreapp3.0</td><td align="right">18.57 ns</td><td align="right">0.1513 ns</td><td align="right">0.1416 ns</td><td align="right">0.62</td><td align="right">–</td><td align="right">–</td><td align="right">–</td><td align="right">–</td></tr></tbody></table><p><code>System.IO.Pipelines</code>&nbsp;is another IO-related library that’s received a lot of attention in .NET Core 3.0. Pipelines was introduced in .NET Core 2.1, and provides buffer-management as part of an I/O pipeline, used heavily by ASP.NET Core. A variety of PRs have gone into improving its performance. For example, PR&nbsp;<a href="https://github.com/dotnet/corefx/pull/35171">dotnet/corefx#35171</a>special-cases the common and default case where the&nbsp;<code>Pool</code>&nbsp;specified to be used by a&nbsp;<code>Pipe</code>&nbsp;is the default&nbsp;<code>MemoryPool&lt;byte&gt;.Shared</code>. Rather than go through&nbsp;<code>MemoryPool&lt;byte&gt;.Shared</code>&nbsp;in this case, the&nbsp;<code>Pipe</code>&nbsp;now bypasses it and goes to the underlying&nbsp;<code>ArrayPool&lt;byte&gt;.Shared</code>&nbsp;directly, which removes a layer of indirection but also the allocation of&nbsp;<code>IMemoryOwner&lt;byte&gt;</code>&nbsp;objects returned from&nbsp;<code>MemoryPool&lt;byte&gt;.Rent</code>. (Note that for this benchmark, since&nbsp;<code>System.IO.Pipelines</code>&nbsp;is part of a NuGet package rather than in the shared framework, I’ve added a Benchmark.NET config that specifies what package version to use with each run in order to show the improvements.)</p><div class="highlight highlight-source-cs"><pre class="wrap:false lang:default decode:true">// Run with: dotnet run -c Release -f netcoreapp2.1 --filter *Program*

private sealed class Config : ManualConfig // also add [Config(typeof(Config))] to the Program class
{
    public Config()
    {
        Add(Job.MediumRun.With(CsProjCoreToolchain.NetCoreApp21).WithNuGet("System.IO.Pipelines", "4.5.0").WithId("4.5.0"));
        Add(Job.MediumRun.With(CsProjCoreToolchain.NetCoreApp30).WithNuGet("System.IO.Pipelines", "4.6.0-preview5.19224.8").WithId("4.6.0-preview5.19224.8"));
    }
}

private readonly Pipe _pipe = new Pipe();
private byte[] _buffer = new byte[1024];

[Benchmark]
public async Task ReadWrite()
{
    var reader = _pipe.Reader;
    var writer = _pipe.Writer;

    for (int i = 0; i &lt; 1000; i++)
    {
        ValueTask&lt;ReadResult&gt; vt = reader.ReadAsync();
        await writer.WriteAsync(_buffer);
        ReadResult rr = await vt;
        reader.AdvanceTo(rr.Buffer.End);
    }
}</pre></div><table border="1"><thead><tr><th>Method</th><th>Job</th><th>NuGetReferences</th><th>Toolchain</th><th align="right">Mean</th><th align="right">Error</th><th align="right">StdDev</th><th align="right">Gen 0</th><th align="right">Gen 1</th><th align="right">Gen 2</th></tr></thead><tbody><tr><td>ReadWrite</td><td>4.5.0</td><td>System.IO.Pipelines 4.5.0</td><td>.NET Core 2.1</td><td align="right">406.8 us</td><td align="right">12.774 us</td><td align="right">17.907 us</td><td align="right">11.2305</td><td align="right">–</td><td align="right">–</td></tr><tr><td>ReadWrite</td><td>4.6.0-preview5.19224.8</td><td>System.IO.Pipelines 4.6.0-preview5.19224.8</td><td>.NET Core 3.0</td><td align="right">324.6 us</td><td align="right">3.208 us</td><td align="right">4.702 us</td><td align="right">–</td><td align="right">–</td><td align="right">–</td></tr></tbody></table><p>PR&nbsp;<a href="https://github.com/dotnet/corefx/pull/33658">dotnet/corefx#33658</a>&nbsp;from @benaadams allows&nbsp;<code>Pipe</code>&nbsp;to use the&nbsp;<code>UnsafeQueueUserWorkItem</code>boxing-related optimizations described earlier, PR&nbsp;<a href="https://github.com/dotnet/corefx/pull/33755">dotnet/corefx#33755</a>&nbsp;avoids queueing unnecessary work items, PR&nbsp;<a href="https://github.com/dotnet/corefx/pull/35939">dotnet/corefx#35939</a>&nbsp;tweaks the defaults used to better handle buffering in common cases, PR&nbsp;<a href="https://github.com/dotnet/corefx/pull/35216">dotnet/corefx#35216</a>&nbsp;reduces the amount of slicing performed in various pipe operations, PR&nbsp;<a href="https://github.com/dotnet/corefx/pull/35234">dotnet/corefx#35234</a>&nbsp;from @benaadams reduces the locking used in core operations, PR&nbsp;<a href="https://github.com/dotnet/corefx/pull/35509">dotnet/corefx#35509</a>&nbsp;reduces argument validation (decreasing branching costs), PR&nbsp;<a href="https://github.com/dotnet/corefx/pull/33000">dotnet/corefx#33000</a>&nbsp;focused on reducing costs associated with&nbsp;<code>ReadOnlySequence&lt;byte&gt;</code>&nbsp;that’s the main exchange type pipelines passes around, and PR&nbsp;<a href="https://github.com/dotnet/corefx/pull/29837">dotnet/corefx#29837</a>&nbsp;further optimizes operations like&nbsp;<code>GetSpan</code>&nbsp;and&nbsp;<code>Advance</code>&nbsp;on the&nbsp;<code>Pipe</code>. The net result is to whittle away at already low CPU and allocation overheads.</p><div class="highlight highlight-source-cs"><pre class="wrap:false lang:default decode:true">// Run with: dotnet run -c Release -f netcoreapp2.1 --filter *Program*

private sealed class Config : ManualConfig // also add [Config(typeof(Config))] to the Program class
{
    public Config()
    {
        Add(Job.MediumRun.With(CsProjCoreToolchain.NetCoreApp21).WithNuGet("System.IO.Pipelines", "4.5.0").WithId("4.5.0"));
        Add(Job.MediumRun.With(CsProjCoreToolchain.NetCoreApp30).WithNuGet("System.IO.Pipelines", "4.6.0-preview5.19224.8").WithId("4.6.0-preview5.19224.8"));
    }
}

private readonly Pipe _pipe1 = new Pipe();
private readonly Pipe _pipe2 = new Pipe();
private byte[] _buffer = new byte[1024];

[GlobalSetup]
public void Setup()
{
    Task.Run(async () =&gt;
    {
        var reader = _pipe2.Reader;
        var writer = _pipe1.Writer;
        while (true)
        {
            ReadResult rr = await reader.ReadAsync();
            foreach (ReadOnlyMemory&lt;byte&gt; mem in rr.Buffer)
            {
                await writer.WriteAsync(mem);
            }
            reader.AdvanceTo(rr.Buffer.End);
        }
    });
}

[Benchmark]
public async Task ReadWrite()
{
    var reader = _pipe1.Reader;
    var writer = _pipe2.Writer;

    for (int i = 0; i &lt; 1000; i++)
    {
        await writer.WriteAsync(_buffer);
        long count = 0;
        while (count &lt; _buffer.Length)
        {
            ReadResult rr = await reader.ReadAsync();
            count += rr.Buffer.Length;
            reader.AdvanceTo(rr.Buffer.End);
        }
    }
}</pre></div><table border="1"><thead><tr><th>Method</th><th>Job</th><th>NuGetReferences</th><th>Toolchain</th><th align="right">Mean</th><th align="right">Error</th><th align="right">StdDev</th><th align="right">Gen 0</th><th align="right">Gen 1</th><th align="right">Gen 2</th></tr></thead><tbody><tr><td>ReadWrite</td><td>4.5.0</td><td>System.IO.Pipelines 4.5.0</td><td>.NET Core 2.1</td><td align="right">3.261 ms</td><td align="right">0.0732 ms</td><td align="right">0.1002 ms</td><td align="right">46.8750</td><td align="right">–</td><td align="right">–</td></tr><tr><td>ReadWrite</td><td>4.6.0-preview5.19224.8</td><td>System.IO.Pipelines 4.6.0-preview5.19224.8</td><td>.NET Core 3.0</td><td align="right">2.947 ms</td><td align="right">0.1281 ms</td><td align="right">0.1837 ms</td><td align="right">–</td><td align="right">–</td><td align="right">–</td></tr></tbody></table><h3>System.Console</h3><p><code>Console</code>&nbsp;isn’t something one normally thinks of as being performance-sensitive. However, there are two changes in this release that I think are worth calling attention to here.</p><p>First, there is one area of Console about which we’ve heard numerous concerns related to performance, where the performance impact visibly impacts users. In particular, interactive console applications generally do a lot of manipulation of the cursor, which also entails asking where the cursor currently is. On Windows, both the setting and getting of the cursor are relatively fast operations, with P/Invoke calls made to functions exported from kernel32.dll. On Unix, things are more complicated. There’s no standard POSIX function for getting or setting a terminal’s cursor position. Instead, there’s a standard convention for interacting with the terminal via ANSI escape sequences. To set the cursor position, one writes a sequence of characters to stdout (e.g. “ESC [ 12 ; 34 H” to indicate 12th row, 34th column) and the terminal interprets that and reacts accordingly. Getting the cursor position is more of an ordeal. To get the current cursor position, an application writes to stdout a request (e.g. “ESC [ 6 n”), and in response the terminal writes back to the application’s stdin a response something like “ESC [ 12 ; 34 R”, to indicate the cursor is at the 12th row and 34th column. That response then needs to be read from stdin and parsed. So, in contrast to a fast interop call on Windows, on Unix we need to write, read, and parse text, and do so in a way that doesn’t cause problems with a user sitting at a keyboard using the app concurrently… not particularly cheap. When just getting the cursor position now and then, it’s not a big deal. But when getting it frequently, and when porting code originally written for Windows where the operation was so cheap the code being ported may not have been very frugal with how often it asked for the position (asking for it more than is really needed), this has resulted in visible performance problems. Thankfully, the issue has been addressed in .NET Core 3.0, by PR&nbsp;<a href="https://github.com/dotnet/corefx/pull/36049">dotnet/corefx#36049</a>&nbsp;from @tmds. The change caches the current position and then manually handles updating that cached value based on user interactions, such as handling typing or resizing the terminal window. (Note that Benchmark.NET operates in a way that redirects standard input and output for the process running the test, and that makes Console.CursorLeft/Top return 0 immediately, so for this test, I’ve just done a simple console app with a&nbsp;<code>Stopwatch</code>, which is, as you’ll see, more than sufficient given the discrepancy between costs in versions.)</p><div class="highlight highlight-source-cs"><pre class="lang:default decode:true">using System;
using System.Diagnostics;

public class Program
{
    static void Main()
    {
        var sw = new Stopwatch();
        for (int iter = 0; iter &lt; 5; iter++)
        {
            sw.Restart();
            for (int i = 0; i &lt; 1_000; i++) { _ = Console.CursorLeft; }
            sw.Stop();
            Console.WriteLine(sw.Elapsed.TotalSeconds);
        }
    }
}</pre></div><pre class="lang:default decode:true ">~/BlogPostBenchmarks$ dotnet run -c Release -f netcoreapp2.1
18.2152636
17.9935087
18.2676408
17.7891821
17.4141348
~/BlogPostBenchmarks$ dotnet run -c Release -f netcoreapp3.0
0.0648111
0.0001539
0.00013979999999999998
0.00013529999999999998
0.0001459</pre><p>Another place where&nbsp;<code>Console</code>&nbsp;has been improved affects both Windows and Unix. Interestingly, this change was made for functional reasons (in particular for when running on Windows), but it has performance benefits as well for all OSes. In .NET, most of the times we specify buffer sizes it’s for performance reasons and represents a trade-off: the smaller the buffer size, the less memory is used but the more times operations may need to be performed to fill that buffer, and conversely the larger the buffer size, the more memory is used but the fewer times the buffer will need to be filled. It’s rare that the buffer size has a functional impact, but it actually can in&nbsp;<code>Console</code>. On Windows to read from the console, one calls either the&nbsp;<code>ReadFile</code>&nbsp;or&nbsp;<code>ReadConsole</code>&nbsp;functions, both of which accept a buffer to store the read data into. By default on Windows, reading from the console will not return until a newline, but Windows also needs somewhere to store the typed data, and it does so into the supplied buffer. Thus, Windows won’t let the user type more characters than can fit into the buffer, which means the line length a user can type is limited by the buffer size. For whatever historical reason, .NET has used a buffer size of 256 characters, limiting the typeable line length to that amount. PR&nbsp;<a href="https://github.com/dotnet/corefx/pull/36212">dotnet/corefx#36212</a>&nbsp;expands that to 4096 characters, which much better matches other programming environments and allows for a much more reasonable line length. However, as is the case when increasing buffer sizes, relevant throughput involving that buffer improves as well, in particular when reading from files piped to stdin. For example, reading 8K of input data from stdin previously would have required 32 calls to&nbsp;<code>ReadFile</code>; with a 4K buffer, only 2 calls are required. The impact of that can be seen in this benchmark. (Again, this is harder to test with Benchmark.NET, so I’ve again just used a simple console app.)</p><div class="highlight highlight-source-cs"><pre class="lang:default decode:true">using System;
using System.Diagnostics;
using System.IO;

public class Program
{
    static void Main()
    {
        //using (var writer = new StreamWriter(@"tmp.dat"))
        //{
        //    for (int i = 0; i &lt; 10_000_000; i++)
        //    {
        //        writer.WriteLine("This is a test.  This is only a test.");
        //    }
        //}

        var sw = Stopwatch.StartNew();
        while (Console.ReadLine() != null) ;
        Console.WriteLine(sw.Elapsed.TotalSeconds);
    }
}</pre></div><pre class="wrap:false lang:default decode:true">c:\BlogPostBenchmarks&gt;dotnet run -c Release -f netcoreapp2.1 &lt; c:\BlogPostBenchmarks\bin\Release\netcoreapp2.1\tmp.dat
4.8151814

c:\BlogPostBenchmarks&gt;dotnet run -c Release -f netcoreapp3.0 &lt; c:\BlogPostBenchmarks\bin\Release\netcoreapp2.1\tmp.dat
1.3161175999999999</pre><h3>System.Diagnostics.Process</h3><p>There have been various functional improvements to the&nbsp;<code>Process</code>&nbsp;class in .NET Core 3.0, in particular on Unix, but there are a couple of performance-focused improvements I want to call out.</p><p>PR&nbsp;<a href="https://github.com/dotnet/corefx/pull/31236">dotnet/corefx#31236</a>&nbsp;is another nice example of introducing a new performance-focused API and, at the same time, using it within .NET Core to further improve the performance of core libraries. In this case, it’s a low-level API on MemoryMarshal that enables efficiently reading structs from spans, something that’s done in spades as part of the interop in&nbsp;<code>System.Diagnostics.Process</code>. I like that example, not because it makes for a massive performance improvement, but because it highlights the general pattern I like to see: adding new APIs for others to consume and in the same breath using those APIs to better the technology itself.</p><p>A more impactful example, though, comes from @joshudson in PR&nbsp;<a href="https://github.com/dotnet/corefx/pull/33289">dotnet/corefx#33289</a>, which changed the native code used to fork a new process from using the&nbsp;<code>fork</code>&nbsp;function to instead using the&nbsp;<code>vfork</code>&nbsp;function. The benefit of&nbsp;<code>vfork</code>&nbsp;is that it avoids copying the page tables of the parent process into the child process, with the assumption that the child process is then just going to overwrite everything anyway via an almost immediate&nbsp;<code>exec</code>&nbsp;call.&nbsp;<code>fork</code>&nbsp;does copy-on-write, but if the process is modifying a lot of state concurrently (e.g. with the garbage collector running), this can get expensive quickly and unnecessarily. For this benchmark, I’ve just written a nop C program in a test.c file:</p><p>and compiled it with GCC:</p><pre class="lang:default decode:true ">gcc -o test test.c</pre><p>to give us a target for Process.Start to invoke.</p><div class="highlight highlight-source-cs"><pre class="wrap:false lang:default decode:true">[Benchmark]
public void ProcessStartWait() =&gt; Process.Start("/home/stephentoub/BlogPostBenchmarks/test").WaitForExit();</pre></div><table border="1"><thead><tr><th>Method</th><th>Toolchain</th><th align="right">Mean</th><th align="right">Error</th><th align="right">StdDev</th><th align="right">Ratio</th><th align="right">RatioSD</th><th align="right">Gen 0</th><th align="right">Gen 1</th><th align="right">Gen 2</th><th align="right">Allocated</th></tr></thead><tbody><tr><td>ProcessStartWait</td><td>netcoreapp2.1</td><td align="right">1,663.0 us</td><td align="right">32.79 us</td><td align="right">67.72 us</td><td align="right">1.00</td><td align="right">0.00</td><td align="right">–</td><td align="right">–</td><td align="right">–</td><td align="right">21.45 KB</td></tr><tr><td>ProcessStartWait</td><td>netcoreapp3.0</td><td align="right">536.0 us</td><td align="right">10.64 us</td><td align="right">28.40 us</td><td align="right">0.32</td><td align="right">0.02</td><td align="right">1.9531</td><td align="right">–</td><td align="right">–</td><td align="right">16.65 KB</td></tr></tbody></table><h3>LINQ</h3><p>Previous releases have seen a ton of investment in optimizing LINQ. There’s less of that in .NET Core 3.0, as a lot of the common patterns have already been covered well. However, there are still some nice improvements to be found in the release.</p><p>It’s relatively rare that new operators are added to&nbsp;<code>System.Linq</code>&nbsp;itself, as the very nature of extension methods makes it easy for anyone to build up and share their own library of extension methods they consider to be useful (and several well-established such libraries exist). Even so, .NET Core 2.0 saw a new&nbsp;<code>TakeLast</code>&nbsp;method added. In .NET Core 3.0, PR&nbsp;<a href="https://github.com/dotnet/corefx/pull/36051">dotnet/corefx#36051</a>&nbsp;by @Romasz updated&nbsp;<code>TakeLast</code>&nbsp;to integrate with the internal&nbsp;<code>IPartition&lt;T&gt;</code>&nbsp;interface that enables several operators to cooperate, helping to optimize (in some situations quite heavily) various uses of the operator.</p><div class="highlight highlight-source-cs"><pre class="lang:default decode:true">private IEnumerable&lt;int&gt; _enumerable = new int[1000].Select(i =&gt; i);

[Benchmark]
public int SumLast10() =&gt; _enumerable.TakeLast(10).Sum();</pre></div><table border="1"><thead><tr><th>Method</th><th>Toolchain</th><th align="right">Mean</th><th align="right">Error</th><th align="right">StdDev</th><th align="right">Ratio</th><th align="right">Gen 0</th><th align="right">Gen 1</th><th align="right">Gen 2</th><th align="right">Allocated</th></tr></thead><tbody><tr><td>SumLast10</td><td>netcoreapp2.1</td><td align="right">11,935.5 ns</td><td align="right">102.793 ns</td><td align="right">85.837 ns</td><td align="right">1.00</td><td align="right">0.1526</td><td align="right">–</td><td align="right">–</td><td align="right">344 B</td></tr><tr><td>SumLast10</td><td>netcoreapp3.0</td><td align="right">141.4 ns</td><td align="right">1.310 ns</td><td align="right">1.225 ns</td><td align="right">0.01</td><td align="right">0.0267</td><td align="right">–</td><td align="right">–</td><td align="right">56 B</td></tr></tbody></table><p>Just recently, PR&nbsp;<a href="https://github.com/dotnet/corefx/pull/37410">dotnet/corefx#37410</a>&nbsp;optimized the relatively common pattern of using&nbsp;<code>Enumerable.Range(...).Select(…)</code>, teaching&nbsp;<code>Select</code>&nbsp;about the object generated by&nbsp;<code>Range</code>&nbsp;and allowing for the enumeration performed by&nbsp;<code>Select</code>&nbsp;to skip going through&nbsp;<code>IEnumerable&lt;T&gt;</code>&nbsp;and instead just loop through the intended numerical range directly.</p><div class="highlight highlight-source-cs"><pre class="lang:default decode:true">[Benchmark]
public int[] RangeSelectToArray() =&gt; Enumerable.Range(0, 100).Select(i =&gt; i * 2).ToArray();</pre></div><table border="1"><thead><tr><th>Method</th><th>Toolchain</th><th align="right">Mean</th><th align="right">Error</th><th align="right">StdDev</th><th align="right">Ratio</th><th align="right">RatioSD</th><th align="right">Gen 0</th><th align="right">Gen 1</th><th align="right">Gen 2</th><th align="right">Allocated</th></tr></thead><tbody><tr><td>RangeSelectToArray</td><td>netcoreapp2.1</td><td align="right">953.9 ns</td><td align="right">20.232 ns</td><td align="right">28.363 ns</td><td align="right">1.00</td><td align="right">0.00</td><td align="right">0.2460</td><td align="right">–</td><td align="right">–</td><td align="right">520 B</td></tr><tr><td>RangeSelectToArray</td><td>netcoreapp3.0</td><td align="right">358.0 ns</td><td align="right">7.650 ns</td><td align="right">7.156 ns</td><td align="right">0.37</td><td align="right">0.02</td><td align="right">0.2441</td><td align="right">–</td><td align="right">–</td><td align="right">512 B</td></tr></tbody></table><p><code>Enumerable.Empty&lt;T&gt;()</code>&nbsp;was also changed in PR&nbsp;<a href="https://github.com/dotnet/corefx/pull/31025">dotnet/corefx#31025</a>&nbsp;to better compose with optimizations already elsewhere in .NET Core’s System.Linq implementation. While no one should be writing code that explicitly calls additional LINQ operators directly on the result of&nbsp;<code>Enumerable.Empty&lt;T&gt;()</code>, it is common to return the result of&nbsp;<code>Empty&lt;T&gt;()</code>&nbsp;as one possible return value from an&nbsp;<code>IEnumerable&lt;T&gt;</code>-returning method, and then for the caller to tack on additional operators, such that this optimization does actually have a meaningful effect.</p><pre class="lang:default decode:true">[Benchmark]
public int[] EmptyTakeSelectToArray() =&gt; Enumerable.Empty&lt;int&gt;().Take(10).Select(i =&gt; i).ToArray();</pre><table border="1"><thead><tr><th>Method</th><th>Toolchain</th><th align="right">Mean</th><th align="right">Error</th><th align="right">StdDev</th><th align="right">Ratio</th><th align="right">Gen 0</th><th align="right">Gen 1</th><th align="right">Gen 2</th><th align="right">Allocated</th></tr></thead><tbody><tr><td>EmptyTakeSelectToArray</td><td>netcoreapp2.1</td><td align="right">71.80 ns</td><td align="right">1.4205 ns</td><td align="right">1.1861 ns</td><td align="right">1.00</td><td align="right">0.0495</td><td align="right">–</td><td align="right">–</td><td align="right">104 B</td></tr><tr><td>EmptyTakeSelectToArray</td><td>netcoreapp3.0</td><td align="right">30.09 ns</td><td align="right">0.1550 ns</td><td align="right">0.1295 ns</td><td align="right">0.42</td><td align="right">–</td><td align="right">–</td><td align="right">–</td><td align="right">–</td></tr></tbody></table><p>Across .NET Core, we’re also paying more attention to assembly size, in particular as it can impact ahead-of-time (AOT) compilation. PRs like&nbsp;<a href="https://github.com/dotnet/corefx/pull/35213">dotnet/corefx#35213</a>, which employs “ThrowHelpers” in the heavily-generic LINQ, help to reduce generated code size, which has benefits in and of itself but can also help with other areas of performance.</p><h3>Interop</h3><p>Interop is another one of those areas that’s critically important both to customers of .NET as well as to .NET itself, as a lot of functionality in .NET is layered on top of underlying operating system functionality that requires interop to access. As such, performance improvements in interop itself end up impacting a wide array of components.</p><p>One notable improvement is in&nbsp;<code>SafeHandle</code>, and it’s another example of where moving code from native to managed helped improve performance. SafeHandle is the recommended way for managing the lifetime of native resources, whether represented by handles on Windows or by file descriptors on Unix, and it’s used in exactly that way internally in all of our managed libraries in coreclr and corefx. One of the reasons it’s the recommended solution is that it uses appropriate synchronization to ensure that these native resources aren’t closed from managed code while they’re still being used, and that means that the interop layer needs to track every time a P/Invoke call is made with a SafeHandle, invoking DangerousAddRef prior to the call, DangerousRelease after the call, and DangerousGetHandle to extract the actual pointer value to pass to the native function. In previous releases of .NET, the core pieces of those implementations were in the runtime, which meant managed code needed to make&nbsp;<code>InternalCall</code>s to native code in the runtime for each of those operations. In .NET Core 3.0 as of PR&nbsp;<a href="https://github.com/dotnet/coreclr/pull/22564">dotnet/coreclr#22564</a>, those operations have been ported to managed code, removing the overhead associated with each of those transitions.</p><div class="highlight highlight-source-cs"><pre class="lang:default decode:true">private SafeFileHandle _sfh = new SafeFileHandle((IntPtr)12345, ownsHandle: false);

[Benchmark]
public IntPtr SafeHandleOps()
{
    bool success = false;
    try
    {
        _sfh.DangerousAddRef(ref success);
        return _sfh.DangerousGetHandle();
    }
    finally
    {
        if (success)
        {
            _sfh.DangerousRelease();
        }
    }
}</pre></div><table border="1"><thead><tr><th>Method</th><th>Toolchain</th><th align="right">Mean</th><th align="right">Error</th><th align="right">StdDev</th><th align="right">Ratio</th></tr></thead><tbody><tr><td>SafeHandleOps</td><td>netcoreapp2.1</td><td align="right">36.72 ns</td><td align="right">0.7285 ns</td><td align="right">0.6458 ns</td><td align="right">1.00</td></tr><tr><td>SafeHandleOps</td><td>netcoreapp3.0</td><td align="right">16.04 ns</td><td align="right">0.1322 ns</td><td align="right">0.1104 ns</td><td align="right">0.44</td></tr></tbody></table><p>There are also examples for improvements to marshaling. Earlier in this post, I highlighted a variety of cases where&nbsp;<code>StringBuilder</code>&nbsp;was used as part of marshaling and interop. For the record, I personally dislike&nbsp;<code>StringBuilder</code>&nbsp;being used in interop, as it adds cost and complexity for relatively little benefit, and as a result did work in PRs like&nbsp;<a href="https://github.com/dotnet/corefx/pull/33780">dotnet/corefx#33780</a>&nbsp;and&nbsp;<a href="https://github.com/dotnet/coreclr/pull/21120">dotnet/coreclr#21120</a>&nbsp;to remove almost all use of&nbsp;<code>StringBuilder</code>&nbsp;marshaling in coreclr and corefx. However, there is still a lot of code built around&nbsp;<code>StringBuilder</code>, and it deserves to be as fast as possible. PR&nbsp;<a href="https://github.com/dotnet/coreclr/pull/17928">dotnet/coreclr#17928</a>&nbsp;avoids a bunch of unnecessary work and allocation that happens as part of&nbsp;<code>StringBuilder</code>&nbsp;marshaling, and leads to improvements like this:</p><div class="highlight highlight-source-cs"><pre class="lang:default decode:true">private const int MAX_PATH = 260;
private StringBuilder _sb = new StringBuilder(MAX_PATH);

[DllImport("kernel32", CharSet = CharSet.Unicode, SetLastError = true)]
private static extern uint GetTempPathW(int bufferLen, [Out]StringBuilder buffer);

[Benchmark]
public void StringBuilderMarshal() =&gt; GetTempPathW(MAX_PATH, _sb);</pre></div><table border="1"><thead><tr><th>Method</th><th>Toolchain</th><th align="right">Mean</th><th align="right">Error</th><th align="right">StdDev</th><th align="right">Ratio</th><th align="right">RatioSD</th><th align="right">Gen 0</th><th align="right">Gen 1</th><th align="right">Gen 2</th><th align="right">Allocated</th></tr></thead><tbody><tr><td>StringBuilderMarshal</td><td>netcoreapp2.1</td><td align="right">359.4 ns</td><td align="right">7.643 ns</td><td align="right">13.386 ns</td><td align="right">1.00</td><td align="right">0.00</td><td align="right">0.2584</td><td align="right">–</td><td align="right">–</td><td align="right">544 B</td></tr><tr><td>StringBuilderMarshal</td><td>netcoreapp3.0</td><td align="right">289.1 ns</td><td align="right">5.773 ns</td><td align="right">7.707 ns</td><td align="right">0.80</td><td align="right">0.04</td><td align="right">–</td><td align="right">–</td><td align="right">–</td><td align="right">–</td></tr></tbody></table><p>And of course, specific uses of interop and marshaling have also improved. For example,&nbsp;<code>FileSystemWatcher</code>‘s interop on macOS had been using&nbsp;<code>MarshalAs</code>&nbsp;attributes, which forced the runtime to do additional marshaling work on every OS callback, including allocating arrays. PR&nbsp;<a href="https://github.com/dotnet/corefx/pull/34715">dotnet/corefx#34715</a>&nbsp;moved&nbsp;<code>FileSystemWatcher</code>‘s interop to use a more efficient scheme that doesn’t entail additional allocations nor marshaling directives. Or consider&nbsp;<a href="https://github.com/dotnet/corefx/pull/30099">dotnet/corefx#30099</a>, where&nbsp;<code>System.Drawing</code>&nbsp;was switched to using a much more efficient scheme of marshaling and interop, with a managed array being pinned and passed directly to native code instead of allocating additional memory and copying to it.</p><div class="highlight highlight-source-cs"><pre class="wrap:false lang:default decode:true">// Run with: dotnet run -c Release -f netcoreapp2.1 --filter *Program*

private sealed class Config : ManualConfig // also add [Config(typeof(Config))] to the Program class
{
    public Config()
    {
        Add(Job.MediumRun.With(CsProjCoreToolchain.NetCoreApp21).WithNuGet("System.Drawing.Common", "4.5.1").WithId("4.5.1"));
        Add(Job.MediumRun.With(CsProjCoreToolchain.NetCoreApp30).WithNuGet("System.Drawing.Common", "4.6.0-preview5.19224.8").WithId("4.6.0-preview5.19224.8"));
    }
}

private Bitmap _image;
private Graphics _graphics;
private Point[] _points;

[GlobalSetup]
public void Setup()
{
    _image = new Bitmap(100, 100);
    _graphics = Graphics.FromImage(_image);
    _points = new[]
    {
        new Point(10, 10), new Point(20, 1), new Point(35, 5), new Point(50, 10),
        new Point(60, 15), new Point(65, 25), new Point(50, 30)
    };
}

[Benchmark]
public void TransformPoints()
{
    _graphics.TransformPoints(CoordinateSpace.World, CoordinateSpace.Page, _points);
    _graphics.TransformPoints(CoordinateSpace.Device, CoordinateSpace.World, _points);
    _graphics.TransformPoints(CoordinateSpace.Page, CoordinateSpace.Device, _points);
}</pre></div><table border="1"><thead><tr><th>Method</th><th>Job</th><th>NuGetReferences</th><th>Toolchain</th><th align="right">Mean</th><th align="right">Error</th><th align="right">StdDev</th><th align="right">Gen 0</th><th align="right">Gen 1</th><th align="right">Gen 2</th><th align="right">Allocated</th></tr></thead><tbody><tr><td>TransformPoints</td><td>4.5.1</td><td>System.Drawing.Common 4.5.1</td><td>.NET Core 2.1</td><td align="right">11,010.3 ns</td><td align="right">490.050 ns</td><td align="right">718.309 ns</td><td align="right">0.5798</td><td align="right">–</td><td align="right">–</td><td align="right">1248 B</td></tr><tr><td>TransformPoints</td><td>4.6.0-preview5.19224.8</td><td>System.Drawing.Common 4.6.0-preview5.19224.8</td><td>.NET Core 3.0</td><td align="right">364.0 ns</td><td align="right">6.704 ns</td><td align="right">9.827 ns</td><td align="right">–</td><td align="right">–</td><td align="right">–</td><td align="right">–</td></tr></tbody></table><h3>Peanut butter</h3><p>In previous sections of this post, I highlighted groups of PRs that addressed various areas of .NET in an impactful way, where some piece of mainstream functionality was significantly improved. But those aren’t the only areas or kinds of PRs that matter.</p><p>In .NET we also have what we sometimes refer to as “peanut butter”. We have a ton of code that’s generally great for most applications but that has a myriad of small opportunities for improvements. Those improvements alone don’t make anything better, but they fix a smearing of performance penalties across a large swath of code, and the more of such issues we can fix, the better performance becomes overall. An allocation removed here, some unnecessary cycles eliminated there, some unnecessary code removed there. Here are just a sampling of PRs that went in to address such “peanut butter”:</p><ul><li><strong>Lower bounds explicitly provided to&nbsp;<code>Array.Copy</code>.</strong>&nbsp;Calling&nbsp;<code>Array.Copy(src, dst, length)</code>&nbsp;requires the runtime to call&nbsp;<code>GetLowerBound</code>&nbsp;on each of the src and the dst arrays. When working with&nbsp;<code>T[]</code>s, the lower bound is 0, and we can just explicitly pass in 0 for both bounds and avoid the implicit&nbsp;<code>GetLowerBound</code>&nbsp;calls. PR&nbsp;<a href="https://github.com/dotnet/coreclr/pull/21756">dotnet/coreclr#21756</a>&nbsp;does that in a variety of places.</li><li><strong>Cheaper copying to new arrays.</strong>&nbsp;In a variety of places, a&nbsp;<code>List&lt;T&gt;</code>&nbsp;stored some data, a new array was then allocated based on the length of the list, and the contents then copied to the array with&nbsp;<code>CopyTo</code>. PR&nbsp;<a href="https://github.com/dotnet/coreclr/pull/22101">dotnet/coreclr#22101</a>&nbsp;from @benaadams recognized the silliness of this and replaced that pattern with simply using&nbsp;<code>List&lt;T&gt;.ToArray</code>.</li><li><strong><code>Nullable&lt;T&gt;.Value</code>&nbsp;vs&nbsp;<code>GetValueOrDefault</code>.</strong>&nbsp;<code>Nullable&lt;T&gt;</code>&nbsp;has two main members to access the value:&nbsp;<code>Value</code>&nbsp;and&nbsp;<code>GetValueOrDefault</code>. It’s initially counter-intuitive, but&nbsp;<code>GetValueOrDefault</code>&nbsp;is actually cheaper:&nbsp;<code>Value</code>&nbsp;needs to check whether the instance has a value or not, throwing if it doesn’t, whereas&nbsp;<code>GetValueOrDefault</code>&nbsp;just always returns the value field, and it’ll be&nbsp;<code>default</code>&nbsp;if there was no value. PR&nbsp;<a href="https://github.com/dotnet/coreclr/pull/22297">dotnet/coreclr#22297</a>&nbsp;fixed up a variety of call sites where&nbsp;<code>GetValueOrDefault</code>&nbsp;could be used instead.</li><li><strong><code>Array.Empty&lt;T&gt;()</code>.</strong>&nbsp;In previous releases, lots of zero-length array allocations were changed to instead use&nbsp;<code>Array.Empty&lt;T&gt;()</code>, both in libraries and via compiler changes for things like&nbsp;<code>params</code>&nbsp;arrays. That trend continues in .NET Core 3.0, with PR&nbsp;<a href="https://github.com/dotnet/corefx/pull/30235">dotnet/corefx#30235</a>&nbsp;doing another sweep through corefx and replacing even more zero-length allocations with the cached&nbsp;<code>Array.Empty&lt;T&gt;()</code>.</li><li><strong>Avoiding lots of little allocations all over the place.</strong>&nbsp;For new code being written, we’re very cost-conscious and keep an eye out for allocations that, even if small and rare, could be easily replaced by something less expensive. For existing code, the most impactful allocations show up in profiling of key scenarios and are squashed whenever possible. But there are a lot of small allocations here and there that generally don’t pop up on our radar until we have another reason to review and profile the relevant code. In every release, we end up removing a bunch of these. For example, all of these PRs contributed to reducing the allocation peanut butter across coreclr and corefx in .NET Core 3.0:<ul><li>In System.Collections:&nbsp;<a href="https://github.com/dotnet/corefx/pull/30528">dotnet/corefx#30528</a></li><li>In System.Data:&nbsp;<a href="https://github.com/dotnet/corefx/pull/30130">dotnet/corefx#30130</a></li><li>In System.Data.SqlClient:&nbsp;<a href="https://github.com/dotnet/corefx/pull/34044">dotnet/corefx#34044</a>,&nbsp;<a href="https://github.com/dotnet/corefx/pull/34047">dotnet/corefx#34047</a>,&nbsp;<a href="https://github.com/dotnet/corefx/pull/34234">dotnet/corefx#34234</a>,&nbsp; <a href="https://github.com/dotnet/corefx/pull/34999">dotnet/corefx#34999</a>,&nbsp;<a href="https://github.com/dotnet/corefx/pull/35549">dotnet/corefx#35549</a>,&nbsp;<a href="https://github.com/dotnet/corefx/pull/34048">dotnet/corefx#34048</a>,&nbsp;<a href="https://github.com/dotnet/corefx/pull/34390">dotnet/corefx#34390</a>, and&nbsp;<a href="https://github.com/dotnet/corefx/pull/34393">dotnet/corefx#34393</a>, all from @Wraith2</li><li>In System.Diagnostics:&nbsp;<a href="https://github.com/dotnet/coreclr/pull/21752">dotnet/coreclr#21752</a></li><li>In System.IO: <a href="https://github.com/dotnet/corefx/pull/30509">dotnet/corefx#30509</a>,&nbsp;<a href="https://github.com/dotnet/corefx/pull/30514">dotnet/corefx#30514</a>,&nbsp;<a href="https://github.com/dotnet/coreclr/pull/21760">dotnet/coreclr#21760</a>,&nbsp;<a href="https://github.com/dotnet/corefx/pull/37546">dotnet/corefx#37546</a></li><li>In System.Globalization: <a href="https://github.com/dotnet/coreclr/pull/18546">dotnet/coreclr#18546</a>,&nbsp;<a href="https://github.com/dotnet/coreclr/pull/21121">dotnet/coreclr#21121</a></li><li>In System.Net: <a href="https://github.com/dotnet/corefx/pull/30521">dotnet/corefx#30521</a>,&nbsp;<a href="https://github.com/dotnet/corefx/pull/30530">dotnet/corefx#30530</a>, <a href="https://github.com/dotnet/corefx/pull/30508">dotnet/corefx#30508</a>,&nbsp;<a href="https://github.com/dotnet/corefx/pull/30529">dotnet/corefx#30529</a>,&nbsp;<a href="https://github.com/dotnet/corefx/pull/34356">dotnet/corefx#34356</a>,&nbsp;<a href="https://github.com/dotnet/corefx/pull/36021">dotnet/corefx#36021</a></li><li>In System.Reflection:&nbsp;<a href="https://github.com/dotnet/coreclr/pull/21770">dotnet/coreclr#21770</a>,&nbsp;<a href="https://github.com/dotnet/coreclr/pull/21758">dotnet/coreclr#21758</a></li><li>In System.Security:&nbsp;<a href="https://github.com/dotnet/corefx/pull/30512">dotnet/corefx#30512</a>, <a href="https://github.com/dotnet/corefx/pull/29612">dotnet/corefx#29612</a></li><li>In System.Uri:&nbsp;<a href="https://github.com/dotnet/corefx/pull/33641">dotnet/corefx#33641</a>,&nbsp;<a href="https://github.com/dotnet/corefx/pull/36056">dotnet/corefx#36056</a></li><li>In System.Xml: <a href="https://github.com/dotnet/corefx/pull/34196">dotnet/corefx#34196</a></li></ul></li><li><strong>Avoiding explicit static cctors.</strong>&nbsp;Any type that has static fields initialized ends up with a static constructor (cctor) to run that initialization. But depending on how the initialization is authored can impact performance. In particular, if the developer explicitly writes a static cctor rather than initializing the fields as part of the static field declarations, the C# compiler will not mark the type as&nbsp;<code>beforefieldinit</code>. Having the type marked&nbsp;<code>beforefieldinit</code>&nbsp;can be beneficial for performance, because it allows the runtime more flexibility in when it performs the initialization, which in turn allows the JIT more flexibility about how it can optimize, and whether locking might be needed when accessing static methods on the type. PRs like&nbsp;<a href="https://github.com/dotnet/coreclr/pull/21718">dotnet/coreclr#21718</a>&nbsp;and&nbsp;<a href="https://github.com/dotnet/coreclr/pull/21715">dotnet/coreclr#21715</a>&nbsp;from @benaadams have removed such static cctors that can layer in small costs across a wide swath of accessing code.</li><li><strong>Using a cheaper, sufficient equivalent.</strong>&nbsp;<code>IndexOf</code>&nbsp;on strings and spans returns the position of a found element, whereas&nbsp;<code>Contains</code>&nbsp;just returns whether the element was found. The latter can be slightly more efficient, because it doesn’t need to track the exact location of an element, just that it existed. Even so, lots of call sites that could have used&nbsp;<code>Contains</code>&nbsp;instead used&nbsp;<code>IndexOf</code>. PRs&nbsp;<a href="https://github.com/dotnet/coreclr/pull/19874">dotnet/coreclr#19874</a>&nbsp;and&nbsp;<a href="https://github.com/dotnet/corefx/pull/32249">dotnet/corefx#32249</a>&nbsp;by @grant-d addressed that. Another example,&nbsp;<code>SocketsHttpHandler</code>(the default&nbsp;<code>HttpMessageHandler</code>&nbsp;behind&nbsp;<code>HttpClient</code>) was using&nbsp;<code>DateTime.UtcNow</code>&nbsp;when determining whether a connection could be reused for the next request or not, but&nbsp;<code>Environment.TickCount</code>&nbsp;is cheaper and has sufficient resolution and accuracy for this purpose, so PR&nbsp;<a href="https://github.com/dotnet/corefx/pull/35401">dotnet/corefx#35401</a>&nbsp;switched it to use that. Another example, PR&nbsp;<a href="https://github.com/dotnet/corefx/pull/37548">dotnet/corefx#37548</a>&nbsp;tweaks the overloads of Array.Copy used in a bunch of places to avoid unnecessary&nbsp;<code>GetLowerBound()</code>&nbsp;calls to lookup the lower bound for arrays we know have a lower bound of 0.</li><li><strong>Simplifying interop.</strong>&nbsp;The interop infrastructure in .NET is quite powerful and comprehensive, with lots of knobs that allow for specifying how calls should be made and how data should be transformed. However, many come with a cost, such as needing the runtime to generate a marshaling stub to perform the various required transformations. PRs&nbsp;<a href="https://github.com/dotnet/corefx/pull/36544">dotnet/corefx#36544</a>&nbsp;and&nbsp;<a href="https://github.com/dotnet/corefx/pull/36071">dotnet/corefx#36071</a>, for example, tweaked interop signatures to avoid overheads associated with such marshaling code.</li><li><strong>Avoiding unnecessary globalization.</strong>&nbsp;Due to how various&nbsp;<code>System.String</code>&nbsp;APIs were designed almost two decades ago, it can be easy to accidentally employ culture-aware string comparisons when it’s not intended. Such comparisons can be functionally incorrect for a given task and also more costly, involving more expensive calls to the operating system or globalization library. In particular,&nbsp;<code>String.IndexOf</code>&nbsp;with a&nbsp;<code>char</code>&nbsp;argument uses ordinal comparison, but&nbsp;<code>String.IndexOf</code>&nbsp;with a&nbsp;<code>string</code>&nbsp;argument (even if it’s a single character) uses the current culture to perform the comparison. PRs&nbsp;<a href="https://github.com/dotnet/corefx/pull/37499">dotnet/corefx#37499</a>&nbsp;addresses a bunch of such cases in&nbsp;<code>System.Net</code>, an area in which one almost always wants to do ordinal comparisons, generally the case when doing parsing for text-based protocols.</li><li><strong>Avoiding unnecessary&nbsp;<code>ExecutionContext</code>&nbsp;flow.</strong>&nbsp;<code>ExecutionContext</code>&nbsp;is the primary vehicle for ambient state “flowing” through a program and across asynchronous calls, in particular&nbsp;<code>AsyncLocal&lt;T&gt;</code>. In order to achieve such flow, code that spawns an async operation (e.g.&nbsp;<code>Task.Run</code>,&nbsp;<code>Timer</code>, etc.) or code that creates a continuation to run when some other operation finishes (e.g.&nbsp;<code>await</code>) needs to “capture” the current&nbsp;<code>ExecutionContext</code>, hang on to it, and then later when executing the relevant work, use that captured&nbsp;<code>ExecutionContext</code>‘s&nbsp;<code>Run</code>&nbsp;method to do so. If the work being performed doesn’t actually require the&nbsp;<code>ExecutionContext</code>, we can avoid flowing it to avoid the small associated overhead. PRs&nbsp;<a href="https://github.com/dotnet/corefx/pull/37551">dotnet/corefx#37551</a>,&nbsp;<a href="https://github.com/dotnet/corefx/pull/33235">dotnet/corefx#33235</a>, and&nbsp;<a href="https://github.com/dotnet/corefx/pull/33080">dotnet/corefx#33080</a>&nbsp;are examples: they switch several uses of&nbsp;<code>CancellationToken.Register</code>over to the new&nbsp;<code>CancellationToken.UnsafeRegister</code>&nbsp;method, the only difference compared to&nbsp;<code>Register</code>&nbsp;being that it doesn’t flow&nbsp;<code>ExecutionContext</code>. As another example, PR&nbsp;<a href="https://github.com/dotnet/coreclr/pull/18670">dotnet/coreclr#18670</a>&nbsp;changed&nbsp;<code>CancellationTokenSource</code>&nbsp;so that when it creates a&nbsp;<code>Timer</code>, it doesn’t unnecessarily capture&nbsp;<code>ExecutionContext</code>. Or consider PR&nbsp;<a href="https://github.com/dotnet/coreclr/pull/20294">dotnet/coreclr#20294</a>, which ensures that any such captured&nbsp;<code>ExecutionContext</code>&nbsp;is dropped as soon as it’s not needed from completed&nbsp;<code>Task</code>s.</li><li><strong>Centralized / optimized bit operations.</strong>&nbsp;PR&nbsp;<a href="https://github.com/dotnet/coreclr/pull/22118">dotnet/coreclr#22118</a>&nbsp;from @benaadams introduced a&nbsp;<code>BitOperations</code>&nbsp;class that serves to centralize a bunch of bit-twiddling operations (rotating, leading zero count, population count, log, etc.). This type was later augmented and enhanced in PRs from @grant-d like&nbsp;<a href="https://github.com/dotnet/coreclr/pull/22497">dotnet/coreclr#22497</a>,&nbsp;<a href="https://github.com/dotnet/coreclr/pull/22584">dotnet/coreclr#22584</a>, and&nbsp;<a href="https://github.com/dotnet/coreclr/pull/22630">dotnet/coreclr#22630</a>, which also serve to use these shared helpers from everywhere across&nbsp;<code>System.Private.Corelib</code>&nbsp;where such bit-twiddling operations are required. This ensures that all such call sites (of which there are currently ~70) get the best implementation the runtime can muster, whether that be an implementation that takes advantage of the current hardware’s instruction set or one that utilizes a software fallback.</li></ul><h3>GC</h3><p>No blog post on performance would be complete without discussing the garbage collector. Many of the improvements cited thus far have involved reducing allocations, which is in part about reducing direct costs but more so about reducing the load placed on the garbage collector and minimizing the work it needs to do. But improving the GC itself is also a key focus, and one that’s gotten attention in this release, as it has in previous releases.</p><p>PR&nbsp;<a href="https://github.com/dotnet/coreclr/pull/21523">dotnet/coreclr#21523</a>&nbsp;includes a variety of performance improvements, from improvements to locking to better free list management. PR&nbsp;<a href="https://github.com/dotnet/coreclr/pull/23251">dotnet/coreclr#23251</a>&nbsp;from @mjsabby adds support to the GC for Large Pages (“Huge Pages” on Linux), which can be opted-into by very large applications that experience bottlenecks due to the translation lookaside buffer (TLB). And PR&nbsp;<a href="https://github.com/dotnet/coreclr/pull/22003">dotnet/coreclr#22003</a>&nbsp;further optimized the write barriers employed by the GC.</p><p>One notable piece of work is improving behavior on machines with a large number of processors, e.g. PR&nbsp;<a href="https://github.com/dotnet/coreclr/pull/23824">dotnet/coreclr#23824</a>. Rather than trying to explain it here, I’ll simply refer to @Maoni0’s blog post on the subject:&nbsp;<a href="https://blogs.msdn.microsoft.com/maoni/2019/04/03/making-cpu-configuration-better-for-gc-on-machines-with-64-cpus/" rel="nofollow">https://blogs.msdn.microsoft.com/maoni/2019/04/03/making-cpu-configuration-better-for-gc-on-machines-with-64-cpus/</a>.</p><p>Similarly, a lot of work has gone into the release to improve the behavior and performance of the GC when operating in a containerized environment (and in particular in one that’s heavily constrained), such as in PR&nbsp;<a href="https://github.com/dotnet/coreclr/pull/22180">dotnet/coreclr#22180</a>. Again, @Maoni0 can do a much better job than I can describing this work, and you can read all about it her two blog posts,&nbsp;<a href="https://blogs.msdn.microsoft.com/maoni/2018/11/16/running-with-server-gc-in-a-small-container-scenario-part-0/" rel="nofollow">running-with-server-gc-in-a-small-container-scenario-part-0</a>&nbsp;and&nbsp;<a href="https://blogs.msdn.microsoft.com/maoni/2019/02/04/running-with-server-gc-in-a-small-container-scenario-part-1-hard-limit-for-the-gc-heap/" rel="nofollow">running-with-server-gc-in-a-small-container-scenario-part-1-hard-limit-for-the-gc-heap</a>.</p><h3>JIT</h3><p>A lot of goodness has gone into the just-in-time (JIT) compiler in .NET Core 3.0.</p><p>One of the most impactful changes is tiered compilation (this is split across many PRs, but for example PR&nbsp;<a href="https://github.com/dotnet/coreclr/pull/23599">dotnet/coreclr#23599</a>). Tiered compilation is a solution for the problem that very good compilation from MSIL to native code takes time; the more analysis to be done, the more optimizations to be applied, the longer it takes. But with a JIT compiler that does that code generation at runtime, that time comes at the direct expense of application start-up, and so you’re left with a trade-off: do you spend more time generating better code but take longer to get going, or do you spend less time generating less-good code but get going faster? Tiered compilation is a scheme for accomplishing both. The idea is that methods are first compiled with a fast pass that applies few-to-no optimizations but that completes very quickly, and then as methods are seen to execute again and again, those methods are re-JIT’d, this time with more time spent on code quality.</p><p>Interestingly, though, tiered compilation isn’t just about start-up time. There are optimizations that the re-compilation can take advantage of that weren’t available the first time around. For example, tiered compilation can apply to ready-to-run (R2R) images, a form of precompilation employed by assemblies in the .NET Core shared framework. These assemblies contain precompiled native code, but in some ways the optimizations that can be applied during that native code generation are limited in order to aid in version resiliency, e.g. cross-module inlining doesn’t happen with R2R. So, the R2R code can help enable faster start-up, but then methods found to be used frequently can be re-compiled via tiered compilation, thereby taking advantage of such optimizations the original precompiled code was restricted from using.</p><p>Here’s an example of that. First, we can run the following benchmark.</p><div class="highlight highlight-source-cs"><pre class="wrap:false lang:default decode:true">private XmlDocument _doc = new XmlDocument();

[Benchmark]
public void LoadXml()
{
    _doc.RemoveAll();
    _doc.LoadXml("&lt;Root&gt;&lt;Element attrib=\"foo\" attrib2=\"foo2\"&gt;foo&lt;/Element&gt;&lt;Element attrib=\"foo\" attrib2=\"foo2\"&gt;foo&lt;/Element&gt;&lt;Element attrib=\"foo\" attrib2=\"foo2\"&gt;foo&lt;/Element&gt;&lt;Element attrib=\"foo\" attrib2=\"foo2\"&gt;foo&lt;/Element&gt;&lt;Element attrib=\"foo\" attrib2=\"foo2\"&gt;foo&lt;/Element&gt;&lt;Element attrib=\"foo\" attrib2=\"foo2\"&gt;foo&lt;/Element&gt;&lt;Element attrib=\"foo\" attrib2=\"foo2\"&gt;foo&lt;/Element&gt;&lt;Element attrib=\"foo\" attrib2=\"foo2\"&gt;foo&lt;/Element&gt;&lt;/Root&gt;");
}</pre></div><table border="1"><thead><tr><th>Method</th><th>Toolchain</th><th align="right">Mean</th><th align="right">Error</th><th align="right">StdDev</th><th align="right">Ratio</th></tr></thead><tbody><tr><td>LoadXml</td><td>netcoreapp2.1</td><td align="right">9.576 us</td><td align="right">0.1523 us</td><td align="right">0.1425 us</td><td align="right">1.00</td></tr><tr><td>LoadXml</td><td>netcoreapp3.0</td><td align="right">7.414 us</td><td align="right">0.0980 us</td><td align="right">0.0868 us</td><td align="right">0.78</td></tr></tbody></table><p>Then, we can run it again, but this time with tiered compilation disabled by setting the&nbsp;<code>COMPlus_TieredCompilation</code>environment variable to 0.</p><table border="1"><thead><tr><th>Method</th><th>Toolchain</th><th align="right">Mean</th><th align="right">Error</th><th align="right">StdDev</th><th align="right">Ratio</th><th align="right">RatioSD</th></tr></thead><tbody><tr><td>LoadXml</td><td>netcoreapp2.1</td><td align="right">9.650 us</td><td align="right">0.1638 us</td><td align="right">0.1279 us</td><td align="right">1.00</td><td align="right">0.00</td></tr><tr><td>LoadXml</td><td>netcoreapp3.0</td><td align="right">9.002 us</td><td align="right">0.2018 us</td><td align="right">0.2073 us</td><td align="right">0.93</td><td align="right">0.03</td></tr></tbody></table><p>There are a variety of environment variables that configure tiered compilation and in what situations it’s enabled. For more details, see&nbsp;<a href="https://github.com/dotnet/coreclr/issues/24064">https://github.com/dotnet/coreclr/issues/24064</a>.</p><p>Another really cool improvement in the JIT comes in PR&nbsp;<a href="https://github.com/dotnet/coreclr/pull/20886">dotnet/coreclr#20886</a>. In previous releases of .NET, the JIT could optimize the usage of some primitive type&nbsp;<code>static readonly</code>&nbsp;fields as if they were constants. For example, if a&nbsp;<code>static readonly int</code>&nbsp;field were initialized to the value&nbsp;<code>42</code>&nbsp;by the time some code that used that field was JIT compiled, the JIT compiler would effectively treat that field instead as a&nbsp;<code>const</code>, and do constant folding and all other forms of optimizations that would otherwise apply. In .NET Core 3.0, the JIT can now utilize the type of&nbsp;<code>static readonly</code>fields to do additional optimizations. For example, if a&nbsp;<code>static readonly</code>&nbsp;field is typed as a base type but is then set to a derived type, the JIT might be able to see the actual type of the object stored in the field, and then when a virtual method is called on it, devirtualize the call and even potentially inline it.</p><div class="highlight highlight-source-cs"><pre class="lang:default decode:true">private static readonly Base s_base;

static Program() =&gt; s_base = new Derived();

[Benchmark]
public void AccessStatic() =&gt; s_base.Method();

private sealed class Derived : Base { public override void Method() { } }
private abstract class Base { public abstract void Method(); }</pre></div><table border="1"><thead><tr><th>Method</th><th>Toolchain</th><th align="right">Mean</th><th align="right">Error</th><th align="right">StdDev</th><th align="right">Median</th><th align="right">Ratio</th></tr></thead><tbody><tr><td>AccessStatic</td><td>netcoreapp2.1</td><td align="right">0.5625 ns</td><td align="right">0.0147 ns</td><td align="right">0.0130 ns</td><td align="right">0.5616 ns</td><td align="right">1.000</td></tr><tr><td>AccessStatic</td><td>netcoreapp3.0</td><td align="right">0.0015 ns</td><td align="right">0.0060 ns</td><td align="right">0.0062 ns</td><td align="right">0.0000 ns</td><td align="right">0.003</td></tr></tbody></table><p>That highlights some improvements that have gone into devirtualization, but there are others, such as in PRs&nbsp;<a href="https://github.com/dotnet/coreclr/pull/20447">dotnet/coreclr#20447</a>,&nbsp;<a href="https://github.com/dotnet/coreclr/pull/20292">dotnet/coreclr#20292</a>, and&nbsp;<a href="https://github.com/dotnet/coreclr/pull/20640">dotnet/coreclr#20640</a>&nbsp;which, when combined with PRs like&nbsp;<a href="https://github.com/dotnet/coreclr/pull/20637">dotnet/coreclr#20637</a>&nbsp;from @benaadams, help with APIs like&nbsp;<code>ArrayPool&lt;T&gt;.Shared<span><span>.</span></span></code></p><pre class="lang:default decode:true">[Benchmark]
public void RentReturn() =&gt; ArrayPool&lt;byte&gt;.Shared.Return(ArrayPool&lt;byte&gt;.Shared.Rent(256));</pre><table border="1"><thead><tr><th>Method</th><th>Toolchain</th><th align="right">Mean</th><th align="right">Error</th><th align="right">StdDev</th><th align="right">Ratio</th></tr></thead><tbody><tr><td>RentReturn</td><td>netcoreapp2.1</td><td align="right">32.92 ns</td><td align="right">0.3357 ns</td><td align="right">0.2803 ns</td><td align="right">1.00</td></tr><tr><td>RentReturn</td><td>netcoreapp3.0</td><td align="right">25.74 ns</td><td align="right">0.2392 ns</td><td align="right">0.1867 ns</td><td align="right">0.78</td></tr></tbody></table><p>Another nice improvement is around zeroing of locals. Even when the&nbsp;<code>initlocals</code>&nbsp;flag isn’t set (as of PR&nbsp;<a href="https://github.com/dotnet/corefx/pull/34406">dotnet/corefx#34406</a>, it’s cleared for all assemblies in coreclr and corefx), the JIT still needs to zero out references in locals so that the GC doesn’t see and misinterpret garbage, and that zero’ing can take a measurable amount of time, in particular in methods that do a lot of work with spans. PRs&nbsp;<a href="https://github.com/dotnet/coreclr/pull/23498">dotnet/coreclr#23498</a>&nbsp;and&nbsp;<a href="https://github.com/dotnet/coreclr/pull/13868">dotnet/coreclr#13868</a>&nbsp;make some nice improvements in this area.</p><div class="highlight highlight-source-cs"><pre class="lang:default decode:true">private byte[] _bytes = new byte[1];

[Benchmark]
public void StackZero()
{
    Span&lt;byte&gt; a, b;
    a = _bytes;
    b = _bytes;
    Nop(a, b);
}

[MethodImpl(MethodImplOptions.NoInlining)]
private void Nop(Span&lt;byte&gt; a, Span&lt;byte&gt; b) { }</pre></div><table border="1"><thead><tr><th>Method</th><th>Toolchain</th><th align="right">Mean</th><th align="right">Error</th><th align="right">StdDev</th><th align="right">Ratio</th></tr></thead><tbody><tr><td>StackZero</td><td>netcoreapp2.1</td><td align="right">8.948 ns</td><td align="right">0.2479 ns</td><td align="right">0.2546 ns</td><td align="right">1.00</td></tr><tr><td>StackZero</td><td>netcoreapp3.0</td><td align="right">2.389 ns</td><td align="right">0.0740 ns</td><td align="right">0.0727 ns</td><td align="right">0.27</td></tr></tbody></table><p>Another example relates to structs. As more and more recognition has come to .NET performance, in particular around allocation, there’s been a significant increase in the use of value types, often wrapping one another. For example, awaiting a&nbsp;<code>ValueTask&lt;T&gt;</code>&nbsp;results in calling&nbsp;<code>GetAwaiter()</code>&nbsp;on that value task, and that returns a&nbsp;<code>ValueTaskAwaiter&lt;T&gt;</code>&nbsp;that wraps the&nbsp;<code>ValueTask&lt;T&gt;</code>. PR&nbsp;<a href="https://github.com/dotnet/coreclr/pull/19429">dotnet/coreclr#19429</a>&nbsp;improves the situation by removing unnecessary copies involved in these operations.</p><pre class="lang:default decode:true">[Benchmark]
public int WrapUnwrap() =&gt; ValueTuple.Create(ValueTuple.Create(ValueTuple.Create(42))).Item1.Item1.Item1;</pre><table border="1"><thead><tr><th>Method</th><th>Toolchain</th><th align="right">Mean</th><th align="right">Error</th><th align="right">StdDev</th><th align="right">Median</th><th align="right">Ratio</th></tr></thead><tbody><tr><td>WrapUnwrap</td><td>netcoreapp2.1</td><td align="right">1.2198 ns</td><td align="right">0.0717 ns</td><td align="right">0.0599 ns</td><td align="right">1.2095 ns</td><td align="right">1.000</td></tr><tr><td>WrapUnwrap</td><td>netcoreapp3.0</td><td align="right">0.0002 ns</td><td align="right">0.0007 ns</td><td align="right">0.0006 ns</td><td align="right">0.0000 ns</td><td align="right">0.000</td></tr></tbody></table><h3>What’s Next?</h3><p>As I write this post, I count 29 pending performance-focused PRs in the coreclr repo and another 8&nbsp; in the corefx repo. Some of those are likely to be merged in time for the .NET Core 3.0 release, as will, I’m sure, additional PRs that haven’t even been opened yet. In short, even after all of the improvements detailed in for&nbsp;<a href="https://blogs.msdn.microsoft.com/dotnet/2017/06/07/performance-improvements-in-net-core/" rel="nofollow">.NET Core 2.0</a>,&nbsp;<a href="https://devblogs.microsoft.com/dotnet/performance-improvements-in-net-core-2-1" rel="nofollow">.NET Core 2.1</a>, and now in this post for .NET Core 3.0, and even with all of those improvements contributing to ASP.NET Core being one of the fastest web servers on the planet, there is still incredible opportunity for performance to keep getting better and better, and for you to help achieve that. Hopefully this post has made you excited about the potential .NET Core 3.0 holds. I look forward to reviewing your PRs as we all contribute to this exciting future together!</p><div class="authorinfoarea"></div></div></div></div></div>
</div>
    </div>
    <footer>
        <div>created by buildstarted &copy; 2020 <a href="/about">about</a></div>
        <div>Share this page on social media: copy and paste this url https://linksfor.dev/</div>
        <div>If you prefer RSS: <a href="https://linksfor.dev/feed.xml">https://linksfor.dev/feed.xml</a></div>
    </footer>
    
    <script>
        (function () {
            var COLLECT_URL = "https://dna.buildstarted.com/t";
            var SITE_ID = "linksfor.devs";
            var GLOBAL_VAR_NAME = "__DNA__";

            window[GLOBAL_VAR_NAME] = {};

            window[GLOBAL_VAR_NAME].sendPageView = function () {
                var path = location.pathname;
                var referrer = document.referrer;

                var url = COLLECT_URL + "?siteid=" + SITE_ID + "&p=" + encodeURIComponent(path) + "&r=" + encodeURIComponent(referrer);

                try { fetch(url, { method: "GET" }); } catch (e) { }
            };

            window[GLOBAL_VAR_NAME].sendPageView();
        })();
    </script>
</body>
</html>