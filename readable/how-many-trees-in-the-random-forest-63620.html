<!DOCTYPE html>
<html lang="en">
<head>
    <title>
How many trees in the Random Forest? - linksfor.dev(s)    </title>
    <meta charset="utf-8">
    <link rel="alternate" type="application/rss+xml" title="Linksfor.dev(s) feed" href="https://linksfor.dev/feed.rss" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta name="google" value="notranslate">
    <link rel="stylesheet" type="text/css" href="/style.css" />
    <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon">
    <link rel="icon" href="/favicon.ico" type="image/x-icon">
    <meta property="og:title" content="How many trees in the Random Forest? - linksfor.dev(s)"/>
    <meta property="article:author" content="Piotr P&#x142;o&#x144;ski"/>
    <meta property="og:description" content="I have trained 3,600 Random Forest Classifiers (each with 1,000 trees) on 72 data sets (from OpenML-CC18 benchmark) to check how many trees should be used in the Random Forest. What I&#x2019;ve found:"/>
    <meta property="og:type" content="website"/>
    <meta property="og:url" content="https://mljar.com/blog/how-many-trees-in-random-forest/"/>

<meta property="og:site_name" content="linksfor.dev(s)" />
</head>
<body>
    <div class="devring" style="background: #222">
        <div style="text-align:center">Explore other dev related sites in this ring. If you would like to join this ring <a href="https://devring.club">click here</a>.</div>
        <div class="grid">
            <div style="display: grid; grid-template-columns: .5fr 1fr 1fr 1fr; text-align: center;">
                <span class="devring-title"><a href="https://devring.club/">devring.club</a></span>
                <a href="https://devring.club/sites/1/prev" class="devring-previous">Previous</a>
                <a href="https://devring.club/random" class="devring-random">Random</a>
                <a href="https://devring.club/sites/1/next" class="devring-next">Next</a>
            </div>
        </div>
    </div>
    <div class="grid">
        <h1>
<a href="/" style="color:inherit">linksfor.dev(s)</a>
        </h1>
        <title>linksfor.dev(s) - How many trees in the Random Forest?</title>
<div class="readable">
        <h1>How many trees in the Random Forest?</h1>
            <div>by Piotr P&#x142;o&#x144;ski</div>
            <div>Reading time: 11-14 minutes</div>
        <div>Posted here: 01 Jul 2020</div>
        <p><a href="https://mljar.com/blog/how-many-trees-in-random-forest/">https://mljar.com/blog/how-many-trees-in-random-forest/</a></p>
        <hr/>
<div id="readability-page-1" class="page"><div>
			
		<p>I have trained <code>3,600</code> Random Forest Classifiers (each with <code>1,000</code> trees) on <code>72</code> data sets (from <a href="https://docs.openml.org/benchmark/">OpenML-CC18 benchmark</a>) to check how many trees should be used in the Random Forest. What I’ve found:</p>

<ul>
  <li>the best performance is obtained by selecting the optimal tree number with single tree precision (I’ve tried to tune the number of trees with <code>100</code> trees and single tree step precision),</li>
  <li>the optimal number of trees in the Random Forest depends on the number of rows in the data set. The more rows in the data, the more trees are needed (the mean of the optimal number of trees is <code>464</code>),</li>
  <li>when tuning the number of trees in the Random Forest train it with maximum number of trees and then check how does the Random Forest perform with using different subset of trees (with single tree precision) on validation dataset (code example below).</li>
</ul>

<p>The details of my study is below. The link to all code in <a href="https://github.com/mljar/website_snippets/blob/master/how_many_trees/how_many_trees_in_RF.ipynb">Jupyter Notebook</a>. If you have any questions, you can email me at: <a href="mailto:piotr@mljar.com">
			piotr@mljar.com
	</a> or leave a comment.</p>

<h2 id="the-data-sets-from-openml-cc18-benchmark">The Data Sets From OpenML-CC18 Benchmark</h2>

<p>The data sets used in this study are from <a href="https://docs.openml.org/benchmark/#openml-cc18">OpenML-CC18 benchamrk</a>. Each dataset were carefully selected from thousands of data sets on OpenML by creators of the benchmark.</p>

<p>In the plot below are presented distribution of the number of rows and columns in the datasets, and scatter plot with data sizes:</p>

<p><img src="https://raw.githubusercontent.com/mljar/website_snippets/master/how_many_trees/dataset_description.png" alt="OpenML-CC18 dataset description"></p>

<p>Most of the datasets have up to few thousands rows and up to hundred columns - they are small and medium sized datasets. Eeach data set in the benchmark suite has a defined train and test splits for <code>10</code>-fold cross-validation.</p>

<h2 id="training-the-random-forest-classifiers-with-scikit-learn">Training the Random Forest Classifiers with Scikit-Learn</h2>

<p>For each train and test split I added <code>5</code>-fold cross-validation (internally), so I have a validation data which can be used to tune the number of trees. To make it clear:</p>

<ul>
  <li>each data set has defined train and test split (with <code>10</code>-fold CV),</li>
  <li>for each train data, I additionally split it into train and validation with <code>5</code>-fold CV,</li>
  <li>for each set of (train, validation, test) data I have trained a Random Forest classifier from <code>scikit-learn</code> with <code>n_estimators=1000</code> (other hyper-parameters are left default).</li>
</ul>

<p>The code to train and save the Random Forests models:</p>

<div><div><pre><code><span># load packages</span>
<span>import</span> <span>os</span>
<span>import</span> <span>openml</span>
<span>import</span> <span>joblib</span>

<span>import</span> <span>pandas</span> <span>as</span> <span>pd</span>
<span>import</span> <span>numpy</span> <span>as</span> <span>np</span>

<span>from</span> <span>sklearn.ensemble</span> <span>import</span> <span>RandomForestClassifier</span>
<span>from</span> <span>sklearn.metrics</span> <span>import</span> <span>log_loss</span>
<span>from</span> <span>sklearn.preprocessing</span> <span>import</span> <span>LabelEncoder</span>
<span>from</span> <span>sklearn.model_selection</span> <span>import</span> <span>StratifiedKFold</span>
</code></pre></div></div>

<p>Set constants:</p>

<div><div><pre><code><span>openml</span><span>.</span><span>config</span><span>.</span><span>apikey</span> <span>=</span> <span>"af03aa0fe70d3aed007035c74222e658"</span> <span># my old OpenML API</span>
<span>DIR</span> <span>=</span> <span>"where-to-save-rf-models"</span>
<span>TOTAL_TREES</span> <span>=</span> <span>1000</span>

<span>benchmark_suite</span> <span>=</span> <span>openml</span><span>.</span><span>study</span><span>.</span><span>get_suite</span><span>(</span><span>'OpenML-CC18'</span><span>)</span> <span># obtain the benchmark suite</span>
</code></pre></div></div>

<p>Train and save the Random Forests. Please note, that while saving the model (with <code>joblib.dump</code>) I set the <code>compress=3</code> which <a href="https://mljar.com/blog/save-load-random-forest/">reduces the model file size (sometimes even up to 5 times)</a>.</p>

<div><div><pre><code><span>def</span> <span>train_random_forest</span><span>(</span><span>X</span><span>,</span> <span>y</span><span>,</span> <span>fname_prefix</span><span>):</span>
    <span># train Random Forest with internal 5-fold CV</span>
    <span>skf</span> <span>=</span> <span>StratifiedKFold</span><span>(</span><span>n_splits</span><span>=</span><span>5</span><span>,</span> <span>shuffle</span><span>=</span><span>True</span><span>,</span> <span>random_state</span><span>=</span><span>1234</span><span>)</span>
    <span>for</span> <span>fold</span><span>,</span> <span>(</span><span>train_index</span><span>,</span> <span>validation_index</span><span>)</span> <span>in</span> <span>enumerate</span><span>(</span><span>skf</span><span>.</span><span>split</span><span>(</span><span>X</span><span>,</span> <span>y</span><span>)):</span>
        <span>X_train</span><span>,</span> <span>X_validation</span> <span>=</span> <span>X</span><span>[</span><span>train_index</span><span>],</span> <span>X</span><span>[</span><span>validation_index</span><span>]</span>
        <span>y_train</span><span>,</span> <span>y_validation</span> <span>=</span> <span>y</span><span>[</span><span>train_index</span><span>],</span> <span>y</span><span>[</span><span>validation_index</span><span>]</span>
        
        <span>model</span> <span>=</span> <span>RandomForestClassifier</span><span>(</span><span>n_estimators</span><span>=</span><span>TOTAL_TREES</span><span>,</span> <span>random_state</span><span>=</span><span>1234</span><span>,</span> <span>n_jobs</span><span>=-</span><span>1</span><span>)</span>
        <span>model</span><span>.</span><span>fit</span><span>(</span><span>X_train</span><span>,</span> <span>y_train</span><span>)</span>
        <span>joblib</span><span>.</span><span>dump</span><span>(</span><span>model</span><span>,</span> <span>fname_prefix</span> <span>+</span> <span>f</span><span>"_f_{fold}"</span><span>,</span> <span>compress</span><span>=</span><span>3</span><span>)</span>
        

<span>for</span> <span>task_id</span> <span>in</span> <span>benchmark_suite</span><span>.</span><span>tasks</span><span>:</span>  <span># iterate over all tasks</span>
    <span>task</span> <span>=</span> <span>openml</span><span>.</span><span>tasks</span><span>.</span><span>get_task</span><span>(</span><span>task_id</span><span>)</span>  <span># download the OpenML task</span>
    <span>X</span><span>,</span> <span>y</span> <span>=</span> <span>task</span><span>.</span><span>get_X_and_y</span><span>()</span>  <span># get the data</span>
    
    <span># basic preprocessing</span>
    <span>df</span> <span>=</span> <span>pd</span><span>.</span><span>DataFrame</span><span>(</span><span>X</span><span>)</span>
    <span>to_remove</span> <span>=</span> <span>[]</span>
    <span>for</span> <span>col</span> <span>in</span> <span>df</span><span>.</span><span>columns</span><span>:</span>
        <span>empty_column</span> <span>=</span> <span>np</span><span>.</span><span>sum</span><span>(</span><span>pd</span><span>.</span><span>isnull</span><span>(</span><span>df</span><span>[</span><span>col</span><span>])</span> <span>==</span> <span>True</span><span>)</span> <span>==</span> <span>df</span><span>.</span><span>shape</span><span>[</span><span>0</span><span>]</span>
        <span>constant_column</span> <span>=</span> <span>len</span><span>(</span><span>np</span><span>.</span><span>unique</span><span>(</span><span>df</span><span>.</span><span>loc</span><span>[</span><span>~</span><span>pd</span><span>.</span><span>isnull</span><span>(</span><span>df</span><span>[</span><span>col</span><span>]),</span> <span>col</span><span>]))</span> <span>==</span> <span>1</span>
        <span>if</span> <span>empty_column</span> <span>or</span> <span>constant_column</span><span>:</span>
            <span>to_remove</span> <span>+=</span> <span>[</span><span>col</span><span>]</span>
    <span># remove constant and empty columns</span>
    <span>df</span><span>.</span><span>drop</span><span>(</span><span>to_remove</span><span>,</span> <span>inplace</span><span>=</span><span>True</span><span>,</span> <span>axis</span><span>=</span><span>1</span><span>)</span>
    <span># fill missing values with mode</span>
    <span>df_mode</span> <span>=</span> <span>df</span><span>.</span><span>mode</span><span>()</span><span>.</span><span>iloc</span><span>[</span><span>0</span><span>]</span>
    <span>df</span> <span>=</span> <span>df</span><span>.</span><span>fillna</span><span>(</span><span>df_mode</span><span>)</span>
    <span>X</span> <span>=</span> <span>np</span><span>.</span><span>array</span><span>(</span><span>df</span><span>)</span> <span># back to numpy array</span>
    
    <span># repeat 10 times (10-fold CV)</span>
    <span>n_repeats</span><span>,</span> <span>n_folds</span><span>,</span> <span>n_samples</span> <span>=</span> <span>task</span><span>.</span><span>get_split_dimensions</span><span>()</span>
    <span>for</span> <span>fold</span> <span>in</span> <span>range</span><span>(</span><span>n_folds</span><span>):</span>
        <span>train_indices</span><span>,</span> <span>test_indices</span> <span>=</span> <span>task</span><span>.</span><span>get_train_test_split_indices</span><span>(</span><span>repeat</span><span>=</span><span>0</span><span>,</span> <span>fold</span><span>=</span><span>fold</span><span>,</span> <span>sample</span><span>=</span><span>0</span><span>)</span>
        <span>X_train</span> <span>=</span> <span>X</span><span>[</span><span>train_indices</span><span>]</span>
        <span>y_train</span> <span>=</span> <span>y</span><span>[</span><span>train_indices</span><span>]</span>
        <span>X_test</span> <span>=</span> <span>X</span><span>[</span><span>test_indices</span><span>]</span>
        <span>y_test</span> <span>=</span> <span>y</span><span>[</span><span>test_indices</span><span>]</span>

        <span>fname_prefix</span> <span>=</span> <span>os</span><span>.</span><span>path</span><span>.</span><span>join</span><span>(</span><span>DIR</span><span>,</span> <span>f</span><span>"task_{task_id}_repeat_{fold}_rf"</span><span>)</span>
        <span>train_random_forest</span><span>(</span><span>X_train</span><span>,</span> <span>y_train</span><span>,</span> <span>fname_prefix</span><span>)</span>
</code></pre></div></div>

<h2 id="compute-the-performance-of-the-random-forest-classifier">Compute the Performance of the Random Forest Classifier</h2>

<p>To compute the model’s performance I use the test data and <a href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.log_loss.html"><code>logloss</code></a> metric. I check what is the performance of the Random Forest with <code>[100, 200, 300, 400, 500, 600, 700, 800, 900, 1000]</code> trees. <strong>Please notice, that I’m not training <code>10</code> Random Forests models with different number of trees! I’m reusing the Random Forest with <code>1000</code> trees, with setting different numer of <code>n_estimators</code> before prediction.</strong> This saves a lot of computational time when doing a hyper-parameters search.  The final response is the average prediction from the <code>5</code> Random Forests (trained with internal <code>5</code>-fold CV).</p>

<p>In the second approach, I search for the number of trees which will minimize the <code>logloss</code> on validation data. In this method, I iterate over all trees in the forest and compute predictions from single trees. The prediction of the Random Forest is the average from all trees in the subset (I’m doing manually what is done internally in <code>predict_proba</code> in the Random Forest). As ealier, the final response is the average over all <code>5</code> models (from internal CV). Notice, that each model from internal CV can have (and have) different number of trees.</p>

<p>The code to compute reponses from Random Forest with <code>100</code> trees and <code>1</code> tree step.</p>

<div><div><pre><code><span>def</span> <span>compute_performance</span><span>(</span><span>X</span><span>,</span> <span>y</span><span>,</span> <span>X_test</span><span>,</span> <span>y_test</span><span>,</span> <span>fname_prefix</span><span>):</span>
    <span>FOLDS</span> <span>=</span> <span>5</span>
    <span>results</span> <span>=</span> <span>{}</span>
    <span># iterate over trees, with 100 trees step</span>
    <span>for</span> <span>t</span> <span>in</span> <span>range</span><span>(</span><span>10</span><span>):</span>
        <span>trees</span> <span>=</span> <span>100</span><span>*</span><span>(</span><span>t</span><span>+</span><span>1</span><span>)</span>
        <span>response</span> <span>=</span> <span>None</span> 
        <span>for</span> <span>cv_fold</span> <span>in</span> <span>range</span><span>(</span><span>FOLDS</span><span>):</span>
            <span>model</span> <span>=</span> <span>joblib</span><span>.</span><span>load</span><span>(</span><span>fname_prefix</span> <span>+</span> <span>f</span><span>"_f_{cv_fold}"</span><span>)</span> <span># load model</span>
            <span>model</span><span>.</span><span>estimators_</span> <span>=</span> <span>model</span><span>.</span><span>estimators_</span><span>[:</span><span>trees</span><span>]</span>
            <span>proba</span> <span>=</span> <span>model</span><span>.</span><span>predict_proba</span><span>(</span><span>X_test</span><span>)</span>
            <span>response</span> <span>=</span> <span>proba</span> <span>if</span> <span>response</span> <span>is</span> <span>None</span> <span>else</span> <span>response</span> <span>+</span> <span>proba</span>
        <span>response</span> <span>/=</span> <span>float</span><span>(</span><span>FOLDS</span><span>)</span> <span># the final prediction is average from internal cross-validation</span>
        <span>ll</span> <span>=</span> <span>log_loss</span><span>(</span><span>y_test</span><span>,</span> <span>response</span><span>)</span>
        <span>results</span><span>[</span><span>trees</span><span>]</span> <span>=</span> <span>ll</span>
    
    <span># check the number of trees with minimum loss on validation data set (1 tree step)</span>
    <span>skf</span> <span>=</span> <span>StratifiedKFold</span><span>(</span><span>n_splits</span><span>=</span><span>FOLDS</span><span>,</span> <span>shuffle</span><span>=</span><span>True</span><span>,</span> <span>random_state</span><span>=</span><span>1234</span><span>)</span>
    <span>selected_trees</span> <span>=</span> <span>[]</span>
    <span>response</span> <span>=</span> <span>None</span>
    <span>for</span> <span>cv_fold</span><span>,</span> <span>(</span><span>train_index</span><span>,</span> <span>validation_index</span><span>)</span> <span>in</span> <span>enumerate</span><span>(</span><span>skf</span><span>.</span><span>split</span><span>(</span><span>X</span><span>,</span> <span>y</span><span>)):</span>
        <span>X_validation</span> <span>=</span> <span>X</span><span>[</span><span>validation_index</span><span>]</span>
        <span>y_validation</span> <span>=</span> <span>y</span><span>[</span><span>validation_index</span><span>]</span>
        
        <span>model</span> <span>=</span> <span>joblib</span><span>.</span><span>load</span><span>(</span><span>fname_prefix</span> <span>+</span> <span>f</span><span>"_f_{cv_fold}"</span><span>)</span> <span># load model</span>
        <span>y_validation_predicted</span> <span>=</span> <span>None</span>
        <span>losses</span> <span>=</span> <span>[]</span>
        <span># iterate over each tree</span>
        <span>for</span> <span>count</span><span>,</span> <span>estimator</span> <span>in</span> <span>enumerate</span><span>(</span><span>model</span><span>.</span><span>estimators_</span><span>):</span>
            <span>proba</span> <span>=</span> <span>estimator</span><span>.</span><span>predict_proba</span><span>(</span><span>X_validation</span><span>)</span>
            <span>y_validation_predicted</span> <span>=</span> <span>proba</span> <span>if</span> <span>y_validation_predicted</span> <span>is</span> <span>None</span> <span>else</span> <span>proba</span> <span>+</span> <span>y_validation_predicted</span>
            <span>losses</span> <span>+=</span> <span>[</span><span>log_loss</span><span>(</span><span>y_validation</span><span>,</span> <span>y_validation_predicted</span> <span>/</span> <span>float</span><span>(</span><span>count</span><span>+</span><span>1.0</span><span>))]</span>
         
        <span># set selected number of trees    </span>
        <span>selected_trees</span> <span>+=</span> <span>[</span><span>np</span><span>.</span><span>argmin</span><span>(</span><span>losses</span><span>)]</span>
        <span>model</span><span>.</span><span>estimators_</span> <span>=</span> <span>model</span><span>.</span><span>estimators_</span><span>[:</span><span>selected_trees</span><span>[</span><span>-</span><span>1</span><span>]]</span>
        <span>proba</span> <span>=</span> <span>model</span><span>.</span><span>predict_proba</span><span>(</span><span>X_test</span><span>)</span>
        <span>response</span> <span>=</span> <span>proba</span> <span>if</span> <span>response</span> <span>is</span> <span>None</span> <span>else</span> <span>response</span> <span>+</span> <span>proba</span>
        
    <span>response</span> <span>/=</span> <span>float</span><span>(</span><span>FOLDS</span><span>)</span>
    <span>ll</span> <span>=</span> <span>log_loss</span><span>(</span><span>y_test</span><span>,</span> <span>response</span><span>)</span>
    <span>results</span><span>[</span><span>"Selected"</span><span>]</span> <span>=</span> <span>ll</span>
        
    <span>return</span> <span>results</span><span>,</span> <span>selected_trees</span>
</code></pre></div></div>

<p>The code to aggregate all results:</p>

<div><div><pre><code><span>all_results</span> <span>=</span> <span>{}</span>
<span>all_selected_trees</span> <span>=</span> <span>{}</span>
<span>for</span> <span>task_id</span> <span>in</span> <span>benchmark_suite</span><span>.</span><span>tasks</span><span>:</span>  <span># iterate over all tasks</span>
    <span>task</span> <span>=</span> <span>openml</span><span>.</span><span>tasks</span><span>.</span><span>get_task</span><span>(</span><span>task_id</span><span>)</span>  <span># download the OpenML task</span>
    <span>X</span><span>,</span> <span>y</span> <span>=</span> <span>task</span><span>.</span><span>get_X_and_y</span><span>()</span>  <span># get the data</span>
    
    <span># basic preprocessing</span>
    <span>df</span> <span>=</span> <span>pd</span><span>.</span><span>DataFrame</span><span>(</span><span>X</span><span>)</span>
    <span>to_remove</span> <span>=</span> <span>[]</span>
    <span>for</span> <span>col</span> <span>in</span> <span>df</span><span>.</span><span>columns</span><span>:</span>
        <span>empty_column</span> <span>=</span> <span>np</span><span>.</span><span>sum</span><span>(</span><span>pd</span><span>.</span><span>isnull</span><span>(</span><span>df</span><span>[</span><span>col</span><span>])</span> <span>==</span> <span>True</span><span>)</span> <span>==</span> <span>df</span><span>.</span><span>shape</span><span>[</span><span>0</span><span>]</span>
        <span>constant_column</span> <span>=</span> <span>len</span><span>(</span><span>np</span><span>.</span><span>unique</span><span>(</span><span>df</span><span>.</span><span>loc</span><span>[</span><span>~</span><span>pd</span><span>.</span><span>isnull</span><span>(</span><span>df</span><span>[</span><span>col</span><span>]),</span> <span>col</span><span>]))</span> <span>==</span> <span>1</span>
        <span>if</span> <span>empty_column</span> <span>or</span> <span>constant_column</span><span>:</span>
            <span>to_remove</span> <span>+=</span> <span>[</span><span>col</span><span>]</span>

    <span>df</span><span>.</span><span>drop</span><span>(</span><span>to_remove</span><span>,</span> <span>inplace</span><span>=</span><span>True</span><span>,</span> <span>axis</span><span>=</span><span>1</span><span>)</span>    
    <span>df_mode</span> <span>=</span> <span>df</span><span>.</span><span>mode</span><span>()</span><span>.</span><span>iloc</span><span>[</span><span>0</span><span>]</span>
    <span>df</span> <span>=</span> <span>df</span><span>.</span><span>fillna</span><span>(</span><span>df_mode</span><span>)</span>
    <span>X</span> <span>=</span> <span>np</span><span>.</span><span>array</span><span>(</span><span>df</span><span>)</span>
    
    <span>all_results</span><span>[</span><span>task_id</span><span>]</span> <span>=</span> <span>[]</span>
    <span>all_selected_trees</span><span>[</span><span>task_id</span><span>]</span> <span>=</span> <span>[]</span>
    <span>n_repeats</span><span>,</span> <span>n_folds</span><span>,</span> <span>n_samples</span> <span>=</span> <span>task</span><span>.</span><span>get_split_dimensions</span><span>()</span>
    <span>for</span> <span>fold</span> <span>in</span> <span>range</span><span>(</span><span>n_folds</span><span>):</span>
        <span>train_indices</span><span>,</span> <span>test_indices</span> <span>=</span> <span>task</span><span>.</span><span>get_train_test_split_indices</span><span>(</span><span>repeat</span><span>=</span><span>0</span><span>,</span> <span>fold</span><span>=</span><span>fold</span><span>,</span> <span>sample</span><span>=</span><span>0</span><span>)</span>

        <span>X_train</span> <span>=</span> <span>X</span><span>[</span><span>train_indices</span><span>]</span>
        <span>y_train</span> <span>=</span> <span>y</span><span>[</span><span>train_indices</span><span>]</span>
        <span>X_test</span> <span>=</span> <span>X</span><span>[</span><span>test_indices</span><span>]</span>
        <span>y_test</span> <span>=</span> <span>y</span><span>[</span><span>test_indices</span><span>]</span>

        <span>fname_prefix</span> <span>=</span> <span>os</span><span>.</span><span>path</span><span>.</span><span>join</span><span>(</span><span>DIR</span><span>,</span> <span>f</span><span>"task_{task_id}_repeat_{fold}_rf"</span><span>)</span>
        <span>results</span><span>,</span> <span>selected_trees</span> <span>=</span> <span>compute_performance</span><span>(</span><span>X_train</span><span>,</span> <span>y_train</span><span>,</span> <span>X_test</span><span>,</span> <span>y_test</span><span>,</span> <span>fname_prefix</span><span>)</span>
        
        <span>all_results</span><span>[</span><span>task_id</span><span>]</span> <span>+=</span> <span>[</span><span>results</span><span>]</span>
        <span>all_selected_trees</span><span>[</span><span>task_id</span><span>]</span> <span>+=</span> <span>[</span><span>selected_trees</span><span>]</span>
</code></pre></div></div>

<h2 id="results">Results</h2>

<p>The results are presented as boxplots for each data set. On the x-axis, there is a number of trees. The last box in each plot is the result from optimized number of trees (on x-axis marked as ‘Selected’). On the y-axis, there is <code>logloss</code> metric.</p>

<p><img src="https://raw.githubusercontent.com/mljar/website_snippets/master/how_many_trees/boxplots.png" alt="The results of analysis"></p>

<p>To compare all results in the single plot, I used ranking (for each data set and repetition, I ordered the losses and assign rank from <code>0</code> to <code>10</code> from the best to the worst predictions, so the lowest rank is the best). The average rank is presented below:</p>

<p><img src="https://raw.githubusercontent.com/mljar/website_snippets/master/how_many_trees/ranked.png" alt="The ranked results"></p>

<p>Two things worth to remeber from the above plot:</p>

<ul>
  <li>the more trees in the Random Forest the performance is better (lower rank),</li>
  <li>however, if the number of trees is tuned with <code>1</code> tree step, the performance is significantly better than in the largest forest.</li>
</ul>

<p>I comupted the mean percentage different between performance of the Random Forest with selected tree number and tree number set with <code>100</code> trees step, it is presented below:</p>

<p><img src="https://raw.githubusercontent.com/mljar/website_snippets/master/how_many_trees/percent_change.png" alt="The percent change in preformance comparing to selected number of trees"></p>

<p>The smallest Random Forest with <code>100</code> trees loses about <code>2%</code> to tuned Random Forest. For ensembles with more than <code>600</code> trees, the mean difference is about <code>0.25%</code>. (Is the <code>0.25-2%</code> a big improvement? It depends on your task.)</p>

<p>The distribution of the optimized tree number in the Random Forest:</p>

<p><img src="https://raw.githubusercontent.com/mljar/website_snippets/master/how_many_trees/optimized_tree_number_distribution.png" alt="The number of trees distribution"></p>

<p>The mean of above distribution is <code>464</code>, but as you see it is very wide and it will be hard to infer one optmial number of trees in the Random Forest (it strongly depends on the data).</p>

<p>It is interesting, how does the selected number of trees depend on the data set size. There is a strong dependency between number of trees selected in the forest and rows count in the data:</p>

<p><img src="https://raw.githubusercontent.com/mljar/website_snippets/master/how_many_trees/trees_vs_nrows.png" alt="The dependency between tree size and data size, rows"></p>

<p>The more rows in the data the more trees are needed. There is not such clear dependency between the selected tree number and number of columns in the data set:</p>

<p><img src="https://raw.githubusercontent.com/mljar/website_snippets/master/how_many_trees/trees_vs_cols.png" alt="The dependency between tree size and data size, cols"></p>

<h2 id="summary">Summary</h2>

<ul>
  <li>It is important to tune the number of trees in the Random Forest.</li>
  <li>To tune number of trees in the Random Forest, train the model with large number of trees (for example <code>1000</code> trees) and select from it optimal subset of trees. There is no need to train new Random Forest with different tree numbers each time.</li>
  <li>The number of trees needed in the Random Forest depends on the number of rows in the data set. The more rows in the data, the more trees are needed.</li>
  <li>In MLJAR’s open-source AutoML python package <a href="https://github.com/mljar/mljar-supervised"><code>mljar-supervised</code></a> the number of trees is tuned with <code>1</code> tree step. The same approach is for Extra Trees algorithm. The details of the implmentation can be found in the <a href="https://github.com/mljar/mljar-supervised/blob/master/supervised/algorithms/sklearn.py#L98">Github repository</a>. Maybe it will be nice to have early stopping implementation in the <code>scikit-learn</code> Random Forest.</li>
</ul>


		
		
		
			
			
			
		
	</div></div></div>
    </div>
    <footer>
        <div>created by buildstarted &copy; 2020 <a href="/about">about</a></div>
        <div>Share this page on social media: copy and paste this url https://linksfor.dev/</div>
        <div>If you prefer RSS: <a href="https://linksfor.dev/feed.xml">https://linksfor.dev/feed.xml</a></div>
        <div>Customer satisfaction guaranteed to be optional.</div>
    </footer>
    
    <script async defer>
        _dna = window._dna || {};
        _dna.siteId = "linksfor.devs";
        _dna.outlink = true;

        (function() {
            let dna = document.createElement('script');
            dna.type = 'text/javascript';
            dna.async = true;
            dna.src = '//dna.buildstarted.com/t.js';
            let s = document.getElementsByTagName('script')[0];
            s.parentNode.insertBefore(dna, s);
        })();
    </script>
    <noscript><img src="//dna.buildstarted.com/g?siteId=linksfor.devs"/></noscript>
</body>
</html>