<!DOCTYPE html>
<html lang="en">
<head>
    <title>
Unity Labs Publications | Unity - linksfor.dev(s)    </title>
    <meta charset="utf-8">
    <link rel="alternate" type="application/rss+xml" title="Linksfor.dev(s) feed" href="https://linksfor.dev/feed.rss" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta name="google" value="notranslate">
    <link rel="stylesheet" type="text/css" href="/style.min.css" />
    <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon">
    <link rel="icon" href="/favicon.ico" type="image/x-icon">
    <meta property="og:title" content="Unity Labs Publications | Unity - linksfor.dev(s)"/>
    <meta property="article:author" content="Unity Technologies"/>
    <meta property="og:description" content="The team in Grenoble does research only and their output is papers. We will use this page to share said papers."/>
    <meta property="og:type" content="website"/>
    <meta property="og:url" content="https://unity.com/labs/publications"/>

<meta property="og:site_name" content="linksfor.dev(s)" />
</head>
<body>
    <div class="devring" style="background: #222">
        <div style="text-align:center">Explore other dev related sites in this ring. If you would like to join this ring <a href="https://devring.club">click here</a>.</div>
        <div class="grid">
            <div style="display: grid; grid-template-columns: .5fr 1fr 1fr 1fr; text-align: center;">
                <span class="devring-title"><a href="https://devring.club/">devring.club</a></span>
                <a href="https://devring.club/sites/1/prev" class="devring-previous">Previous</a>
                <a href="https://devring.club/random" class="devring-random">Random</a>
                <a href="https://devring.club/sites/1/next" class="devring-next">Next</a>
            </div>
        </div>
    </div>
    <div class="grid">
        <h1 style="margin: unset">
<a href="/" style="color:inherit">linksfor.dev(s)</a>
        </h1>
        <title>linksfor.dev(s) - Unity Labs Publications | Unity</title>
<div class="readable">
        <h1>Unity Labs Publications | Unity</h1>
            <div>by Unity Technologies</div>
            <div>Reading time: 25-31 minutes</div>
        <div>Posted here: 27 Feb 2019</div>
        <p><a href="https://unity.com/labs/publications">https://unity.com/labs/publications</a></p>
        <hr/>
<div id="readability-page-1" class="page"><div id="master-wrapper">
      <div data-off-canvas-main-canvas="">
    
  




<section id="main">
    
<main>
  
<article role="article" id="unity-labs-publications">

  
    

  
  <div>
    
      <div>
              <div>

  
  <section>
    
  <div>
    <div>
      <div>
        <div>
                    
                      <p>At Unity, we do research for Graphics, AI, Performance and much more. We share that research with you and the community through talks, conferences and journals. See below for the latest publications.</p>
                                
                  </div>
      </div>
    </div>
  </div>
</section>
</div>
              <div>


      <section id="tabs-8650--2">
    <div>

      
      <div>
        
        <div>
                      
      <div>
              <div>
  <div id="multi-stylization-video-games-real-time-hpg-2019">

    
                    <h2>
            <p>Multi-Stylization of Video Games in Real-Time guided by G-buffer Information</p>
      </h2>
      
      <div>
        
            <div><p><strong>Adèle Saint-Denis, Kenneth Vanhoey, Thomas Deliot</strong> <strong>HPG 2019</strong></p>
<p>We investigate how to take advantage of modern neural style transfer techniques to modify the style of video games at runtime. Recent style transfer neural networks are pre-trained, and allow for fast style transfer of any style at runtime. However, a single style applies globally, over the full image, whereas we would like to provide finer authoring tools to the user. In this work, we allow the user to assign styles (by means of a style image) to various physical quantities found in the G-buffer of a deferred rendering pipeline, like depth, normals, or object ID. Our algorithm then interpolates those styles smoothly according to the scene to be rendered: e.g., a different style arises for different objects, depths, or orientations.</p>
<p><a href="http://kenneth.vanhoey.free.fr/data/research/poster_HPG2019.pdf">Poster</a><br>
&nbsp;</p>
</div>
      
      </div>

                    
      
      
      </div>
</div>
              <div>
  <div id="surface-gradient-based-bump-mapping-framework-2019">

    
              <p><img src="https://unity.com/sites/default/files/styles/810_scale_width/public/2019-10/botd_rock_detail.png?itok=HlWBlVHF" alt="">
                  </p>
                    <h2>
            <p>Surface Gradient Based Bump Mapping Framework</p>
      </h2>
      
      <div>
        
            <div><p><strong>Morten Mikkelsen 2019</strong></p>
<div>
<div>
<div><p>
In this paper a new framework is proposed for layering/compositing of bump/normal maps including support for both multiple sets of texture coordinates as well as procedurally generated texture coordinates and geometry. Furthermore, we provide proper support and integration for bump maps defined on a volume such as decal projectors, triplanar projection and noise–based functions.</p>
<p><a href="https://mmikk.github.io/papers3d/sgp.pdf">Paper</a>
</p></div>
</div>
</div>
</div>
      
      </div>

                    
      
      
      </div>
</div>
              <div>
  <div id="distributing-monte-carlo-errors-blue-noise-screen-space-permuting-pixel-seeds-between-frames-egsr">

    
              <p><img src="https://unity.com/sites/default/files/styles/810_scale_width/public/2019-07/Bluenoiseerror.jpg?itok=u7BIlKtP" alt="">
                  </p>
                    <h2>
            <p>Distributing Monte Carlo Errors as a Blue Noise in Screen Space by Permuting Pixel Seeds Between Frames</p>
      </h2>
      
      <div>
        
            <div><p><strong>Eric Heitz, Laurent Belcour -</strong> <strong>EGSR</strong>&nbsp; <strong>2019</strong></p>
<p>We introduce a sampler that generates per-pixel samples achieving high visual quality thanks to two key properties related to the Monte Carlo errors that it produces. First, the sequence of each pixel is an Owen-scrambled Sobol sequence that has state-of-the-art convergence properties. The Monte Carlo errors have thus low magnitudes. Second, these errors are distributed as a blue noise in screen space. This makes them visually even more acceptable. Our sam-pler is lightweight and fast. We implement it with a small texture and two xor operations. Our supplemental material provides comparisons against previous work for different scenes and sample counts.</p>
<p><a href="https://drive.google.com/file/d/1znhbmKGeHphfae1tz3YnroOzOA5-sYcd/view">Paper</a><br>
<a href="https://belcour.github.io/blog/supp/2019-animation-bluenoise/index.html">Supplemental Material</a><br>
&nbsp;</p>
</div>
      
      </div>

                    
      
      
      </div>
</div>
              <div>
  <div id="low-discrepancy-sampler-distributes-monte-carlo-errors-blue-noise-screen-space-acm-siggraph-talk">

    
              <p><img src="https://unity.com/sites/default/files/styles/810_scale_width/public/2019-07/Samplerbluenoise_0.jpg?itok=yCb6jVQ8" alt="">
                  </p>
                    <h2>
            <p>A Low-Discrepancy Sampler that Distributes Monte Carlo Errors as a Blue Noise in Screen Space</p>
      </h2>
      
      <div>
        
            <div><p><strong>Eric Heitz, Laurent Belcour -</strong> <strong>ACM SIGGRAPH Talk 2019</strong></p>
<p>We introduce a sampler that generates per-pixel samples achieving high visual quality thanks to two key properties related to the Monte Carlo errors that it produces. First, the sequence of each pixel is an Owen-scrambled Sobol sequence that has state-of-the-art convergence properties. The Monte Carlo errors have thus low magnitudes. Second, these errors are distributed as a blue noise in screen space. This makes them visually even more acceptable. Our sampler is lightweight and fast. We implement it with a small texture and two xor operations. Our supplemental material provides comparisons against previous work for different scenes and sample counts.</p>
<p><a href="https://hal.archives-ouvertes.fr/hal-02150657">Paper</a><br>
<a href="https://perso.liris.cnrs.fr/david.coeurjolly/publications/heitz19-supplemental/index.html">Supplemental Material</a><br>
<a href="https://hal.archives-ouvertes.fr/hal-02150657/file/samplerBlueNoiseErrors2019_video.mp4">Video</a><br>
<a href="https://hal.archives-ouvertes.fr/hal-02150657/file/samplerCPP.zip">Sampler Code</a></p>
</div>
      
      </div>

                    
      
      
      </div>
</div>
              <div>
  <div id="low-distortion-map-between-triangle-and-square-tech-report-2019">

    
              <p><img src="https://unity.com/sites/default/files/styles/810_scale_width/public/2019-05/lowdistortion.png?itok=M5nmZV1t" alt="">
                  </p>
                    <h2>
            <p>A Low-Distortion Map Between Triangle and Square</p>
      </h2>
      
      <div>
        
            <div><p><strong>Eric Heitz - Tech Report 2019</strong></p>
<p>We introduce a low-distortion map between triangle and square. This mapping yields an area-preserving parameterization that can be used for sampling random points with a uniform density in arbitrary triangles. This parameterization presents two advantages compared to the square-root param-eterization typically used for triangle sampling. First, it has lower distortions and better preserves the blue-noise properties of the input samples. Second, its computation relies only on arithmetic operations (+, *), which makes it faster to evaluate.</p>
<p><a href="https://hal.archives-ouvertes.fr/hal-02073696">Paper</a><br>
&nbsp;</p>
</div>
      
      </div>

                    
      
      
      </div>
</div>
              <div>
  <div id="ggx-vndf-sampling-jcgt-2018">

    
                    <h2>
            <p>Sampling the GGX Distribution of Visible Normals</p>
      </h2>
      
      <div>
        
            <div><p><strong>Eric Heitz - JCGT 2018</strong></p>
<p>Importance sampling microfacet BSDFs using their Distribution of Visible Normals (VNDF) yields significant variance reduction in Monte Carlo rendering. In this article, we describe an efficient and exact sampling routine for the VNDF of the GGX microfacet distribution. This routine leverages the property that GGX is the distribution of normals of a truncated ellipsoid and sampling the GGX VNDF is equivalent to sampling the 2D projection of this truncated ellipsoid. To do that, we simplify the problem by using the linear transformation that maps the truncated ellipsoid to a hemisphere. Since linear transformations preserve the uniformity of projected areas, sampling in the hemisphere configuration and transforming the samples back to the ellipsoid configuration yields valid samples from the GGX VNDF.</p>
<p><a href="http://jcgt.org/published/0007/04/01/paper.pdf">Paper</a><br>
<a href="http://jcgt.org/published/0007/04/01/">Bib</a><br>
<a href="http://jcgt.org/published/0007/04/01/">Sample Code</a></p>
</div>
      
      </div>

                    
      
      
      </div>
</div>
              <div>
  <div id="solid-angle-ellipsoid-physics-research-2018">

    
              <p><img src="https://unity.com/sites/default/files/styles/810_scale_width/public/2018-12/AnalyticalCalculation-8_12-810x540%402x.png?itok=ZxBO54_F" alt="">
                  </p>
                    <h2>
            <p>Analytical Calculation of the Solid Angle Subtended by an Arbitrarily Positioned Ellipsoid to a Point Source</p>
      </h2>
      
      <div>
        
            <div><p><strong>Eric Heitz - &nbsp;Nuclear Instruments and Methods in Physics Research 2018</strong></p>
<p>We present a geometric method for computing an ellipse that subtends the same solid-angle domain as an arbitrarily positioned ellipsoid. With this method we can extend existing analytical solid-angle calculations of ellipses to ellipsoids. Our idea consists of applying a linear transformation on the ellipsoid such that it is transformed into a sphere from which a disk that covers the same solid-angle domain can be computed. We demonstrate that by applying the inverse linear transformation on this disk we obtain an ellipse that subtends the same solid-angle domain as the ellipsoid. We provide a MATLAB implementation of our algorithm and we validate it numerically.</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0168900217301857">Publisher’s Version</a><strong> </strong><br>
<a href="https://hal.archives-ouvertes.fr/hal-01402302">Author's free print</a></p>
</div>
      
      </div>

                    
      
      
      </div>
</div>
              <div>
  <div id="non-exponential-track-length-sampling-tech-report-2018">

    
                    <h2>
            <p>A note on track-length sampling with non-exponential distributions </p>
      </h2>
      
      <div>
        
            <div><p><strong>Eric Heitz, Laurent Belcour - Tech Report 2018</strong></p>
<p>Track-length sampling is the process of sampling random intervals according to a&nbsp;distance distribution. It means that, instead of sampling a punctual distance from the&nbsp;distance distribution, track-length sampling generates an interval of possible distances.The track-length sampling process is correct if the expectation of the intervals is the&nbsp;target distance distribution. In other words, averaging all the sampled intervals should&nbsp;converge towards the distance distribution as their number increases. In this note, we emphasize that the distance distribution that is used for sampling punctual distances and the track-length distribution that is used for sampling intervals are not the same in general. This difference can be surprising because, to our knowledge, track-length sampling has been mostly studied in the context of transport theory where the distance distribution is typically exponential: in this special case, the distance distribution and the track-length distribution happens to be both the same exponential distribution. However, they are not the same in general when the distance distribution is non-exponential.</p>
<p><a href="https://drive.google.com/file/d/1gmwg0Qh5UkAvfAJRwGKA2Hqcl_psE4CA/view">Paper</a></p>
</div>
      
      </div>

                    
      
      
      </div>
</div>
              <div>
  <div id="stochastic-shadows-i3d-2018">

    
              <p><img src="https://unity.com/sites/default/files/styles/810_scale_width/public/2018-11/CombiningAnalyticalDirect.jpg?itok=7-XF7RAg" alt="">
                  </p>
                    <h2>
            <p>Combining Analytic Direct Illumination and Stochastic Shadows</p>
      </h2>
      
      <div>
        
            <div><p><strong>Eric Heitz, Stephen Hill (Lucasfilm), Morgan McGuire (NVIDIA) &nbsp;- I3D 2018 (short paper) (Best Paper Presentation Award)</strong></p>
<p>In this paper, we propose a ratio estimator of the direct-illumination equation that allows us to combine analytic illumination techniques with stochastic raytraced shadows while maintaining correctness. Our main contribution is to show that the shadowed illumination can be split into the product of the unshadowed illumination and the illumination-weighted shadow. These terms can be computed separately — possibly using different techniques — without affecting the exactness of the final result given by their product. This formulation broadens the utility of analytic illumination techniques to raytracing applications, where they were hitherto avoided&nbsp;because they did not incorporate shadows. We use such methods to obtain sharp and noise-free shading in the unshadowed-illumination image and we compute the weighted-shadow image with stochastic raytracing. The advantage of restricting stochastic evaluation to the weighted-shadow image is that the final result exhibits noise only&nbsp;in the shadows. Furthermore, we denoise shadows separately from illumination so that even aggressive denoising only overblurs shadows, while high-frequency shading details (textures, normal maps, etc.) are preserved.</p>
<p><a href="https://hal.archives-ouvertes.fr/hal-01761558/document">Paper</a><br>
<a href="https://hal.archives-ouvertes.fr/hal-01761558/file/heitzI3D2018_supplemental.pdf">Supplemental</a><br>
<a href="https://hal.archives-ouvertes.fr/hal-01761558/file/heitzI3D2018_slides.pdf">Slides</a><br>
<a href="https://casual-effects.com/research/Heitz2018Shadow/Heitz2018Shadow-reference-code.zip">Code + demo</a></p>
</div>
      
      </div>

                    
      
      
      </div>
</div>
              <div>
  <div id="tiling-procedural-noise-hpg-2018">

    
              <p><img src="https://unity.com/sites/default/files/styles/810_scale_width/public/2019-01/teaser1.png?itok=C-TLfqgw" alt="">
                  </p>
                    <h2>
            <p>Non-Periodic Tiling of Procedural Noise Functions</p>
      </h2>
      
      <div>
        
            <div><p><strong>Aleksandr Kirillov - HPG 2018</strong></p>
<p>Procedural noise functions have many applications in computer graphics, ranging from texture synthesis to atmospheric effect simulation or to landscape geometry specification. Noise can either be precomputed and stored into a texture, or evaluated directly at application runtime. This choice offers a trade-off between image variance, memory consumption and performance.</p>
<p>Advanced tiling algorithms can be used to decrease visual repetition. Wang tiles allow a plane to be tiled in a non-periodic way, using a relatively small set of textures. Tiles can be arranged in a single texture map to enable the GPU to use hardware filtering.</p>
<p>In this paper, we present modifications to several popular procedural noise functions that directly produce texture maps containing the smallest complete Wang tile set. The findings presented in this paper enable non-periodic tiling of these noise functions and textures based on them, both at runtime and as a preprocessing step. These findings also allow decreasing repetition of noise-based effects in computer-generated images at a small performance cost, while maintaining or even reducing the memory consumption.</p>
<p><a href="https://drive.google.com/open?id=1qH2ldaagIDapkYa_4aApVKpo8fCS9DFp">Paper</a></p>
<p><a href="https://drive.google.com/open?id=1CUG4bQzRuDO284dQtrF42WVna-apEYpS">Supplemental</a></p>
</div>
      
      </div>

                    
      
      
      </div>
</div>
              <div>
  <div id="example-noise-hpg-2018">

    
              <p><img src="https://unity.com/sites/default/files/styles/810_scale_width/public/2018-12/Highperofrmancenarrow.jpg?itok=9SkmmwHI" alt="">
                  </p>
                    <h2>
            <p>High-Performance By-Example Noise using a Histogram-Preserving Blending Operator</p>
      </h2>
      
      <div>
        
            <div><p><strong>Eric Heitz, Fabrice Neyret (Inria) - HPG 2018 (Best Paper Award)</strong></p>
<p>We propose a new by-example noise algorithm that takes as input a small example of a stochastic texture and synthesizes an infinite output with the same appearance. It works on any kind of random-phase inputs as well as on many non-random-phase inputs that are stochastic and non-periodic, typically natural textures such as moss, granite, sand, bark, etc. Our algorithm achieves high-quality results comparable to state-of-the-art procedural-noise techniques but is more than 20 times faster</p>
<p><a href="https://hal.inria.fr/hal-01824773/document">Paper</a><br>
<a href="https://hal.inria.fr/hal-01824773/file/HPN2018_supplemental.pdf">Supplemental</a><br>
<a href="https://www.highperformancegraphics.org/wp-content/uploads/2018/Papers-Session3/HPG2018_ByExampleNoise.pdf">Slides</a><br>
<a href="https://drive.google.com/file/d/1YS8RHNcYff7ReroiBbeXHZ79L0_KrSK6/view">Video</a></p>
</div>
      
      </div>

                    
      
      
      </div>
</div>
              <div>
  <div id="unsupervised-deep-intrinsic-decomposition-pacific-graphics-2018">

    
              <p><img src="https://unity.com/sites/default/files/styles/810_scale_width/public/2018-12/unsupervisednarrow.jpg?itok=uUtH-8V1" alt="">
                  </p>
                    <h2>
            <p>Unsupervised Deep Single-Image Intrinsic Decomposition using Illumination-Varying Image Sequences</p>
      </h2>
      
      <div>
        
            <div><p><strong>Louis Lettry (ETH Zürich), Kenneth Vanhoey, Luc Van Gool (ETH Zürich) - Pacific Graphics 2018 / Computer Graphics Forum</strong></p>
<p>Intrinsic Decomposition decomposes a photographed scene into albedo and shading. Removing shading &nbsp;allows to "delight" images, which can then be reused in virtually relit scenes. We propose an unsupervised learning method to solve this problem.</p>
<p>Recent techniques use supervised learning: it requires a large set of known decompositions, which are hard to obtain. Instead, we train on unannotated images by using time lapse imagery gained from static webcams. We exploit the assumption that albedo is static by definition, and shading varies with lighting. We transcribe this into a siamese training for deep learning.</p>
<p><a href="http://kenneth.vanhoey.free.fr/data/research/LVvG18b.pdf">Paper</a><br>
<a href="http://kenneth.vanhoey.free.fr/data/research/LVvG18b_supplemental.zip">Supplemental<br>
Supplemental code</a><br>
<a href="http://kenneth.vanhoey.free.fr/data/research/LVvG18b.bib">Bib</a><br>
<a href="https://docs.google.com/presentation/d/13E5Tog95OU4xokwIb_GZxIvQJ3h0B1AdMbiiMYmsdJo/edit?usp=sharing">Slides</a><br>
&nbsp;</p>
</div>
      
      </div>

                    
      
      
      </div>
</div>
              <div>
  <div id="real-time-layered-materials-acm-siggraph-2018">

    
              <p><img src="https://unity.com/sites/default/files/styles/810_scale_width/public/2018-12/Efficient%20Rendering%20of%20Layered%20.jpg?itok=ociYnYB7" alt="">
                  </p>
                    <h2>
            <p>Efficient Rendering of Layered Materials using an Atomic Decomposition with Statistical Operators</p>
      </h2>
      
      <div>
        
            <div><p><strong>Laurent Belcour - ACM SIGGRAPH 2018</strong></p>
<p>We derive a novel framework for the efficient analysis and computation of light transport within layered materials. Our derivation consists of two steps. First, we decompose light transport into a set of atomic operators that act on its directional statistics. Specically, our operators consist of reflection, refraction, scattering, and absorption, whose combinations are sufficient to describe the statistics of light scattering multiple times within layered structures. We show that the first three directional moments (energy, mean and variance) already provide an accurate summary. Second, we extend the adding-doubling method to support arbitrary combinations of such operators eciently. During shading, we map the directional moments to BSDF lobes. We validate that the resulting BSDF closely matches the ground truth in a lightweight and efficient form. Unlike previous methods, we support an arbitrary number of textured layers, and demonstrate a practical and accurate rendering of layered materials with both an offline and real-time implementation that are free from per-material precomputation.</p>
<p><a href="https://hal.archives-ouvertes.fr/hal-01785457/document">Paper</a><br>
<a href="https://hal.archives-ouvertes.fr/hal-01785457v2/file/suppl.pdf">Supplemental</a><br>
<a href="https://hal.archives-ouvertes.fr/hal-01785457v3/file/suppl.zip">Supplemental code</a><br>
<a href="https://hal.archives-ouvertes.fr/hal-01785457/bibtex">Bib</a><br>
<a href="https://www.youtube.com/watch?v=wM5E-NJtaug&amp;feature=youtu.be">Video</a><br>
<a href="https://belcour.github.io/blog/slides/2018-brdf-realtime-layered/slides.html#/">Slides</a></p>
</div>
      
      </div>

                    
      
      
      </div>
</div>
              <div>
  <div id="material-acquisition-acm-siggraph-asia-2018">

    
              <p><img src="https://unity.com/sites/default/files/styles/810_scale_width/public/2018-12/AdaptivePara.jpg?itok=MoCqjH7O" alt="">
                  </p>
                    <h2>
            <p>An Adaptive Parameterization for Material Acquisition and Rendering </p>
      </h2>
      
      <div>
        
            <div><p><strong>Jonathan Dupuy and Wenzel Jakob (EPFL) - ACM SIGGRAPH Asia 2018 </strong></p>
<p>One of the key ingredients of any physically based rendering system is a detailed specification characterizing the interaction of light and matter of all materials present in a scene, typically via the Bidirectional Reflectance Distribution Function (BRDF). Despite their utility, access to real-world BRDF datasets remains limited: this is because measurements involve scanning&nbsp;a four-dimensional domain at sufficient resolution, a tedious and often infeasible time-consuming process. We propose a new parameterization that automatically adapts to the&nbsp;behavior of a material, warping the underlying 4D domain so that most of the volume maps to regions where the BRDF takes on non-negligible values, while irrelevant regions are strongly compressed. This adaptation only requires a brief 1D or 2D measurement of the material’s retro-reflective properties. Our parameterization is unified in the sense that it combines&nbsp;several steps that previously required intermediate data conversions: the same mapping can simultaneously be used for BRDF acquisition, storage, and it supports efficient Monte Carlo sample generation.</p>
<p><a href="http://rgl.s3.eu-central-1.amazonaws.com/media/papers/Dupuy2018Adaptive.pdf">Paper</a><br>
<a href="http://rgl.s3.eu-central-1.amazonaws.com/media/papers/Dupuy2018Adaptive.mp4">Video</a><br>
<a href="http://rgl.s3.eu-central-1.amazonaws.com/media/papers/Dupuy2018Adaptive_1.pdf">Isotropic BRDF Dataset</a><br>
<a href="http://rgl.s3.eu-central-1.amazonaws.com/media/papers/Dupuy2018Adaptive_2.pdf">Anisotropic BRDF Dataset</a><br>
<a href="http://rgl.s3.eu-central-1.amazonaws.com/media/papers/Dupuy2018Adaptive_3.pdf">MERL Database Validation</a><br>
<a href="https://github.com/rgl-epfl/brdf-loader">C++ &amp; Python code</a><br>
<a href="http://rgl.epfl.ch/materials">Material Database</a></p>
</div>
      
      </div>

                    
      
      
      </div>
</div>
              <div>
  <div id="stochastic-shadows">

    
              <p><img src="https://unity.com/sites/default/files/styles/810_scale_width/public/2019-01/combining_teaser.png?itok=nu6ZWrNF" alt="">
                  </p>
                    <h2>
            <p>Stochastic Shadows</p>
      </h2>
      
      <div>
        
            <div><p><strong>Eric Heitz, Stephen Hill (Lucasfilm), Morgan McGuire (NVIDIA)&nbsp;</strong></p>
<p>In this paper, we propose a ratio estimator of the direct-illumination equation that allows us to combine analytic illumination techniques with stochastic raytraced shadows while maintaining correctness. Our main contribution is to show that the shadowed illumination can be split into the product of the&nbsp;unshadowed illumination&nbsp;and the&nbsp;illumination-weighted shadow. These terms can be computed separately — possibly using different techniques — without affect- ing the exactness of the final result given by their product.</p>
<p>This formulation broadens the utility of analytic illumination tech- niques to raytracing applications, where they were hitherto avoided because they did not incorporate shadows. We use such methods to obtain sharp and noise-free shading in the unshadowed-illumination image and we compute the weighted-shadow image with stochastic raytracing. The advantage of restricting stochastic evaluation to the weighted-shadow image is that the final result exhibits noise only in the shadows. Furthermore, we denoise shadows separately from illumination so that even aggressive denoising only overblurs shad- ows, while high-frequency shading details (textures, normal maps, etc.) are preserved.</p>
<p><a href="https://hal.archives-ouvertes.fr/hal-01761558/document">Paper</a></p>
</div>
      
      </div>

                    
      
      
      </div>
</div>
              <div>
  <div id="adaptive-gpu-tessellation-compute-shaders-gpu-zen-2">

    
              <p><img src="https://unity.com/sites/default/files/styles/810_scale_width/public/2018-12/adaptiveGPU.jpg?itok=Z0nVwagv" alt="">
                  </p>
                    <h2>
            <p>Adaptive GPU Tessellation with Compute Shaders </p>
      </h2>
      
      <div>
        
            <div><p><strong>Jad Khoury, Jonathan Dupuy, and Christophe Riccio - GPU Zen 2 </strong></p>
<p>GPU rasterizers are most efficient when primitives project into more than a few pixels. Below this limit, the Z-buffer starts aliasing, and shading rate decreases dramatically [Riccio 12]; this makes the rendering of geometrically-complex scenes challenging, as any moderately distant polygon will project to sub-pixel size. In order to minimize such sub-pixel projections, a simple solution consists in procedurally refining coarse meshes as they get closer to the camera. In this chapter, we are interested in deriving such a procedural refinement technique for arbitrary polygon meshes.</p>
<p><a href="http://onrendering.com/data/papers/isubd/isubd.pdf">Paper</a><br>
<a href="https://github.com/jadkhoury/TessellationDemo">Code</a></p>
</div>
      
      </div>

                    
      
      
      </div>
</div>
              <div>
  <div id="line-and-disk-lights-acm-siggraph-courses-2017">

    
              <p><img src="https://unity.com/sites/default/files/styles/810_scale_width/public/2018-12/Realtime%20line%20and%20disk%20lgiht.jpg?itok=kfIkGz8L" alt="">
                  </p>
                    <h2>
            <p>Real-Time Line- and Disk-Light Shading with Linearly Transformed Cosines</p>
      </h2>
      
      <div>
        
            <div><p><strong>Eric Heitz (Unity Technologies) and Stephen Hill (Lucasfilm) - ACM SIGGRAPH Courses 2017</strong></p>
<p>We recently introduced a new real-time area-light shading technique dedicated to <a href="https://labs.unity.com/article/real-time-polygonal-light-shading-linearly-transformed-cosines">lights with polygonal shapes</a>. In this talk, we extend this area-lighting framework to support lights shaped as lines, spheres and disks in addition to polygons.</p>
<p><a href="https://blog.selfshadow.com/publications/s2017-shading-course/#course_content">Slides</a><br>
<a href="https://github.com/selfshadow/ltc_code">Demo code</a><br>
WebGL demo for <a href="http://blog.selfshadow.com/ltc/webgl/ltc_quad.html">quad</a>, <a href="http://blog.selfshadow.com/ltc/webgl/ltc_line.html">line</a> and <a href="http://blog.selfshadow.com/ltc/webgl/ltc_disk.html">disk lights</a></p>
</div>
      
      </div>

                    
      
      
      </div>
</div>
              <div>
  <div id="microfacet-based-normal-mapping-acm-siggraph-asia-2017">

    
              <p><img src="https://unity.com/sites/default/files/styles/810_scale_width/public/2018-12/microfacetNormapMapping.jpg?itok=OKNMhUuU" alt="">
                  </p>
                    <h2>
            <p>Microfacet-based Normal Mapping for Robust Monte Carlo Path Tracing</p>
      </h2>
      
      <div>
        
            <div><p><strong>Vincent Schüssler (KIT), Eric Heitz (Unity Technologies), Johannes Hanika (KIT) and Carsten Dachsbacher (KIT) - ACM SIGGRAPH ASIA 2017</strong></p>
<p>Normal mapping imitates visual details on surfaces by using fake shading normals. However, the resulting surface model is geometrically impossible and normal mapping is thus often considered a fundamentally flawed approach with unavoidable problems for Monte Carlo path tracing: it breaks either the appearance (black fringes, energy loss) or the integrator (different forward and backward light transport). In this paper, we present microfacet-based normal mapping, an alternative way of faking geometric details without corrupting the robustness of Monte Carlo path tracing such that these problems do not arise.</p>
<p><a href="https://drive.google.com/file/d/0BzvWIdpUpRx_ZHI1X2Z4czhqclk/view">Paper</a></p>
</div>
      
      </div>

                    
      
      
      </div>
</div>
              <div>
  <div id="spherical-cap-preserving-parameterization-acm-siggraph-2017">

    
              <p><img src="https://unity.com/sites/default/files/styles/810_scale_width/public/2018-12/aspherical.jpg?itok=Bdi-HL9d" alt="">
                  </p>
                    <h2>
            <p>A Spherical Cap Preserving Parameterization for Spherical Distributions</p>
      </h2>
      
      <div>
        
            <div><p><strong>Jonathan Dupuy, Eric Heitz and Laurent Belcour - ACM SIGGRAPH 2017</strong></p>
<p>We introduce a novel parameterization for spherical distributions that is based on a point located inside the sphere, which we call a pivot. The pivot serves as the center of a straight-line projection that maps solid angles onto the opposite side of the sphere. By transforming spherical distributions in this&nbsp;way, we derive novel parametric spherical distributions that can be evaluated and importance-sampled from the original distributions using simple, closed-form expressions. Moreover, we prove that if the original distribution can be sampled and/or integrated over a spherical cap, then so can the transformed distribution. We exploit the properties of our parameterization to derive efficient spherical lighting techniques for both real-time and offline rendering. Our techniques are robust, fast, easy to implement, and achieve quality that is superior to previous work.</p>
<p><a href="http://onrendering.com/data/papers/pivot/pivot.pdf">Paper</a><br>
<a href="https://www.youtube.com/watch?v=9aZMRjbflpo&amp;feature=youtu.be">Video</a></p>
</div>
      
      </div>

                    
      
      
      </div>
</div>
              <div>
  <div id="thin-film-iridescence-acm-siggraph-2017">

    
              <p><img src="https://unity.com/sites/default/files/styles/810_scale_width/public/2018-11/brdf-thin-film-header.jpg?itok=pTVNzcDs" alt="">
                  </p>
                    <h2>
            <p>A Practical Extension to Microfacet Theory for the Modeling of Varying Iridescence</p>
      </h2>
      
      <div>
        
            <div><p><strong>Laurent Belcour (Unity), Pascal Barla (Inria) - ACM SIGGRAPH 2017</strong></p>
<p>Thin film iridescence permits to reproduce the appearance of leather. However, this theory requires spectral rendering engines (such as Maxwell Render) to correctly integrate the change of appearance with respect to viewpoint (known as goniochromatism). This is due to aliasing in the spectral domain as real-time renderers only work with three components (RGB) for the entire range of visible light. In this work, we show how to anti-alias a thin-film model, how to incorporate it in microfacet theory, and how to integrate it in a real-time rendering engine. This widens the range of reproducible appearances with microfacet models.</p>
<p><a href="https://hal.archives-ouvertes.fr/hal-01518344/document">Paper</a><br>
<a href="https://hal.archives-ouvertes.fr/hal-01518344v2/file/supp-mat-small%20(1).pdf">Supplemental</a><br>
<a href="https://hal.inria.fr/hal-01518344/bibtex">Bib</a><br>
<a href="https://www.youtube.com/watch?v=4nKb9hRYbPA&amp;feature=youtu.be">Video</a><br>
<a href="https://hal.inria.fr/hal-01518344v2/file/supplemental-code%20%282%29.zip">Code</a><br>
<a href="https://belcour.github.io/blog/slides/2017-brdf-thin-film/slides.html#/">Slides</a></p>
</div>
      
      </div>

                    
      
      
      </div>
</div>
              <div>
  <div id="linear-light-shading-gpu-zen">

    
              <p><img src="https://unity.com/sites/default/files/styles/810_scale_width/public/2018-12/LinearLightShadingnarrow.jpg?itok=VModpcTe" alt="">
                  </p>
                    <h2>
            <p>Linear-Light Shading with Linearly Transformed Cosines</p>
      </h2>
      
      <div>
        
            <div><p><strong>Eric Heitz, Stephen Hill (Lucasfilm) - GPU Zen (book)</strong></p>
<p>In this book chapter, we extend our area-light framework based on Linearly Transformed Cosines to support linear (or line) lights. Linear lights are a good approximation for cylindrical lights with a small but non-zero radius. We describe how to approximate these lights with linear lights that have similar power and shading, and discuss the validity of this approximation.</p>
<p><a href="https://drive.google.com/file/d/0BzvWIdpUpRx_SzZ2NWVwLWxwck0/view">Paper</a></p>
</div>
      
      </div>

                    
      
      
      </div>
</div>
              <div>
  <div id="frequency-analysis-light-transport-acm-siggraph-courses-2016">

    
              <p><img src="https://unity.com/sites/default/files/styles/810_scale_width/public/2018-12/Frequencyanalysis.jpg?itok=QLE-MdTZ" alt="">
                  </p>
                    <h2>
            <p>A Practical Introduction to Frequency Analysis of Light Transport</p>
      </h2>
      
      <div>
        
            <div><p><strong>Laurent Belcour - ACM SIGGRAPH Courses 2016</strong></p>
<p>Frequency Analysis of Light Transport expresses Physically Based Rendering (PBR) using signal processing tools. It is thus tailored to predict sampling rate, perform denoising, perform anti-aliasing, etc. Many method have been proposed to deal with specific cases of light transport (motion, lenses, etc). This course aims to introduce concepts and present practical application scenario of frequency analysis of light transport in a unified context. To ease the understanding of theoretical elements, frequency analysis will be introduced in pair with an implementation.</p>
<p><a href="https://belcour.github.io/blog/siggraph-2016-course.html">Course</a></p>
</div>
      
      </div>

                    
      
      
      </div>
</div>
              <div>
  <div id="polygonal-light-shading-acm-siggraph-2016">

    
              <p><img src="https://unity.com/sites/default/files/styles/810_scale_width/public/2018-12/realtimepolygonal.jpg?itok=zZjUSFF7" alt="">
                  </p>
                    <h2>
            <p>Real-Time Polygonal-Light Shading with Linearly Transformed Cosines </p>
      </h2>
      
      

                    
      
      
      </div>
</div>
              <div>
  <div id="microfacet-and-microflake-theories-egsr-2016-ei">

    
              <p><img src="https://unity.com/sites/default/files/styles/810_scale_width/public/2018-12/additionalprogress.jpg?itok=OXRSVj3L" alt="">
                  </p>
                    <h2>
            <p>Additional Progress Towards the Unification of Microfacet and Microflake Theories </p>
      </h2>
      
      <div>
        
            <div><p><strong>Jonathan Dupuy and Eric Heitz - EGSR 2016 (E&amp;I) </strong></p>
<p>We study the links between microfacet and microflake theories from the perspective of linear transport theory. In doing so, we gain additional insights, find several simplifications and touch upon important open questions as well as possible paths forward in extending the unification of surface and volume scattering models. First, we introduce a semi-infinite homogeneous exponential-free-path medium that (a) produces exactly the same light transport as the Smith microsurface scattering model&nbsp;and the inhomogeneous Smith medium that was recently introduced by Heitz et al, and (b) allows us to rederive all the Smith masking and shadowing functions in a simple way. Second, we investigate in detail what new aspects of linear transport theory enable a volume to act like a rough surface. We show that this is mostly due to the use of non-symmetric distributions of normals and explore how the violation of this symmetry impacts light transport within the microflake volume without breaking global reciprocity. Finally, we argue that the surface profiles that would be consistent with very rough Smith microsurfaces have geometrically implausible shapes. To overcome this, we discuss an extension of Smith theory in the volume setting that includes NDFs on the entire sphere in order to produce a single unified reflectance model capable of describing everything from a smooth flat mirror all the way to a semi-infinite isotropically scattering medium with both low and high roughness regimes in&nbsp;between.</p>
<p><a href="http://onrendering.com/data/papers/ms16/ms16.pdf">Paper</a><br>
<a href="http://onrendering.com/data/papers/ms16/slides/index.html">Slides</a></p>
</div>
      
      </div>

                    
      
      
      </div>
</div>
          </div>
  
                  </div>
      </div>
    </div>
  </section>
</div>
          </div>
  
  </div>

</article>

</main>


</section>



  </div>

  </div></div></div>
    </div>
    <footer>
        <div>created by <a href="https://buildstarted.com">buildstarted</a> &copy; 2020 <a href="/about">about</a></div>
        <div>Share this page on social media: copy and paste this url https://linksfor.dev/</div>
        <div>If you prefer RSS: <a href="https://linksfor.dev/feed.xml">https://linksfor.dev/feed.xml</a></div>
        <div>Customer satisfaction guaranteed to be optional.</div>
    </footer>
    
    <script async defer>
        _dna = window._dna || {};
        _dna.siteId = "linksfor.devs";
        _dna.outlink = true;

        (function() {
            let dna = document.createElement('script');
            dna.type = 'text/javascript';
            dna.async = true;
            dna.src = '//dna.buildstarted.com/t.js';
            let s = document.getElementsByTagName('script')[0];
            s.parentNode.insertBefore(dna, s);
        })();
    </script>
    <noscript><img src="//dna.buildstarted.com/g?siteId=linksfor.devs"/></noscript>
</body>
</html>