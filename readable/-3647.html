<!DOCTYPE html>
<html lang="en">
<head>
    <title>
Training Neural Networks with Local Error Signals - linksfor.dev(s)    </title>
    <meta charset="utf-8">
    <link rel="alternate" type="application/rss+xml" title="Linksfor.dev(s) feed" href="https://linksfor.dev/feed.rss" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta name="google" value="notranslate">
    <link rel="stylesheet" type="text/css" href="/style.css" />
    <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon">
    <link rel="icon" href="/favicon.ico" type="image/x-icon">
    <meta property="og:title" content="Training Neural Networks with Local Error Signals - linksfor.dev(s)"/>
    <meta property="article:author" content="Authors:Arild N&#xF8;kland, Lars Hiller Eidnes"/>
    <meta property="og:description" content="Supervised training of neural networks for classification is typically&#xA;performed with a global loss function. The loss function provides a gradient&#xA;for the output layer, and this gradient is back-propagated to hidden layers to&#xA;dictate an update direction for the weights. An alternative approach is to&#xA;train the network with layer-wise loss functions. In this paper we demonstrate,&#xA;for the first time, that layer-wise training can approach the state-of-the-art&#xA;on a variety of image datasets. We use single-layer sub-networks and two&#xA;different supervised loss functions to generate local error signals for the&#xA;hidden layers, and we show that the combination of these losses help with&#xA;optimization in the context of local learning. Using local errors could be a&#xA;step towards more biologically plausible deep learning because the global error&#xA;does not have to be transported back to hidden layers. A completely backprop&#xA;free variant outperforms previously reported results among methods aiming for&#xA;higher biological plausibility. Code is available&#xA;https://github.com/anokland/local-loss"/>
    <meta property="og:type" content="website"/>
    <meta property="og:url" content="https://arxiv.org/abs/1901.06656"/>

<meta property="og:site_name" content="linksfor.dev(s)" />
</head>
<body>
    <div class="grid">
        <h1>
                <span style="cursor: default" title="linksfor.dev(s) has been running for 1 year! :partypopper:">ðŸŽ‰</span>
<a href="/" style="color:inherit">linksfor.dev(s)</a>
        </h1>
        <title>linksfor.dev(s) - Training Neural Networks with Local Error Signals</title>
<div class="readable">
        <h1>Training Neural Networks with Local Error Signals</h1>
            <div>by Authors:Arild N&#xF8;kland, Lars Hiller Eidnes</div>
            <div>Watching time: 2 minutes</div>
        <div>Posted here: 27 Feb 2019</div>
        <p><a href="https://arxiv.org/abs/1901.06656">https://arxiv.org/abs/1901.06656</a></p>
        <hr/>
<div id="readability-page-1" class="page"><div id="content-inner">
  <div id="abs">

    
    
    <p>
  
  
  
    
  
  
    
    
  

  (Submitted on 20 Jan 2019 (<a href="https://arxiv.org/abs/1901.06656v1">v1</a>), last revised 7 May 2019 (this version, v2))</p>
    <blockquote><span>Abstract:</span>  Supervised training of neural networks for classification is typically
performed with a global loss function. The loss function provides a gradient
for the output layer, and this gradient is back-propagated to hidden layers to
dictate an update direction for the weights. An alternative approach is to
train the network with layer-wise loss functions. In this paper we demonstrate,
for the first time, that layer-wise training can approach the state-of-the-art
on a variety of image datasets. We use single-layer sub-networks and two
different supervised loss functions to generate local error signals for the
hidden layers, and we show that the combination of these losses help with
optimization in the context of local learning. Using local errors could be a
step towards more biologically plausible deep learning because the global error
does not have to be transported back to hidden layers. A completely backprop
free variant outperforms previously reported results among methods aiming for
higher biological plausibility. Code is available
<a href="https://github.com/anokland/local-loss" rel="external noopener nofollow">this https URL</a>
</blockquote>
    <!--CONTEXT-->
    
  </div>
</div><div>
      <h2>Submission history</h2><p> From: Arild NÃ¸kland [<a href="https://arxiv.org/show-email/c9da506d/1901.06656">view email</a>]
      <br>
  <strong><a href="https://arxiv.org/abs/1901.06656v1">[v1]</a></strong>
  Sun, 20 Jan 2019 10:59:53 UTC (584 KB)<br><strong>[v2]</strong>
Tue, 7 May 2019 18:49:45 UTC (584 KB)<br></p></div></div></div>
    </div>
    <footer>
        <div>created by buildstarted &copy; 2020 <a href="/about">about</a></div>
        <div>Share this page on social media: copy and paste this url https://linksfor.dev/</div>
        <div>If you prefer RSS: <a href="https://linksfor.dev/feed.xml">https://linksfor.dev/feed.xml</a></div>
    </footer>
    
    <script async defer>
        _dna = window._dna || {};
        _dna.siteId = "linksfor.devs";
        _dna.outlink = true;

        (function () {
            let dna = document.createElement('script');
            dna.type = 'text/javascript';
            dna.async = true;
            dna.src = '//dna.buildstarted.com/t.js';
            let s = document.getElementsByTagName('script')[0];
            s.parentNode.insertBefore(dna, s);
        })();
    </script>
    <noscript><img src="//dna.buildstarted.com/g?siteId=linksfor.devs"/></noscript>
</body>
</html>