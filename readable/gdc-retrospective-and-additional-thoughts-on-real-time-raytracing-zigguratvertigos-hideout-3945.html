<!DOCTYPE html>
<html lang="en">
<head>
    <title>
GDC Retrospective and Additional Thoughts on Real-Time Raytracing - linksfor.dev(s)    </title>
    <meta charset="utf-8">
    <link rel="alternate" type="application/rss+xml" title="Linksfor.dev(s) feed" href="https://linksfor.dev/feed.rss" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta name="google" value="notranslate">
    <link rel="stylesheet" type="text/css" href="/style.css" />
    <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon">
    <link rel="icon" href="/favicon.ico" type="image/x-icon">
    <meta property="og:title" content="GDC Retrospective and Additional Thoughts on Real-Time Raytracing - linksfor.dev(s)"/>
    <meta property="og:description" content="This post is part of the series &#x201C;Finding Next-Gen&#x201D;. Just got back from GDC. Had a great time showcasing the hard work we&#x2019;ve been up to at SEED. In case you missed it, we did two p&#x2026;"/>
    <meta property="og:type" content="website"/>
    <meta property="og:url" content="https://colinbarrebrisebois.com/2018/04/07/some-thoughts-on-real-time-raytracing/"/>

<meta property="og:site_name" content="linksfor.dev(s)" />
</head>
<body>
	<div class="devring" style="background: #222">
		<div class="grid">
			<div style="display: grid; grid-template-columns: .5fr 1fr 1fr 1fr; text-align: center;">
				<span class="devring-title"><a href="https://devring.club/">devring.club</a></span>
				<a href="https://devring.club/sites/1/prev" class="devring-previous">Previous</a>
				<a href="https://devring.club/random" class="devring-random">Random</a>
				<a href="https://devring.club/sites/1/next" class="devring-next">Next</a>
			</div>
		</div>
	</div>
    <div class="grid">
        <h1>
<a href="/" style="color:inherit">linksfor.dev(s)</a>
        </h1>
        <title>linksfor.dev(s) - GDC Retrospective and Additional Thoughts on Real-Time Raytracing</title>
<div class="readable">
        <h1>GDC Retrospective and Additional Thoughts on Real-Time Raytracing</h1>
            <div>Reading time: 9-11 minutes</div>
        <div>Posted here: 27 Feb 2019</div>
        <p><a href="https://colinbarrebrisebois.com/2018/04/07/some-thoughts-on-real-time-raytracing/">https://colinbarrebrisebois.com/2018/04/07/some-thoughts-on-real-time-raytracing/</a></p>
        <hr/>
<div id="readability-page-1" class="page"><article id="post-3088">
	<!-- .entry-header -->

	<div>
		<p><em>This post is part of the series â€œ<a href="https://colinbarrebrisebois.com/2015/11/04/finding-next-gen/">Finding Next-Gen</a>â€œ.</em></p>
<p><span>Just got back from GDC. Had a great time showcasing the hard work weâ€™ve been up to at </span><a href="https://www.ea.com/seed">SEED</a><span>. In case you missed it, we did two presentations on real-time raytracing:</span></p>
<h6><img data-attachment-id="3092" data-permalink="https://colinbarrebrisebois.com/2018/04/07/some-thoughts-on-real-time-raytracing/gdc1-3/" data-orig-file="https://zigguratvertigodotcom.files.wordpress.com/2018/04/gdc12.png" data-orig-size="4474,1316" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="gdc1" data-image-description="" data-medium-file="https://zigguratvertigodotcom.files.wordpress.com/2018/04/gdc12.png?w=300" data-large-file="https://zigguratvertigodotcom.files.wordpress.com/2018/04/gdc12.png?w=863" src="https://zigguratvertigodotcom.files.wordpress.com/2018/04/gdc12.png?w=863" alt="gdc1" srcset="https://zigguratvertigodotcom.files.wordpress.com/2018/04/gdc12.png?w=863 863w, https://zigguratvertigodotcom.files.wordpress.com/2018/04/gdc12.png?w=1726 1726w, https://zigguratvertigodotcom.files.wordpress.com/2018/04/gdc12.png?w=150 150w, https://zigguratvertigodotcom.files.wordpress.com/2018/04/gdc12.png?w=300 300w, https://zigguratvertigodotcom.files.wordpress.com/2018/04/gdc12.png?w=768 768w, https://zigguratvertigodotcom.files.wordpress.com/2018/04/gdc12.png?w=1024 1024w" sizes="(max-width: 863px) 100vw, 863px"><em>DirectX Raytracing Announcement (Microsoft) and <strong>Shiny Pixels and Beyond: Real-Time Raytracing at SEED (NVIDIA)</strong></em></h6>
<p><strong><em>In case you were at GDC and saw the presentation, you can skip directly <a href="#additional">here</a>.</em></strong></p>
<p>During the first session Matt Sandy from Microsoft <a href="https://blogs.msdn.microsoft.com/directx/2018/03/19/announcing-microsoft-directx-raytracing/" target="_blank" rel="noopener">announced</a> <em>DirectX Raytracing (DXR)</em>. He went into great detail over the API changes, and showed how <em>DirectX 12</em> has evolved to support raytracing. We then followed with our own presentations, where we showcased <em><a href="https://www.ea.com/seed/news/seed-project-picapica" target="_blank" rel="noopener">Project PICA PICA</a></em>,&nbsp;a real-time raytracing experiment featuring a mini-game for self-learning AI agents in a procedurally-assembled world. The team has worked super hard on this demo, and the results really show it! ðŸ™‚</p>
<p><span><iframe type="text/html" src="https://www.youtube.com/embed/LXo0WdlELJk?version=3&amp;rel=1&amp;fs=1&amp;autohide=2&amp;showsearch=0&amp;showinfo=1&amp;iv_load_policy=1&amp;wmode=transparent" allowfullscreen="true" data-ratio="0.5631517960602549" data-width="863" data-height="486"></iframe></span></p>
<p><em>PICA PICA</em> is powered by DXR.</p>
<h2>DirectX Raytracing?</h2>
<p>The addition of raytracing to <em>DirectX 12</em> is exposed via simple concepts: <em>acceleration structures</em> (<em>bottom &amp; top), new shader types (ray-generation,&nbsp;closest-hit,&nbsp;any-hit, and&nbsp;miss), new HLSL types and intrinsics,&nbsp;commandlist-level DispatchRays(â€¦) and a raytracing pipeline state. </em>You can read more about it <a href="https://blogs.msdn.microsoft.com/directx/2018/03/19/announcing-microsoft-directx-raytracing/" target="_blank" rel="noopener">here</a>.</p>
<p>Taken from our presentation, hereâ€™s a brief overview of how this works in PICA PICA:</p>
<h5><img data-attachment-id="3093" data-permalink="https://colinbarrebrisebois.com/2018/04/07/some-thoughts-on-real-time-raytracing/gdc3/" data-orig-file="https://zigguratvertigodotcom.files.wordpress.com/2018/04/gdc3.png" data-orig-size="2736,1824" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="gdc3" data-image-description="" data-medium-file="https://zigguratvertigodotcom.files.wordpress.com/2018/04/gdc3.png?w=300" data-large-file="https://zigguratvertigodotcom.files.wordpress.com/2018/04/gdc3.png?w=863" src="https://zigguratvertigodotcom.files.wordpress.com/2018/04/gdc3.png?w=863" alt="gdc3.png" srcset="https://zigguratvertigodotcom.files.wordpress.com/2018/04/gdc3.png?w=863 863w, https://zigguratvertigodotcom.files.wordpress.com/2018/04/gdc3.png?w=1726 1726w, https://zigguratvertigodotcom.files.wordpress.com/2018/04/gdc3.png?w=150 150w, https://zigguratvertigodotcom.files.wordpress.com/2018/04/gdc3.png?w=300 300w, https://zigguratvertigodotcom.files.wordpress.com/2018/04/gdc3.png?w=768 768w, https://zigguratvertigodotcom.files.wordpress.com/2018/04/gdc3.png?w=1024 1024w" sizes="(max-width: 863px) 100vw, 863px"><em>Using Bottom/top acceleration structures and shader table (from GDC slides)</em></h5>
<h5><img data-attachment-id="3110" data-permalink="https://colinbarrebrisebois.com/2018/04/07/some-thoughts-on-real-time-raytracing/picapica_hlslpseudocode/" data-orig-file="https://zigguratvertigodotcom.files.wordpress.com/2018/04/picapica_hlslpseudocode.png" data-orig-size="1169,1226" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="picapica_hlslPseudoCode" data-image-description="" data-medium-file="https://zigguratvertigodotcom.files.wordpress.com/2018/04/picapica_hlslpseudocode.png?w=286" data-large-file="https://zigguratvertigodotcom.files.wordpress.com/2018/04/picapica_hlslpseudocode.png?w=863" src="https://zigguratvertigodotcom.files.wordpress.com/2018/04/picapica_hlslpseudocode.png?w=863" alt="picapica_hlslPseudoCode.png" srcset="https://zigguratvertigodotcom.files.wordpress.com/2018/04/picapica_hlslpseudocode.png?w=863 863w, https://zigguratvertigodotcom.files.wordpress.com/2018/04/picapica_hlslpseudocode.png?w=143 143w, https://zigguratvertigodotcom.files.wordpress.com/2018/04/picapica_hlslpseudocode.png?w=286 286w, https://zigguratvertigodotcom.files.wordpress.com/2018/04/picapica_hlslpseudocode.png?w=768 768w, https://zigguratvertigodotcom.files.wordpress.com/2018/04/picapica_hlslpseudocode.png?w=976 976w, https://zigguratvertigodotcom.files.wordpress.com/2018/04/picapica_hlslpseudocode.png 1169w" sizes="(max-width: 863px) 100vw, 863px"><em>Ray Generation Shadow â€“ HLSL Pseudo Code â€“ Does Not Compile (from GDC slides)</em></h5>
<p>While you donâ€™t necessarily need to use DXR to do real-time raytracing on current GPUs (<a href="https://www.dropbox.com/s/s9tzmyj0wqkymmz/Claybook_Simulation_Raytracing_GDC18.pptx?dl=0" target="_blank" rel="noopener">see Sebastian Aaltonenâ€™s Claybook rendering presentation</a>), itâ€™s a flexible new tool in the toolbox. From the code above, you benefit from the fact that itâ€™s unified with the rest of <em>DirectX 12.&nbsp;</em>DXR relies on well known HLSL functionality and types, allowing you to share code between rasterization, compute and raytracing. More than just raytracing, DXR also allows to solve more sparse and incoherent problems that you canâ€™t easily solve with rasterization and compute. Itâ€™s also a centralized implementation for hardware vendors to optimize, and now becomes common language for every developer that wants to do raytracing in <em>DirectX 12</em>. Itâ€™s not perfect, but itâ€™s a good start and it works well.</p>
<h2>Presentation Retrospective</h2>
<p>During the presentation we talked about our hybrid rendering pipeline where rasterization, compute and raytracing work together:</p>
<h5><img data-attachment-id="3090" data-permalink="https://colinbarrebrisebois.com/2018/04/07/some-thoughts-on-real-time-raytracing/gdc2/" data-orig-file="https://zigguratvertigodotcom.files.wordpress.com/2018/04/gdc2.png" data-orig-size="2736,1824" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="gdc2" data-image-description="" data-medium-file="https://zigguratvertigodotcom.files.wordpress.com/2018/04/gdc2.png?w=300" data-large-file="https://zigguratvertigodotcom.files.wordpress.com/2018/04/gdc2.png?w=863" src="https://zigguratvertigodotcom.files.wordpress.com/2018/04/gdc2.png?w=863" alt="gdc2.png" srcset="https://zigguratvertigodotcom.files.wordpress.com/2018/04/gdc2.png?w=863 863w, https://zigguratvertigodotcom.files.wordpress.com/2018/04/gdc2.png?w=1726 1726w, https://zigguratvertigodotcom.files.wordpress.com/2018/04/gdc2.png?w=150 150w, https://zigguratvertigodotcom.files.wordpress.com/2018/04/gdc2.png?w=300 300w, https://zigguratvertigodotcom.files.wordpress.com/2018/04/gdc2.png?w=768 768w, https://zigguratvertigodotcom.files.wordpress.com/2018/04/gdc2.png?w=1024 1024w" sizes="(max-width: 863px) 100vw, 863px"><em>PICA PICAâ€™s Hybrid Rendering Pipeline&nbsp;(from GDC slides)</em></h5>
<p>Our hybrid approach allows us to solve, develop and apply several interesting techniques and algorithms that rely on rasterization, compute or raytracing while balancing quality and performance. This shows the flexibility of the API, where one is free to choose a specific pipeline to solve a specific problem. Again since raytracing is another tool in the toolbox, it can be used where it makes sense and doesnâ€™t prevent you from using other available pipelines.</p>
<p>First we talked about how we raytrace reflections from the G-Buffer at half resolution, reconstruct at full resolution, and how it allows us to handle varying levels of roughness. We also presented our multi-layer material system, shared between rasterization, compute and raytracing.</p>
<h5><img data-attachment-id="3109" data-permalink="https://colinbarrebrisebois.com/2018/04/07/some-thoughts-on-real-time-raytracing/picapica_reflectionsmaterials/" data-orig-file="https://zigguratvertigodotcom.files.wordpress.com/2018/04/picapica_reflectionsmaterials.png" data-orig-size="2002,1529" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="picapica_reflectionsMaterials" data-image-description="" data-medium-file="https://zigguratvertigodotcom.files.wordpress.com/2018/04/picapica_reflectionsmaterials.png?w=300" data-large-file="https://zigguratvertigodotcom.files.wordpress.com/2018/04/picapica_reflectionsmaterials.png?w=863" src="https://zigguratvertigodotcom.files.wordpress.com/2018/04/picapica_reflectionsmaterials.png?w=863" alt="picapica_reflectionsMaterials.png" srcset="https://zigguratvertigodotcom.files.wordpress.com/2018/04/picapica_reflectionsmaterials.png?w=863 863w, https://zigguratvertigodotcom.files.wordpress.com/2018/04/picapica_reflectionsmaterials.png?w=1726 1726w, https://zigguratvertigodotcom.files.wordpress.com/2018/04/picapica_reflectionsmaterials.png?w=150 150w, https://zigguratvertigodotcom.files.wordpress.com/2018/04/picapica_reflectionsmaterials.png?w=300 300w, https://zigguratvertigodotcom.files.wordpress.com/2018/04/picapica_reflectionsmaterials.png?w=768 768w, https://zigguratvertigodotcom.files.wordpress.com/2018/04/picapica_reflectionsmaterials.png?w=1024 1024w" sizes="(max-width: 863px) 100vw, 863px"><em>Raytraced Reflections (left) and Multi-Layer Materials (right) (from GDC slides)</em></h5>
<p>We then followed by describing a novel texture-space approach for order-independent transparency, translucency and subsurface scattering:</p>
<h5><img data-attachment-id="3108" data-permalink="https://colinbarrebrisebois.com/2018/04/07/some-thoughts-on-real-time-raytracing/picapica_translucencyglass/" data-orig-file="https://zigguratvertigodotcom.files.wordpress.com/2018/04/picapica_translucencyglass2.png" data-orig-size="2087,1359" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="picapica_translucencyGlass" data-image-description="" data-medium-file="https://zigguratvertigodotcom.files.wordpress.com/2018/04/picapica_translucencyglass2.png?w=300" data-large-file="https://zigguratvertigodotcom.files.wordpress.com/2018/04/picapica_translucencyglass2.png?w=863" src="https://zigguratvertigodotcom.files.wordpress.com/2018/04/picapica_translucencyglass2.png?w=863" alt="picapica_translucencyGlass.png" srcset="https://zigguratvertigodotcom.files.wordpress.com/2018/04/picapica_translucencyglass2.png?w=863 863w, https://zigguratvertigodotcom.files.wordpress.com/2018/04/picapica_translucencyglass2.png?w=1726 1726w, https://zigguratvertigodotcom.files.wordpress.com/2018/04/picapica_translucencyglass2.png?w=150 150w, https://zigguratvertigodotcom.files.wordpress.com/2018/04/picapica_translucencyglass2.png?w=300 300w, https://zigguratvertigodotcom.files.wordpress.com/2018/04/picapica_translucencyglass2.png?w=768 768w, https://zigguratvertigodotcom.files.wordpress.com/2018/04/picapica_translucencyglass2.png?w=1024 1024w" sizes="(max-width: 863px) 100vw, 863px"><em>Glass and Translucency (from GDC slides)</em></h5>
<p>We then presented a sparse surfel-based approach where we use raytracing to pathtrace irradiance from surfels spawned from the camera.</p>
<h5><img data-attachment-id="3114" data-permalink="https://colinbarrebrisebois.com/2018/04/07/some-thoughts-on-real-time-raytracing/picapica_gi/" data-orig-file="https://zigguratvertigodotcom.files.wordpress.com/2018/04/picapica_gi.png" data-orig-size="2306,1365" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="picapica_gi" data-image-description="" data-medium-file="https://zigguratvertigodotcom.files.wordpress.com/2018/04/picapica_gi.png?w=300" data-large-file="https://zigguratvertigodotcom.files.wordpress.com/2018/04/picapica_gi.png?w=863" src="https://zigguratvertigodotcom.files.wordpress.com/2018/04/picapica_gi.png?w=863" alt="picapica_gi.png" srcset="https://zigguratvertigodotcom.files.wordpress.com/2018/04/picapica_gi.png?w=863 863w, https://zigguratvertigodotcom.files.wordpress.com/2018/04/picapica_gi.png?w=1726 1726w, https://zigguratvertigodotcom.files.wordpress.com/2018/04/picapica_gi.png?w=150 150w, https://zigguratvertigodotcom.files.wordpress.com/2018/04/picapica_gi.png?w=300 300w, https://zigguratvertigodotcom.files.wordpress.com/2018/04/picapica_gi.png?w=768 768w, https://zigguratvertigodotcom.files.wordpress.com/2018/04/picapica_gi.png?w=1024 1024w" sizes="(max-width: 863px) 100vw, 863px"><em>Surfel-based Global Illumination (from GDC slides)</em></h5>
<p>We also covered ambient occlusion (AO), and how raytraced AO compares to screen-space AO.</p>
<div id="gallery-3088-3-slideshow" data-trans="fade" data-autostart="1" data-gallery="[{&quot;src&quot;:&quot;https:\/\/zigguratvertigodotcom.files.wordpress.com\/2018\/04\/picapia_rtao.png&quot;,&quot;id&quot;:&quot;3112&quot;,&quot;title&quot;:&quot;picapia_rtao&quot;,&quot;alt&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;itemprop&quot;:&quot;image&quot;},{&quot;src&quot;:&quot;https:\/\/zigguratvertigodotcom.files.wordpress.com\/2018\/04\/picapia_ssao.png&quot;,&quot;id&quot;:&quot;3113&quot;,&quot;title&quot;:&quot;picapia_ssao&quot;,&quot;alt&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;itemprop&quot;:&quot;image&quot;}]" itemscope="" itemtype="https://schema.org/ImageGallery"><div itemprop="associatedMedia" itemscope="" itemtype="https://schema.org/ImageObject"><p><img src="https://zigguratvertigodotcom.files.wordpress.com/2018/04/picapia_rtao.png" title="picapia_rtao" alt="" itemprop="image"><span>&nbsp;</span></p></div></div>
<p>Inspired from Schied/NVIDIAâ€™s <em><a href="http://research.nvidia.com/publication/2017-07_Spatiotemporal-Variance-Guided-Filtering%3A" target="_blank" rel="noopener">Spatiotemporal Variance-Guided Filtering</a></em> (SVGF), we also presented a super-optimized denoising filter specialized for soft shadows with varying penumbra.</p>
<h5><img data-attachment-id="3111" data-permalink="https://colinbarrebrisebois.com/2018/04/07/some-thoughts-on-real-time-raytracing/picapica_shadows/" data-orig-file="https://zigguratvertigodotcom.files.wordpress.com/2018/04/picapica_shadows.png" data-orig-size="2711,1243" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="picapica_shadows" data-image-description="" data-medium-file="https://zigguratvertigodotcom.files.wordpress.com/2018/04/picapica_shadows.png?w=300" data-large-file="https://zigguratvertigodotcom.files.wordpress.com/2018/04/picapica_shadows.png?w=863" src="https://zigguratvertigodotcom.files.wordpress.com/2018/04/picapica_shadows.png?w=863" alt="picapica_shadows.png" srcset="https://zigguratvertigodotcom.files.wordpress.com/2018/04/picapica_shadows.png?w=863 863w, https://zigguratvertigodotcom.files.wordpress.com/2018/04/picapica_shadows.png?w=1726 1726w, https://zigguratvertigodotcom.files.wordpress.com/2018/04/picapica_shadows.png?w=150 150w, https://zigguratvertigodotcom.files.wordpress.com/2018/04/picapica_shadows.png?w=300 300w, https://zigguratvertigodotcom.files.wordpress.com/2018/04/picapica_shadows.png?w=768 768w, https://zigguratvertigodotcom.files.wordpress.com/2018/04/picapica_shadows.png?w=1024 1024w" sizes="(max-width: 863px) 100vw, 863px"><em>Surfel-based Global Illumination (from GDC slides)</em></h5>
<p>Finally we talked about how we handle multiple GPUs (mGPU) and split the frame, relying on the first GPU to act as an arbiter that dispatches work to secondary GPUs in parallel fork-join style.</p>
<h5><img data-attachment-id="3115" data-permalink="https://colinbarrebrisebois.com/2018/04/07/some-thoughts-on-real-time-raytracing/picapica_mgpu/" data-orig-file="https://zigguratvertigodotcom.files.wordpress.com/2018/04/picapica_mgpu.png" data-orig-size="1155,1435" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="picapica_mgpu" data-image-description="" data-medium-file="https://zigguratvertigodotcom.files.wordpress.com/2018/04/picapica_mgpu.png?w=241" data-large-file="https://zigguratvertigodotcom.files.wordpress.com/2018/04/picapica_mgpu.png?w=824" src="https://zigguratvertigodotcom.files.wordpress.com/2018/04/picapica_mgpu.png?w=863" alt="picapica_mgpu.png" srcset="https://zigguratvertigodotcom.files.wordpress.com/2018/04/picapica_mgpu.png?w=863 863w, https://zigguratvertigodotcom.files.wordpress.com/2018/04/picapica_mgpu.png?w=121 121w, https://zigguratvertigodotcom.files.wordpress.com/2018/04/picapica_mgpu.png?w=241 241w, https://zigguratvertigodotcom.files.wordpress.com/2018/04/picapica_mgpu.png?w=768 768w, https://zigguratvertigodotcom.files.wordpress.com/2018/04/picapica_mgpu.png?w=824 824w, https://zigguratvertigodotcom.files.wordpress.com/2018/04/picapica_mgpu.png 1155w" sizes="(max-width: 863px) 100vw, 863px"><em>mGPU in PICA pica (from GDC slides)</em></h5>
<p>All-and-all, it was a lot of content for the time slot we had. In case you want more info, check out the presentation:</p>
<p><em>You can also download the slides: <a href="https://media.contentapi.ea.com/content/dam/ea/seed/presentations/gdc2018-seed-shiny-pixels-and-beyond-real-time-raytracing-at-seed.pptx">Powerpoint</a>&nbsp;and <a href="https://media.contentapi.ea.com/content/dam/ea/seed/presentations/gdc2018-seed-shiny-pixels-and-beyond-real-time-raytracing-at-seed.pdf" target="_blank" rel="noopener">PDF</a>. You can also watch the presentation live <a href="http://www.gdcvault.com/play/1024801/" target="_blank" rel="noopener">here</a>&nbsp;(starts around 21:30).</em></p>
<p>Here are a few additional links that talk about <i>DirectX Raytracing </i>and <em>Project PICA PICA</em>:</p>
<ul>
<li><strong>Microsoft</strong>: <a href="https://blogs.msdn.microsoft.com/directx/2018/03/19/announcing-microsoft-directx-raytracing/">Announcing Microsoft DirectX Raytracing</a></li>
<li><strong>Ars Technica</strong>:&nbsp;<a href="https://arstechnica.com/gadgets/2018/03/microsoft-announces-the-next-step-in-gaming-graphics-directx-raytracing/" target="_blank" rel="noopener">DirectX Raytracing is the first step toward a graphics revolution</a></li>
<li><strong>PC World</strong>:&nbsp;<a href="https://www.pcworld.com/article/3263746/components-graphics/microsoft-directx-raytracing-dx12-features-hardware-software.html" target="_blank" rel="noopener">Microsoftâ€™s DirectX Raytracing paves the way for lifelike gaming, the graphics holy grail</a></li>
<li><strong>PC Gamer</strong>:&nbsp;&nbsp;<a href="https://www.pcgamer.com/what-microsofts-directx-raytracing-means-for-gaming/" target="_blank" rel="noopener">What Microsoftâ€™s DirectX Raytracing means for gaming</a></li>
<li><strong>Anandtech</strong>:&nbsp;<a href="https://www.anandtech.com/show/12547/expanding-directx-12-microsoft-announces-directx-raytracing" target="_blank" rel="noopener">Expanding DirectX 12: Microsoft Announces DirectX Raytracing</a></li>
<li><strong>Rock Paper Shotgun</strong>:&nbsp;<a href="https://www.rockpapershotgun.com/2018/03/20/eas-project-pica-pica-leads-new-wave-of-photorealistic-ray-tracing-graphics-demos-at-gdc-2018/amp/">EAâ€™s Project Pica Pica leads new wave of photorealistic ray-tracing graphics demos at GDC 2018</a></li>
<li><strong>Eurogamer</strong>:&nbsp;<a href="https://www.eurogamer.net/articles/2018-03-20-ea-just-showed-off-project-pica-pica-an-adorable-new-demo" target="_blank" rel="noopener">EA just showed off Project Pica Pica, an adorable new tech demo</a></li>
<li><strong>Polygon</strong>:&nbsp;<a href="https://www.polygon.com/2018/3/19/17139358/nvidia-rtx-real-time-ray-tracing-volta-gpu-gdc-2018" target="_blank" rel="noopener">Nvidiaâ€™s latest tech will enable â€˜cinematic-qualityâ€™ graphics â€” on unannounced GPUs</a></li>
</ul>
<h2><a id="additional"></a>Additional Thoughts</h2>
<p>As mentioned at GDC weâ€™ve had the chance to be involved early with DXR, to experiment and provide feedback as the API evolved. Super glad to have been part of this initiative. We still have a lot to explore, and the future is exciting! Some additional thoughts:</p>
<h3>Noise vs Ghosting vs Performance</h3>
<p>DXR opens the door to an entirely new class of techniques that have never been achieved in games. With real-time raytracing it feels like the upcoming years will be about managing complex tradeoffs, such as noise, ghosting, quality vs performance. While you can add more samples to reduce noise (and improve convergence) during stochastic sampling, it decreases performance. Alternatively you can reuse samples from previous frames (via temporal filtering), but it can add ghosting. It feels like achieving the right balance here will be important. As DXR gets adopted in games this topic will generate a lot of good presentations at conferences.</p>
<h3>Comparing Against Ground Truth</h3>
<p>We also mentioned that we built our own pathtracer inside our framework. This pathtracer acts as reference implementation, which at any point we can toggle when working on a feature for our hybrid renderer. This allows us to rapidly compare results, and see how a feature looks against ground truth. Since a lot of code is shared between the reference and various hybrids techniques, no significant additional maintenance is required. At the end of the day, having a reference implementation will help you make the best decision in order to achieve the balance between quality and performance for your (hybrid) techniques.</p>
<p>If raytracing is new to you and building a reference ray/pathtracer is of interest, many books and online resources are available. Peter Shirleyâ€™s <em><a href="https://www.amazon.com/Ray-Tracing-Weekend-Minibooks-Book-ebook/dp/B01B5AODD8" target="_blank" rel="noopener">Ray Tracing in One Weekend</a>&nbsp;</em>is quite popular. You should check it out! ðŸ™‚</p>
<h3>Specialized Denoising and Reconstruction</h3>
<p>Also mentioned during the presentation, we built a denoising filter specialized for soft penumbra shadows. While one can use general denoising algorithms like SVGF on the whole image, building a denoising filter around a specific term will undeniably achieve greater quality and performance. This is true since you can really customize the filter around the constraints of that term. In the near future one can expect that significant time and energy will be spent on specialized denoisers, and custom reconstruction of stochastically sampled terms.</p>
<h3>DXR Interop</h3>
<p>As mentioned earlier we share a lot of code between raytracing, rasterization and compute. In the event where one wants to bake lightmaps inside their engine (see <a href="https://twitter.com/SebHillaire" target="_blank" rel="noopener">SÃ©bastien Hillaire</a>â€˜s talk on <a href="https://www.ea.com/frostbite/news/real-time-raytracing-for-interactive-global-illumination-workflows-in-frostbite">Real-Time Raytracing For Interactive Global Illumination Workflows in Frostbite</a>), DXR is very appealing because you can evaluate your actual HLSL material shaders. No need for (limited) parameter conversion, which is often necessary when using an external lightmap baking tool.</p>
<p>This is awesome!</p>
<h2>Wrapping-up</h2>
<p>Even though the API is there and available to everyone, this is just the beginning.&nbsp;Itâ€™s an important tool going forward that will enable new techniques in games, and could end up pushing the industry to new heights.&nbsp;Iâ€™m looking forward to the new techniques that evolve from everyone having access to DXR, and what kind of rendering problems get solved. I also find it quite appealing for the research community to be able to try and solve problems closer to the realm of real-time raytracing, where researchers can implement their solutions using a raytracing API that everyone can use.</p>
<p>Because itâ€™s unified, it should also be easy for you to pick up the API, experiment and integrate in your own engine. Again, one doesnâ€™t need this API to do real-time raytracing, but it provides a really nice package and a common language that all DirectX 12 developers can talk around. Itâ€™s also a clear focus point for hardware makers to focus on optimization. Also compute hasnâ€™t really changed in a while, so hopefully these improvements will drive improvements in compute and in the the pipelines as well. <strong>That being said, the API is obviously not perfect, and&nbsp;is still at the proposal stage. Microsoft is open to additional feedback and discussion. Try it out and send your feedback!</strong></p>
<p>Canâ€™t wait to see what you will do with DXR! ðŸ™‚</p>
			</div><!-- .entry-content -->

	
	<!-- .entry-footer -->
</article></div></div>
    </div>
    <footer>
        <div>created by buildstarted &copy; 2020 <a href="/about">about</a></div>
        <div>Share this page on social media: copy and paste this url https://linksfor.dev/</div>
        <div>If you prefer RSS: <a href="https://linksfor.dev/feed.xml">https://linksfor.dev/feed.xml</a></div>
		<div>Customer satisfaction guaranteed to be optional.</div>
    </footer>
    
    <script async defer>
        _dna = window._dna || {};
        _dna.siteId = "linksfor.devs";
        _dna.outlink = true;

        (function () {
            let dna = document.createElement('script');
            dna.type = 'text/javascript';
            dna.async = true;
            dna.src = '//dna.buildstarted.com/t.js';
            let s = document.getElementsByTagName('script')[0];
            s.parentNode.insertBefore(dna, s);
        })();
    </script>
    <noscript><img src="//dna.buildstarted.com/g?siteId=linksfor.devs"/></noscript>
</body>
</html>