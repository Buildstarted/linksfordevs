<!DOCTYPE html>
<html lang="en">
<head>
    <title>
Revisiting Unreasonable Effectiveness of Data in Deep Learning Era - linksfor.dev(s)    </title>
    <meta charset="utf-8">
    <link rel="alternate" type="application/rss+xml" title="Linksfor.dev(s) feed" href="https://linksfor.dev/feed.rss" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta name="google" value="notranslate">
    <link rel="stylesheet" type="text/css" href="/style.css" />
    <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon">
    <link rel="icon" href="/favicon.ico" type="image/x-icon">
    <meta property="og:title" content="Revisiting Unreasonable Effectiveness of Data in Deep Learning Era - linksfor.dev(s)"/>
    <meta property="article:author" content="Authors:Chen Sun, Abhinav Shrivastava, Saurabh Singh, Abhinav Gupta"/>
    <meta property="og:description" content="The success of deep learning in vision can be attributed to: (a) models with&#xA;high capacity; (b) increased computational power; and (c) availability of&#xA;large-scale labeled data. Since 2012, there have been significant advances in&#xA;representation capabilities of the models and computational capabilities of&#xA;GPUs. But the size of the biggest dataset has surprisingly remained constant.&#xA;What will happen if we increase the dataset size by 10x or 100x? This paper&#xA;takes a step towards clearing the clouds of mystery surrounding the&#xA;relationship between `enormous data&#x27; and visual deep learning. By exploiting&#xA;the JFT-300M dataset which has more than 375M noisy labels for 300M images, we&#xA;investigate how the performance of current vision tasks would change if this&#xA;data was used for representation learning. Our paper delivers some surprising&#xA;(and some expected) findings. First, we find that the performance on vision&#xA;tasks increases logarithmically based on volume of training data size. Second,&#xA;we show that representation learning (or pre-training) still holds a lot of&#xA;promise. One can improve performance on many vision tasks by just training a&#xA;better base model. Finally, as expected, we present new state-of-the-art&#xA;results for different vision tasks including image classification, object&#xA;detection, semantic segmentation and human pose estimation. Our sincere hope is&#xA;that this inspires vision community to not undervalue the data and develop&#xA;collective efforts in building larger datasets."/>
    <meta property="og:type" content="website"/>
    <meta property="og:url" content="https://arxiv.org/abs/1707.02968"/>

<meta property="og:site_name" content="linksfor.dev(s)" />
</head>
<body>
	<div class="devring" style="background: #222">
		<div class="grid">
			<div style="display: grid; grid-template-columns: .5fr 1fr 1fr 1fr; text-align: center;">
				<span class="devring-title">devring.club</span>
				<a href="https://devring.club/site/1/previous" class="devring-previous">Previous</a>
				<a href="https://devring.club/random" class="devring-random">Random</a>
				<a href="https://devring.club/sites/1/next" class="devring-next">Next</a>
			</div>
		</div>
	</div>
    <div class="grid">
        <h1>
<a href="/" style="color:inherit">linksfor.dev(s)</a>
        </h1>
        <title>linksfor.dev(s) - Revisiting Unreasonable Effectiveness of Data in Deep Learning Era</title>
<div class="readable">
        <h1>Revisiting Unreasonable Effectiveness of Data in Deep Learning Era</h1>
            <div>by Authors:Chen Sun, Abhinav Shrivastava, Saurabh Singh, Abhinav Gupta</div>
            <div>Reading time: 2-3 minutes</div>
        <div>Posted here: 27 Feb 2019</div>
        <p><a href="https://arxiv.org/abs/1707.02968">https://arxiv.org/abs/1707.02968</a></p>
        <hr/>
<div id="readability-page-1" class="page"><div id="content-inner">
  <div id="abs">

    
    
    <p>
  
  
  
    
  
  
    
    
  

  (Submitted on 10 Jul 2017 (<a href="https://arxiv.org/abs/1707.02968v1">v1</a>), last revised 4 Aug 2017 (this version, v2))</p>
    <blockquote><span>Abstract:</span>  The success of deep learning in vision can be attributed to: (a) models with
high capacity; (b) increased computational power; and (c) availability of
large-scale labeled data. Since 2012, there have been significant advances in
representation capabilities of the models and computational capabilities of
GPUs. But the size of the biggest dataset has surprisingly remained constant.
What will happen if we increase the dataset size by 10x or 100x? This paper
takes a step towards clearing the clouds of mystery surrounding the
relationship between `enormous data' and visual deep learning. By exploiting
the JFT-300M dataset which has more than 375M noisy labels for 300M images, we
investigate how the performance of current vision tasks would change if this
data was used for representation learning. Our paper delivers some surprising
(and some expected) findings. First, we find that the performance on vision
tasks increases logarithmically based on volume of training data size. Second,
we show that representation learning (or pre-training) still holds a lot of
promise. One can improve performance on many vision tasks by just training a
better base model. Finally, as expected, we present new state-of-the-art
results for different vision tasks including image classification, object
detection, semantic segmentation and human pose estimation. Our sincere hope is
that this inspires vision community to not undervalue the data and develop
collective efforts in building larger datasets.
</blockquote>
    <!--CONTEXT-->
    
  </div>
</div><div>
      <h2>Submission history</h2><p> From: Chen Sun [<a href="https://arxiv.org/show-email/1a7ef483/1707.02968">view email</a>]
      <br>
  <strong><a href="https://arxiv.org/abs/1707.02968v1">[v1]</a></strong>
  Mon, 10 Jul 2017 17:54:31 UTC (834 KB)<br><strong>[v2]</strong>
Fri, 4 Aug 2017 01:33:22 UTC (834 KB)<br></p></div></div></div>
    </div>
    <footer>
        <div>created by buildstarted &copy; 2020 <a href="/about">about</a></div>
        <div>Share this page on social media: copy and paste this url https://linksfor.dev/</div>
        <div>If you prefer RSS: <a href="https://linksfor.dev/feed.xml">https://linksfor.dev/feed.xml</a></div>
		<div>Customer satisfaction guaranteed to be optional.</div>
    </footer>
    
    <script async defer>
        _dna = window._dna || {};
        _dna.siteId = "linksfor.devs";
        _dna.outlink = true;

        (function () {
            let dna = document.createElement('script');
            dna.type = 'text/javascript';
            dna.async = true;
            dna.src = '//dna.buildstarted.com/t.js';
            let s = document.getElementsByTagName('script')[0];
            s.parentNode.insertBefore(dna, s);
        })();
    </script>
    <noscript><img src="//dna.buildstarted.com/g?siteId=linksfor.devs"/></noscript>
</body>
</html>