<!DOCTYPE html>
<html lang="en">
<head>
    <title>
SLIDE : In Defense of Smart Algorithms over Hardware Acceleration for Large-Scale Deep Learning Systems - linksfor.dev(s)    </title>
    <meta charset="utf-8">
    <link rel="alternate" type="application/rss+xml" title="Linksfor.dev(s) feed" href="https://linksfor.dev/feed.rss" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta name="google" value="notranslate">
    <link rel="stylesheet" type="text/css" href="/style.css" />
    <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon">
    <link rel="icon" href="/favicon.ico" type="image/x-icon">
    <meta property="og:title" content="SLIDE : In Defense of Smart Algorithms over Hardware Acceleration for Large-Scale Deep Learning Systems - linksfor.dev(s)"/>
    <meta property="article:author" content="Authors:Beidi Chen, Tharun Medini, James Farwell, Sameh Gobriel, Charlie Tai, Anshumali Shrivastava"/>
    <meta property="og:description" content="Deep Learning (DL) algorithms are the central focus of modern machine&#xA;learning systems. As data volumes keep growing, it has become customary to&#xA;train large neural networks with hundreds of millions of parameters to maintain&#xA;enough capacity to memorize these volumes and obtain state-of-the-art accuracy.&#xA;To get around the costly computations associated with large models and data,&#xA;the community is increasingly investing in specialized hardware for model&#xA;training. However, specialized hardware is expensive and hard to generalize to&#xA;a multitude of tasks. The progress on the algorithmic front has failed to&#xA;demonstrate a direct advantage over powerful hardware such as NVIDIA-V100 GPUs.&#xA;This paper provides an exception. We propose SLIDE (Sub-LInear Deep learning&#xA;Engine) that uniquely blends smart randomized algorithms, with multi-core&#xA;parallelism and workload optimization. Using just a CPU, SLIDE drastically&#xA;reduces the computations during both training and inference outperforming an&#xA;optimized implementation of Tensorflow (TF) on the best available GPU. Our&#xA;evaluations on industry-scale recommendation datasets, with large fully&#xA;connected architectures, show that training with SLIDE on a 44 core CPU is more&#xA;than 3.5 times (1 hour vs. 3.5 hours) faster than the same network trained&#xA;using TF on Tesla V100 at any given accuracy level. On the same CPU hardware,&#xA;SLIDE is over 10x faster than TF. We provide codes and scripts for&#xA;reproducibility."/>
    <meta property="og:type" content="website"/>
    <meta property="og:url" content="https://arxiv.org/abs/1903.03129"/>

<meta property="og:site_name" content="linksfor.dev(s)" />
</head>
<body>
    <div class="devring" style="background: #222">
        <div style="text-align:center">Explore other dev related sites in this ring. If you would like to join this ring <a href="https://devring.club">click here</a>.</div>
        <div class="grid">
            <div style="display: grid; grid-template-columns: .5fr 1fr 1fr 1fr; text-align: center;">
                <span class="devring-title"><a href="https://devring.club/">devring.club</a></span>
                <a href="https://devring.club/sites/1/prev" class="devring-previous">Previous</a>
                <a href="https://devring.club/random" class="devring-random">Random</a>
                <a href="https://devring.club/sites/1/next" class="devring-next">Next</a>
            </div>
        </div>
    </div>
    <div class="grid">
        <h1>
<a href="/" style="color:inherit">linksfor.dev(s)</a>
        </h1>
        <title>linksfor.dev(s) - SLIDE : In Defense of Smart Algorithms over Hardware Acceleration for Large-Scale Deep Learning Systems</title>
<div class="readable">
        <h1>SLIDE : In Defense of Smart Algorithms over Hardware Acceleration for Large-Scale Deep Learning Systems</h1>
            <div>by Authors:Beidi Chen, Tharun Medini, James Farwell, Sameh Gobriel, Charlie Tai, Anshumali Shrivastava</div>
            <div>Reading time: 2-3 minutes</div>
        <div>Posted here: 06 Mar 2020</div>
        <p><a href="https://arxiv.org/abs/1903.03129">https://arxiv.org/abs/1903.03129</a></p>
        <hr/>
<div id="readability-page-1" class="page"><div id="content-inner">
  <div id="abs">

    
    
    <p>
  
  
  
    
  
  
    
    
  

  (Submitted on 7 Mar 2019 (<a href="https://arxiv.org/abs/1903.03129v1">v1</a>), last revised 1 Mar 2020 (this version, v2))</p>
    <blockquote><span>Abstract:</span>  Deep Learning (DL) algorithms are the central focus of modern machine
learning systems. As data volumes keep growing, it has become customary to
train large neural networks with hundreds of millions of parameters to maintain
enough capacity to memorize these volumes and obtain state-of-the-art accuracy.
To get around the costly computations associated with large models and data,
the community is increasingly investing in specialized hardware for model
training. However, specialized hardware is expensive and hard to generalize to
a multitude of tasks. The progress on the algorithmic front has failed to
demonstrate a direct advantage over powerful hardware such as NVIDIA-V100 GPUs.
This paper provides an exception. We propose SLIDE (Sub-LInear Deep learning
Engine) that uniquely blends smart randomized algorithms, with multi-core
parallelism and workload optimization. Using just a CPU, SLIDE drastically
reduces the computations during both training and inference outperforming an
optimized implementation of Tensorflow (TF) on the best available GPU. Our
evaluations on industry-scale recommendation datasets, with large fully
connected architectures, show that training with SLIDE on a 44 core CPU is more
than 3.5 times (1 hour vs. 3.5 hours) faster than the same network trained
using TF on Tesla V100 at any given accuracy level. On the same CPU hardware,
SLIDE is over 10x faster than TF. We provide codes and scripts for
reproducibility.
</blockquote>
    <!--CONTEXT-->
    
  </div>
</div><div>
      <h2>Submission history</h2><p> From: Beidi Chen [<a href="https://arxiv.org/show-email/24d7e217/1903.03129">view email</a>]
      <br>
  <strong><a href="https://arxiv.org/abs/1903.03129v1">[v1]</a></strong>
  Thu, 7 Mar 2019 19:12:07 UTC (1,971 KB)<br><strong>[v2]</strong>
Sun, 1 Mar 2020 03:17:52 UTC (3,650 KB)<br></p></div></div></div>
    </div>
    <footer>
        <div>created by buildstarted &copy; 2020 <a href="/about">about</a></div>
        <div>Share this page on social media: copy and paste this url https://linksfor.dev/</div>
        <div>If you prefer RSS: <a href="https://linksfor.dev/feed.xml">https://linksfor.dev/feed.xml</a></div>
        <div>Customer satisfaction guaranteed to be optional.</div>
    </footer>
    
    <script async defer>
        _dna = window._dna || {};
        _dna.siteId = "linksfor.devs";
        _dna.outlink = true;

        (function() {
            let dna = document.createElement('script');
            dna.type = 'text/javascript';
            dna.async = true;
            dna.src = '//dna.buildstarted.com/t.js';
            let s = document.getElementsByTagName('script')[0];
            s.parentNode.insertBefore(dna, s);
        })();
    </script>
    <noscript><img src="//dna.buildstarted.com/g?siteId=linksfor.devs"/></noscript>
</body>
</html>