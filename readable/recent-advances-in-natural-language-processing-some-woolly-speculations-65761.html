<!DOCTYPE html>
<html lang="en">
<head>
    <title>
Recent advances in Natural Language Processing- Some Woolly speculations - linksfor.dev(s)    </title>
    <meta charset="utf-8">
    <link rel="alternate" type="application/rss+xml" title="Linksfor.dev(s) feed" href="https://linksfor.dev/feed.rss" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta name="google" value="notranslate">
    <link rel="stylesheet" type="text/css" href="/style.min.css" />
    <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon">
    <link rel="icon" href="/favicon.ico" type="image/x-icon">
    <meta property="og:title" content="Recent advances in Natural Language Processing- Some Woolly speculations - linksfor.dev(s)"/>
    <meta property="article:author" content="Published by deponysum&#xA;&#x9;&#x9;&#xA;&#xA;&#x9;&#x9;&#xA;&#x9;&#x9;&#x9;&#x9;&#x9;&#x9;&#xA;&#x9;&#x9;&#x9;&#x9;View all posts by deponysum"/>
    <meta property="og:description" content="If you enjoy this article, please check out my free book by clicking Here: &#x201C;Something to Read in Quarantine: Essays 2018-2020.&#x201D; Natural Language Processing (NLP) per Wikipedia: &#x201C;Is a su&#x2026;"/>
    <meta property="og:type" content="website"/>
    <meta property="og:url" content="https://deponysum.com/2020/01/16/recent-advances-in-natural-language-processing-some-woolly-speculations/"/>

<meta property="og:site_name" content="linksfor.dev(s)" />
</head>
<body>
    <div class="devring" style="background: #222">
        <div style="text-align:center">Explore other dev related sites in this ring. If you would like to join this ring <a href="https://devring.club">click here</a>.</div>
        <div class="grid">
            <div style="display: grid; grid-template-columns: .5fr 1fr 1fr 1fr; text-align: center;">
                <span class="devring-title"><a href="https://devring.club/">devring.club</a></span>
                <a href="https://devring.club/sites/1/prev" class="devring-previous">Previous</a>
                <a href="https://devring.club/random" class="devring-random">Random</a>
                <a href="https://devring.club/sites/1/next" class="devring-next">Next</a>
            </div>
        </div>
    </div>
    <div class="grid">
        <h1 style="margin: unset">
<a href="/" style="color:inherit">linksfor.dev(s)</a>
        </h1>
        <title>linksfor.dev(s) - Recent advances in Natural Language Processing- Some Woolly speculations</title>
<div class="readable">
        <h1>Recent advances in Natural Language Processing- Some Woolly speculations</h1>
            <div>by Published by deponysum&#xA;&#x9;&#x9;&#xA;&#xA;&#x9;&#x9;&#xA;&#x9;&#x9;&#x9;&#x9;&#x9;&#x9;&#xA;&#x9;&#x9;&#x9;&#x9;View all posts by deponysum</div>
            <div>Reading time: 8-10 minutes</div>
        <div>Posted here: 17 Aug 2020</div>
        <p><a href="https://deponysum.com/2020/01/16/recent-advances-in-natural-language-processing-some-woolly-speculations/">https://deponysum.com/2020/01/16/recent-advances-in-natural-language-processing-some-woolly-speculations/</a></p>
        <hr/>
<div id="readability-page-1" class="page"><article id="post-1118">
	<!-- .entry-header -->

	<div>
		<p><em>If you enjoy this article, please check out my free book by clicking <a href="https://deponysum.com/2020/03/30/something-to-read-in-quarantine-essays-2018-to-2020/">Here: “Something to Read in Quarantine: Essays 2018-2020.”</a></em></p>
<p id="b9a9">Natural Language Processing (NLP) per Wikipedia:</p>
<p id="34cf">“Is a subfield of linguistics, computer science, information engineering, and artificial intelligence concerned with the interactions between computers and human (natural) languages, in particular how to program computers to process and analyze large amounts of natural language data.”</p>
<p id="7dd8">The field has seen tremendous advances during the recent explosion of progress in machine learning techniques.</p>
<p id="b5f8">Here are some of its more impressive recent achievements:</p>
<p id="cb15"><strong>A)</strong>&nbsp;The Winograd Schema is a test of common sense reasoning- easy for humans, but historically almost impossible for computers- which requires the test taker to indicate which noun an ambiguous pronoun stands for. The correct answer hinges on a single word, which is different between two separate versions of the question. For example:</p>
<p id="512d"><em>The city councilmen refused the demonstrators a permit because they feared violence.</em></p>
<p id="ece3"><em>The city councilmen refused the demonstrators a permit because they advocated violence.</em></p>
<p id="0ddf">Who does the pronoun “They” refer to in each of the instances?</p>
<p id="d33c">The Winograd schema test was originally intended to be a more rigorous replacement for the Turing test, because it seems to require deep knowledge of how things fit together in the world, and the ability to reason about that knowledge in a linguistic context. Recent advances in NLP have allowed computers to achieve near human scores:(<a href="https://gluebenchmark.com/leaderboard/" target="_blank" rel="noopener nofollow">https://gluebenchmark.com/leaderboard/</a>).</p>
<p id="0a43"><strong>B)</strong>&nbsp;The New York Regent’s science exam is a test requiring both scientific knowledge and reasoning skills, covering an extremely broad range of topics. Some of the questions include:</p>
<p id="a36d"><em>1.Which equipment will best separate a mixture of iron filings and black pepper? (1) magnet (2) filter paper (3) triplebeam balance (4) voltmeter</em></p>
<p id="9797"><em>2. Which form of energy is produced when a rubber band vibrates? (1) chemical (2) light (3) electrical (4) sound</em></p>
<p id="966a"><em>3. Because copper is a metal, it is (1) liquid at room temperature (2) nonreactive with other substances (3) a poor conductor of electricity (4) a good conductor of heat</em></p>
<p id="6d70"><em>4. Which process in an apple tree primarily results from cell division? (1) growth (2) photosynthesis (3) gas exchange (4) waste removal</em></p>
<p id="3a8a">On the 8th grade, non-diagram based questions of the test, a program was recently able to score 90%. (&nbsp;<a href="https://arxiv.org/pdf/1909.01958.pdf" target="_blank" rel="noopener nofollow">https://arxiv.org/pdf/1909.01958.pdf</a>&nbsp;)</p>
<p id="2f49"><strong>C)</strong></p>
<p id="267a">It’s not just about answer selection either. Progress in text generation has been impressive. See, for example, some of the text samples created by Megatron:&nbsp;<a href="https://arxiv.org/pdf/1909.08053.pdf" target="_blank" rel="noopener nofollow">https://arxiv.org/pdf/1909.08053.pdf</a></p>
<p id="b8f7"><strong>2.</strong></p>
<p id="db13">Much of this progress has been rapid. Big progress on the Winograd schema, for example, still looked like it might be decades away back in (from memory) much of 2018. The computer science is advancing very fast, but it’s not clear our concepts have kept up.</p>
<p id="1a6c">I found this relatively sudden progress in NLP surprising. In my head- and maybe this was naive- I had thought that, in order to attempt these sorts of tasks with any facility, it wouldn’t be sufficient to simply feed a computer lots of text. Instead, any “proper” attempt to understand language would have to integrate different modalities of experience and understanding, like visual and auditory, in order to build up a full picture of how things relate to each other in the world. Only on the basis of this extra-linguistic grounding could it deal flexibly with problems involving rich meanings- we might call this the multi-modality thesis. Whether the multi-modality thesis is true for some kinds of problems or not, it’s certainly true for far fewer problems than I, and many others, had suspected.</p>
<p id="5a4d">I think science-fictiony speculations generally backed me up on this (false) hunch. Most people imagined that this kind of high-level language “understanding” would be the capstone of AI research, the thing that comes after the program already has a sophisticated extra-linguistic model of the world. This sort of just seemed obvious- a great example of how assumptions you didn’t even know you were making can ruin attempts to predict the future.</p>
<p id="f943">In hindsight it makes a certain sense that reams and reams of text alone can be used to build the capabilities needed to answer questions like these. A lot of people remind us that these programs are really just statistical analyses of the co-occurence of words, however complex and glorified. However we should not forget that the relationships between words are isomorphic to the relations between things- that isomorphism is why language works. This is to say the patterns in language use mirror the patterns of how things are(1). Models are transitive- if x models y, and y models z, then x models z. The upshot of these facts are that if you have a really good statistical model of how words relate to each other, that model is also implicitly a model of the world.</p>
<p id="12db">It might be instructive to think about what it would take to create a program which has a model of eighth grade science sufficient to understand and answer questions about hundreds of different things like “growth is driven by cell division”, and “What can magnets be used for” that wasn’t NLP led. It would be a nightmare of many different (probably handcrafted) models. Speaking somewhat loosely, language allows for intellectual capacities to be greatly compressed. From this point of view, it shouldn’t be surprising that some of the first signs of really broad capacity- common sense reasoning, wide ranging problem solving etc., have been found in language based programs- words and their relationships are just a vastly more efficient way of representing knowledge than the alternatives.</p>
<p id="1e8b">So I find myself wondering if language is not the crown of general intelligence, but a potential shortcut to it.</p>
<p id="beea"><strong>3.</strong></p>
<p id="0095">A couple of weeks ago I finished this essay, read through it, and decided it was not good enough to publish. The point about language being isomorphic to the world, and that therefore any sufficiently good model of language&nbsp;<em>is</em>&nbsp;a model of the world, is important, but it’s kind of abstract, and far from original.</p>
<p id="e0e2">Then today I read&nbsp;<a href="https://slatestarcodex.com/2020/01/06/a-very-unlikely-chess-game/" target="_blank" rel="noopener nofollow">this report</a>&nbsp;by Scott Alexander of having trained GPT-2 (a language program) to play chess. I realised this was the perfect example. GPT-2 has no (visual) understanding of things like the arrangement of a chess board. But if you feed it enough sequences of alphanumerically encoded games- 1.Kt-f3, d5 and so on- it begins to understand patterns in these strings of characters which are isomorphic to chess itself. Thus, for all intents and purposes, it develops a model of chess.</p>
<p id="1a14">Exactly how strong this approach is- whether GPT-2 is capable of some limited analysis, or can only overfit openings- remains to be seen. We might have a better idea as it is optimised — for example, once it is fed board states instead of sequences of moves. Either way though, it illustrates the point about isomorphism.</p>
<p id="a87a">Of course everyday language stands in a woolier relation to sheep, pine cones, desire and quarks than the formal language of chess moves stands in relation to chess moves, and the patterns are far more complex. Modality, uncertainty, vagueness and other complexities enter but the isomorphism between world and language is there, even if inexact.</p>
<p id="b6c0"><strong>Postscript- The Chinese Room Argument</strong></p>
<p id="f901">After similar arguments are made, someone usually mentions the Chinese room thought experiment. There are, I think, two useful things to say about it:</p>
<p id="3d7e">A) The thought experiment is an argument about understanding in itself, separate from capacity to handle tasks, a difficult thing to quantify or understand. It’s unclear that there is a practical upshot for what <em>AI can actually do.</em></p>
<p id="1f79">B) A lot of the power of the thought experiment hinges on the fact that the room solves questions using a lookup table, this stacks the deck. Perhaps we be more willing to say that the room as a whole understood language if it formed an (implicit) model of how things are, and of the current context, and used those models to answer questions? Even if this doesn’t deal with all the intuition that the room cannot understand Chinese, I think it takes a bite from it.</p>
<p id="96a9">— — — — — — — — — — — — — — — — — — — — — — — — — — — — — —</p>
<p id="4642">(1)- Strictly of course only the patterns in true sentences mirror, or are isomorphic to, the arrangement of the world, but most sentences people utter are at least approximately true.</p>
			</div><!-- .entry-content -->

	<!-- .entry-footer -->

		<!-- .entry-auhtor -->
</article></div></div>
    </div>
    <footer>
        <div>created by <a href="https://buildstarted.com">buildstarted</a> &copy; 2020 <a href="/about">about</a></div>
        <div>Share this page on social media: copy and paste this url https://linksfor.dev/</div>
        <div>If you prefer RSS: <a href="https://linksfor.dev/feed.xml">https://linksfor.dev/feed.xml</a></div>
        <div>Customer satisfaction guaranteed to be optional.</div>
    </footer>
    
    <script async defer>
        _dna = window._dna || {};
        _dna.siteId = "linksfor.devs";
        _dna.outlink = true;

        (function() {
            let dna = document.createElement('script');
            dna.type = 'text/javascript';
            dna.async = true;
            dna.src = '//dna.buildstarted.com/t.js';
            let s = document.getElementsByTagName('script')[0];
            s.parentNode.insertBefore(dna, s);
        })();
    </script>
    <noscript><img src="//dna.buildstarted.com/g?siteId=linksfor.devs"/></noscript>
</body>
</html>