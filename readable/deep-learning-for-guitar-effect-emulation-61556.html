<!DOCTYPE html>
<html lang="en">
<head>
    <title>
Deep Learning for Guitar Effect Emulation - linksfor.dev(s)    </title>
    <meta charset="utf-8">
    <link rel="alternate" type="application/rss+xml" title="Linksfor.dev(s) feed" href="https://linksfor.dev/feed.rss" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta name="google" value="notranslate">
    <link rel="stylesheet" type="text/css" href="/style.css" />
    <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon">
    <link rel="icon" href="/favicon.ico" type="image/x-icon">
    <meta property="og:title" content="Deep Learning for Guitar Effect Emulation - linksfor.dev(s)"/>
    <meta property="og:description" content="Since the 1940s, electric guitarists, keyboardists, and other instrumentalists have been using effects pedals, devices that modify the sound of the original audio source. Typical effects include distortion, compression, chorus, reverb, and delay. Early effects pedals consisted of basic analog circuits, often along with vacuum tubes, which were later replaced with transistors. Although many pedals today apply effects digitally with modern signal processing techniques, many purists argue that the sound of analog pedals can not be replaced by their digital counterparts. We&#x2019;ll follow a deep learning approach to see if we can use machine learning to replicate the sound of an iconic analog effect pedal, the Ibanez Tube Screamer. This post will be mostly a reproduction of the work done by Alec Wright et al.&#xA0;in Real-Time Guitar Amplifier Emulation with Deep Learning1. Alec Wright et al., &#x201C;Real-Time Guitar Amplifier Emulation with&#xA0;&#x21A9;"/>
    <meta property="og:type" content="website"/>
    <meta property="og:url" content="https://teddykoker.com/2020/05/deep-learning-for-guitar-effect-emulation/"/>

<meta property="og:site_name" content="linksfor.dev(s)" />
</head>
<body>
    <div class="devring" style="background: #222">
        <div style="text-align:center">Explore other dev related sites in this ring. If you would like to join this ring <a href="https://devring.club">click here</a>.</div>
        <div class="grid">
            <div style="display: grid; grid-template-columns: .5fr 1fr 1fr 1fr; text-align: center;">
                <span class="devring-title"><a href="https://devring.club/">devring.club</a></span>
                <a href="https://devring.club/sites/1/prev" class="devring-previous">Previous</a>
                <a href="https://devring.club/random" class="devring-random">Random</a>
                <a href="https://devring.club/sites/1/next" class="devring-next">Next</a>
            </div>
        </div>
    </div>
    <div class="grid">
        <h1>
<a href="/" style="color:inherit">linksfor.dev(s)</a>
        </h1>
        <title>linksfor.dev(s) - Deep Learning for Guitar Effect Emulation</title>
<div class="readable">
        <h1>Deep Learning for Guitar Effect Emulation</h1>
            <div>Reading time: 6-7 minutes</div>
        <div>Posted here: 11 May 2020</div>
        <p><a href="https://teddykoker.com/2020/05/deep-learning-for-guitar-effect-emulation/">https://teddykoker.com/2020/05/deep-learning-for-guitar-effect-emulation/</a></p>
        <hr/>
<div id="readability-page-1" class="page"><div aria-label="Content">
      <div>
        <article itemscope="" itemtype="http://schema.org/BlogPosting">

  

  <div itemprop="articleBody">
    <p>Since the 1940s, electric guitarists, keyboardists, and other
instrumentalists have been using <a href="https://en.wikipedia.org/wiki/Effects_unit">effects
pedals</a>, devices that modify
the sound of the original audio source. Typical effects include
distortion, compression, chorus, reverb, and delay. Early effects pedals
consisted of basic analog circuits, often along with vacuum tubes, which
were later replaced with transistors. Although many pedals today apply
effects digitally with modern signal processing techniques, many purists
argue that the sound of analog pedals can not be replaced by their
digital counterparts. We’ll follow a deep learning approach to see if we
can use machine learning to replicate the sound of an iconic analog
effect pedal, the <a href="https://en.wikipedia.org/wiki/Ibanez_Tube_Screamer">Ibanez Tube
Screamer</a>. This post
will be mostly a reproduction of the work done by Alec Wright et al.&nbsp;in
<em>Real-Time Guitar Amplifier Emulation with Deep Learning</em><sup id="fnref:1"><a href="#fn:1">1</a></sup>.</p>

<!--more-->
<p>The code for this model (and training data) is available here:
<a href="https://github.com/teddykoker/pedalnet">github.com/teddykoker/pedalnet</a>.</p>

<h2 id="data">Data</h2>

<p>Popularized by blues guitarist Stevie Ray Vaughan, the Ibanez Tube
Screamer is used by many well known guitarists including Gary Clark Jr.,
The Edge (U2), Noel Gallagher (Oasis), Billie Joe Armstrong (Green Day),
John Mayer, Eric Johnson, Carlos Santana, and many more<sup id="fnref:2"><a href="#fn:2">2</a></sup>. Using my
own Ibanez TS9 Tube Screamer, we collect data by connecting the pedal to
an audio interface and recording the output of a dataset of prerecorded
guitar playing. The
<a href="https://www.idmt.fraunhofer.de/en/business_units/m2d/smt/guitar.html">IDMT-SMT-Guitar</a>
dataset contains dry signal recordings of many different electric
guitars with both monophonic and polyphonic phrases over different
genres and playing techniques<sup id="fnref:3"><a href="#fn:3">3</a></sup>. We’ll use a 5 minute subset of this
data, and store both the original audio as well as the output of the
pedal when the audio is passed through it. To maintain reproducibility,
we set all of the knobs on both the pedal and audio interface to 12
o’clock:</p>

<p><img src="https://teddykoker.com/images/signal_chain.png" height="400" width="auto"></p>

<h2 id="model">Model</h2>

<p>Our model architecture will be nearly identical to that of <em>WaveNet: A
Generative Model for Raw Audio</em><sup id="fnref:4"><a href="#fn:4">4</a></sup>. WaveNet models are able to generate
audio that is both qualitatively and quantitatively better than more
traditional LSTM and statistical-based models.</p>

<h3 id="dilated-convolutions">Dilated Convolutions</h3>

<p>The “main ingredient” of the WaveNet architecture consists of a stack of
dilated convolutions, or <em>à trous</em>, layers. By doubling the dilation –
increasing the spacing between each parameter in the filter – for each
layer , the receptive field of the model grows exponentially with depth
of the model. This allows for computationally efficient models with
large receptive fields, which is needed for audio effect emulation.</p>

<p><img src="https://teddykoker.com/images/dilated_conv.png" alt=""> Figure 3 from WaveNet: Visualization of a
stack of <em>dilated</em> convolutional layers.</p>

<h3 id="gated-activation-units">Gated Activation Units</h3>

<p>Another notable feature of WaveNet architecture is the gated activation
unit. The output of each layer is computed as:</p>

<p><span><span><span><span><math><semantics><mrow><mi>z</mi><mo>=</mo><mi>tanh</mi><mo>⁡</mo><mrow><mo fence="true">(</mo><msub><mi>W</mi><mrow><mi>f</mi><mo separator="true">,</mo><mi>k</mi></mrow></msub><mo>∗</mo><mi>x</mi><mo fence="true">)</mo></mrow><mo>⊙</mo><mi>σ</mi><mrow><mo fence="true">(</mo><msub><mi>W</mi><mrow><mi>g</mi><mo separator="true">,</mo><mi>k</mi></mrow></msub><mo>∗</mo><mi>x</mi><mo fence="true">)</mo></mrow></mrow><annotation encoding="application/x-tex">
z = \tanh \left(W_{f, k} \ast x\right) \odot \sigma \left(W_{g, k} \ast x
\right)
</annotation></semantics></math></span></span></span></span></p>

<p>where <span><span><span><math><semantics><mrow><mo>∗</mo></mrow><annotation encoding="application/x-tex">\ast</annotation></semantics></math></span></span></span>, <span><span><span><math><semantics><mrow><mo>⊙</mo></mrow><annotation encoding="application/x-tex">\odot</annotation></semantics></math></span></span></span>, and <span><span><span><math><semantics><mrow><mi>σ</mi><mo stretchy="false">(</mo><mo>⋅</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\sigma(\cdot)</annotation></semantics></math></span></span></span> denote convolution,
element-wise multiplication, and the sigmoid function, respectively.
<span><span><span><math><semantics><mrow><msub><mi>W</mi><mrow><mi>f</mi><mo separator="true">,</mo><mi>k</mi></mrow></msub></mrow><annotation encoding="application/x-tex">W_{f, k}</annotation></semantics></math></span></span></span> and <span><span><span><math><semantics><mrow><msub><mi>W</mi><mrow><mi>g</mi><mo separator="true">,</mo><mi>k</mi></mrow></msub></mrow><annotation encoding="application/x-tex">W_{g, k}</annotation></semantics></math></span></span></span> are the learned convolutionial filters at
layer <span><span><span><math><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span></span></span>. This was found to produce better results than the
traditionally used rectified linear activation unit (ReLU).</p>

<h3 id="differences-from-wavenet">Differences From WaveNet</h3>

<p>The WaveNet model originally quantizes 16-bit audio time samples into
256 bins, and the model is trained to produce a probability distribution
over these 256 possible values. In order to reduce the size of the model
and increase its inference speed, we replace the 256 channel discrete
output with a single continuous output. This is done by performing a
<span><span><span><math><semantics><mrow><mn>1</mn><mo>×</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">1 \times 1</annotation></semantics></math></span></span></span> convolution on the concatenation of each layer’s output.</p>

<h2 id="training">Training</h2>

<p>To train our network, we minimize error-to-signal ratio. This is similar
to Mean Squared Error (MSE), however the addition of the term in the
denominator normalizes the loss with respect to the amplitude of the
target signal:</p>

<p><span><span><span><span><math><semantics><mrow><msub><mi>L</mi><mtext>ESR</mtext></msub><mo>=</mo><mfrac><mrow><munder><mo>∑</mo><mi>t</mi></munder><mo stretchy="false">(</mo><mi>H</mi><mo stretchy="false">(</mo><msub><mi>y</mi><mi>t</mi></msub><mo stretchy="false">)</mo><mo>−</mo><mi>H</mi><mo stretchy="false">(</mo><msub><mover accent="true"><mi>y</mi><mo>^</mo></mover><mi>t</mi></msub><mo stretchy="false">)</mo><msup><mo stretchy="false">)</mo><mn>2</mn></msup></mrow><mrow><munder><mo>∑</mo><mi>t</mi></munder><mi>H</mi><mo stretchy="false">(</mo><msub><mi>y</mi><mi>t</mi></msub><msup><mo stretchy="false">)</mo><mn>2</mn></msup></mrow></mfrac></mrow><annotation encoding="application/x-tex">
L_\text{ESR} = \frac
{\sum_{t} (H(y_t) - H(\hat{y}_t))^2}
{\sum_{t} H(y_t)^2}
</annotation></semantics></math></span></span></span></span></p>

<p>where <span><span><span><math><semantics><mrow><mover accent="true"><mi>y</mi><mo>^</mo></mover></mrow><annotation encoding="application/x-tex">\hat{y}</annotation></semantics></math></span></span></span> is the predicted signal, and <span><span><span><math><semantics><mrow><mi>y</mi></mrow><annotation encoding="application/x-tex">y</annotation></semantics></math></span></span></span> is the original output
of the guitar pedal. <span><span><span><math><semantics><mrow><mi>H</mi><mo stretchy="false">(</mo><mo>⋅</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">H(\cdot)</annotation></semantics></math></span></span></span> is a pre-emphasis filter to emphasize
frequencies within the audible spectrum:</p>

<p><span><span><span><span><math><semantics><mrow><mi>H</mi><mo stretchy="false">(</mo><msub><mi>z</mi><mi>t</mi></msub><mo stretchy="false">)</mo><mo>=</mo><mn>1</mn><mo>−</mo><mn>0.95</mn><msub><mi>z</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub></mrow><annotation encoding="application/x-tex">
H(z_t) = 1 - 0.95 z_{t-1}
</annotation></semantics></math></span></span></span></span></p>

<p>When selecting the number of layers and channels for the model, we find
that a a stack of 24 layers, each with 16 channels, and a dilatation
pattern of:</p>

<p><span><span><span><span><math><semantics><mrow><mn>1</mn><mo separator="true">,</mo><mn>2</mn><mo separator="true">,</mo><mn>4</mn><mo separator="true">,</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mo separator="true">,</mo><mn>256</mn><mo separator="true">,</mo><mn>1</mn><mo separator="true">,</mo><mn>2</mn><mo separator="true">,</mo><mn>4</mn><mo separator="true">,</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mo separator="true">,</mo><mn>256</mn><mo separator="true">,</mo><mn>1</mn><mo separator="true">,</mo><mn>2</mn><mo separator="true">,</mo><mn>4</mn><mo separator="true">,</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mo separator="true">,</mo><mn>256</mn></mrow><annotation encoding="application/x-tex">
1, 2, 4,..., 256, 1, 2, 4,..., 256, 1, 2, 4, ..., 256
</annotation></semantics></math></span></span></span></span></p>

<p>was capable of replicating the sound well, while being small enough to
run in real time on a CPU. The model is then trained for 1500 epochs
using the Adam optimizer. This takes about 2 hours on a single Nvidia
2070 GPU.</p>

<h2 id="results">Results</h2>

<p>After training our network, we can listen to the models performance on
the held-out test set. See if you can differentiate between <strong>Output A</strong>
and <strong>Output B</strong> (you may need to wear headphones).</p>

<h4 id="input-dry-signal">Input (Dry Signal)</h4>



<h4 id="output-a">Output A</h4>



<h4 id="output-b">Output B</h4>



<details>
<summary>Reveal Outputs</summary>
<p>
<br> <b>Output A</b> is from the neural net; <b>Output B</b> is from the
real pedal.
</p>
</details>


<p>We find that the model is able to reproduce a sound nearly
indistinguishable from the real analog pedal. Best of all, the model is
small and efficient enough to be used in real time. Using this
technique, many analog effect pedals can likely be modeled with just a
few minutes of sample audio.</p>

<p>As always, thank you for reading! For any questions regarding this post
or others, feel free to reach out on twitter:
<a href="https://twitter.com/teddykoker">@teddykoker</a>.</p>



  </div>
</article>

      </div>
    </div></div></div>
    </div>
    <footer>
        <div>created by buildstarted &copy; 2020 <a href="/about">about</a></div>
        <div>Share this page on social media: copy and paste this url https://linksfor.dev/</div>
        <div>If you prefer RSS: <a href="https://linksfor.dev/feed.xml">https://linksfor.dev/feed.xml</a></div>
        <div>Customer satisfaction guaranteed to be optional.</div>
    </footer>
    
    <script async defer>
        _dna = window._dna || {};
        _dna.siteId = "linksfor.devs";
        _dna.outlink = true;

        (function() {
            let dna = document.createElement('script');
            dna.type = 'text/javascript';
            dna.async = true;
            dna.src = '//dna.buildstarted.com/t.js';
            let s = document.getElementsByTagName('script')[0];
            s.parentNode.insertBefore(dna, s);
        })();
    </script>
    <noscript><img src="//dna.buildstarted.com/g?siteId=linksfor.devs"/></noscript>
</body>
</html>