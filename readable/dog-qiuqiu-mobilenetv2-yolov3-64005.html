<!DOCTYPE html>
<html lang="en">
<head>
    <title>
dog-qiuqiu/MobileNetv2-YOLOV3 - linksfor.dev(s)    </title>
    <meta charset="utf-8">
    <link rel="alternate" type="application/rss+xml" title="Linksfor.dev(s) feed" href="https://linksfor.dev/feed.rss" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta name="google" value="notranslate">
    <link rel="stylesheet" type="text/css" href="/style.min.css" />
    <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon">
    <link rel="icon" href="/favicon.ico" type="image/x-icon">
    <meta property="og:title" content="dog-qiuqiu/MobileNetv2-YOLOV3 - linksfor.dev(s)"/>
    <meta property="og:description" content="MobileNetV2-YoloV3-Nano: 0.5BFlops 3MB HUAWEI P40: 6ms/img, YoloFace-500k:0.1Bflops500KB:fire::fire::fire: - dog-qiuqiu/MobileNetv2-YOLOV3"/>
    <meta property="og:type" content="website"/>
    <meta property="og:url" content="https://github.com/dog-qiuqiu/MobileNetv2-YOLOV3#500kb%E7%9A%84yolo-face-detection"/>

<meta property="og:site_name" content="linksfor.dev(s)" />
</head>
<body>
    <div class="devring" style="background: #222">
        <div style="text-align:center">Explore other dev related sites in this ring. If you would like to join this ring <a href="https://devring.club">click here</a>.</div>
        <div class="grid">
            <div style="display: grid; grid-template-columns: .5fr 1fr 1fr 1fr; text-align: center;">
                <span class="devring-title"><a href="https://devring.club/">devring.club</a></span>
                <a href="https://devring.club/sites/1/prev" class="devring-previous">Previous</a>
                <a href="https://devring.club/random" class="devring-random">Random</a>
                <a href="https://devring.club/sites/1/next" class="devring-next">Next</a>
            </div>
        </div>
    </div>
    <div class="grid">
        <h1 style="margin: unset">
<a href="/" style="color:inherit">linksfor.dev(s)</a>
        </h1>
        <title>linksfor.dev(s) - dog-qiuqiu/MobileNetv2-YOLOV3</title>
<div class="readable">
        <h1>dog-qiuqiu/MobileNetv2-YOLOV3</h1>
            <div>Reading time: 5-7 minutes</div>
        <div>Posted here: 08 Jul 2020</div>
        <p><a href="https://github.com/dog-qiuqiu/MobileNetv2-YOLOV3#500kb%E7%9A%84yolo-face-detection">https://github.com/dog-qiuqiu/MobileNetv2-YOLOV3#500kb%E7%9A%84yolo-face-detection</a></p>
        <hr/>
<div id="readability-page-1" class="page"><div id="readme">
    
        

      <div>
        <article itemprop="text"><h2>ÂæÖÂäû</h2>
<ul>
<li>ÂºÄÊîæandooidÁ§∫‰æãÈ°πÁõÆ(Ê±ÇÂ§ß‰Ω¨)</li>
</ul>
<h2>MobileNetv2-YOLOv3-SPP Darknet</h2>
<p>A darknet implementation of MobileNetv2-YOLOv3-SPP detection network</p>
<table>
<thead>
<tr>
<th>Network</th>
<th>COCO mAP(0.5)</th>
<th>Resolution</th>
<th>FLOPS</th>
<th>Weight size</th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="https://github.com/dog-qiuqiu/MobileNetv2-YOLOV3/tree/master/MobileNetV2-YOLOv3-SPP">MobileNetV2-YOLOv3-SPP</a></td>
<td>42.6</td>
<td>416</td>
<td>6.1BFlops</td>
<td>17.6MB</td>
</tr>
<tr>
<td><a href="https://github.com/AlexeyAB/darknet#pre-trained-models">YOLOv4-Tiny</a></td>
<td>40.2</td>
<td>416</td>
<td>6.9BFlops</td>
<td>23.1MB</td>
</tr>
</tbody>
</table>
<p>*emmmm...Ëøô‰∏™ÊáíÂæóËÆ≠ÁªÉÔºåmAPÂ∞±ÂáëÂêàËøôÊ†∑Âêß</p>
<h2><em><strong>Darknet Group convolution is not well supported on some GPUs such as NVIDIA PASCAL!!! The MobileNetV2-YOLOv3-SPP	inference time is 100ms at GTX1080ti, but RTX2080 inference time is 5ms!!!</strong></em></h2>
<ul>
<li><a href="https://github.com/AlexeyAB/darknet/issues/6091#issuecomment-651667469">https://github.com/AlexeyAB/darknet/issues/6091#issuecomment-651667469</a></li>
</ul>
<h2>MobileNetV2-YOLOv3-Lite&amp;Nano Darknet</h2>
<h4>Mobile inference frameworks benchmark (4*ARM_CPU)</h4>
<table>
<thead>
<tr>
<th>Network</th>
<th>VOC mAP(0.5)</th>
<th>COCO mAP(0.5)</th>
<th>Resolution</th>
<th>Inference time (NCNN/Kirin 990)</th>
<th>Inference time (MNN arm82/Kirin 990)</th>
<th>FLOPS</th>
<th>Weight size</th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="https://github.com/dog-qiuqiu/MobileNetv2-YOLOV3/tree/master/MobileNetV2-YOLOv3-Lite">MobileNetV2-YOLOv3-Lite</a></td>
<td>72.61</td>
<td>36.57</td>
<td>320</td>
<td>31.58 ms</td>
<td>18 ms</td>
<td>1.8BFlops</td>
<td>8.0MB</td>
</tr>
<tr>
<td><a href="https://github.com/dog-qiuqiu/MobileNetv2-YOLOV3/tree/master/MobileNetV2-YOLOv3-Nano">MobileNetV2-YOLOv3-Nano</a></td>
<td>65.27</td>
<td>30.13</td>
<td>320</td>
<td>13 ms</td>
<td>5 ms</td>
<td>0.5BFlops</td>
<td>3.0MB</td>
</tr>
<tr>
<td><a href="https://github.com/AlexeyAB/darknet#pre-trained-models">YOLOv3-Tiny-Prn</a></td>
<td>&amp;</td>
<td>33.1</td>
<td>416</td>
<td>36.6 ms</td>
<td>&amp; ms</td>
<td>3.5BFlops</td>
<td>18.8MB</td>
</tr>
<tr>
<td><a href="https://github.com/liux0614/yolo_nano">YOLO-Nano</a></td>
<td>69.1</td>
<td>&amp;</td>
<td>416</td>
<td>&amp; ms</td>
<td>&amp; ms</td>
<td>4.57BFlops</td>
<td>4.0MB</td>
</tr>
</tbody>
</table>
<ul>
<li>Support mobile inference frameworks such as NCNN&amp;MNN</li>
<li>The mnn benchmark only includes the forward inference time</li>
<li>The ncnn benchmark is the forward inference time + post-processing time(NMS...) of the convolution feature map.</li>
<li>Darknet Train Configuration: CUDA-version: 10010 (10020), cuDNN: 7.6.4,OpenCV version: 4 GPU:RTX2080ti</li>
</ul>
<h2>MobileNetV2-YOLOv3-Lite-COCO Test results</h2>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/dog-qiuqiu/MobileNetv2-YOLOV3/blob/master/data/predictions.jpg"><img src="https://github.com/dog-qiuqiu/MobileNetv2-YOLOV3/raw/master/data/predictions.jpg" alt="image"></a></p>
<h2>MobileNetV2-YOLO-Fastest</h2>
<table>
<thead>
<tr>
<th>Network</th>
<th>Resolution</th>
<th>VOC mAP(0.5)</th>
<th>Inference time (DarkNet/i7-6700)</th>
<th>Inference time (NCNN/Kirin 990)</th>
<th>Inference time (MNN arm82/Kirin 990)</th>
<th>FLOPS</th>
<th>Weight size</th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="https://github.com/dog-qiuqiu/MobileNetv2-YOLOV3/tree/master/MobileNetV2-YOLO-Fastest">MobileNetV2-YOLOv3-Fastest</a></td>
<td>320</td>
<td>46.55</td>
<td>26 ms</td>
<td>8.2 ms</td>
<td>2.4 ms</td>
<td>0.13BFlops</td>
<td>700KB</td>
</tr>
</tbody>
</table>
<ul>
<li>ÈÉΩ2.4ms‰∫ÜÔºåË¶ÅÂï•mAP<g-emoji alias="sunglasses" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f60e.png">üòé</g-emoji></li>
<li>Suitable for hardware with extremely tight computing resources</li>
<li>The mnn benchmark only includes the forward inference time</li>
<li>The ncnn benchmark is the forward inference time + post-processing time(NMS...) of the convolution feature map.</li>
<li>This model is recommended to do some simple single object detection suitable for simple application scenarios</li>
</ul>
<h2>MobileNetV2-YOLO-Fastest Test results</h2>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/dog-qiuqiu/MobileNetv2-YOLOV3/blob/master/data/Fastest.jpg"><img src="https://github.com/dog-qiuqiu/MobileNetv2-YOLOV3/raw/master/data/Fastest.jpg" alt="image"></a></p>
<h2>500kbÁöÑyolo-Face-Detection</h2>
<table>
<thead>
<tr>
<th>Network</th>
<th>Resolution</th>
<th>Inference time (NCNN/Kirin 990)</th>
<th>Inference time (MNN arm82/Kirin 990)</th>
<th>FLOPS</th>
<th>Weight size</th>
</tr>
</thead>
<tbody>
<tr>
<td>UltraFace-version-RFB</td>
<td>320x240</td>
<td>&amp;ms</td>
<td>3.36ms</td>
<td>0.1BFlops</td>
<td>1.3MB</td>
</tr>
<tr>
<td>UltraFace-version-Slim</td>
<td>320x240</td>
<td>&amp;ms</td>
<td>3.06ms</td>
<td>0.1BFlops</td>
<td>1.2MB</td>
</tr>
<tr>
<td><a href="https://github.com/dog-qiuqiu/MobileNetv2-YOLOV3/tree/master/yoloface-500k">yoloface-500k</a></td>
<td>320x256</td>
<td>5.5ms</td>
<td>2.4ms</td>
<td>0.1BFlops</td>
<td>0.5MB</td>
</tr>
</tbody>
</table>
<ul>
<li>ÈÉΩ500k‰∫ÜÔºåË¶ÅÂï•mAP<g-emoji alias="sunglasses" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f60e.png">üòé</g-emoji></li>
<li>Inference time (DarkNet/i7-6700):13ms</li>
<li>The mnn benchmark only includes the forward inference time</li>
<li>The ncnn benchmark is the forward inference time + post-processing time(NMS...) of the convolution feature map.</li>
</ul>
<h2>Wider Face Val</h2>
<table>
<thead>
<tr>
<th>Model</th>
<th>Easy Set</th>
<th>Medium Set</th>
<th>Hard Set</th>
</tr>
</thead>
<tbody>
<tr>
<td>libfacedetection v1ÔºàcaffeÔºâ</td>
<td>0.65</td>
<td>0.5</td>
<td>0.233</td>
</tr>
<tr>
<td>libfacedetection v2ÔºàcaffeÔºâ</td>
<td>0.714</td>
<td>0.585</td>
<td>0.306</td>
</tr>
<tr>
<td>Retinaface-Mobilenet-0.25 (Mxnet)</td>
<td>0.745</td>
<td>0.553</td>
<td>0.232</td>
</tr>
<tr>
<td>version-slim-320</td>
<td>0.77</td>
<td>0.671</td>
<td>0.395</td>
</tr>
<tr>
<td>version-RFB-320</td>
<td>0.787</td>
<td>0.698</td>
<td>0.438</td>
</tr>
<tr>
<td><a href="https://github.com/dog-qiuqiu/MobileNetv2-YOLOV3/tree/master/yoloface-500k">yoloface-500k-320</a></td>
<td><strong>0.728</strong></td>
<td><strong>0.682</strong></td>
<td><strong>0.431</strong></td>
</tr>
</tbody>
</table>
<h2>YoloFace-500k Test results</h2>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/dog-qiuqiu/MobileNetv2-YOLOV3/blob/master/data/p1.jpg"><img src="https://github.com/dog-qiuqiu/MobileNetv2-YOLOV3/raw/master/data/p1.jpg" alt="image"></a></p>
<h2>Reference&amp;Framework instructions&amp;How to Train</h2>
<ul>
<li><a href="https://github.com/AlexeyAB/darknet">https://github.com/AlexeyAB/darknet</a></li>
<li>You must use a pre-trained model to train your own data set. You can make a pre-trained model based on the weights of COCO training in this project to initialize the network parameters</li>
<li>‰∫§ÊµÅqqÁæ§:1062122604</li>
</ul>
<h2>About model selection</h2>
<ul>
<li>MobileNetV2-YOLOv3-SPP:  Nvidia Jeston, Intel Movidius, TensorRTÔºåNPUÔºåOPENVINO...High-performance embedded side</li>
<li>MobileNetV2-YOLOv3-Lite: High Performance ARM-CPUÔºåQualcomm Adreno GPUÔºå ARM82...High-performance mobile</li>
<li>MobileNetV2-YOLOv3-NANOÔºö ARM-CPU...Computing resources are limited</li>
<li>MobileNetV2-YOLOv3-FastestÔºö ....... Can you do personal face detection???It‚Äôs better than nothing</li>
</ul>
<h2>DarkNet2Caffe tutorial</h2>
<h3>Environmental requirements</h3>
<ul>
<li>Python2.7</li>
<li>python-opencv</li>
<li>Caffe(add upsample layer <a href="https://github.com/dog-qiuqiu/caffe">https://github.com/dog-qiuqiu/caffe</a>)</li>
<li>You have to compile cpu version of caffeÔºÅÔºÅÔºÅ
<pre><code>  cd darknet2caffe/
  python darknet2caffe.py MobileNetV2-YOLOv3-Nano-voc.cfg MobileNetV2-YOLOv3-Nano-voc.weights MobileNetV2-YOLOv3-Nano-voc.prototxt MobileNetV2-YOLOv3-Nano-voc.caffemodel
  cp MobileNetV2-YOLOv3-Nano-voc.prototxt sample
  cp MobileNetV2-YOLOv3-Nano-voc.caffemodel sample
  cd sample
  python detector.py
</code></pre>
</li>
</ul>
<h3>MNN conversion tutorial</h3>
<ul>
<li>Benchmark:<a href="https://www.yuque.com/mnn/cn/tool_benchmark" rel="nofollow">https://www.yuque.com/mnn/cn/tool_benchmark</a></li>
<li>Convert darknet model to caffemodel through darknet2caffe</li>
<li>Manually replace the upsample layer in prototxt with the interp layer</li>
<li>Take the modification of MobileNetV2-YOLOv3-Nano-voc.prototxt as an example</li>
</ul>
<pre><code>	#layer {
	#    bottom: "layer71-route"
	#    top: "layer72-upsample"
	#    name: "layer72-upsample"
	#    type: "Upsample"
	#    upsample_param {
	#        scale: 2
	#    }
	#}
	layer {
	    bottom: "layer71-route"
	    top: "layer72-upsample"
	    name: "layer72-upsample"
	    type: "Interp"
	    interp_param {
		height:20  #upsample h size
		width:20   #upsample w size
	    }
	}

</code></pre>
<ul>
<li>MNN conversion: <a href="https://www.yuque.com/mnn/cn/model_convert" rel="nofollow">https://www.yuque.com/mnn/cn/model_convert</a></li>
</ul>
<h2>NCNN conversion tutorial</h2>
<ul>
<li>Benchmark:<a href="https://github.com/Tencent/ncnn/tree/master/benchmark">https://github.com/Tencent/ncnn/tree/master/benchmark</a></li>
<li>NCNN supports direct conversion of darknet models</li>
<li>darknet2ncnn: <a href="https://github.com/Tencent/ncnn/tree/master/tools/darknet">https://github.com/Tencent/ncnn/tree/master/tools/darknet</a></li>
</ul>
<h2>NCNN Android Sample</h2>
<ul>
<li>ÁôΩÂ´ñ‰∏≠....</li>
</ul>
<h2>Thanks</h2>
<ul>
<li><a href="https://github.com/shicai/MobileNet-Caffe">https://github.com/shicai/MobileNet-Caffe</a></li>
<li><a href="https://github.com/AlexeyAB/darknet">https://github.com/AlexeyAB/darknet</a></li>
<li><a href="https://github.com/Tencent/ncnn">https://github.com/Tencent/ncnn</a></li>
<li><a href="https://gluon-cv.mxnet.io/" rel="nofollow">https://gluon-cv.mxnet.io/</a></li>
</ul>
</article>
      </div>
  </div></div></div>
    </div>
    <footer>
        <div>created by <a href="https://buildstarted.com">buildstarted</a> &copy; 2020 <a href="/about">about</a></div>
        <div>Share this page on social media: copy and paste this url https://linksfor.dev/</div>
        <div>If you prefer RSS: <a href="https://linksfor.dev/feed.xml">https://linksfor.dev/feed.xml</a></div>
        <div>Customer satisfaction guaranteed to be optional.</div>
    </footer>
    
    <script async defer>
        _dna = window._dna || {};
        _dna.siteId = "linksfor.devs";
        _dna.outlink = true;

        (function() {
            let dna = document.createElement('script');
            dna.type = 'text/javascript';
            dna.async = true;
            dna.src = '//dna.buildstarted.com/t.js';
            let s = document.getElementsByTagName('script')[0];
            s.parentNode.insertBefore(dna, s);
        })();
    </script>
    <noscript><img src="//dna.buildstarted.com/g?siteId=linksfor.devs"/></noscript>
</body>
</html>