<!DOCTYPE html>
<html lang="en">
<head>
    <title>
Exploration Strategies in Deep Reinforcement Learning - linksfor.dev(s)    </title>
    <meta charset="utf-8">
    <link rel="alternate" type="application/rss+xml" title="Linksfor.dev(s) feed" href="https://linksfor.dev/feed.rss" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta name="google" value="notranslate">
    <link rel="stylesheet" type="text/css" href="/style.css" />
    <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon">
    <link rel="icon" href="/favicon.ico" type="image/x-icon">
    <meta property="og:title" content="Exploration Strategies in Deep Reinforcement Learning - linksfor.dev(s)"/>
    <meta property="article:author" content="https://lilianweng.github.io/about/"/>
    <meta property="og:description" content="Exploitation versus exploration is a critical topic in reinforcement learning. This post introduces several common approaches for better exploration in Deep RL."/>
    <meta property="og:type" content="website"/>
    <meta property="og:url" content="https://lilianweng.github.io/lil-log/2020/06/07/exploration-strategies-in-deep-reinforcement-learning.html"/>

<meta property="og:site_name" content="linksfor.dev(s)" />
</head>
<body>
	<div class="devring" style="background: #222">
		<div class="grid">
			<div style="display: grid; grid-template-columns: .5fr 1fr 1fr 1fr; text-align: center;">
				<span class="devring-title"><a href="https://devring.club/">devring.club</a></span>
				<a href="https://devring.club/sites/1/prev" class="devring-previous">Previous</a>
				<a href="https://devring.club/random" class="devring-random">Random</a>
				<a href="https://devring.club/sites/1/next" class="devring-next">Next</a>
			</div>
		</div>
	</div>
    <div class="grid">
        <h1>
<a href="/" style="color:inherit">linksfor.dev(s)</a>
        </h1>
        <title>linksfor.dev(s) - Exploration Strategies in Deep Reinforcement Learning</title>
<div class="readable">
        <h1>Exploration Strategies in Deep Reinforcement Learning</h1>
            <div>by https://lilianweng.github.io/about/</div>
            <div>Reading time: 42-53 minutes</div>
        <div>Posted here: 10 Jun 2020</div>
        <p><a href="https://lilianweng.github.io/lil-log/2020/06/07/exploration-strategies-in-deep-reinforcement-learning.html">https://lilianweng.github.io/lil-log/2020/06/07/exploration-strategies-in-deep-reinforcement-learning.html</a></p>
        <hr/>
<div id="readability-page-1" class="page"><div itemprop="articleBody">
    <blockquote>
  <p>Exploitation versus exploration is a critical topic in reinforcement learning. This post introduces several common approaches for better exploration in Deep RL.</p>
</blockquote>

<!--more-->

<p><a href="https://lilianweng.github.io/lil-log/2018/01/23/the-multi-armed-bandit-problem-and-its-solutions.html">Exploitation versus exploration</a> is a critical topic in Reinforcement Learning. We’d like the RL agent to find the best solution as fast as possible. However, in the meantime, committing to solutions too quickly without enough exploration sounds pretty bad, as it could lead to local minima or total failure. Modern <a href="https://lilianweng.github.io/lil-log/2018/02/19/a-long-peek-into-reinforcement-learning.html">RL</a> <a href="https://lilianweng.github.io/lil-log/2018/04/08/policy-gradient-algorithms.html">algorithms</a> that optimize for the best returns can achieve good exploitation quite efficiently, while exploration remains more like an open topic.</p>

<p>I would like to discuss several common exploration strategies in Deep RL here. As this is a very big topic, my post by no means can cover all the important subtopics. I plan to update it periodically and keep further enriching the content gradually in time.</p>



<h2 id="classic-exploration-strategies">Classic Exploration Strategies</h2>

<p>As a quick recap, let’s first go through several classic exploration algorithms that work out pretty well in the multi-armed bandit problem or simple tabular RL.</p>
<ul>
  <li><strong>Epsilon-greedy</strong>: The agent does random exploration occasionally with probability <span><span id="MJXp-Span-1"><span id="MJXp-Span-2">ϵ</span></span></span><span id="MathJax-Element-1-Frame" tabindex="0"><nobr><span id="MathJax-Span-1"><span><span><span id="MathJax-Span-2"><span id="MathJax-Span-3">ϵ</span></span></span></span></span></nobr></span> and takes the optimal action most of the time with probability <span><span id="MJXp-Span-3"><span id="MJXp-Span-4">1</span><span id="MJXp-Span-5">−</span><span id="MJXp-Span-6">ϵ</span></span></span><span id="MathJax-Element-2-Frame" tabindex="0"><nobr><span id="MathJax-Span-4"><span><span><span id="MathJax-Span-5"><span id="MathJax-Span-6">1</span><span id="MathJax-Span-7">−</span><span id="MathJax-Span-8">ϵ</span></span></span></span></span></nobr></span>.</li>
  <li><strong>Upper confidence bounds</strong>: The agent selects the greediest action to maximize the upper confidence bound <span><span id="MJXp-Span-7"><span id="MJXp-Span-8"><span id="MJXp-Span-9"><span id="MJXp-Span-10"><span><span><span><span id="MJXp-Span-12">ˆ</span></span><span><span id="MJXp-Span-11">Q</span></span></span></span></span></span><span id="MJXp-Span-13">t</span></span><span id="MJXp-Span-14">(</span><span id="MJXp-Span-15">a</span><span id="MJXp-Span-16">)</span><span id="MJXp-Span-17">+</span><span id="MJXp-Span-18"><span id="MJXp-Span-19"><span id="MJXp-Span-20"><span><span><span><span id="MJXp-Span-22">ˆ</span></span><span><span id="MJXp-Span-21">U</span></span></span></span></span></span><span id="MJXp-Span-23">t</span></span><span id="MJXp-Span-24">(</span><span id="MJXp-Span-25">a</span><span id="MJXp-Span-26">)</span></span></span><span id="MathJax-Element-3-Frame" tabindex="0"><nobr><span id="MathJax-Span-9"><span><span><span id="MathJax-Span-10"><span id="MathJax-Span-11"><span><span><span id="MathJax-Span-12"><span id="MathJax-Span-13"><span id="MathJax-Span-14"><span><span><span id="MathJax-Span-15">Q</span><span></span></span><span><span id="MathJax-Span-16">^</span><span></span></span></span></span></span></span><span></span></span><span><span id="MathJax-Span-17">t</span><span></span></span></span></span><span id="MathJax-Span-18">(</span><span id="MathJax-Span-19">a</span><span id="MathJax-Span-20">)</span><span id="MathJax-Span-21">+</span><span id="MathJax-Span-22"><span><span><span id="MathJax-Span-23"><span id="MathJax-Span-24"><span id="MathJax-Span-25"><span><span><span id="MathJax-Span-26">U<span></span></span><span></span></span><span><span id="MathJax-Span-27">^</span><span></span></span></span></span></span></span><span></span></span><span><span id="MathJax-Span-28">t</span><span></span></span></span></span><span id="MathJax-Span-29">(</span><span id="MathJax-Span-30">a</span><span id="MathJax-Span-31">)</span></span></span></span></span></nobr></span>, where <span><span id="MJXp-Span-27"><span id="MJXp-Span-28"><span id="MJXp-Span-29"><span id="MJXp-Span-30"><span><span><span><span id="MJXp-Span-32">ˆ</span></span><span><span id="MJXp-Span-31">Q</span></span></span></span></span></span><span id="MJXp-Span-33">t</span></span><span id="MJXp-Span-34">(</span><span id="MJXp-Span-35">a</span><span id="MJXp-Span-36">)</span></span></span><span id="MathJax-Element-4-Frame" tabindex="0"><nobr><span id="MathJax-Span-32"><span><span><span id="MathJax-Span-33"><span id="MathJax-Span-34"><span><span><span id="MathJax-Span-35"><span id="MathJax-Span-36"><span id="MathJax-Span-37"><span><span><span id="MathJax-Span-38">Q</span><span></span></span><span><span id="MathJax-Span-39">^</span><span></span></span></span></span></span></span><span></span></span><span><span id="MathJax-Span-40">t</span><span></span></span></span></span><span id="MathJax-Span-41">(</span><span id="MathJax-Span-42">a</span><span id="MathJax-Span-43">)</span></span></span></span></span></nobr></span> is the average rewards associated with action <span><span id="MJXp-Span-37"><span id="MJXp-Span-38">a</span></span></span><span id="MathJax-Element-5-Frame" tabindex="0"><nobr><span id="MathJax-Span-44"><span><span><span id="MathJax-Span-45"><span id="MathJax-Span-46">a</span></span></span></span></span></nobr></span> up to time <span><span id="MJXp-Span-39"><span id="MJXp-Span-40">t</span></span></span><span id="MathJax-Element-6-Frame" tabindex="0"><nobr><span id="MathJax-Span-47"><span><span><span id="MathJax-Span-48"><span id="MathJax-Span-49">t</span></span></span></span></span></nobr></span> and <span><span id="MJXp-Span-41"><span id="MJXp-Span-42"><span id="MJXp-Span-43"><span id="MJXp-Span-44"><span><span><span><span id="MJXp-Span-46">ˆ</span></span><span><span id="MJXp-Span-45">U</span></span></span></span></span></span><span id="MJXp-Span-47">t</span></span><span id="MJXp-Span-48">(</span><span id="MJXp-Span-49">a</span><span id="MJXp-Span-50">)</span></span></span><span id="MathJax-Element-7-Frame" tabindex="0"><nobr><span id="MathJax-Span-50"><span><span><span id="MathJax-Span-51"><span id="MathJax-Span-52"><span><span><span id="MathJax-Span-53"><span id="MathJax-Span-54"><span id="MathJax-Span-55"><span><span><span id="MathJax-Span-56">U<span></span></span><span></span></span><span><span id="MathJax-Span-57">^</span><span></span></span></span></span></span></span><span></span></span><span><span id="MathJax-Span-58">t</span><span></span></span></span></span><span id="MathJax-Span-59">(</span><span id="MathJax-Span-60">a</span><span id="MathJax-Span-61">)</span></span></span></span></span></nobr></span> is a function reversely proportional to how many times action <span><span id="MJXp-Span-51"><span id="MJXp-Span-52">a</span></span></span><span id="MathJax-Element-8-Frame" tabindex="0"><nobr><span id="MathJax-Span-62"><span><span><span id="MathJax-Span-63"><span id="MathJax-Span-64">a</span></span></span></span></span></nobr></span> has been taken. See <a href="https://lilianweng.github.io/lil-log/2018/01/23/the-multi-armed-bandit-problem-and-its-solutions.html#upper-confidence-bounds">here</a> for more details.</li>
  <li><strong>Boltzmann exploration</strong>: The agent draws actions from a <a href="https://en.wikipedia.org/wiki/Boltzmann_distribution">boltzmann distribution</a> (softmax) over the learned Q values, regulated by a temperature parameter <span><span id="MJXp-Span-53"><span id="MJXp-Span-54">τ</span></span></span><span id="MathJax-Element-9-Frame" tabindex="0"><nobr><span id="MathJax-Span-65"><span><span><span id="MathJax-Span-66"><span id="MathJax-Span-67">τ<span></span></span></span></span></span></span></nobr></span>.</li>
  <li><strong>Thompson sampling</strong>: The agent keeps track of a belief over the probability of optimal actions and samples from this distribution. See <a href="https://lilianweng.github.io/lil-log/2018/01/23/the-multi-armed-bandit-problem-and-its-solutions.html#thompson-sampling">here</a> for more details.</li>
</ul>

<p>The following strategies could be used for better exploration in deep RL training when neural networks are used for function approximation:</p>
<ul>
  <li><strong>Entropy loss term</strong>: Add an entropy term <span><span id="MJXp-Span-55"><span id="MJXp-Span-56">H</span><span id="MJXp-Span-57">(</span><span id="MJXp-Span-58">π</span><span id="MJXp-Span-59">(</span><span id="MJXp-Span-60">a</span><span id="MJXp-Span-61">|</span><span id="MJXp-Span-62">s</span><span id="MJXp-Span-63">)</span><span id="MJXp-Span-64">)</span></span></span><span id="MathJax-Element-10-Frame" tabindex="0"><nobr><span id="MathJax-Span-68"><span><span><span id="MathJax-Span-69"><span id="MathJax-Span-70">H<span></span></span><span id="MathJax-Span-71">(</span><span id="MathJax-Span-72">π<span></span></span><span id="MathJax-Span-73">(</span><span id="MathJax-Span-74">a</span><span id="MathJax-Span-75">|</span><span id="MathJax-Span-76">s</span><span id="MathJax-Span-77">)</span><span id="MathJax-Span-78">)</span></span></span></span></span></nobr></span> into the loss function, encouraging the policy to take diverse actions.</li>
  <li><strong>Noise-based Exploration</strong>: Add noise into the observation, action or even parameter space (<a href="https://arxiv.org/abs/1706.10295">Fortunato, et al. 2017</a>, <a href="https://arxiv.org/abs/1706.01905">Plappert, et al. 2017</a>).</li>
</ul>

<h2 id="key-exploration-problems">Key Exploration Problems</h2>

<p>Good exploration becomes especially hard when the environment rarely provides rewards as feedback or the environment has distracting noise. Many exploration strategies are proposed to solve one or both of the following problems.</p>

<h3 id="the-hard-exploration-problem">The Hard-Exploration Problem</h3>

<p>The “hard-exploration” problem refers to exploration in an environment with very sparse or even deceptive reward. It is difficult because random exploration in such scenarios can rarely discover successful states or obtain meaningful feedback.</p>

<p><a href="https://en.wikipedia.org/wiki/Montezuma%27s_Revenge_(video_game)">Montezuma’s Revenge</a> is a concrete example for the hard-exploration problem. It remains as a few challenging games in Atari for DRL to solve. Many papers use Montezuma’s Revenge to benchmark their results.</p>

<h3 id="the-noisy-tv-problem">The Noisy-TV Problem</h3>

<p>The “Noisy-TV” problem started as a thought experiment in <a href="https://arxiv.org/abs/1810.12894">Burda, et al (2018)</a>. Imagine that an RL agent is rewarded with seeking novel experience, a TV with uncontrollable &amp; unpredictable random noise outputs would be able to attract the agent’s attention forever. The agent obtains new rewards from noisy TV consistently, but it fails to make any meaningful progress and becomes a “couch potato”.</p>

<p><img src="https://lilianweng.github.io/lil-log/assets/images/the-noisy-TV-problem.gif" alt="The noisy-TV problem"></p>

<p><em>Fig. 1. An agent is rewarded with novel experience in the experiment. If a maze has a noisy TC set up, the agent would be attracted and stop moving in the maze. (Image source: <a href="https://openai.com/blog/reinforcement-learning-with-prediction-based-rewards/">OpenAI Blog: “Reinforcement Learning with Prediction-Based Rewards”</a>)</em></p>

<h2 id="intrinsic-rewards-as-exploration-bonuses">Intrinsic Rewards as Exploration Bonuses</h2>

<p>One common approach to better exploration, especially for solving the <a href="#the-hard-exploration-problem">hard-exploration</a> problem, is to augment the environment reward with an additional bonus signal to encourage extra exploration. The policy is thus trained with a reward composed of two terms, <span><span id="MJXp-Span-65"><span id="MJXp-Span-66"><span id="MJXp-Span-67">r</span><span id="MJXp-Span-68">t</span></span><span id="MJXp-Span-69">=</span><span id="MJXp-Span-70"><span id="MJXp-Span-71">r</span><span><span><span><span><span id="MJXp-Span-73">e</span></span></span></span><span><span><span><span id="MJXp-Span-72">t</span></span></span></span></span></span><span id="MJXp-Span-74">+</span><span id="MJXp-Span-75">β</span><span id="MJXp-Span-76"><span id="MJXp-Span-77">r</span><span><span><span><span><span id="MJXp-Span-79">i</span></span></span></span><span><span><span><span id="MJXp-Span-78">t</span></span></span></span></span></span></span></span><span id="MathJax-Element-11-Frame" tabindex="0"><nobr><span id="MathJax-Span-79"><span><span><span id="MathJax-Span-80"><span id="MathJax-Span-81"><span><span><span id="MathJax-Span-82">r</span><span></span></span><span><span id="MathJax-Span-83">t</span><span></span></span></span></span><span id="MathJax-Span-84">=</span><span id="MathJax-Span-85"><span><span><span id="MathJax-Span-86">r</span><span></span></span><span><span id="MathJax-Span-87">e</span><span></span></span><span><span id="MathJax-Span-88">t</span><span></span></span></span></span><span id="MathJax-Span-89">+</span><span id="MathJax-Span-90">β<span></span></span><span id="MathJax-Span-91"><span><span><span id="MathJax-Span-92">r</span><span></span></span><span><span id="MathJax-Span-93">i</span><span></span></span><span><span id="MathJax-Span-94">t</span><span></span></span></span></span></span></span></span></span></nobr></span>, where <span><span id="MJXp-Span-80"><span id="MJXp-Span-81">β</span></span></span><span id="MathJax-Element-12-Frame" tabindex="0"><nobr><span id="MathJax-Span-95"><span><span><span id="MathJax-Span-96"><span id="MathJax-Span-97">β<span></span></span></span></span></span></span></nobr></span> is a hyperparameter adjusting the balance between exploitation and exploration.</p>
<ul>
  <li><span><span id="MJXp-Span-82"><span id="MJXp-Span-83"><span id="MJXp-Span-84">r</span><span><span><span><span><span id="MJXp-Span-86">e</span></span></span></span><span><span><span><span id="MJXp-Span-85">t</span></span></span></span></span></span></span></span><span id="MathJax-Element-13-Frame" tabindex="0"><nobr><span id="MathJax-Span-98"><span><span><span id="MathJax-Span-99"><span id="MathJax-Span-100"><span><span><span id="MathJax-Span-101">r</span><span></span></span><span><span id="MathJax-Span-102">e</span><span></span></span><span><span id="MathJax-Span-103">t</span><span></span></span></span></span></span></span></span></span></nobr></span> is an <em>extrinsic</em> reward from the environment at time <span><span id="MJXp-Span-87"><span id="MJXp-Span-88">t</span></span></span><span id="MathJax-Element-14-Frame" tabindex="0"><nobr><span id="MathJax-Span-104"><span><span><span id="MathJax-Span-105"><span id="MathJax-Span-106">t</span></span></span></span></span></nobr></span>, defined according to the task in hand.</li>
  <li><span><span id="MJXp-Span-89"><span id="MJXp-Span-90"><span id="MJXp-Span-91">r</span><span><span><span><span><span id="MJXp-Span-93">i</span></span></span></span><span><span><span><span id="MJXp-Span-92">t</span></span></span></span></span></span></span></span><span id="MathJax-Element-15-Frame" tabindex="0"><nobr><span id="MathJax-Span-107"><span><span><span id="MathJax-Span-108"><span id="MathJax-Span-109"><span><span><span id="MathJax-Span-110">r</span><span></span></span><span><span id="MathJax-Span-111">i</span><span></span></span><span><span id="MathJax-Span-112">t</span><span></span></span></span></span></span></span></span></span></nobr></span> is an <em>intrinsic</em> exploration bonus at time <span><span id="MJXp-Span-94"><span id="MJXp-Span-95">t</span></span></span><span id="MathJax-Element-16-Frame" tabindex="0"><nobr><span id="MathJax-Span-113"><span><span><span id="MathJax-Span-114"><span id="MathJax-Span-115">t</span></span></span></span></span></nobr></span>.</li>
</ul>

<p>This intrinsic reward is somewhat inspired by <em>intrinsic motivation</em> in psychology (<a href="https://www.researchgate.net/profile/Pierre-Yves_Oudeyer/publication/29614795_How_can_we_define_intrinsic_motivation/links/09e415107f1b4c8041000000/How-can-we-define-intrinsic-motivation.pdf">Oudeyer &amp; Kaplan, 2008</a>). Exploration driven by curiosity might be an important way for children to grow and learn. In other words, exploratory activities should be rewarding intrinsically in the human mind to encourage such behavior. The intrinsic rewards could be correlated with curiosity, surprise, familiarity of the state, and many other factors.</p>

<p>Same ideas can be applied to RL algorithms. In the following sections, methods of bonus-based exploration rewards are roughly grouped into two categories:</p>
<ol>
  <li>Discovery of novel states</li>
  <li>Improvement of the agent’s knowledge about the environment.</li>
</ol>

<h3 id="count-based-exploration">Count-based Exploration</h3>

<p>If we consider intrinsic rewards as rewarding conditions that surprise us, we need a way to measure whether a state is novel or appears often. One intuitive way is to count how many times a state has been encountered and to assign a bonus accordingly. The bonus guides the agent’s behavior to prefer rarely visited states to common states. This is known as the <strong>count-based exploration</strong> method.</p>

<p>Let <span><span id="MJXp-Span-96"><span id="MJXp-Span-97"><span id="MJXp-Span-98">N</span><span id="MJXp-Span-99">n</span></span><span id="MJXp-Span-100">(</span><span id="MJXp-Span-101">s</span><span id="MJXp-Span-102">)</span></span></span><span id="MathJax-Element-17-Frame" tabindex="0"><nobr><span id="MathJax-Span-116"><span><span><span id="MathJax-Span-117"><span id="MathJax-Span-118"><span><span><span id="MathJax-Span-119">N<span></span></span><span></span></span><span><span id="MathJax-Span-120">n</span><span></span></span></span></span><span id="MathJax-Span-121">(</span><span id="MathJax-Span-122">s</span><span id="MathJax-Span-123">)</span></span></span></span></span></nobr></span> be the <em>empirical count</em> function that tracks the real number of visits of a state <span><span id="MJXp-Span-103"><span id="MJXp-Span-104">s</span></span></span><span id="MathJax-Element-18-Frame" tabindex="0"><nobr><span id="MathJax-Span-124"><span><span><span id="MathJax-Span-125"><span id="MathJax-Span-126">s</span></span></span></span></span></nobr></span> in the sequence of <span><span id="MJXp-Span-105"><span id="MJXp-Span-106"><span id="MJXp-Span-107">s</span><span id="MJXp-Span-108"><span id="MJXp-Span-109">1</span><span id="MJXp-Span-110">:</span><span id="MJXp-Span-111">n</span></span></span></span></span><span id="MathJax-Element-19-Frame" tabindex="0"><nobr><span id="MathJax-Span-127"><span><span><span id="MathJax-Span-128"><span id="MathJax-Span-129"><span><span><span id="MathJax-Span-130">s</span><span></span></span><span><span id="MathJax-Span-131"><span id="MathJax-Span-132"><span id="MathJax-Span-133">1</span><span id="MathJax-Span-134">:</span><span id="MathJax-Span-135">n</span></span></span><span></span></span></span></span></span></span></span></span></nobr></span>. Unfortunately, using <span><span id="MJXp-Span-112"><span id="MJXp-Span-113"><span id="MJXp-Span-114">N</span><span id="MJXp-Span-115">n</span></span><span id="MJXp-Span-116">(</span><span id="MJXp-Span-117">s</span><span id="MJXp-Span-118">)</span></span></span><span id="MathJax-Element-20-Frame" tabindex="0"><nobr><span id="MathJax-Span-136"><span><span><span id="MathJax-Span-137"><span id="MathJax-Span-138"><span><span><span id="MathJax-Span-139">N<span></span></span><span></span></span><span><span id="MathJax-Span-140">n</span><span></span></span></span></span><span id="MathJax-Span-141">(</span><span id="MathJax-Span-142">s</span><span id="MathJax-Span-143">)</span></span></span></span></span></nobr></span> for exploration directly is not practical, because most of the states would have <span><span id="MJXp-Span-119"><span id="MJXp-Span-120"><span id="MJXp-Span-121">N</span><span id="MJXp-Span-122">n</span></span><span id="MJXp-Span-123">(</span><span id="MJXp-Span-124">s</span><span id="MJXp-Span-125">)</span><span id="MJXp-Span-126">=</span><span id="MJXp-Span-127">0</span></span></span><span id="MathJax-Element-21-Frame" tabindex="0"><nobr><span id="MathJax-Span-144"><span><span><span id="MathJax-Span-145"><span id="MathJax-Span-146"><span><span><span id="MathJax-Span-147">N<span></span></span><span></span></span><span><span id="MathJax-Span-148">n</span><span></span></span></span></span><span id="MathJax-Span-149">(</span><span id="MathJax-Span-150">s</span><span id="MathJax-Span-151">)</span><span id="MathJax-Span-152">=</span><span id="MathJax-Span-153">0</span></span></span></span></span></nobr></span>, especially considering that the state space is often continuous or high-dimensional. We need an non-zero count for most states, even when they haven’t been seen before.</p>

<h4 id="counting-by-density-model">Counting by Density Model</h4>

<p><a href="https://arxiv.org/abs/1606.01868">Bellemare, et al. (2016)</a> used a <strong>density model</strong> to approximate the frequency of state visits and a novel algorithm for deriving a <em>pseudo-count</em> from this density model. Let’s first define a conditional probability over the state space, <span><span id="MJXp-Span-128"><span id="MJXp-Span-129"><span id="MJXp-Span-130">ρ</span><span id="MJXp-Span-131">n</span></span><span id="MJXp-Span-132">(</span><span id="MJXp-Span-133">s</span><span id="MJXp-Span-134">)</span><span id="MJXp-Span-135">=</span><span id="MJXp-Span-136">ρ</span><span id="MJXp-Span-137">(</span><span id="MJXp-Span-138">s</span><span id="MJXp-Span-139">|</span><span id="MJXp-Span-140"><span id="MJXp-Span-141">s</span><span id="MJXp-Span-142"><span id="MJXp-Span-143">1</span><span id="MJXp-Span-144">:</span><span id="MJXp-Span-145">n</span></span></span><span id="MJXp-Span-146">)</span></span></span><span id="MathJax-Element-22-Frame" tabindex="0"><nobr><span id="MathJax-Span-154"><span><span><span id="MathJax-Span-155"><span id="MathJax-Span-156"><span><span><span id="MathJax-Span-157">ρ</span><span></span></span><span><span id="MathJax-Span-158">n</span><span></span></span></span></span><span id="MathJax-Span-159">(</span><span id="MathJax-Span-160">s</span><span id="MathJax-Span-161">)</span><span id="MathJax-Span-162">=</span><span id="MathJax-Span-163">ρ</span><span id="MathJax-Span-164">(</span><span id="MathJax-Span-165">s</span><span id="MathJax-Span-166">|</span><span id="MathJax-Span-167"><span><span><span id="MathJax-Span-168">s</span><span></span></span><span><span id="MathJax-Span-169"><span id="MathJax-Span-170"><span id="MathJax-Span-171">1</span><span id="MathJax-Span-172">:</span><span id="MathJax-Span-173">n</span></span></span><span></span></span></span></span><span id="MathJax-Span-174">)</span></span></span></span></span></nobr></span> as the probability of the <span><span id="MJXp-Span-147"><span id="MJXp-Span-148">(</span><span id="MJXp-Span-149">n</span><span id="MJXp-Span-150">+</span><span id="MJXp-Span-151">1</span><span id="MJXp-Span-152">)</span></span></span><span id="MathJax-Element-23-Frame" tabindex="0"><nobr><span id="MathJax-Span-175"><span><span><span id="MathJax-Span-176"><span id="MathJax-Span-177">(</span><span id="MathJax-Span-178">n</span><span id="MathJax-Span-179">+</span><span id="MathJax-Span-180">1</span><span id="MathJax-Span-181">)</span></span></span></span></span></nobr></span>-th state being <span><span id="MJXp-Span-153"><span id="MJXp-Span-154">s</span></span></span><span id="MathJax-Element-24-Frame" tabindex="0"><nobr><span id="MathJax-Span-182"><span><span><span id="MathJax-Span-183"><span id="MathJax-Span-184">s</span></span></span></span></span></nobr></span> given the first <span><span id="MJXp-Span-155"><span id="MJXp-Span-156">n</span></span></span><span id="MathJax-Element-25-Frame" tabindex="0"><nobr><span id="MathJax-Span-185"><span><span><span id="MathJax-Span-186"><span id="MathJax-Span-187">n</span></span></span></span></span></nobr></span> states are <span><span id="MJXp-Span-157"><span id="MJXp-Span-158"><span id="MJXp-Span-159">s</span><span id="MJXp-Span-160"><span id="MJXp-Span-161">1</span><span id="MJXp-Span-162">:</span><span id="MJXp-Span-163">n</span></span></span></span></span><span id="MathJax-Element-26-Frame" tabindex="0"><nobr><span id="MathJax-Span-188"><span><span><span id="MathJax-Span-189"><span id="MathJax-Span-190"><span><span><span id="MathJax-Span-191">s</span><span></span></span><span><span id="MathJax-Span-192"><span id="MathJax-Span-193"><span id="MathJax-Span-194">1</span><span id="MathJax-Span-195">:</span><span id="MathJax-Span-196">n</span></span></span><span></span></span></span></span></span></span></span></span></nobr></span>. To measure this empirically, we can simply use <span><span id="MJXp-Span-164"><span id="MJXp-Span-165"><span id="MJXp-Span-166">N</span><span id="MJXp-Span-167">n</span></span><span id="MJXp-Span-168">(</span><span id="MJXp-Span-169">s</span><span id="MJXp-Span-170">)</span><span id="MJXp-Span-171"><span id="MJXp-Span-172">/</span></span><span id="MJXp-Span-173">n</span></span></span><span id="MathJax-Element-27-Frame" tabindex="0"><nobr><span id="MathJax-Span-197"><span><span><span id="MathJax-Span-198"><span id="MathJax-Span-199"><span><span><span id="MathJax-Span-200">N<span></span></span><span></span></span><span><span id="MathJax-Span-201">n</span><span></span></span></span></span><span id="MathJax-Span-202">(</span><span id="MathJax-Span-203">s</span><span id="MathJax-Span-204">)</span><span id="MathJax-Span-205"><span id="MathJax-Span-206"><span id="MathJax-Span-207">/</span></span></span><span id="MathJax-Span-208">n</span></span></span></span></span></nobr></span>.</p>

<p>Let’s also define a <em>recoding probability</em> of a state <span><span id="MJXp-Span-174"><span id="MJXp-Span-175">s</span></span></span><span id="MathJax-Element-28-Frame" tabindex="0"><nobr><span id="MathJax-Span-209"><span><span><span id="MathJax-Span-210"><span id="MathJax-Span-211">s</span></span></span></span></span></nobr></span> as the probability assigned by the density model to <span><span id="MJXp-Span-176"><span id="MJXp-Span-177">s</span></span></span><span id="MathJax-Element-29-Frame" tabindex="0"><nobr><span id="MathJax-Span-212"><span><span><span id="MathJax-Span-213"><span id="MathJax-Span-214">s</span></span></span></span></span></nobr></span> <em>after observing a new occurrence of</em> <span><span id="MJXp-Span-178"><span id="MJXp-Span-179">s</span></span></span><span id="MathJax-Element-30-Frame" tabindex="0"><nobr><span id="MathJax-Span-215"><span><span><span id="MathJax-Span-216"><span id="MathJax-Span-217">s</span></span></span></span></span></nobr></span>, <span><span id="MJXp-Span-180"><span id="MJXp-Span-181"><span id="MJXp-Span-182">ρ</span><span><span><span><span><span id="MJXp-Span-184">′</span></span></span></span><span><span><span><span id="MJXp-Span-183">n</span></span></span></span></span></span><span id="MJXp-Span-185">(</span><span id="MJXp-Span-186">s</span><span id="MJXp-Span-187">)</span><span id="MJXp-Span-188">=</span><span id="MJXp-Span-189">ρ</span><span id="MJXp-Span-190">(</span><span id="MJXp-Span-191">s</span><span id="MJXp-Span-192">|</span><span id="MJXp-Span-193"><span id="MJXp-Span-194">s</span><span id="MJXp-Span-195"><span id="MJXp-Span-196">1</span><span id="MJXp-Span-197">:</span><span id="MJXp-Span-198">n</span></span></span><span id="MJXp-Span-199">s</span><span id="MJXp-Span-200">)</span></span></span><span id="MathJax-Element-31-Frame" tabindex="0"><nobr><span id="MathJax-Span-218"><span><span><span id="MathJax-Span-219"><span id="MathJax-Span-220"><span><span><span id="MathJax-Span-221">ρ</span><span></span></span><span><span id="MathJax-Span-222">′</span><span></span></span><span><span id="MathJax-Span-223">n</span><span></span></span></span></span><span id="MathJax-Span-224">(</span><span id="MathJax-Span-225">s</span><span id="MathJax-Span-226">)</span><span id="MathJax-Span-227">=</span><span id="MathJax-Span-228">ρ</span><span id="MathJax-Span-229">(</span><span id="MathJax-Span-230">s</span><span id="MathJax-Span-231">|</span><span id="MathJax-Span-232"><span><span><span id="MathJax-Span-233">s</span><span></span></span><span><span id="MathJax-Span-234"><span id="MathJax-Span-235"><span id="MathJax-Span-236">1</span><span id="MathJax-Span-237">:</span><span id="MathJax-Span-238">n</span></span></span><span></span></span></span></span><span id="MathJax-Span-239">s</span><span id="MathJax-Span-240">)</span></span></span></span></span></nobr></span>.</p>

<p>The paper introduced two concepts to better regulate the density model, a <em>pseudo-count</em> function <span><span id="MJXp-Span-201"><span id="MJXp-Span-202"><span id="MJXp-Span-203"><span id="MJXp-Span-204"><span><span><span><span id="MJXp-Span-206">ˆ</span></span><span><span id="MJXp-Span-205">N</span></span></span></span></span></span><span id="MJXp-Span-207">n</span></span><span id="MJXp-Span-208">(</span><span id="MJXp-Span-209">s</span><span id="MJXp-Span-210">)</span></span></span><span id="MathJax-Element-32-Frame" tabindex="0"><nobr><span id="MathJax-Span-241"><span><span><span id="MathJax-Span-242"><span id="MathJax-Span-243"><span><span><span id="MathJax-Span-244"><span id="MathJax-Span-245"><span id="MathJax-Span-246"><span><span><span id="MathJax-Span-247">N<span></span></span><span></span></span><span><span id="MathJax-Span-248">^</span><span></span></span></span></span></span></span><span></span></span><span><span id="MathJax-Span-249">n</span><span></span></span></span></span><span id="MathJax-Span-250">(</span><span id="MathJax-Span-251">s</span><span id="MathJax-Span-252">)</span></span></span></span></span></nobr></span> and a <em>pseudo-count total</em> <span><span id="MJXp-Span-211"><span id="MJXp-Span-212"><span id="MJXp-Span-213"><span><span><span><span id="MJXp-Span-215">ˆ</span></span><span><span id="MJXp-Span-214">n</span></span></span></span></span></span></span></span><span id="MathJax-Element-33-Frame" tabindex="0"><nobr><span id="MathJax-Span-253"><span><span><span id="MathJax-Span-254"><span id="MathJax-Span-255"><span id="MathJax-Span-256"><span id="MathJax-Span-257"><span><span><span id="MathJax-Span-258">n</span><span></span></span><span><span id="MathJax-Span-259">^</span><span></span></span></span></span></span></span></span></span></span></span></nobr></span>. As they are designed to imitate an empirical count function, we would have:</p>

<p><span><span id="MJXp-Span-216"><span id="MJXp-Span-217"><span id="MJXp-Span-218">ρ</span><span id="MJXp-Span-219">n</span></span><span id="MJXp-Span-220">(</span><span id="MJXp-Span-221">s</span><span id="MJXp-Span-222">)</span><span id="MJXp-Span-223">=</span><span id="MJXp-Span-224"><span><span id="MJXp-Span-225"><span id="MJXp-Span-226"><span id="MJXp-Span-227"><span><span><span><span id="MJXp-Span-229">ˆ</span></span><span><span id="MJXp-Span-228">N</span></span></span></span></span></span><span id="MJXp-Span-230">n</span></span><span id="MJXp-Span-231">(</span><span id="MJXp-Span-232">s</span><span id="MJXp-Span-233">)</span></span><span><span><span><span></span></span><span><span><span id="MJXp-Span-234"><span id="MJXp-Span-235"><span><span><span><span id="MJXp-Span-237">ˆ</span></span><span><span id="MJXp-Span-236">n</span></span></span></span></span></span></span></span></span></span></span><span id="MJXp-Span-238">≤</span><span id="MJXp-Span-239"><span id="MJXp-Span-240">ρ</span><span><span><span><span><span id="MJXp-Span-242">′</span></span></span></span><span><span><span><span id="MJXp-Span-241">n</span></span></span></span></span></span><span id="MJXp-Span-243">(</span><span id="MJXp-Span-244">s</span><span id="MJXp-Span-245">)</span><span id="MJXp-Span-246">=</span><span id="MJXp-Span-247"><span><span id="MJXp-Span-248"><span id="MJXp-Span-249"><span id="MJXp-Span-250"><span><span><span><span id="MJXp-Span-252">ˆ</span></span><span><span id="MJXp-Span-251">N</span></span></span></span></span></span><span id="MJXp-Span-253">n</span></span><span id="MJXp-Span-254">(</span><span id="MJXp-Span-255">s</span><span id="MJXp-Span-256">)</span><span id="MJXp-Span-257">+</span><span id="MJXp-Span-258">1</span></span><span><span><span><span></span></span><span><span><span id="MJXp-Span-259"><span id="MJXp-Span-260"><span><span><span><span id="MJXp-Span-262">ˆ</span></span><span><span id="MJXp-Span-261">n</span></span></span></span></span></span><span id="MJXp-Span-263">+</span><span id="MJXp-Span-264">1</span></span></span></span></span></span></span></span></p><p><span id="MathJax-Element-34-Frame" tabindex="0"><nobr><span id="MathJax-Span-260"><span><span><span id="MathJax-Span-261"><span id="MathJax-Span-262"><span><span><span id="MathJax-Span-263">ρ</span><span></span></span><span><span id="MathJax-Span-264">n</span><span></span></span></span></span><span id="MathJax-Span-265">(</span><span id="MathJax-Span-266">s</span><span id="MathJax-Span-267">)</span><span id="MathJax-Span-268">=</span><span id="MathJax-Span-269"><span><span><span id="MathJax-Span-270"><span id="MathJax-Span-271"><span><span><span id="MathJax-Span-272"><span id="MathJax-Span-273"><span id="MathJax-Span-274"><span><span><span id="MathJax-Span-275">N<span></span></span><span></span></span><span><span id="MathJax-Span-276">^</span><span></span></span></span></span></span></span><span></span></span><span><span id="MathJax-Span-277">n</span><span></span></span></span></span><span id="MathJax-Span-278">(</span><span id="MathJax-Span-279">s</span><span id="MathJax-Span-280">)</span></span><span></span></span><span><span id="MathJax-Span-281"><span id="MathJax-Span-282"><span id="MathJax-Span-283"><span><span><span id="MathJax-Span-284">n</span><span></span></span><span><span id="MathJax-Span-285">^</span><span></span></span></span></span></span></span><span></span></span><span><span></span><span></span></span></span></span><span id="MathJax-Span-286">≤</span><span id="MathJax-Span-287"><span><span><span id="MathJax-Span-288">ρ</span><span></span></span><span><span id="MathJax-Span-289">′</span><span></span></span><span><span id="MathJax-Span-290">n</span><span></span></span></span></span><span id="MathJax-Span-291">(</span><span id="MathJax-Span-292">s</span><span id="MathJax-Span-293">)</span><span id="MathJax-Span-294">=</span><span id="MathJax-Span-295"><span><span><span id="MathJax-Span-296"><span id="MathJax-Span-297"><span><span><span id="MathJax-Span-298"><span id="MathJax-Span-299"><span id="MathJax-Span-300"><span><span><span id="MathJax-Span-301">N<span></span></span><span></span></span><span><span id="MathJax-Span-302">^</span><span></span></span></span></span></span></span><span></span></span><span><span id="MathJax-Span-303">n</span><span></span></span></span></span><span id="MathJax-Span-304">(</span><span id="MathJax-Span-305">s</span><span id="MathJax-Span-306">)</span><span id="MathJax-Span-307">+</span><span id="MathJax-Span-308">1</span></span><span></span></span><span><span id="MathJax-Span-309"><span id="MathJax-Span-310"><span id="MathJax-Span-311"><span id="MathJax-Span-312"><span><span><span id="MathJax-Span-313">n</span><span></span></span><span><span id="MathJax-Span-314">^</span><span></span></span></span></span></span></span><span id="MathJax-Span-315">+</span><span id="MathJax-Span-316">1</span></span><span></span></span><span><span></span><span></span></span></span></span></span></span></span></span></nobr></span></p>

<p>The relationship between <span><span id="MJXp-Span-265"><span id="MJXp-Span-266"><span id="MJXp-Span-267">ρ</span><span id="MJXp-Span-268">n</span></span><span id="MJXp-Span-269">(</span><span id="MJXp-Span-270">x</span><span id="MJXp-Span-271">)</span></span></span><span id="MathJax-Element-35-Frame" tabindex="0"><nobr><span id="MathJax-Span-317"><span><span><span id="MathJax-Span-318"><span id="MathJax-Span-319"><span><span><span id="MathJax-Span-320">ρ</span><span></span></span><span><span id="MathJax-Span-321">n</span><span></span></span></span></span><span id="MathJax-Span-322">(</span><span id="MathJax-Span-323">x</span><span id="MathJax-Span-324">)</span></span></span></span></span></nobr></span> and <span><span id="MJXp-Span-272"><span id="MJXp-Span-273"><span id="MJXp-Span-274">ρ</span><span><span><span><span><span id="MJXp-Span-276">′</span></span></span></span><span><span><span><span id="MJXp-Span-275">n</span></span></span></span></span></span><span id="MJXp-Span-277">(</span><span id="MJXp-Span-278">x</span><span id="MJXp-Span-279">)</span></span></span><span id="MathJax-Element-36-Frame" tabindex="0"><nobr><span id="MathJax-Span-325"><span><span><span id="MathJax-Span-326"><span id="MathJax-Span-327"><span><span><span id="MathJax-Span-328">ρ</span><span></span></span><span><span id="MathJax-Span-329">′</span><span></span></span><span><span id="MathJax-Span-330">n</span><span></span></span></span></span><span id="MathJax-Span-331">(</span><span id="MathJax-Span-332">x</span><span id="MathJax-Span-333">)</span></span></span></span></span></nobr></span> requires the density model to be <em>learning-positive</em>:  for all <span><span id="MJXp-Span-280"><span id="MJXp-Span-281"><span id="MJXp-Span-282">s</span><span id="MJXp-Span-283"><span id="MJXp-Span-284">1</span><span id="MJXp-Span-285">:</span><span id="MJXp-Span-286">n</span></span></span><span id="MJXp-Span-287">∈</span><span id="MJXp-Span-288"><span id="MJXp-Span-289"><span id="MJXp-Span-290">S</span></span><span id="MJXp-Span-291">n</span></span></span></span><span id="MathJax-Element-37-Frame" tabindex="0"><nobr><span id="MathJax-Span-334"><span><span><span id="MathJax-Span-335"><span id="MathJax-Span-336"><span><span><span id="MathJax-Span-337">s</span><span></span></span><span><span id="MathJax-Span-338"><span id="MathJax-Span-339"><span id="MathJax-Span-340">1</span><span id="MathJax-Span-341">:</span><span id="MathJax-Span-342">n</span></span></span><span></span></span></span></span><span id="MathJax-Span-343">∈</span><span id="MathJax-Span-344"><span><span><span id="MathJax-Span-345"><span id="MathJax-Span-346"><span id="MathJax-Span-347">S<span></span></span></span></span><span></span></span><span><span id="MathJax-Span-348">n</span><span></span></span></span></span></span></span></span></span></nobr></span> and all <span><span id="MJXp-Span-292"><span id="MJXp-Span-293">s</span><span id="MJXp-Span-294">∈</span><span id="MJXp-Span-295"><span id="MJXp-Span-296">S</span></span></span></span><span id="MathJax-Element-38-Frame" tabindex="0"><nobr><span id="MathJax-Span-349"><span><span><span id="MathJax-Span-350"><span id="MathJax-Span-351">s</span><span id="MathJax-Span-352">∈</span><span id="MathJax-Span-353"><span id="MathJax-Span-354"><span id="MathJax-Span-355">S<span></span></span></span></span></span></span></span></span></nobr></span>, <span><span id="MJXp-Span-297"><span id="MJXp-Span-298"><span id="MJXp-Span-299">ρ</span><span id="MJXp-Span-300">n</span></span><span id="MJXp-Span-301">(</span><span id="MJXp-Span-302">s</span><span id="MJXp-Span-303">)</span><span id="MJXp-Span-304">≤</span><span id="MJXp-Span-305"><span id="MJXp-Span-306">ρ</span><span><span><span><span><span id="MJXp-Span-308">′</span></span></span></span><span><span><span><span id="MJXp-Span-307">n</span></span></span></span></span></span><span id="MJXp-Span-309">(</span><span id="MJXp-Span-310">s</span><span id="MJXp-Span-311">)</span></span></span><span id="MathJax-Element-39-Frame" tabindex="0"><nobr><span id="MathJax-Span-356"><span><span><span id="MathJax-Span-357"><span id="MathJax-Span-358"><span><span><span id="MathJax-Span-359">ρ</span><span></span></span><span><span id="MathJax-Span-360">n</span><span></span></span></span></span><span id="MathJax-Span-361">(</span><span id="MathJax-Span-362">s</span><span id="MathJax-Span-363">)</span><span id="MathJax-Span-364">≤</span><span id="MathJax-Span-365"><span><span><span id="MathJax-Span-366">ρ</span><span></span></span><span><span id="MathJax-Span-367">′</span><span></span></span><span><span id="MathJax-Span-368">n</span><span></span></span></span></span><span id="MathJax-Span-369">(</span><span id="MathJax-Span-370">s</span><span id="MathJax-Span-371">)</span></span></span></span></span></nobr></span>. In other words, After observing one instance of <span><span id="MJXp-Span-312"><span id="MJXp-Span-313">s</span></span></span><span id="MathJax-Element-40-Frame" tabindex="0"><nobr><span id="MathJax-Span-372"><span><span><span id="MathJax-Span-373"><span id="MathJax-Span-374">s</span></span></span></span></span></nobr></span>, the density model’s prediction of that same <span><span id="MJXp-Span-314"><span id="MJXp-Span-315">s</span></span></span><span id="MathJax-Element-41-Frame" tabindex="0"><nobr><span id="MathJax-Span-375"><span><span><span id="MathJax-Span-376"><span id="MathJax-Span-377">s</span></span></span></span></span></nobr></span> should increase. Apart from being learning-positive, the density model should be trained completely <em>online</em> with non-randomized mini-batches of experienced states, so naturally we have <span><span id="MJXp-Span-316"><span id="MJXp-Span-317"><span id="MJXp-Span-318">ρ</span><span><span><span><span><span id="MJXp-Span-320">′</span></span></span></span><span><span><span><span id="MJXp-Span-319">n</span></span></span></span></span></span><span id="MJXp-Span-321">=</span><span id="MJXp-Span-322"><span id="MJXp-Span-323">ρ</span><span id="MJXp-Span-324"><span id="MJXp-Span-325">n</span><span id="MJXp-Span-326">+</span><span id="MJXp-Span-327">1</span></span></span></span></span><span id="MathJax-Element-42-Frame" tabindex="0"><nobr><span id="MathJax-Span-378"><span><span><span id="MathJax-Span-379"><span id="MathJax-Span-380"><span><span><span id="MathJax-Span-381">ρ</span><span></span></span><span><span id="MathJax-Span-382">′</span><span></span></span><span><span id="MathJax-Span-383">n</span><span></span></span></span></span><span id="MathJax-Span-384">=</span><span id="MathJax-Span-385"><span><span><span id="MathJax-Span-386">ρ</span><span></span></span><span><span id="MathJax-Span-387"><span id="MathJax-Span-388"><span id="MathJax-Span-389">n</span><span id="MathJax-Span-390">+</span><span id="MathJax-Span-391">1</span></span></span><span></span></span></span></span></span></span></span></span></nobr></span>.</p>

<p>The pseudo-count can be computed from <span><span id="MJXp-Span-328"><span id="MJXp-Span-329"><span id="MJXp-Span-330">ρ</span><span id="MJXp-Span-331">n</span></span><span id="MJXp-Span-332">(</span><span id="MJXp-Span-333">s</span><span id="MJXp-Span-334">)</span></span></span><span id="MathJax-Element-43-Frame" tabindex="0"><nobr><span id="MathJax-Span-392"><span><span><span id="MathJax-Span-393"><span id="MathJax-Span-394"><span><span><span id="MathJax-Span-395">ρ</span><span></span></span><span><span id="MathJax-Span-396">n</span><span></span></span></span></span><span id="MathJax-Span-397">(</span><span id="MathJax-Span-398">s</span><span id="MathJax-Span-399">)</span></span></span></span></span></nobr></span> and <span><span id="MJXp-Span-335"><span id="MJXp-Span-336"><span id="MJXp-Span-337">ρ</span><span><span><span><span><span id="MJXp-Span-339">′</span></span></span></span><span><span><span><span id="MJXp-Span-338">n</span></span></span></span></span></span><span id="MJXp-Span-340">(</span><span id="MJXp-Span-341">s</span><span id="MJXp-Span-342">)</span></span></span><span id="MathJax-Element-44-Frame" tabindex="0"><nobr><span id="MathJax-Span-400"><span><span><span id="MathJax-Span-401"><span id="MathJax-Span-402"><span><span><span id="MathJax-Span-403">ρ</span><span></span></span><span><span id="MathJax-Span-404">′</span><span></span></span><span><span id="MathJax-Span-405">n</span><span></span></span></span></span><span id="MathJax-Span-406">(</span><span id="MathJax-Span-407">s</span><span id="MathJax-Span-408">)</span></span></span></span></span></nobr></span> after solving the above linear system:</p>

<p><span><span id="MJXp-Span-343"><span id="MJXp-Span-344"><span id="MJXp-Span-345"><span id="MJXp-Span-346"><span><span><span><span id="MJXp-Span-348">ˆ</span></span><span><span id="MJXp-Span-347">N</span></span></span></span></span></span><span id="MJXp-Span-349">n</span></span><span id="MJXp-Span-350">(</span><span id="MJXp-Span-351">s</span><span id="MJXp-Span-352">)</span><span id="MJXp-Span-353">=</span><span id="MJXp-Span-354"><span id="MJXp-Span-355"><span><span><span><span id="MJXp-Span-357">ˆ</span></span><span><span id="MJXp-Span-356">n</span></span></span></span></span></span><span id="MJXp-Span-358"><span id="MJXp-Span-359">ρ</span><span id="MJXp-Span-360">n</span></span><span id="MJXp-Span-361">(</span><span id="MJXp-Span-362">s</span><span id="MJXp-Span-363">)</span><span id="MJXp-Span-364">=</span><span id="MJXp-Span-365"><span><span id="MJXp-Span-366"><span id="MJXp-Span-367">ρ</span><span id="MJXp-Span-368">n</span></span><span id="MJXp-Span-369">(</span><span id="MJXp-Span-370">s</span><span id="MJXp-Span-371">)</span><span id="MJXp-Span-372">(</span><span id="MJXp-Span-373">1</span><span id="MJXp-Span-374">−</span><span id="MJXp-Span-375"><span id="MJXp-Span-376">ρ</span><span><span><span><span><span id="MJXp-Span-378">′</span></span></span></span><span><span><span><span id="MJXp-Span-377">n</span></span></span></span></span></span><span id="MJXp-Span-379">(</span><span id="MJXp-Span-380">s</span><span id="MJXp-Span-381">)</span><span id="MJXp-Span-382">)</span></span><span><span><span><span></span></span><span><span><span id="MJXp-Span-383"><span id="MJXp-Span-384">ρ</span><span><span><span><span><span id="MJXp-Span-386">′</span></span></span></span><span><span><span><span id="MJXp-Span-385">n</span></span></span></span></span></span><span id="MJXp-Span-387">(</span><span id="MJXp-Span-388">s</span><span id="MJXp-Span-389">)</span><span id="MJXp-Span-390">−</span><span id="MJXp-Span-391"><span id="MJXp-Span-392">ρ</span><span id="MJXp-Span-393">n</span></span><span id="MJXp-Span-394">(</span><span id="MJXp-Span-395">s</span><span id="MJXp-Span-396">)</span></span></span></span></span></span></span></span></p><p><span id="MathJax-Element-45-Frame" tabindex="0"><nobr><span id="MathJax-Span-409"><span><span><span id="MathJax-Span-410"><span id="MathJax-Span-411"><span><span><span id="MathJax-Span-412"><span id="MathJax-Span-413"><span id="MathJax-Span-414"><span><span><span id="MathJax-Span-415">N<span></span></span><span></span></span><span><span id="MathJax-Span-416">^</span><span></span></span></span></span></span></span><span></span></span><span><span id="MathJax-Span-417">n</span><span></span></span></span></span><span id="MathJax-Span-418">(</span><span id="MathJax-Span-419">s</span><span id="MathJax-Span-420">)</span><span id="MathJax-Span-421">=</span><span id="MathJax-Span-422"><span id="MathJax-Span-423"><span id="MathJax-Span-424"><span><span><span id="MathJax-Span-425">n</span><span></span></span><span><span id="MathJax-Span-426">^</span><span></span></span></span></span></span></span><span id="MathJax-Span-427"><span><span><span id="MathJax-Span-428">ρ</span><span></span></span><span><span id="MathJax-Span-429">n</span><span></span></span></span></span><span id="MathJax-Span-430">(</span><span id="MathJax-Span-431">s</span><span id="MathJax-Span-432">)</span><span id="MathJax-Span-433">=</span><span id="MathJax-Span-434"><span><span><span id="MathJax-Span-435"><span id="MathJax-Span-436"><span><span><span id="MathJax-Span-437">ρ</span><span></span></span><span><span id="MathJax-Span-438">n</span><span></span></span></span></span><span id="MathJax-Span-439">(</span><span id="MathJax-Span-440">s</span><span id="MathJax-Span-441">)</span><span id="MathJax-Span-442">(</span><span id="MathJax-Span-443">1</span><span id="MathJax-Span-444">−</span><span id="MathJax-Span-445"><span><span><span id="MathJax-Span-446">ρ</span><span></span></span><span><span id="MathJax-Span-447">′</span><span></span></span><span><span id="MathJax-Span-448">n</span><span></span></span></span></span><span id="MathJax-Span-449">(</span><span id="MathJax-Span-450">s</span><span id="MathJax-Span-451">)</span><span id="MathJax-Span-452">)</span></span><span></span></span><span><span id="MathJax-Span-453"><span id="MathJax-Span-454"><span><span><span id="MathJax-Span-455">ρ</span><span></span></span><span><span id="MathJax-Span-456">′</span><span></span></span><span><span id="MathJax-Span-457">n</span><span></span></span></span></span><span id="MathJax-Span-458">(</span><span id="MathJax-Span-459">s</span><span id="MathJax-Span-460">)</span><span id="MathJax-Span-461">−</span><span id="MathJax-Span-462"><span><span><span id="MathJax-Span-463">ρ</span><span></span></span><span><span id="MathJax-Span-464">n</span><span></span></span></span></span><span id="MathJax-Span-465">(</span><span id="MathJax-Span-466">s</span><span id="MathJax-Span-467">)</span></span><span></span></span><span><span></span><span></span></span></span></span></span></span></span></span></nobr></span></p>

<p>Or estimated by the <em>prediction gain (PG)</em>:</p>

<p><span><span id="MJXp-Span-397"><span id="MJXp-Span-398"><span id="MJXp-Span-399"><span id="MJXp-Span-400"><span><span><span><span id="MJXp-Span-402">ˆ</span></span><span><span id="MJXp-Span-401">N</span></span></span></span></span></span><span id="MJXp-Span-403">n</span></span><span id="MJXp-Span-404">(</span><span id="MJXp-Span-405">s</span><span id="MJXp-Span-406">)</span><span id="MJXp-Span-407">≈</span><span id="MJXp-Span-408">(</span><span id="MJXp-Span-409"><span id="MJXp-Span-410">e</span><span id="MJXp-Span-411"><span id="MJXp-Span-412"><span id="MJXp-Span-413">PG</span><span id="MJXp-Span-414">n</span></span><span id="MJXp-Span-415">(</span><span id="MJXp-Span-416">s</span><span id="MJXp-Span-417">)</span></span></span><span id="MJXp-Span-418">−</span><span id="MJXp-Span-419">1</span><span id="MJXp-Span-420"><span id="MJXp-Span-421">)</span><span id="MJXp-Span-422"><span id="MJXp-Span-423">−</span><span id="MJXp-Span-424">1</span></span></span><span id="MJXp-Span-425">=</span><span id="MJXp-Span-426">(</span><span id="MJXp-Span-427"><span id="MJXp-Span-428">e</span><span id="MJXp-Span-429"><span id="MJXp-Span-430">log</span><span id="MJXp-Span-431"></span><span id="MJXp-Span-432"><span id="MJXp-Span-433">ρ</span><span><span><span><span><span id="MJXp-Span-435">′</span></span></span></span><span><span><span><span id="MJXp-Span-434">n</span></span></span></span></span></span><span id="MJXp-Span-436">(</span><span id="MJXp-Span-437">s</span><span id="MJXp-Span-438">)</span><span id="MJXp-Span-439">−</span><span id="MJXp-Span-440">log</span><span id="MJXp-Span-441"></span><span id="MJXp-Span-442">ρ</span><span id="MJXp-Span-443">(</span><span id="MJXp-Span-444">s</span><span id="MJXp-Span-445">)</span></span></span><span id="MJXp-Span-446">−</span><span id="MJXp-Span-447">1</span><span id="MJXp-Span-448"><span id="MJXp-Span-449">)</span><span id="MJXp-Span-450"><span id="MJXp-Span-451">−</span><span id="MJXp-Span-452">1</span></span></span></span></span></p><p><span id="MathJax-Element-46-Frame" tabindex="0"><nobr><span id="MathJax-Span-468"><span><span><span id="MathJax-Span-469"><span id="MathJax-Span-470"><span><span><span id="MathJax-Span-471"><span id="MathJax-Span-472"><span id="MathJax-Span-473"><span><span><span id="MathJax-Span-474">N<span></span></span><span></span></span><span><span id="MathJax-Span-475">^</span><span></span></span></span></span></span></span><span></span></span><span><span id="MathJax-Span-476">n</span><span></span></span></span></span><span id="MathJax-Span-477">(</span><span id="MathJax-Span-478">s</span><span id="MathJax-Span-479">)</span><span id="MathJax-Span-480">≈</span><span id="MathJax-Span-481">(</span><span id="MathJax-Span-482"><span><span><span id="MathJax-Span-483">e</span><span></span></span><span><span id="MathJax-Span-484"><span id="MathJax-Span-485"><span id="MathJax-Span-486"><span><span><span id="MathJax-Span-487">PG</span><span></span></span><span><span id="MathJax-Span-488">n</span><span></span></span></span></span><span id="MathJax-Span-489">(</span><span id="MathJax-Span-490">s</span><span id="MathJax-Span-491">)</span></span></span><span></span></span></span></span><span id="MathJax-Span-492">−</span><span id="MathJax-Span-493">1</span><span id="MathJax-Span-494"><span><span><span id="MathJax-Span-495">)</span><span></span></span><span><span id="MathJax-Span-496"><span id="MathJax-Span-497"><span id="MathJax-Span-498">−</span><span id="MathJax-Span-499">1</span></span></span><span></span></span></span></span><span id="MathJax-Span-500">=</span><span id="MathJax-Span-501">(</span><span id="MathJax-Span-502"><span><span><span id="MathJax-Span-503">e</span><span></span></span><span><span id="MathJax-Span-504"><span id="MathJax-Span-505"><span id="MathJax-Span-506">log</span><span id="MathJax-Span-507"></span><span id="MathJax-Span-508"><span><span><span id="MathJax-Span-509">ρ</span><span></span></span><span><span id="MathJax-Span-510">′</span><span></span></span><span><span id="MathJax-Span-511">n</span><span></span></span></span></span><span id="MathJax-Span-512">(</span><span id="MathJax-Span-513">s</span><span id="MathJax-Span-514">)</span><span id="MathJax-Span-515">−</span><span id="MathJax-Span-516">log</span><span id="MathJax-Span-517"></span><span id="MathJax-Span-518">ρ</span><span id="MathJax-Span-519">(</span><span id="MathJax-Span-520">s</span><span id="MathJax-Span-521">)</span></span></span><span></span></span></span></span><span id="MathJax-Span-522">−</span><span id="MathJax-Span-523">1</span><span id="MathJax-Span-524"><span><span><span id="MathJax-Span-525">)</span><span></span></span><span><span id="MathJax-Span-526"><span id="MathJax-Span-527"><span id="MathJax-Span-528">−</span><span id="MathJax-Span-529">1</span></span></span><span></span></span></span></span></span></span></span></span></nobr></span></p>

<p>A common choice of a count-based intrinsic bonus is <span><span id="MJXp-Span-453"><span id="MJXp-Span-454"><span id="MJXp-Span-455">r</span><span><span><span><span><span id="MJXp-Span-457">i</span></span></span></span><span><span><span><span id="MJXp-Span-456">t</span></span></span></span></span></span><span id="MJXp-Span-458">=</span><span id="MJXp-Span-459">N</span><span id="MJXp-Span-460">(</span><span id="MJXp-Span-461"><span id="MJXp-Span-462">s</span><span id="MJXp-Span-463">t</span></span><span id="MJXp-Span-464">,</span><span id="MJXp-Span-465"><span id="MJXp-Span-466">a</span><span id="MJXp-Span-467">t</span></span><span id="MJXp-Span-468"><span id="MJXp-Span-469">)</span><span id="MJXp-Span-470"><span id="MJXp-Span-471">−</span><span id="MJXp-Span-472">1</span><span id="MJXp-Span-473"><span id="MJXp-Span-474">/</span></span><span id="MJXp-Span-475">2</span></span></span></span></span><span id="MathJax-Element-47-Frame" tabindex="0"><nobr><span id="MathJax-Span-530"><span><span><span id="MathJax-Span-531"><span id="MathJax-Span-532"><span><span><span id="MathJax-Span-533">r</span><span></span></span><span><span id="MathJax-Span-534">i</span><span></span></span><span><span id="MathJax-Span-535">t</span><span></span></span></span></span><span id="MathJax-Span-536">=</span><span id="MathJax-Span-537">N<span></span></span><span id="MathJax-Span-538">(</span><span id="MathJax-Span-539"><span><span><span id="MathJax-Span-540">s</span><span></span></span><span><span id="MathJax-Span-541">t</span><span></span></span></span></span><span id="MathJax-Span-542">,</span><span id="MathJax-Span-543"><span><span><span id="MathJax-Span-544">a</span><span></span></span><span><span id="MathJax-Span-545">t</span><span></span></span></span></span><span id="MathJax-Span-546"><span><span><span id="MathJax-Span-547">)</span><span></span></span><span><span id="MathJax-Span-548"><span id="MathJax-Span-549"><span id="MathJax-Span-550">−</span><span id="MathJax-Span-551">1</span><span id="MathJax-Span-552"><span id="MathJax-Span-553"><span id="MathJax-Span-554">/</span></span></span><span id="MathJax-Span-555">2</span></span></span><span></span></span></span></span></span></span></span></span></nobr></span> (as in MBIE-EB; <a href="https://www.ics.uci.edu/~dechter/courses/ics-295/fall-2019/papers/2008-littman-aij-main.pdf">Strehl &amp; Littman, 2008</a>). The pseudo-count-based exploration bonus is shaped in a similar form, <span><span id="MJXp-Span-476"><span id="MJXp-Span-477"><span id="MJXp-Span-478">r</span><span><span><span><span><span id="MJXp-Span-480">i</span></span></span></span><span><span><span><span id="MJXp-Span-479">t</span></span></span></span></span></span><span id="MJXp-Span-481">=</span><span id="MJXp-Span-482"><span id="MJXp-Span-483"><span>(</span></span></span><span id="MJXp-Span-484"><span id="MJXp-Span-485"><span id="MJXp-Span-486"><span><span><span><span id="MJXp-Span-488">ˆ</span></span><span><span id="MJXp-Span-487">N</span></span></span></span></span></span><span id="MJXp-Span-489">n</span></span><span id="MJXp-Span-490">(</span><span id="MJXp-Span-491"><span id="MJXp-Span-492">s</span><span id="MJXp-Span-493">t</span></span><span id="MJXp-Span-494">,</span><span id="MJXp-Span-495"><span id="MJXp-Span-496">a</span><span id="MJXp-Span-497">t</span></span><span id="MJXp-Span-498">)</span><span id="MJXp-Span-499">+</span><span id="MJXp-Span-500">0.01</span><span id="MJXp-Span-501"><span id="MJXp-Span-502"><span id="MJXp-Span-503"><span>)</span></span></span><span id="MJXp-Span-504"><span id="MJXp-Span-505">−</span><span id="MJXp-Span-506">1</span><span id="MJXp-Span-507"><span id="MJXp-Span-508">/</span></span><span id="MJXp-Span-509">2</span></span></span></span></span><span id="MathJax-Element-48-Frame" tabindex="0"><nobr><span id="MathJax-Span-556"><span><span><span id="MathJax-Span-557"><span id="MathJax-Span-558"><span><span><span id="MathJax-Span-559">r</span><span></span></span><span><span id="MathJax-Span-560">i</span><span></span></span><span><span id="MathJax-Span-561">t</span><span></span></span></span></span><span id="MathJax-Span-562">=</span><span id="MathJax-Span-563"><span id="MathJax-Span-564"><span id="MathJax-Span-565"><span>(</span></span></span></span><span id="MathJax-Span-566"><span><span><span id="MathJax-Span-567"><span id="MathJax-Span-568"><span id="MathJax-Span-569"><span><span><span id="MathJax-Span-570">N<span></span></span><span></span></span><span><span id="MathJax-Span-571">^</span><span></span></span></span></span></span></span><span></span></span><span><span id="MathJax-Span-572">n</span><span></span></span></span></span><span id="MathJax-Span-573">(</span><span id="MathJax-Span-574"><span><span><span id="MathJax-Span-575">s</span><span></span></span><span><span id="MathJax-Span-576">t</span><span></span></span></span></span><span id="MathJax-Span-577">,</span><span id="MathJax-Span-578"><span><span><span id="MathJax-Span-579">a</span><span></span></span><span><span id="MathJax-Span-580">t</span><span></span></span></span></span><span id="MathJax-Span-581">)</span><span id="MathJax-Span-582">+</span><span id="MathJax-Span-583">0.01</span><span id="MathJax-Span-584"><span><span><span id="MathJax-Span-585"><span id="MathJax-Span-586"><span id="MathJax-Span-587"><span>)</span></span></span></span><span></span></span><span><span id="MathJax-Span-588"><span id="MathJax-Span-589"><span id="MathJax-Span-590">−</span><span id="MathJax-Span-591">1</span><span id="MathJax-Span-592"><span id="MathJax-Span-593"><span id="MathJax-Span-594">/</span></span></span><span id="MathJax-Span-595">2</span></span></span><span></span></span></span></span></span></span></span></span></nobr></span>.</p>

<p>Experiments in <a href="https://arxiv.org/abs/1606.01868">Bellemare et al., (2016)</a> adopted a simple <a href="http://proceedings.mlr.press/v32/bellemare14.html">CTS</a> (Context Tree Switching) density model to estimate pseudo-counts. The CTS model takes as input a 2D image and assigns to it a probability according to the product of location-dependent L-shaped filters, where the prediction of each filter is given by a CTS algorithm trained on past images. The CTS model is simple but limited in expressiveness, scalability, and data efficiency. In a following-up paper, <a href="https://arxiv.org/abs/1703.01310">Georg Ostrovski, et al. (2017)</a> improved the approach by training a PixelCNN (<a href="https://arxiv.org/abs/1606.05328">van den Oord et al., 2016</a>) as the density model.</p>

<p>The density model can also be a Gaussian Mixture Model as in <a href="https://arxiv.org/abs/1902.08039">Zhao &amp; Tresp (2018)</a>. They used a variational GMM to estimate the density of trajectories (e.g. concatenation of a sequence of states) and its predicted probabilities to guide prioritization in experience replay in off-policy setting.</p>

<h4 id="counting-after-hashing">Counting after Hashing</h4>

<p>Another idea to make it possible to count high-dimensional states is to map states into <strong>hash codes</strong> so that the occurrences of states become trackable (<a href="https://arxiv.org/abs/1611.04717">Tang et al. 2017</a>). The state space is discretized with a hash function <span><span id="MJXp-Span-510"><span id="MJXp-Span-511">ϕ</span><span id="MJXp-Span-512">:</span><span id="MJXp-Span-513"><span id="MJXp-Span-514">S</span></span><span id="MJXp-Span-515">↦</span><span id="MJXp-Span-516"><span id="MJXp-Span-517"><span id="MJXp-Span-518">Z</span></span><span id="MJXp-Span-519">k</span></span></span></span><span id="MathJax-Element-49-Frame" tabindex="0"><nobr><span id="MathJax-Span-596"><span><span><span id="MathJax-Span-597"><span id="MathJax-Span-598">ϕ</span><span id="MathJax-Span-599">:</span><span id="MathJax-Span-600"><span id="MathJax-Span-601"><span id="MathJax-Span-602">S<span></span></span></span></span><span id="MathJax-Span-603">↦</span><span id="MathJax-Span-604"><span><span><span id="MathJax-Span-605"><span id="MathJax-Span-606"><span id="MathJax-Span-607">Z</span></span></span><span></span></span><span><span id="MathJax-Span-608">k</span><span></span></span></span></span></span></span></span></span></nobr></span>. An exploration bonus <span><span id="MJXp-Span-520"><span id="MJXp-Span-521"><span id="MJXp-Span-522">r</span><span id="MJXp-Span-523"><span id="MJXp-Span-524">i</span></span></span><span id="MJXp-Span-525">:</span><span id="MJXp-Span-526"><span id="MJXp-Span-527">S</span></span><span id="MJXp-Span-528">↦</span><span id="MJXp-Span-529"><span id="MJXp-Span-530">R</span></span></span></span><span id="MathJax-Element-50-Frame" tabindex="0"><nobr><span id="MathJax-Span-609"><span><span><span id="MathJax-Span-610"><span id="MathJax-Span-611"><span><span><span id="MathJax-Span-612">r</span><span></span></span><span><span id="MathJax-Span-613"><span id="MathJax-Span-614"><span id="MathJax-Span-615">i</span></span></span><span></span></span></span></span><span id="MathJax-Span-616">:</span><span id="MathJax-Span-617"><span id="MathJax-Span-618"><span id="MathJax-Span-619">S<span></span></span></span></span><span id="MathJax-Span-620">↦</span><span id="MathJax-Span-621"><span id="MathJax-Span-622"><span id="MathJax-Span-623">R</span></span></span></span></span></span></span></nobr></span> is added to the reward function, defined as <span><span id="MJXp-Span-531"><span id="MJXp-Span-532"><span id="MJXp-Span-533">r</span><span id="MJXp-Span-534"><span id="MJXp-Span-535">i</span></span></span><span id="MJXp-Span-536">(</span><span id="MJXp-Span-537">s</span><span id="MJXp-Span-538">)</span><span id="MJXp-Span-539">=</span><span id="MJXp-Span-540"><span id="MJXp-Span-541"><span id="MJXp-Span-542">N</span><span id="MJXp-Span-543">(</span><span id="MJXp-Span-544">ϕ</span><span id="MJXp-Span-545">(</span><span id="MJXp-Span-546">s</span><span id="MJXp-Span-547">)</span><span id="MJXp-Span-548">)</span></span><span id="MJXp-Span-549"><span id="MJXp-Span-550">−</span><span id="MJXp-Span-551">1</span><span id="MJXp-Span-552"><span id="MJXp-Span-553">/</span></span><span id="MJXp-Span-554">2</span></span></span></span></span><span id="MathJax-Element-51-Frame" tabindex="0"><nobr><span id="MathJax-Span-624"><span><span><span id="MathJax-Span-625"><span id="MathJax-Span-626"><span><span><span id="MathJax-Span-627">r</span><span></span></span><span><span id="MathJax-Span-628"><span id="MathJax-Span-629"><span id="MathJax-Span-630">i</span></span></span><span></span></span></span></span><span id="MathJax-Span-631">(</span><span id="MathJax-Span-632">s</span><span id="MathJax-Span-633">)</span><span id="MathJax-Span-634">=</span><span id="MathJax-Span-635"><span><span><span id="MathJax-Span-636"><span id="MathJax-Span-637"><span id="MathJax-Span-638">N<span></span></span><span id="MathJax-Span-639">(</span><span id="MathJax-Span-640">ϕ</span><span id="MathJax-Span-641">(</span><span id="MathJax-Span-642">s</span><span id="MathJax-Span-643">)</span><span id="MathJax-Span-644">)</span></span></span><span></span></span><span><span id="MathJax-Span-645"><span id="MathJax-Span-646"><span id="MathJax-Span-647">−</span><span id="MathJax-Span-648">1</span><span id="MathJax-Span-649"><span id="MathJax-Span-650"><span id="MathJax-Span-651">/</span></span></span><span id="MathJax-Span-652">2</span></span></span><span></span></span></span></span></span></span></span></span></nobr></span>, where <span><span id="MJXp-Span-555"><span id="MJXp-Span-556">N</span><span id="MJXp-Span-557">(</span><span id="MJXp-Span-558">ϕ</span><span id="MJXp-Span-559">(</span><span id="MJXp-Span-560">s</span><span id="MJXp-Span-561">)</span><span id="MJXp-Span-562">)</span></span></span><span id="MathJax-Element-52-Frame" tabindex="0"><nobr><span id="MathJax-Span-653"><span><span><span id="MathJax-Span-654"><span id="MathJax-Span-655">N<span></span></span><span id="MathJax-Span-656">(</span><span id="MathJax-Span-657">ϕ</span><span id="MathJax-Span-658">(</span><span id="MathJax-Span-659">s</span><span id="MathJax-Span-660">)</span><span id="MathJax-Span-661">)</span></span></span></span></span></nobr></span> is an empirical count of occurrences of <span><span id="MJXp-Span-563"><span id="MJXp-Span-564">ϕ</span><span id="MJXp-Span-565">(</span><span id="MJXp-Span-566">s</span><span id="MJXp-Span-567">)</span></span></span><span id="MathJax-Element-53-Frame" tabindex="0"><nobr><span id="MathJax-Span-662"><span><span><span id="MathJax-Span-663"><span id="MathJax-Span-664">ϕ</span><span id="MathJax-Span-665">(</span><span id="MathJax-Span-666">s</span><span id="MathJax-Span-667">)</span></span></span></span></span></nobr></span>.</p>

<p><a href="https://arxiv.org/abs/1611.04717">Tang et al. (2017)</a> proposed to use <em>Locality-Sensitive Hashing</em> (<a href="https://en.wikipedia.org/wiki/Locality-sensitive_hashing"><em>LSH</em></a>) to convert continuous, high-dimensional data to discrete hash codes. LSH is a popular class of hash functions for querying nearest neighbors based on certain similarity metrics. A hashing scheme <span><span id="MJXp-Span-568"><span id="MJXp-Span-569">x</span><span id="MJXp-Span-570">↦</span><span id="MJXp-Span-571">h</span><span id="MJXp-Span-572">(</span><span id="MJXp-Span-573">x</span><span id="MJXp-Span-574">)</span></span></span><span id="MathJax-Element-54-Frame" tabindex="0"><nobr><span id="MathJax-Span-668"><span><span><span id="MathJax-Span-669"><span id="MathJax-Span-670">x</span><span id="MathJax-Span-671">↦</span><span id="MathJax-Span-672">h</span><span id="MathJax-Span-673">(</span><span id="MathJax-Span-674">x</span><span id="MathJax-Span-675">)</span></span></span></span></span></nobr></span> is locality-sensitive if it preserves the distancing information between data points, such that close vectors obtain similar hashes while distant vectors have very different ones. (See how LSH is used in <a href="https://lilianweng.github.io/lil-log/2020/04/07/the-transformer-family.html#LSH">Transformer improvement</a> if interested.) <a href="https://www.cs.princeton.edu/courses/archive/spr04/cos598B/bib/CharikarEstim.pdf">SimHash</a> is a type of computationally efficient LSH and it measures similarity by angular distance:</p>

<p><span><span id="MJXp-Span-575"><span id="MJXp-Span-576">ϕ</span><span id="MJXp-Span-577">(</span><span id="MJXp-Span-578">s</span><span id="MJXp-Span-579">)</span><span id="MJXp-Span-580">=</span><span id="MJXp-Span-581">sgn</span><span id="MJXp-Span-582">(</span><span id="MJXp-Span-583">A</span><span id="MJXp-Span-584">g</span><span id="MJXp-Span-585">(</span><span id="MJXp-Span-586">s</span><span id="MJXp-Span-587">)</span><span id="MJXp-Span-588">)</span><span id="MJXp-Span-589">∈</span><span id="MJXp-Span-590">{</span><span id="MJXp-Span-591">−</span><span id="MJXp-Span-592">1</span><span id="MJXp-Span-593">,</span><span id="MJXp-Span-594">1</span><span id="MJXp-Span-595"><span id="MJXp-Span-596">}</span><span id="MJXp-Span-597">k</span></span></span></span></p><p><span id="MathJax-Element-55-Frame" tabindex="0"><nobr><span id="MathJax-Span-676"><span><span><span id="MathJax-Span-677"><span id="MathJax-Span-678">ϕ</span><span id="MathJax-Span-679">(</span><span id="MathJax-Span-680">s</span><span id="MathJax-Span-681">)</span><span id="MathJax-Span-682">=</span><span id="MathJax-Span-683">sgn</span><span id="MathJax-Span-684">(</span><span id="MathJax-Span-685">A</span><span id="MathJax-Span-686">g<span></span></span><span id="MathJax-Span-687">(</span><span id="MathJax-Span-688">s</span><span id="MathJax-Span-689">)</span><span id="MathJax-Span-690">)</span><span id="MathJax-Span-691">∈</span><span id="MathJax-Span-692">{</span><span id="MathJax-Span-693">−</span><span id="MathJax-Span-694">1</span><span id="MathJax-Span-695">,</span><span id="MathJax-Span-696">1</span><span id="MathJax-Span-697"><span><span><span id="MathJax-Span-698">}</span><span></span></span><span><span id="MathJax-Span-699">k</span><span></span></span></span></span></span></span></span></span></nobr></span></p>

<p>where <span><span id="MJXp-Span-598"><span id="MJXp-Span-599">A</span><span id="MJXp-Span-600">∈</span><span id="MJXp-Span-601"><span id="MJXp-Span-602"><span id="MJXp-Span-603">R</span></span><span id="MJXp-Span-604"><span id="MJXp-Span-605">k</span><span id="MJXp-Span-606">×</span><span id="MJXp-Span-607">D</span></span></span></span></span><span id="MathJax-Element-56-Frame" tabindex="0"><nobr><span id="MathJax-Span-700"><span><span><span id="MathJax-Span-701"><span id="MathJax-Span-702">A</span><span id="MathJax-Span-703">∈</span><span id="MathJax-Span-704"><span><span><span id="MathJax-Span-705"><span id="MathJax-Span-706"><span id="MathJax-Span-707">R</span></span></span><span></span></span><span><span id="MathJax-Span-708"><span id="MathJax-Span-709"><span id="MathJax-Span-710">k</span><span id="MathJax-Span-711">×</span><span id="MathJax-Span-712">D</span></span></span><span></span></span></span></span></span></span></span></span></nobr></span> is a matrix with each entry drawn i.i.d. from a standard Gaussian and <span><span id="MJXp-Span-608"><span id="MJXp-Span-609">g</span><span id="MJXp-Span-610">:</span><span id="MJXp-Span-611"><span id="MJXp-Span-612">S</span></span><span id="MJXp-Span-613">↦</span><span id="MJXp-Span-614"><span id="MJXp-Span-615"><span id="MJXp-Span-616">R</span></span><span id="MJXp-Span-617">D</span></span></span></span><span id="MathJax-Element-57-Frame" tabindex="0"><nobr><span id="MathJax-Span-713"><span><span><span id="MathJax-Span-714"><span id="MathJax-Span-715">g<span></span></span><span id="MathJax-Span-716">:</span><span id="MathJax-Span-717"><span id="MathJax-Span-718"><span id="MathJax-Span-719">S<span></span></span></span></span><span id="MathJax-Span-720">↦</span><span id="MathJax-Span-721"><span><span><span id="MathJax-Span-722"><span id="MathJax-Span-723"><span id="MathJax-Span-724">R</span></span></span><span></span></span><span><span id="MathJax-Span-725">D</span><span></span></span></span></span></span></span></span></span></nobr></span> is an optional preprocessing function. The dimension of binary codes is <span><span id="MJXp-Span-618"><span id="MJXp-Span-619">k</span></span></span><span id="MathJax-Element-58-Frame" tabindex="0"><nobr><span id="MathJax-Span-726"><span><span><span id="MathJax-Span-727"><span id="MathJax-Span-728">k</span></span></span></span></span></nobr></span>, controlling the granularity of the state space discretization. A higher <span><span id="MJXp-Span-620"><span id="MJXp-Span-621">k</span></span></span><span id="MathJax-Element-59-Frame" tabindex="0"><nobr><span id="MathJax-Span-729"><span><span><span id="MathJax-Span-730"><span id="MathJax-Span-731">k</span></span></span></span></span></nobr></span> leads to higher granularity and fewer collisions.</p>

<p><img src="https://lilianweng.github.io/lil-log/assets/images/count-hashing-exploration.png" alt="#Exploration"></p>
<p><em>Fig. 2. Algorithm of count-based exploration through hashing high-dimensional states by SimHash. (Image source: <a href="https://arxiv.org/abs/1611.04717">Tang et al. 2017</a>)</em></p>

<p>For high-dimensional images, SimHash may not work well on the raw pixel level. <a href="https://arxiv.org/abs/1611.04717">Tang et al. (2017)</a> designed an autoencoder (AE) which takes as input states <span><span id="MJXp-Span-622"><span id="MJXp-Span-623">s</span></span></span><span id="MathJax-Element-60-Frame" tabindex="0"><nobr><span id="MathJax-Span-732"><span><span><span id="MathJax-Span-733"><span id="MathJax-Span-734">s</span></span></span></span></span></nobr></span> to learn hash codes. It has one special dense layer composed of <span><span id="MJXp-Span-624"><span id="MJXp-Span-625">k</span></span></span><span id="MathJax-Element-61-Frame" tabindex="0"><nobr><span id="MathJax-Span-735"><span><span><span id="MathJax-Span-736"><span id="MathJax-Span-737">k</span></span></span></span></span></nobr></span> sigmoid functions as the latent state in the middle and then the sigmoid activation values <span><span id="MJXp-Span-626"><span id="MJXp-Span-627">b</span><span id="MJXp-Span-628">(</span><span id="MJXp-Span-629">s</span><span id="MJXp-Span-630">)</span></span></span><span id="MathJax-Element-62-Frame" tabindex="0"><nobr><span id="MathJax-Span-738"><span><span><span id="MathJax-Span-739"><span id="MathJax-Span-740">b</span><span id="MathJax-Span-741">(</span><span id="MathJax-Span-742">s</span><span id="MathJax-Span-743">)</span></span></span></span></span></nobr></span> of this layer are binarized by rounding to their closest binary numbers <span><span id="MJXp-Span-631"><span id="MJXp-Span-632">⌊</span><span id="MJXp-Span-633">b</span><span id="MJXp-Span-634">(</span><span id="MJXp-Span-635">s</span><span id="MJXp-Span-636">)</span><span id="MJXp-Span-637">⌉</span><span id="MJXp-Span-638">∈</span><span id="MJXp-Span-639">{</span><span id="MJXp-Span-640">0</span><span id="MJXp-Span-641">,</span><span id="MJXp-Span-642">1</span><span id="MJXp-Span-643"><span id="MJXp-Span-644">}</span><span id="MJXp-Span-645">D</span></span></span></span><span id="MathJax-Element-63-Frame" tabindex="0"><nobr><span id="MathJax-Span-744"><span><span><span id="MathJax-Span-745"><span id="MathJax-Span-746">⌊</span><span id="MathJax-Span-747">b</span><span id="MathJax-Span-748">(</span><span id="MathJax-Span-749">s</span><span id="MathJax-Span-750">)</span><span id="MathJax-Span-751">⌉</span><span id="MathJax-Span-752">∈</span><span id="MathJax-Span-753">{</span><span id="MathJax-Span-754">0</span><span id="MathJax-Span-755">,</span><span id="MathJax-Span-756">1</span><span id="MathJax-Span-757"><span><span><span id="MathJax-Span-758">}</span><span></span></span><span><span id="MathJax-Span-759">D</span><span></span></span></span></span></span></span></span></span></nobr></span> as the binary hash codes for state <span><span id="MJXp-Span-646"><span id="MJXp-Span-647">s</span></span></span><span id="MathJax-Element-64-Frame" tabindex="0"><nobr><span id="MathJax-Span-760"><span><span><span id="MathJax-Span-761"><span id="MathJax-Span-762">s</span></span></span></span></span></nobr></span>. The AE loss over <span><span id="MJXp-Span-648"><span id="MJXp-Span-649">n</span></span></span><span id="MathJax-Element-65-Frame" tabindex="0"><nobr><span id="MathJax-Span-763"><span><span><span id="MathJax-Span-764"><span id="MathJax-Span-765">n</span></span></span></span></span></nobr></span> states includes two terms:</p>

<p><span><span id="MJXp-Span-716"><span id="MJXp-Span-717"><span id="MJXp-Span-718">L</span></span><span id="MJXp-Span-719">(</span><span id="MJXp-Span-720">{</span><span id="MJXp-Span-721"><span id="MJXp-Span-722">s</span><span id="MJXp-Span-723">n</span></span><span id="MJXp-Span-724"><span id="MJXp-Span-725">}</span><span><span><span><span><span id="MJXp-Span-730">N</span></span></span></span><span><span><span><span id="MJXp-Span-726"><span id="MJXp-Span-727">n</span><span id="MJXp-Span-728">=</span><span id="MJXp-Span-729">1</span></span></span></span></span></span></span><span id="MJXp-Span-731">)</span><span id="MJXp-Span-732">=</span><span id="MJXp-Span-733"><span><span id="MJXp-Span-734"><span id="MJXp-Span-735"><span><span id="MJXp-Span-736">−</span><span id="MJXp-Span-737"><span><span id="MJXp-Span-738">1</span></span><span><span><span><span></span></span><span><span><span id="MJXp-Span-739">N</span></span></span></span></span></span><span id="MJXp-Span-740"><span><span><span><span id="MJXp-Span-746">N</span></span><span><span id="MJXp-Span-741"><span>∑</span></span></span></span></span><span><span id="MJXp-Span-742"><span id="MJXp-Span-743">n</span><span id="MJXp-Span-744">=</span><span id="MJXp-Span-745">1</span></span></span></span><span id="MJXp-Span-747">log</span><span id="MJXp-Span-748"></span><span id="MJXp-Span-749">p</span><span id="MJXp-Span-750">(</span><span id="MJXp-Span-751"><span id="MJXp-Span-752">s</span><span id="MJXp-Span-753">n</span></span><span id="MJXp-Span-754">)</span></span><span><span id="MJXp-Span-755">⏟</span></span></span></span></span><span><span id="MJXp-Span-756">reconstruction loss</span></span></span><span id="MJXp-Span-757">+</span><span id="MJXp-Span-758"><span><span id="MJXp-Span-759"><span id="MJXp-Span-760"><span><span id="MJXp-Span-761"><span><span id="MJXp-Span-762">1</span></span><span><span><span><span></span></span><span><span><span id="MJXp-Span-763">N</span></span></span></span></span></span><span id="MJXp-Span-764"><span><span id="MJXp-Span-765">λ</span></span><span><span><span><span></span></span><span><span><span id="MJXp-Span-766">K</span></span></span></span></span></span><span id="MJXp-Span-767"><span><span><span><span id="MJXp-Span-773">N</span></span><span><span id="MJXp-Span-768"><span>∑</span></span></span></span></span><span><span id="MJXp-Span-769"><span id="MJXp-Span-770">n</span><span id="MJXp-Span-771">=</span><span id="MJXp-Span-772">1</span></span></span></span><span id="MJXp-Span-774"><span><span><span><span id="MJXp-Span-780">k</span></span><span><span id="MJXp-Span-775"><span>∑</span></span></span></span></span><span><span id="MJXp-Span-776"><span id="MJXp-Span-777">i</span><span id="MJXp-Span-778">=</span><span id="MJXp-Span-779">1</span></span></span></span><span id="MJXp-Span-781">min</span><span id="MJXp-Span-782"><span id="MJXp-Span-783"><span>{</span></span></span><span id="MJXp-Span-784">(</span><span id="MJXp-Span-785">1</span><span id="MJXp-Span-786">−</span><span id="MJXp-Span-787"><span id="MJXp-Span-788">b</span><span id="MJXp-Span-789">i</span></span><span id="MJXp-Span-790">(</span><span id="MJXp-Span-791"><span id="MJXp-Span-792">s</span><span id="MJXp-Span-793">n</span></span><span id="MJXp-Span-794">)</span><span id="MJXp-Span-795"><span id="MJXp-Span-796">)</span><span id="MJXp-Span-797">2</span></span><span id="MJXp-Span-798">,</span><span id="MJXp-Span-799"><span id="MJXp-Span-800">b</span><span id="MJXp-Span-801">i</span></span><span id="MJXp-Span-802">(</span><span id="MJXp-Span-803"><span id="MJXp-Span-804">s</span><span id="MJXp-Span-805">n</span></span><span id="MJXp-Span-806"><span id="MJXp-Span-807">)</span><span id="MJXp-Span-808">2</span></span><span id="MJXp-Span-809"><span id="MJXp-Span-810"><span>}</span></span></span></span><span><span id="MJXp-Span-811">⏟</span></span></span></span></span><span><span id="MJXp-Span-812">sigmoid activation being closer to binary</span></span></span></span></span></p><p><span id="MathJax-Element-66-Frame" tabindex="0"><nobr><span id="MathJax-Span-766"><span><span><span id="MathJax-Span-767"><span id="MathJax-Span-768"><span id="MathJax-Span-769"><span id="MathJax-Span-770">L</span></span></span><span id="MathJax-Span-771">(</span><span id="MathJax-Span-772">{</span><span id="MathJax-Span-773"><span><span><span id="MathJax-Span-774">s</span><span></span></span><span><span id="MathJax-Span-775">n</span><span></span></span></span></span><span id="MathJax-Span-776"><span><span><span id="MathJax-Span-777">}</span><span></span></span><span><span id="MathJax-Span-778">N<span></span></span><span></span></span><span><span id="MathJax-Span-779"><span id="MathJax-Span-780"><span id="MathJax-Span-781">n</span><span id="MathJax-Span-782">=</span><span id="MathJax-Span-783">1</span></span></span><span></span></span></span></span><span id="MathJax-Span-784">)</span><span id="MathJax-Span-785">=</span><span id="MathJax-Span-786"><span><span><span id="MathJax-Span-787"><span id="MathJax-Span-788"><span id="MathJax-Span-789"><span><span><span id="MathJax-Span-790"><span id="MathJax-Span-791">−</span><span id="MathJax-Span-792"><span><span><span id="MathJax-Span-793">1</span><span></span></span><span><span id="MathJax-Span-794">N<span></span></span><span></span></span><span><span></span><span></span></span></span></span><span id="MathJax-Span-795"><span><span><span id="MathJax-Span-796">∑</span><span></span></span><span><span id="MathJax-Span-797"><span id="MathJax-Span-798"><span id="MathJax-Span-799">n</span><span id="MathJax-Span-800">=</span><span id="MathJax-Span-801">1</span></span></span><span></span></span><span><span id="MathJax-Span-802">N<span></span></span><span></span></span></span></span><span id="MathJax-Span-803">log</span><span id="MathJax-Span-804"></span><span id="MathJax-Span-805">p</span><span id="MathJax-Span-806">(</span><span id="MathJax-Span-807"><span><span><span id="MathJax-Span-808">s</span><span></span></span><span><span id="MathJax-Span-809">n</span><span></span></span></span></span><span id="MathJax-Span-810">)</span></span><span></span></span><span><span id="MathJax-Span-811"><span><span><span></span></span><span><span></span></span><span><span></span></span><span><span></span></span><span><span></span></span><span><span></span></span><span><span></span></span><span><span></span></span><span><span></span></span><span><span></span></span><span><span></span></span><span><span></span></span><span><span></span></span><span><span></span></span><span><span></span></span><span><span></span></span><span><span></span></span><span><span></span></span><span><span></span></span></span></span><span></span></span></span></span></span></span><span></span></span><span><span id="MathJax-Span-812">reconstruction loss</span><span></span></span></span></span><span id="MathJax-Span-813">+</span><span id="MathJax-Span-814"><span><span><span id="MathJax-Span-815"><span id="MathJax-Span-816"><span id="MathJax-Span-817"><span><span><span id="MathJax-Span-818"><span id="MathJax-Span-819"><span><span><span id="MathJax-Span-820">1</span><span></span></span><span><span id="MathJax-Span-821">N<span></span></span><span></span></span><span><span></span><span></span></span></span></span><span id="MathJax-Span-822"><span><span><span id="MathJax-Span-823">λ</span><span></span></span><span><span id="MathJax-Span-824">K<span></span></span><span></span></span><span><span></span><span></span></span></span></span><span id="MathJax-Span-825"><span><span><span id="MathJax-Span-826">∑</span><span></span></span><span><span id="MathJax-Span-827"><span id="MathJax-Span-828"><span id="MathJax-Span-829">n</span><span id="MathJax-Span-830">=</span><span id="MathJax-Span-831">1</span></span></span><span></span></span><span><span id="MathJax-Span-832">N<span></span></span><span></span></span></span></span><span id="MathJax-Span-833"><span><span><span id="MathJax-Span-834">∑</span><span></span></span><span><span id="MathJax-Span-835"><span id="MathJax-Span-836"><span id="MathJax-Span-837">i</span><span id="MathJax-Span-838">=</span><span id="MathJax-Span-839">1</span></span></span><span></span></span><span><span id="MathJax-Span-840">k</span><span></span></span></span></span><span id="MathJax-Span-841">min</span><span id="MathJax-Span-842"><span id="MathJax-Span-843"><span id="MathJax-Span-844"><span>{</span></span></span></span><span id="MathJax-Span-845">(</span><span id="MathJax-Span-846">1</span><span id="MathJax-Span-847">−</span><span id="MathJax-Span-848"><span><span><span id="MathJax-Span-849">b</span><span></span></span><span><span id="MathJax-Span-850">i</span><span></span></span></span></span><span id="MathJax-Span-851">(</span><span id="MathJax-Span-852"><span><span><span id="MathJax-Span-853">s</span><span></span></span><span><span id="MathJax-Span-854">n</span><span></span></span></span></span><span id="MathJax-Span-855">)</span><span id="MathJax-Span-856"><span><span><span id="MathJax-Span-857">)</span><span></span></span><span><span id="MathJax-Span-858">2</span><span></span></span></span></span><span id="MathJax-Span-859">,</span><span id="MathJax-Span-860"><span><span><span id="MathJax-Span-861">b</span><span></span></span><span><span id="MathJax-Span-862">i</span><span></span></span></span></span><span id="MathJax-Span-863">(</span><span id="MathJax-Span-864"><span><span><span id="MathJax-Span-865">s</span><span></span></span><span><span id="MathJax-Span-866">n</span><span></span></span></span></span><span id="MathJax-Span-867"><span><span><span id="MathJax-Span-868">)</span><span></span></span><span><span id="MathJax-Span-869">2</span><span></span></span></span></span><span id="MathJax-Span-870"><span id="MathJax-Span-871"><span id="MathJax-Span-872"><span>}</span></span></span></span></span><span></span></span><span><span id="MathJax-Span-873"><span><span><span></span></span><span><span></span></span><span><span></span></span><span><span></span></span><span><span></span></span><span><span></span></span><span><span></span></span><span><span></span></span><span><span></span></span><span><span></span></span><span><span></span></span><span><span></span></span><span><span></span></span><span><span></span></span><span><span></span></span><span><span></span></span><span><span></span></span><span><span></span></span><span><span></span></span><span><span></span></span><span><span></span></span><span><span></span></span><span><span></span></span><span><span></span></span><span><span></span></span><span><span></span></span><span><span></span></span><span><span></span></span><span><span></span></span><span><span></span></span><span><span></span></span><span><span></span></span><span><span></span></span><span><span></span></span><span><span></span></span><span><span></span></span><span><span></span></span><span><span></span></span><span><span></span></span><span><span></span></span><span><span></span></span><span><span></span></span><span><span></span></span><span><span></span></span><span><span></span></span><span><span></span></span><span><span></span></span></span></span><span></span></span></span></span></span></span><span></span></span><span><span id="MathJax-Span-874">sigmoid activation being closer to binary</span><span></span></span></span></span></span></span></span></span></nobr></span></p>

<p>One problem with this approach is that dissimilar inputs <span><span id="MJXp-Span-813"><span id="MJXp-Span-814"><span id="MJXp-Span-815">s</span><span id="MJXp-Span-816">i</span></span><span id="MJXp-Span-817">,</span><span id="MJXp-Span-818"><span id="MJXp-Span-819">s</span><span id="MJXp-Span-820">j</span></span></span></span><span id="MathJax-Element-67-Frame" tabindex="0"><nobr><span id="MathJax-Span-875"><span><span><span id="MathJax-Span-876"><span id="MathJax-Span-877"><span><span><span id="MathJax-Span-878">s</span><span></span></span><span><span id="MathJax-Span-879">i</span><span></span></span></span></span><span id="MathJax-Span-880">,</span><span id="MathJax-Span-881"><span><span><span id="MathJax-Span-882">s</span><span></span></span><span><span id="MathJax-Span-883">j</span><span></span></span></span></span></span></span></span></span></nobr></span> may be mapped to identical hash codes but the AE still reconstructs them perfectly. One can imagine replacing the bottleneck layer <span><span id="MJXp-Span-821"><span id="MJXp-Span-822">b</span><span id="MJXp-Span-823">(</span><span id="MJXp-Span-824">s</span><span id="MJXp-Span-825">)</span></span></span><span id="MathJax-Element-68-Frame" tabindex="0"><nobr><span id="MathJax-Span-884"><span><span><span id="MathJax-Span-885"><span id="MathJax-Span-886">b</span><span id="MathJax-Span-887">(</span><span id="MathJax-Span-888">s</span><span id="MathJax-Span-889">)</span></span></span></span></span></nobr></span> with the hash codes <span><span id="MJXp-Span-826"><span id="MJXp-Span-827">⌊</span><span id="MJXp-Span-828">b</span><span id="MJXp-Span-829">(</span><span id="MJXp-Span-830">s</span><span id="MJXp-Span-831">)</span><span id="MJXp-Span-832">⌉</span></span></span><span id="MathJax-Element-69-Frame" tabindex="0"><nobr><span id="MathJax-Span-890"><span><span><span id="MathJax-Span-891"><span id="MathJax-Span-892">⌊</span><span id="MathJax-Span-893">b</span><span id="MathJax-Span-894">(</span><span id="MathJax-Span-895">s</span><span id="MathJax-Span-896">)</span><span id="MathJax-Span-897">⌉</span></span></span></span></span></nobr></span>, but then gradients cannot be back-propagated through the rounding function. Injecting uniform noise could mitigate this effect, as the AE has to learn to push the latent variable far apart to counteract the noise.</p>

<h3 id="prediction-based-exploration">Prediction-based Exploration</h3>

<p>The second category of intrinsic exploration bonuses are rewarded for improvement of the agent’s knowledge about the environment. The agent’s familiarity with the environment dynamics can be estimated through a prediction model. This idea of using a prediction model to measure <em>curiosity</em> was actually proposed quite a long time ago (<a href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.45.957">Schmidhuber, 1991</a>).</p>

<h4 id="forward-dynamics">Forward Dynamics</h4>

<p>Learning a <strong>forward dynamics prediction model</strong> is a great way to approximate how much knowledge our model has obtained about the environment and the task MDPs. It captures an agent’s capability of predicting the consequence of its own behavior, <span><span id="MJXp-Span-833"><span id="MJXp-Span-834">f</span><span id="MJXp-Span-835">:</span><span id="MJXp-Span-836">(</span><span id="MJXp-Span-837"><span id="MJXp-Span-838">s</span><span id="MJXp-Span-839">t</span></span><span id="MJXp-Span-840">,</span><span id="MJXp-Span-841"><span id="MJXp-Span-842">a</span><span id="MJXp-Span-843">t</span></span><span id="MJXp-Span-844">)</span><span id="MJXp-Span-845">↦</span><span id="MJXp-Span-846"><span id="MJXp-Span-847">s</span><span id="MJXp-Span-848"><span id="MJXp-Span-849">t</span><span id="MJXp-Span-850">+</span><span id="MJXp-Span-851">1</span></span></span></span></span><span id="MathJax-Element-70-Frame" tabindex="0"><nobr><span id="MathJax-Span-898"><span><span><span id="MathJax-Span-899"><span id="MathJax-Span-900">f<span></span></span><span id="MathJax-Span-901">:</span><span id="MathJax-Span-902">(</span><span id="MathJax-Span-903"><span><span><span id="MathJax-Span-904">s</span><span></span></span><span><span id="MathJax-Span-905">t</span><span></span></span></span></span><span id="MathJax-Span-906">,</span><span id="MathJax-Span-907"><span><span><span id="MathJax-Span-908">a</span><span></span></span><span><span id="MathJax-Span-909">t</span><span></span></span></span></span><span id="MathJax-Span-910">)</span><span id="MathJax-Span-911">↦</span><span id="MathJax-Span-912"><span><span><span id="MathJax-Span-913">s</span><span></span></span><span><span id="MathJax-Span-914"><span id="MathJax-Span-915"><span id="MathJax-Span-916">t</span><span id="MathJax-Span-917">+</span><span id="MathJax-Span-918">1</span></span></span><span></span></span></span></span></span></span></span></span></nobr></span>. Such a model cannot be perfect (e.g. due to partial observation), the error <span><span id="MJXp-Span-865"><span id="MJXp-Span-866">e</span><span id="MJXp-Span-867">(</span><span id="MJXp-Span-868"><span id="MJXp-Span-869">s</span><span id="MJXp-Span-870">t</span></span><span id="MJXp-Span-871">,</span><span id="MJXp-Span-872"><span id="MJXp-Span-873">a</span><span id="MJXp-Span-874">t</span></span><span id="MJXp-Span-875">)</span><span id="MJXp-Span-876">=</span><span id="MJXp-Span-877">‖</span><span id="MJXp-Span-878">f</span><span id="MJXp-Span-879">(</span><span id="MJXp-Span-880"><span id="MJXp-Span-881">s</span><span id="MJXp-Span-882">t</span></span><span id="MJXp-Span-883">,</span><span id="MJXp-Span-884"><span id="MJXp-Span-885">a</span><span id="MJXp-Span-886">t</span></span><span id="MJXp-Span-887">)</span><span id="MJXp-Span-888">−</span><span id="MJXp-Span-889"><span id="MJXp-Span-890">s</span><span id="MJXp-Span-891"><span id="MJXp-Span-892">t</span><span id="MJXp-Span-893">+</span><span id="MJXp-Span-894">1</span></span></span><span id="MJXp-Span-895"><span id="MJXp-Span-896">‖</span><span><span><span><span><span id="MJXp-Span-898">2</span></span></span></span><span><span><span><span id="MJXp-Span-897">2</span></span></span></span></span></span></span></span><span id="MathJax-Element-71-Frame" tabindex="0"><nobr><span id="MathJax-Span-919"><span><span><span id="MathJax-Span-920"><span id="MathJax-Span-921">e</span><span id="MathJax-Span-922">(</span><span id="MathJax-Span-923"><span><span><span id="MathJax-Span-924">s</span><span></span></span><span><span id="MathJax-Span-925">t</span><span></span></span></span></span><span id="MathJax-Span-926">,</span><span id="MathJax-Span-927"><span><span><span id="MathJax-Span-928">a</span><span></span></span><span><span id="MathJax-Span-929">t</span><span></span></span></span></span><span id="MathJax-Span-930">)</span><span id="MathJax-Span-931">=</span><span id="MathJax-Span-932">∥</span><span id="MathJax-Span-933">f<span></span></span><span id="MathJax-Span-934">(</span><span id="MathJax-Span-935"><span><span><span id="MathJax-Span-936">s</span><span></span></span><span><span id="MathJax-Span-937">t</span><span></span></span></span></span><span id="MathJax-Span-938">,</span><span id="MathJax-Span-939"><span><span><span id="MathJax-Span-940">a</span><span></span></span><span><span id="MathJax-Span-941">t</span><span></span></span></span></span><span id="MathJax-Span-942">)</span><span id="MathJax-Span-943">−</span><span id="MathJax-Span-944"><span><span><span id="MathJax-Span-945">s</span><span></span></span><span><span id="MathJax-Span-946"><span id="MathJax-Span-947"><span id="MathJax-Span-948">t</span><span id="MathJax-Span-949">+</span><span id="MathJax-Span-950">1</span></span></span><span></span></span></span></span><span id="MathJax-Span-951"><span><span><span id="MathJax-Span-952">∥</span><span></span></span><span><span id="MathJax-Span-953">2</span><span></span></span><span><span id="MathJax-Span-954">2</span><span></span></span></span></span></span></span></span></span></nobr></span> can be used for providing intrinsic exploration rewards. The higher the prediction error, the less familiar we are with that state.  The faster the error rate drops, the more learning progress signals we acquire.</p>

<p><em>Intelligent Adaptive Curiosity</em> (<strong>IAC</strong>; <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.177.7661&amp;rep=rep1&amp;type=pdf">Oudeyer, et al. 2007</a>) sketched an idea of using a forward dynamics prediction model to estimate learning progress and assigned intrinsic exploration reward accordingly.</p>

<p>IAC relies on a memory which stores all the experiences encountered by the robot, <span><span id="MJXp-Span-899"><span id="MJXp-Span-900">M</span><span id="MJXp-Span-901">=</span><span id="MJXp-Span-902">{</span><span id="MJXp-Span-903">(</span><span id="MJXp-Span-904"><span id="MJXp-Span-905">s</span><span id="MJXp-Span-906">t</span></span><span id="MJXp-Span-907">,</span><span id="MJXp-Span-908"><span id="MJXp-Span-909">a</span><span id="MJXp-Span-910">t</span></span><span id="MJXp-Span-911">,</span><span id="MJXp-Span-912"><span id="MJXp-Span-913">s</span><span id="MJXp-Span-914"><span id="MJXp-Span-915">t</span><span id="MJXp-Span-916">+</span><span id="MJXp-Span-917">1</span></span></span><span id="MJXp-Span-918">)</span><span id="MJXp-Span-919">}</span></span></span><span id="MathJax-Element-72-Frame" tabindex="0"><nobr><span id="MathJax-Span-955"><span><span><span id="MathJax-Span-956"><span id="MathJax-Span-957">M<span></span></span><span id="MathJax-Span-958">=</span><span id="MathJax-Span-959">{</span><span id="MathJax-Span-960">(</span><span id="MathJax-Span-961"><span><span><span id="MathJax-Span-962">s</span><span></span></span><span><span id="MathJax-Span-963">t</span><span></span></span></span></span><span id="MathJax-Span-964">,</span><span id="MathJax-Span-965"><span><span><span id="MathJax-Span-966">a</span><span></span></span><span><span id="MathJax-Span-967">t</span><span></span></span></span></span><span id="MathJax-Span-968">,</span><span id="MathJax-Span-969"><span><span><span id="MathJax-Span-970">s</span><span></span></span><span><span id="MathJax-Span-971"><span id="MathJax-Span-972"><span id="MathJax-Span-973">t</span><span id="MathJax-Span-974">+</span><span id="MathJax-Span-975">1</span></span></span><span></span></span></span></span><span id="MathJax-Span-976">)</span><span id="MathJax-Span-977">}</span></span></span></span></span></nobr></span> and a forward dynamics model <span><span id="MJXp-Span-920"><span id="MJXp-Span-921">f</span></span></span><span id="MathJax-Element-73-Frame" tabindex="0"><nobr><span id="MathJax-Span-978"><span><span><span id="MathJax-Span-979"><span id="MathJax-Span-980">f<span></span></span></span></span></span></span></nobr></span>. IAC incrementally splits the state space (i.e. sensorimotor space in the context of robotics, as discussed in the paper) into separate regions based on the transition samples, using a process similar to how a decision tree is split: The split happens when the number of samples is larger than a threshold, and the variance of states in each leaf should be minimal. Each tree node is characterized by its exclusive set of samples and has its own forward dynamics predictor <span><span id="MJXp-Span-922"><span id="MJXp-Span-923">f</span></span></span><span id="MathJax-Element-74-Frame" tabindex="0"><nobr><span id="MathJax-Span-981"><span><span><span id="MathJax-Span-982"><span id="MathJax-Span-983">f<span></span></span></span></span></span></span></nobr></span>, named “expert”.</p>

<p>The prediction error <span><span id="MJXp-Span-924"><span id="MJXp-Span-925"><span id="MJXp-Span-926">e</span><span id="MJXp-Span-927">t</span></span></span></span><span id="MathJax-Element-75-Frame" tabindex="0"><nobr><span id="MathJax-Span-984"><span><span><span id="MathJax-Span-985"><span id="MathJax-Span-986"><span><span><span id="MathJax-Span-987">e</span><span></span></span><span><span id="MathJax-Span-988">t</span><span></span></span></span></span></span></span></span></span></nobr></span> of an expert is pushed into a list associated with each region. The <em>learning progress</em> is then measured as the difference between the mean error rate of a moving window with offset <span><span id="MJXp-Span-928"><span id="MJXp-Span-929">τ</span></span></span><span id="MathJax-Element-76-Frame" tabindex="0"><nobr><span id="MathJax-Span-989"><span><span><span id="MathJax-Span-990"><span id="MathJax-Span-991">τ<span></span></span></span></span></span></span></nobr></span> and the current moving window. The intrinsic reward is defined for tracking the learning progress: <span><span id="MJXp-Span-930"><span id="MJXp-Span-931"><span id="MJXp-Span-932">r</span><span><span><span><span><span id="MJXp-Span-934">i</span></span></span></span><span><span><span><span id="MJXp-Span-933">t</span></span></span></span></span></span><span id="MJXp-Span-935">=</span><span id="MJXp-Span-936"><span><span id="MJXp-Span-937">1</span></span><span><span><span><span></span></span><span><span><span id="MJXp-Span-938">k</span></span></span></span></span></span><span id="MJXp-Span-939"><span id="MJXp-Span-940">∑</span><span><span><span><span><span id="MJXp-Span-945"><span id="MJXp-Span-946">k</span><span id="MJXp-Span-947">−</span><span id="MJXp-Span-948">1</span></span></span></span></span><span><span><span><span id="MJXp-Span-941"><span id="MJXp-Span-942">i</span><span id="MJXp-Span-943">=</span><span id="MJXp-Span-944">0</span></span></span></span></span></span></span><span id="MJXp-Span-949">(</span><span id="MJXp-Span-950"><span id="MJXp-Span-951">e</span><span id="MJXp-Span-952"><span id="MJXp-Span-953">t</span><span id="MJXp-Span-954">−</span><span id="MJXp-Span-955">i</span><span id="MJXp-Span-956">−</span><span id="MJXp-Span-957">τ</span></span></span><span id="MJXp-Span-958">−</span><span id="MJXp-Span-959"><span id="MJXp-Span-960">e</span><span id="MJXp-Span-961"><span id="MJXp-Span-962">t</span><span id="MJXp-Span-963">−</span><span id="MJXp-Span-964">i</span></span></span><span id="MJXp-Span-965">)</span></span></span><span id="MathJax-Element-77-Frame" tabindex="0"><nobr><span id="MathJax-Span-992"><span><span><span id="MathJax-Span-993"><span id="MathJax-Span-994"><span><span><span id="MathJax-Span-995">r</span><span></span></span><span><span id="MathJax-Span-996">i</span><span></span></span><span><span id="MathJax-Span-997">t</span><span></span></span></span></span><span id="MathJax-Span-998">=</span><span id="MathJax-Span-999"><span><span><span id="MathJax-Span-1000">1</span><span></span></span><span><span id="MathJax-Span-1001">k</span><span></span></span><span><span></span><span></span></span></span></span><span id="MathJax-Span-1002"><span><span><span id="MathJax-Span-1003">∑</span><span></span></span><span><span id="MathJax-Span-1004"><span id="MathJax-Span-1005"><span id="MathJax-Span-1006">k</span><span id="MathJax-Span-1007">−</span><span id="MathJax-Span-1008">1</span></span></span><span></span></span><span><span id="MathJax-Span-1009"><span id="MathJax-Span-1010"><span id="MathJax-Span-1011">i</span><span id="MathJax-Span-1012">=</span><span id="MathJax-Span-1013">0</span></span></span><span></span></span></span></span><span id="MathJax-Span-1014">(</span><span id="MathJax-Span-1015"><span><span><span id="MathJax-Span-1016">e</span><span></span></span><span><span id="MathJax-Span-1017"><span id="MathJax-Span-1018"><span id="MathJax-Span-1019">t</span><span id="MathJax-Span-1020">−</span><span id="MathJax-Span-1021">i</span><span id="MathJax-Span-1022">−</span><span id="MathJax-Span-1023">τ<span></span></span></span></span><span></span></span></span></span><span id="MathJax-Span-1024">−</span><span id="MathJax-Span-1025"><span><span><span id="MathJax-Span-1026">e</span><span></span></span><span><span id="MathJax-Span-1027"><span id="MathJax-Span-1028"><span id="MathJax-Span-1029">t</span><span id="MathJax-Span-1030">−</span><span id="MathJax-Span-1031">i</span></span></span><span></span></span></span></span><span id="MathJax-Span-1032">)</span></span></span></span></span></nobr></span>, where <span><span id="MJXp-Span-966"><span id="MJXp-Span-967">k</span></span></span><span id="MathJax-Element-78-Frame" tabindex="0"><nobr><span id="MathJax-Span-1033"><span><span><span id="MathJax-Span-1034"><span id="MathJax-Span-1035">k</span></span></span></span></span></nobr></span> is the moving window size. So the larger prediction error rate decrease we can achieve, the higher intrinsic reward we would assign to the agent. In other words, the agent is encouraged to take actions to quickly learn about the environment.</p>

<p><img src="https://lilianweng.github.io/lil-log/assets/images/IAC.png" alt="IAC"></p>
<p><em>Fig. 3. Architecture of the IAC (Intelligent Adaptive Curiosity) module: the intrinsic reward is assigned w.r.t the learning progress in reducing prediction error of the dynamics model. (Image source: <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.177.7661&amp;rep=rep1&amp;type=pdf">Oudeyer, et al. 2007</a>)</em></p>

<p><a href="https://arxiv.org/abs/1507.00814">Stadie et al. (2015)</a> trained a forward dynamics model in the encoding space defined by <span><span id="MJXp-Span-968"><span id="MJXp-Span-969">ϕ</span></span></span><span id="MathJax-Element-79-Frame" tabindex="0"><nobr><span id="MathJax-Span-1036"><span><span><span id="MathJax-Span-1037"><span id="MathJax-Span-1038">ϕ</span></span></span></span></span></nobr></span>, <span><span id="MJXp-Span-970"><span id="MJXp-Span-971"><span id="MJXp-Span-972">f</span><span id="MJXp-Span-973">ϕ</span></span><span id="MJXp-Span-974">:</span><span id="MJXp-Span-975">(</span><span id="MJXp-Span-976">ϕ</span><span id="MJXp-Span-977">(</span><span id="MJXp-Span-978"><span id="MJXp-Span-979">s</span><span id="MJXp-Span-980">t</span></span><span id="MJXp-Span-981">)</span><span id="MJXp-Span-982">,</span><span id="MJXp-Span-983"><span id="MJXp-Span-984">a</span><span id="MJXp-Span-985">t</span></span><span id="MJXp-Span-986">)</span><span id="MJXp-Span-987">↦</span><span id="MJXp-Span-988">ϕ</span><span id="MJXp-Span-989">(</span><span id="MJXp-Span-990"><span id="MJXp-Span-991">s</span><span id="MJXp-Span-992"><span id="MJXp-Span-993">t</span><span id="MJXp-Span-994">+</span><span id="MJXp-Span-995">1</span></span></span><span id="MJXp-Span-996">)</span></span></span><span id="MathJax-Element-80-Frame" tabindex="0"><nobr><span id="MathJax-Span-1039"><span><span><span id="MathJax-Span-1040"><span id="MathJax-Span-1041"><span><span><span id="MathJax-Span-1042">f<span></span></span><span></span></span><span><span id="MathJax-Span-1043">ϕ</span><span></span></span></span></span><span id="MathJax-Span-1044">:</span><span id="MathJax-Span-1045">(</span><span id="MathJax-Span-1046">ϕ</span><span id="MathJax-Span-1047">(</span><span id="MathJax-Span-1048"><span><span><span id="MathJax-Span-1049">s</span><span></span></span><span><span id="MathJax-Span-1050">t</span><span></span></span></span></span><span id="MathJax-Span-1051">)</span><span id="MathJax-Span-1052">,</span><span id="MathJax-Span-1053"><span><span><span id="MathJax-Span-1054">a</span><span></span></span><span><span id="MathJax-Span-1055">t</span><span></span></span></span></span><span id="MathJax-Span-1056">)</span><span id="MathJax-Span-1057">↦</span><span id="MathJax-Span-1058">ϕ</span><span id="MathJax-Span-1059">(</span><span id="MathJax-Span-1060"><span><span><span id="MathJax-Span-1061">s</span><span></span></span><span><span id="MathJax-Span-1062"><span id="MathJax-Span-1063"><span id="MathJax-Span-1064">t</span><span id="MathJax-Span-1065">+</span><span id="MathJax-Span-1066">1</span></span></span><span></span></span></span></span><span id="MathJax-Span-1067">)</span></span></span></span></span></nobr></span>. The model’s prediction error at time <span><span id="MJXp-Span-997"><span id="MJXp-Span-998">T</span></span></span><span id="MathJax-Element-81-Frame" tabindex="0"><nobr><span id="MathJax-Span-1068"><span><span><span id="MathJax-Span-1069"><span id="MathJax-Span-1070">T<span></span></span></span></span></span></span></nobr></span> is normalized by the maximum error up to time <span><span id="MJXp-Span-999"><span id="MJXp-Span-1000">t</span></span></span><span id="MathJax-Element-82-Frame" tabindex="0"><nobr><span id="MathJax-Span-1071"><span><span><span id="MathJax-Span-1072"><span id="MathJax-Span-1073">t</span></span></span></span></span></nobr></span>, <span><span id="MJXp-Span-1001"><span id="MJXp-Span-1002"><span id="MJXp-Span-1003"><span id="MJXp-Span-1004"><span><span><span><span id="MJXp-Span-1006">ˉ</span></span><span><span id="MJXp-Span-1005">e</span></span></span></span></span></span><span id="MJXp-Span-1007">t</span></span><span id="MJXp-Span-1008">=</span><span id="MJXp-Span-1009"><span><span id="MJXp-Span-1010"><span id="MJXp-Span-1011">e</span><span id="MJXp-Span-1012">t</span></span></span><span><span><span><span></span></span><span><span><span id="MJXp-Span-1013"><span id="MJXp-Span-1014">max</span><span id="MJXp-Span-1015"><span id="MJXp-Span-1016">i</span><span id="MJXp-Span-1017">≤</span><span id="MJXp-Span-1018">t</span></span></span><span id="MJXp-Span-1019"><span id="MJXp-Span-1020">e</span><span id="MJXp-Span-1021">i</span></span></span></span></span></span></span></span></span><span id="MathJax-Element-83-Frame" tabindex="0"><nobr><span id="MathJax-Span-1074"><span><span><span id="MathJax-Span-1075"><span id="MathJax-Span-1076"><span><span><span id="MathJax-Span-1077"><span id="MathJax-Span-1078"><span id="MathJax-Span-1079"><span><span><span id="MathJax-Span-1080">e</span><span></span></span><span><span id="MathJax-Span-1081">¯</span><span></span></span></span></span></span></span><span></span></span><span><span id="MathJax-Span-1082">t</span><span></span></span></span></span><span id="MathJax-Span-1083">=</span><span id="MathJax-Span-1084"><span><span><span id="MathJax-Span-1085"><span><span><span id="MathJax-Span-1086">e</span><span></span></span><span><span id="MathJax-Span-1087">t</span><span></span></span></span></span><span></span></span><span><span id="MathJax-Span-1088"><span id="MathJax-Span-1089"><span><span><span id="MathJax-Span-1090">max</span><span></span></span><span><span id="MathJax-Span-1091"><span id="MathJax-Span-1092"><span id="MathJax-Span-1093">i</span><span id="MathJax-Span-1094">≤</span><span id="MathJax-Span-1095">t</span></span></span><span></span></span></span></span><span id="MathJax-Span-1096"><span><span><span id="MathJax-Span-1097">e</span><span></span></span><span><span id="MathJax-Span-1098">i</span><span></span></span></span></span></span><span></span></span><span><span></span><span></span></span></span></span></span></span></span></span></nobr></span>, so it is always between 0 and 1. The intrinsic reward is defined accordingly: <span><span id="MJXp-Span-1022"><span id="MJXp-Span-1023"><span id="MJXp-Span-1024">r</span><span><span><span><span><span id="MJXp-Span-1026">i</span></span></span></span><span><span><span><span id="MJXp-Span-1025">t</span></span></span></span></span></span><span id="MJXp-Span-1027">=</span><span id="MJXp-Span-1028">(</span><span id="MJXp-Span-1029"><span><span id="MJXp-Span-1030"><span id="MJXp-Span-1031"><span id="MJXp-Span-1032"><span><span><span><span id="MJXp-Span-1034">ˉ</span></span><span><span id="MJXp-Span-1033">e</span></span></span></span></span></span><span id="MJXp-Span-1035">t</span></span><span id="MJXp-Span-1036">(</span><span id="MJXp-Span-1037"><span id="MJXp-Span-1038">s</span><span id="MJXp-Span-1039">t</span></span><span id="MJXp-Span-1040">,</span><span id="MJXp-Span-1041"><span id="MJXp-Span-1042">a</span><span id="MJXp-Span-1043">t</span></span><span id="MJXp-Span-1044">)</span></span><span><span><span><span></span></span><span><span><span id="MJXp-Span-1045">t</span><span id="MJXp-Span-1046">⋅</span><span id="MJXp-Span-1047">C</span></span></span></span></span></span><span id="MJXp-Span-1048">)</span></span></span><span id="MathJax-Element-84-Frame" tabindex="0"><nobr><span id="MathJax-Span-1099"><span><span><span id="MathJax-Span-1100"><span id="MathJax-Span-1101"><span><span><span id="MathJax-Span-1102">r</span><span></span></span><span><span id="MathJax-Span-1103">i</span><span></span></span><span><span id="MathJax-Span-1104">t</span><span></span></span></span></span><span id="MathJax-Span-1105">=</span><span id="MathJax-Span-1106">(</span><span id="MathJax-Span-1107"><span><span><span id="MathJax-Span-1108"><span id="MathJax-Span-1109"><span><span><span id="MathJax-Span-1110"><span id="MathJax-Span-1111"><span id="MathJax-Span-1112"><span><span><span id="MathJax-Span-1113">e</span><span></span></span><span><span id="MathJax-Span-1114">¯</span><span></span></span></span></span></span></span><span></span></span><span><span id="MathJax-Span-1115">t</span><span></span></span></span></span><span id="MathJax-Span-1116">(</span><span id="MathJax-Span-1117"><span><span><span id="MathJax-Span-1118">s</span><span></span></span><span><span id="MathJax-Span-1119">t</span><span></span></span></span></span><span id="MathJax-Span-1120">,</span><span id="MathJax-Span-1121"><span><span><span id="MathJax-Span-1122">a</span><span></span></span><span><span id="MathJax-Span-1123">t</span><span></span></span></span></span><span id="MathJax-Span-1124">)</span></span><span></span></span><span><span id="MathJax-Span-1125"><span id="MathJax-Span-1126">t</span><span id="MathJax-Span-1127">⋅</span><span id="MathJax-Span-1128">C<span></span></span></span><span></span></span><span><span></span><span></span></span></span></span><span id="MathJax-Span-1129">)</span></span></span></span></span></nobr></span>, where <span><span id="MJXp-Span-1049"><span id="MJXp-Span-1050">C</span><span id="MJXp-Span-1051">&gt;</span><span id="MJXp-Span-1052">0</span></span></span><span id="MathJax-Element-85-Frame" tabindex="0"><nobr><span id="MathJax-Span-1130"><span><span><span id="MathJax-Span-1131"><span id="MathJax-Span-1132">C<span></span></span><span id="MathJax-Span-1133">&gt;</span><span id="MathJax-Span-1134">0</span></span></span></span></span></nobr></span> is a decay constant.</p>

<p>Encoding the state space via <span><span id="MJXp-Span-1053"><span id="MJXp-Span-1054">ϕ</span><span id="MJXp-Span-1055">(</span><span id="MJXp-Span-1056">.</span><span id="MJXp-Span-1057">)</span></span></span><span id="MathJax-Element-86-Frame" tabindex="0"><nobr><span id="MathJax-Span-1135"><span><span><span id="MathJax-Span-1136"><span id="MathJax-Span-1137">ϕ</span><span id="MathJax-Span-1138">(</span><span id="MathJax-Span-1139">.</span><span id="MathJax-Span-1140">)</span></span></span></span></span></nobr></span> is necessary, as experiments in the paper have shown that a dynamics model trained directly on raw pixels has <em>very poor</em> behavior — assigning same exploration bonuses to all the states. In <a href="https://arxiv.org/abs/1507.00814">Stadie et al. (2015)</a>, the encoding function <span><span id="MJXp-Span-1058"><span id="MJXp-Span-1059">ϕ</span></span></span><span id="MathJax-Element-87-Frame" tabindex="0"><nobr><span id="MathJax-Span-1141"><span><span><span id="MathJax-Span-1142"><span id="MathJax-Span-1143">ϕ</span></span></span></span></span></nobr></span> is learned via an autocoder (AE) and <span><span id="MJXp-Span-1060"><span id="MJXp-Span-1061">ϕ</span><span id="MJXp-Span-1062">(</span><span id="MJXp-Span-1063">.</span><span id="MJXp-Span-1064">)</span></span></span><span id="MathJax-Element-88-Frame" tabindex="0"><nobr><span id="MathJax-Span-1144"><span><span><span id="MathJax-Span-1145"><span id="MathJax-Span-1146">ϕ</span><span id="MathJax-Span-1147">(</span><span id="MathJax-Span-1148">.</span><span id="MathJax-Span-1149">)</span></span></span></span></span></nobr></span> is one of the output layers in AE. The AE can be statically trained using a set of images collected by a random agent, or dynamically trained together with the policy where the early frames are gathered using <a href="#classic-exploration-strategies"><span><span id="MJXp-Span-1065"><span id="MJXp-Span-1066">ϵ</span></span></span><span id="MathJax-Element-89-Frame" tabindex="0"><nobr><span id="MathJax-Span-1150"><span><span><span id="MathJax-Span-1151"><span id="MathJax-Span-1152">ϵ</span></span></span></span></span></nobr></span>-greedy</a> exploration.</p>

<p><a name="ICM"></a>Instead of autoencoder, <em>Intrinsic Curiosity Module</em> (<strong>ICM</strong>; <a href="https://arxiv.org/abs/1705.05363">Pathak, et al., 2017</a>) learns the state space encoding <span><span id="MJXp-Span-1067"><span id="MJXp-Span-1068">ϕ</span><span id="MJXp-Span-1069">(</span><span id="MJXp-Span-1070">.</span><span id="MJXp-Span-1071">)</span></span></span><span id="MathJax-Element-90-Frame" tabindex="0"><nobr><span id="MathJax-Span-1153"><span><span><span id="MathJax-Span-1154"><span id="MathJax-Span-1155">ϕ</span><span id="MathJax-Span-1156">(</span><span id="MathJax-Span-1157">.</span><span id="MathJax-Span-1158">)</span></span></span></span></span></nobr></span> with a self-supervised <strong>inverse dynamics</strong> model. Predicting the next state given the agent’s own action is not easy, especially considering that some factors in the environment cannot be controlled by the agent or do not affect the agent. ICM believes that a good state feature space should exclude such factors because <em>they cannot influence the agent’s behavior and thus the agent has no incentive for learning them</em>. By learning an inverse dynamics model <span><span id="MJXp-Span-1072"><span id="MJXp-Span-1073">g</span><span id="MJXp-Span-1074">:</span><span id="MJXp-Span-1075">(</span><span id="MJXp-Span-1076">ϕ</span><span id="MJXp-Span-1077">(</span><span id="MJXp-Span-1078"><span id="MJXp-Span-1079">s</span><span id="MJXp-Span-1080">t</span></span><span id="MJXp-Span-1081">)</span><span id="MJXp-Span-1082">,</span><span id="MJXp-Span-1083">ϕ</span><span id="MJXp-Span-1084">(</span><span id="MJXp-Span-1085"><span id="MJXp-Span-1086">s</span><span id="MJXp-Span-1087"><span id="MJXp-Span-1088">t</span><span id="MJXp-Span-1089">+</span><span id="MJXp-Span-1090">1</span></span></span><span id="MJXp-Span-1091">)</span><span id="MJXp-Span-1092">)</span><span id="MJXp-Span-1093">↦</span><span id="MJXp-Span-1094"><span id="MJXp-Span-1095">a</span><span id="MJXp-Span-1096">t</span></span></span></span><span id="MathJax-Element-91-Frame" tabindex="0"><nobr><span id="MathJax-Span-1159"><span><span><span id="MathJax-Span-1160"><span id="MathJax-Span-1161">g<span></span></span><span id="MathJax-Span-1162">:</span><span id="MathJax-Span-1163">(</span><span id="MathJax-Span-1164">ϕ</span><span id="MathJax-Span-1165">(</span><span id="MathJax-Span-1166"><span><span><span id="MathJax-Span-1167">s</span><span></span></span><span><span id="MathJax-Span-1168">t</span><span></span></span></span></span><span id="MathJax-Span-1169">)</span><span id="MathJax-Span-1170">,</span><span id="MathJax-Span-1171">ϕ</span><span id="MathJax-Span-1172">(</span><span id="MathJax-Span-1173"><span><span><span id="MathJax-Span-1174">s</span><span></span></span><span><span id="MathJax-Span-1175"><span id="MathJax-Span-1176"><span id="MathJax-Span-1177">t</span><span id="MathJax-Span-1178">+</span><span id="MathJax-Span-1179">1</span></span></span><span></span></span></span></span><span id="MathJax-Span-1180">)</span><span id="MathJax-Span-1181">)</span><span id="MathJax-Span-1182">↦</span><span id="MathJax-Span-1183"><span><span><span id="MathJax-Span-1184">a</span><span></span></span><span><span id="MathJax-Span-1185">t</span><span></span></span></span></span></span></span></span></span></nobr></span>, the feature space only captures those changes in the environment related to the actions of our agent, and ignores the rest.</p>

<p>Given a forward model <span><span id="MJXp-Span-1097"><span id="MJXp-Span-1098">f</span></span></span><span id="MathJax-Element-92-Frame" tabindex="0"><nobr><span id="MathJax-Span-1186"><span><span><span id="MathJax-Span-1187"><span id="MathJax-Span-1188">f<span></span></span></span></span></span></span></nobr></span>, an inverse dynamics model <span><span id="MJXp-Span-1099"><span id="MJXp-Span-1100">g</span></span></span><span id="MathJax-Element-93-Frame" tabindex="0"><nobr><span id="MathJax-Span-1189"><span><span><span id="MathJax-Span-1190"><span id="MathJax-Span-1191">g<span></span></span></span></span></span></span></nobr></span> and an observation <span><span id="MJXp-Span-1101"><span id="MJXp-Span-1102">(</span><span id="MJXp-Span-1103"><span id="MJXp-Span-1104">s</span><span id="MJXp-Span-1105">t</span></span><span id="MJXp-Span-1106">,</span><span id="MJXp-Span-1107"><span id="MJXp-Span-1108">a</span><span id="MJXp-Span-1109">t</span></span><span id="MJXp-Span-1110">,</span><span id="MJXp-Span-1111"><span id="MJXp-Span-1112">s</span><span id="MJXp-Span-1113"><span id="MJXp-Span-1114">t</span><span id="MJXp-Span-1115">+</span><span id="MJXp-Span-1116">1</span></span></span><span id="MJXp-Span-1117">)</span></span></span><span id="MathJax-Element-94-Frame" tabindex="0"><nobr><span id="MathJax-Span-1192"><span><span><span id="MathJax-Span-1193"><span id="MathJax-Span-1194">(</span><span id="MathJax-Span-1195"><span><span><span id="MathJax-Span-1196">s</span><span></span></span><span><span id="MathJax-Span-1197">t</span><span></span></span></span></span><span id="MathJax-Span-1198">,</span><span id="MathJax-Span-1199"><span><span><span id="MathJax-Span-1200">a</span><span></span></span><span><span id="MathJax-Span-1201">t</span><span></span></span></span></span><span id="MathJax-Span-1202">,</span><span id="MathJax-Span-1203"><span><span><span id="MathJax-Span-1204">s</span><span></span></span><span><span id="MathJax-Span-1205"><span id="MathJax-Span-1206"><span id="MathJax-Span-1207">t</span><span id="MathJax-Span-1208">+</span><span id="MathJax-Span-1209">1</span></span></span><span></span></span></span></span><span id="MathJax-Span-1210">)</span></span></span></span></span></nobr></span>:</p>

<p><span><span id="MJXp-Span-1118"><span id="MJXp-Span-1119"><span id="MJXp-Span-1120">g</span><span id="MJXp-Span-1121"><span id="MJXp-Span-1122"><span id="MJXp-Span-1123">ψ</span><span id="MJXp-Span-1124">I</span></span></span></span><span id="MJXp-Span-1125">(</span><span id="MJXp-Span-1126">ϕ</span><span id="MJXp-Span-1127">(</span><span id="MJXp-Span-1128"><span id="MJXp-Span-1129">s</span><span id="MJXp-Span-1130">t</span></span><span id="MJXp-Span-1131">)</span><span id="MJXp-Span-1132">,</span><span id="MJXp-Span-1133">ϕ</span><span id="MJXp-Span-1134">(</span><span id="MJXp-Span-1135"><span id="MJXp-Span-1136">s</span><span id="MJXp-Span-1137"><span id="MJXp-Span-1138">t</span><span id="MJXp-Span-1139">+</span><span id="MJXp-Span-1140">1</span></span></span><span id="MJXp-Span-1141">)</span><span id="MJXp-Span-1142">)</span><span id="MJXp-Span-1143">=</span><span id="MJXp-Span-1144"><span id="MJXp-Span-1145"><span id="MJXp-Span-1146"><span><span><span><span id="MJXp-Span-1148">ˆ</span></span><span><span id="MJXp-Span-1147">a</span></span></span></span></span></span><span id="MJXp-Span-1149">t</span></span><span id="MJXp-Span-1150"></span><span id="MJXp-Span-1151"><span id="MJXp-Span-1152">f</span><span id="MJXp-Span-1153"><span id="MJXp-Span-1154"><span id="MJXp-Span-1155">ψ</span><span id="MJXp-Span-1156">F</span></span></span></span><span id="MJXp-Span-1157">(</span><span id="MJXp-Span-1158">ϕ</span><span id="MJXp-Span-1159">(</span><span id="MJXp-Span-1160"><span id="MJXp-Span-1161">s</span><span id="MJXp-Span-1162">t</span></span><span id="MJXp-Span-1163">)</span><span id="MJXp-Span-1164">,</span><span id="MJXp-Span-1165"><span id="MJXp-Span-1166">a</span><span id="MJXp-Span-1167">t</span></span><span id="MJXp-Span-1168">)</span><span id="MJXp-Span-1169">=</span><span id="MJXp-Span-1170"><span id="MJXp-Span-1171"><span><span><span><span id="MJXp-Span-1173">ˆ</span></span><span><span id="MJXp-Span-1172">ϕ</span></span></span></span></span></span><span id="MJXp-Span-1174">(</span><span id="MJXp-Span-1175"><span id="MJXp-Span-1176">s</span><span id="MJXp-Span-1177"><span id="MJXp-Span-1178">t</span><span id="MJXp-Span-1179">+</span><span id="MJXp-Span-1180">1</span></span></span><span id="MJXp-Span-1181">)</span><span id="MJXp-Span-1182"></span><span id="MJXp-Span-1183"><span id="MJXp-Span-1184">r</span><span><span><span><span><span id="MJXp-Span-1186">i</span></span></span></span><span><span><span><span id="MJXp-Span-1185">t</span></span></span></span></span></span><span id="MJXp-Span-1187">=</span><span id="MJXp-Span-1188">‖</span><span id="MJXp-Span-1189"><span id="MJXp-Span-1190"><span><span><span><span id="MJXp-Span-1192">ˆ</span></span><span><span id="MJXp-Span-1191">ϕ</span></span></span></span></span></span><span id="MJXp-Span-1193">(</span><span id="MJXp-Span-1194"><span id="MJXp-Span-1195">s</span><span id="MJXp-Span-1196"><span id="MJXp-Span-1197">t</span><span id="MJXp-Span-1198">+</span><span id="MJXp-Span-1199">1</span></span></span><span id="MJXp-Span-1200">)</span><span id="MJXp-Span-1201">−</span><span id="MJXp-Span-1202">ϕ</span><span id="MJXp-Span-1203">(</span><span id="MJXp-Span-1204"><span id="MJXp-Span-1205">s</span><span id="MJXp-Span-1206"><span id="MJXp-Span-1207">t</span><span id="MJXp-Span-1208">+</span><span id="MJXp-Span-1209">1</span></span></span><span id="MJXp-Span-1210">)</span><span id="MJXp-Span-1211"><span id="MJXp-Span-1212">‖</span><span><span><span><span><span id="MJXp-Span-1214">2</span></span></span></span><span><span><span><span id="MJXp-Span-1213">2</span></span></span></span></span></span></span></span></p><p><span id="MathJax-Element-95-Frame" tabindex="0"><nobr><span id="MathJax-Span-1211"><span><span><span id="MathJax-Span-1212"><span id="MathJax-Span-1213"><span><span><span id="MathJax-Span-1214">g<span></span></span><span></span></span><span><span id="MathJax-Span-1215"><span id="MathJax-Span-1216"><span id="MathJax-Span-1217"><span><span><span id="MathJax-Span-1218">ψ</span><span></span></span><span><span id="MathJax-Span-1219">I<span></span></span><span></span></span></span></span></span></span><span></span></span></span></span><span id="MathJax-Span-1220">(</span><span id="MathJax-Span-1221">ϕ</span><span id="MathJax-Span-1222">(</span><span id="MathJax-Span-1223"><span><span><span id="MathJax-Span-1224">s</span><span></span></span><span><span id="MathJax-Span-1225">t</span><span></span></span></span></span><span id="MathJax-Span-1226">)</span><span id="MathJax-Span-1227">,</span><span id="MathJax-Span-1228">ϕ</span><span id="MathJax-Span-1229">(</span><span id="MathJax-Span-1230"><span><span><span id="MathJax-Span-1231">s</span><span></span></span><span><span id="MathJax-Span-1232"><span id="MathJax-Span-1233"><span id="MathJax-Span-1234">t</span><span id="MathJax-Span-1235">+</span><span id="MathJax-Span-1236">1</span></span></span><span></span></span></span></span><span id="MathJax-Span-1237">)</span><span id="MathJax-Span-1238">)</span><span id="MathJax-Span-1239">=</span><span id="MathJax-Span-1240"><span><span><span id="MathJax-Span-1241"><span id="MathJax-Span-1242"><span id="MathJax-Span-1243"><span><span><span id="MathJax-Span-1244">a</span><span></span></span><span><span id="MathJax-Span-1245">^</span><span></span></span></span></span></span></span><span></span></span><span><span id="MathJax-Span-1246">t</span><span></span></span></span></span><span id="MathJax-Span-1247"></span><span id="MathJax-Span-1248"><span><span><span id="MathJax-Span-1249">f<span></span></span><span></span></span><span><span id="MathJax-Span-1250"><span id="MathJax-Span-1251"><span id="MathJax-Span-1252"><span><span><span id="MathJax-Span-1253">ψ</span><span></span></span><span><span id="MathJax-Span-1254">F<span></span></span><span></span></span></span></span></span></span><span></span></span></span></span><span id="MathJax-Span-1255">(</span><span id="MathJax-Span-1256">ϕ</span><span id="MathJax-Span-1257">(</span><span id="MathJax-Span-1258"><span><span><span id="MathJax-Span-1259">s</span><span></span></span><span><span id="MathJax-Span-1260">t</span><span></span></span></span></span><span id="MathJax-Span-1261">)</span><span id="MathJax-Span-1262">,</span><span id="MathJax-Span-1263"><span><span><span id="MathJax-Span-1264">a</span><span></span></span><span><span id="MathJax-Span-1265">t</span><span></span></span></span></span><span id="MathJax-Span-1266">)</span><span id="MathJax-Span-1267">=</span><span id="MathJax-Span-1268"><span id="MathJax-Span-1269"><span id="MathJax-Span-1270"><span><span><span id="MathJax-Span-1271">ϕ</span><span></span></span><span><span id="MathJax-Span-1272">^</span><span></span></span></span></span></span></span><span id="MathJax-Span-1273">(</span><span id="MathJax-Span-1274"><span><span><span id="MathJax-Span-1275">s</span><span></span></span><span><span id="MathJax-Span-1276"><span id="MathJax-Span-1277"><span id="MathJax-Span-1278">t</span><span id="MathJax-Span-1279">+</span><span id="MathJax-Span-1280">1</span></span></span><span></span></span></span></span><span id="MathJax-Span-1281">)</span><span id="MathJax-Span-1282"></span><span id="MathJax-Span-1283"><span><span><span id="MathJax-Span-1284">r</span><span></span></span><span><span id="MathJax-Span-1285">i</span><span></span></span><span><span id="MathJax-Span-1286">t</span><span></span></span></span></span><span id="MathJax-Span-1287">=</span><span id="MathJax-Span-1288">∥</span><span id="MathJax-Span-1289"><span id="MathJax-Span-1290"><span id="MathJax-Span-1291"><span><span><span id="MathJax-Span-1292">ϕ</span><span></span></span><span><span id="MathJax-Span-1293">^</span><span></span></span></span></span></span></span><span id="MathJax-Span-1294">(</span><span id="MathJax-Span-1295"><span><span><span id="MathJax-Span-1296">s</span><span></span></span><span><span id="MathJax-Span-1297"><span id="MathJax-Span-1298"><span id="MathJax-Span-1299">t</span><span id="MathJax-Span-1300">+</span><span id="MathJax-Span-1301">1</span></span></span><span></span></span></span></span><span id="MathJax-Span-1302">)</span><span id="MathJax-Span-1303">−</span><span id="MathJax-Span-1304">ϕ</span><span id="MathJax-Span-1305">(</span><span id="MathJax-Span-1306"><span><span><span id="MathJax-Span-1307">s</span><span></span></span><span><span id="MathJax-Span-1308"><span id="MathJax-Span-1309"><span id="MathJax-Span-1310">t</span><span id="MathJax-Span-1311">+</span><span id="MathJax-Span-1312">1</span></span></span><span></span></span></span></span><span id="MathJax-Span-1313">)</span><span id="MathJax-Span-1314"><span><span><span id="MathJax-Span-1315">∥</span><span></span></span><span><span id="MathJax-Span-1316">2</span><span></span></span><span><span id="MathJax-Span-1317">2</span><span></span></span></span></span></span></span></span></span></nobr></span></p>

<p>Such <span><span id="MJXp-Span-1215"><span id="MJXp-Span-1216">ϕ</span><span id="MJXp-Span-1217">(</span><span id="MJXp-Span-1218">.</span><span id="MJXp-Span-1219">)</span></span></span><span id="MathJax-Element-96-Frame" tabindex="0"></span> is expected to be robust to uncontrollable aspects of the environment.</p>

<p><img src="https://lilianweng.github.io/lil-log/assets/images/ICM.png" alt="ICM"></p>
<p><em>Fig. 4. ICM (Intrinsic Curiosity Module) assigns the forward dynamics prediction error to the agent as the intrinsic reward. This dynamics model operates in a state encoding space learned through an inverse dynamics model to exclude environmental factors that do not affect the agent’s behavior. (Image source: <a href="https://arxiv.org/abs/1705.05363">Pathak, et al. 2017</a>)</em></p>

<p><a href="https://arxiv.org/abs/1808.04355">Burda, Edwards &amp; Pathak, et al. (2018)</a> did a set of large-scale comparison experiments on purely curiosity-driven learning, meaning that only intrinsic rewards are provided to the agent. In this study, the reward is <span><span id="MJXp-Span-1220"><span id="MJXp-Span-1221"><span id="MJXp-Span-1222">r</span><span id="MJXp-Span-1223">t</span></span><span id="MJXp-Span-1224">=</span><span id="MJXp-Span-1225"><span id="MJXp-Span-1226">r</span><span><span><span><span><span id="MJXp-Span-1228">i</span></span></span></span><span><span><span><span id="MJXp-Span-1227">t</span></span></span></span></span></span><span id="MJXp-Span-1229">=</span><span id="MJXp-Span-1230">‖</span><span id="MJXp-Span-1231">f</span><span id="MJXp-Span-1232">(</span><span id="MJXp-Span-1233"><span id="MJXp-Span-1234">s</span><span id="MJXp-Span-1235">t</span></span><span id="MJXp-Span-1236">,</span><span id="MJXp-Span-1237"><span id="MJXp-Span-1238">a</span><span id="MJXp-Span-1239">t</span></span><span id="MJXp-Span-1240">)</span><span id="MJXp-Span-1241">−</span><span id="MJXp-Span-1242">ϕ</span><span id="MJXp-Span-1243">(</span><span id="MJXp-Span-1244"><span id="MJXp-Span-1245">s</span><span id="MJXp-Span-1246"><span id="MJXp-Span-1247">t</span><span id="MJXp-Span-1248">+</span><span id="MJXp-Span-1249">1</span></span></span><span id="MJXp-Span-1250">)</span><span id="MJXp-Span-1251"><span id="MJXp-Span-1252">‖</span><span><span><span><span><span id="MJXp-Span-1254">2</span></span></span></span><span><span><span><span id="MJXp-Span-1253">2</span></span></span></span></span></span></span></span><span id="MathJax-Element-97-Frame" tabindex="0"></span>. A good choice of <span><span id="MJXp-Span-1255"><span id="MJXp-Span-1256">ϕ</span></span></span><span id="MathJax-Element-98-Frame" tabindex="0"></span> is crucial to learning forward dynamics, which is expected to be <em>compact</em>, <em>sufficient</em> and <em>stable</em>, making the prediction task more tractable and filtering out irrelevant observation.</p>

<p>In comparison of 4 encoding functions:</p>
<ol>
  <li>Raw image pixels: No encoding, <span><span id="MJXp-Span-1257"><span id="MJXp-Span-1258">ϕ</span><span id="MJXp-Span-1259">(</span><span id="MJXp-Span-1260">x</span><span id="MJXp-Span-1261">)</span><span id="MJXp-Span-1262">=</span><span id="MJXp-Span-1263">x</span></span></span><span id="MathJax-Element-99-Frame" tabindex="0"></span>.</li>
  <li>Random features (RF): Each state is compressed through a fixed random neural network.</li>
  <li><a href="https://lilianweng.github.io/lil-log/2018/08/12/from-autoencoder-to-beta-vae.html#vae-variational-autoencoder">VAE</a>: The probabilistic encoder is used for encoding, <span><span id="MJXp-Span-1264"><span id="MJXp-Span-1265">ϕ</span><span id="MJXp-Span-1266">(</span><span id="MJXp-Span-1267">x</span><span id="MJXp-Span-1268">)</span><span id="MJXp-Span-1269">=</span><span id="MJXp-Span-1270">q</span><span id="MJXp-Span-1271">(</span><span id="MJXp-Span-1272">z</span><span id="MJXp-Span-1273">|</span><span id="MJXp-Span-1274">x</span><span id="MJXp-Span-1275">)</span></span></span><span id="MathJax-Element-100-Frame" tabindex="0"></span>.</li>
  <li>Inverse dynamic features (IDF): The same feature space as used in <a href="#ICM">ICM</a>.</li>
</ol>

<p>All the experiments have the reward signals normalized by a running estimation of standard deviation of the cumulative returns. And all the experiments are running in an infinite horizon setting to avoid “done” flag leaking information.</p>

<p><img src="https://lilianweng.github.io/lil-log/assets/images/large-scale-curiosity-learning.png" alt="Large-scale curiosity learning"></p>
<p><em>Fig. 5. The mean reward in different games when training with only curiosity signals, generated by different state encoding functions.
(Image source: <a href="https://arxiv.org/abs/1808.04355">Burda, Edwards &amp; Pathak, et al. 2018</a>)</em></p>

<p>Interestingly <em>random features</em> turn out to be quite competitive, but in feature transfer experiments (i.e. train an agent in Super Mario Bros level 1-1 and then test it in another level), learned IDF features can generalize better.</p>

<p>They also compared RF and IDF in an environment with a <a href="#the-noisy-tv-problem">noisy TV</a> on. Unsurprisingly the noisy TV drastically slows down the learning and extrinsic rewards are much lower in time.</p>

<p><img src="https://lilianweng.github.io/lil-log/assets/images/noisy-TV-experiment.png" alt="Noisy TV experiment"></p>
<p><em>Fig. 6. Experiments using RF and IDF feature encoding in an environment with noisy TV on or off. The plot tracks extrinsic reward per episode as the training progresses. (Image source: <a href="https://arxiv.org/abs/1808.04355">Burda, Edwards &amp; Pathak, et al. 2018</a>)</em></p>

<p>The forward dynamics optimization can be modeled via variational inference as well. <strong>VIME</strong> (short for <em>“Variational information maximizing exploration”</em>; <a href="https://arxiv.org/abs/1605.09674">Houthooft, et al. 2017</a>) is an exploration strategy based on maximization of <em>information gain</em> about the agent’s belief of environment dynamics. How much additional information has been obtained about the forward dynamics can be measured as the reduction in entropy.</p>

<p>Let <span><span id="MJXp-Span-1276"><span id="MJXp-Span-1277"><span id="MJXp-Span-1278">P</span></span></span></span><span id="MathJax-Element-101-Frame" tabindex="0"></span> be the environment transition function, <span><span id="MJXp-Span-1279"><span id="MJXp-Span-1280">p</span><span id="MJXp-Span-1281">(</span><span id="MJXp-Span-1282"><span id="MJXp-Span-1283">s</span><span id="MJXp-Span-1284"><span id="MJXp-Span-1285">t</span><span id="MJXp-Span-1286">+</span><span id="MJXp-Span-1287">1</span></span></span><span id="MJXp-Span-1288">|</span><span id="MJXp-Span-1289"><span id="MJXp-Span-1290">s</span><span id="MJXp-Span-1291">t</span></span><span id="MJXp-Span-1292">,</span><span id="MJXp-Span-1293"><span id="MJXp-Span-1294">a</span><span id="MJXp-Span-1295">t</span></span><span id="MJXp-Span-1296">;</span><span id="MJXp-Span-1297">θ</span><span id="MJXp-Span-1298">)</span></span></span><span id="MathJax-Element-102-Frame" tabindex="0"></span> be the forward prediction model, parameterized by <span><span id="MJXp-Span-1299"><span id="MJXp-Span-1300">θ</span><span id="MJXp-Span-1301">∈</span><span id="MJXp-Span-1302">Θ</span></span></span><span id="MathJax-Element-103-Frame" tabindex="0"></span>, and <span><span id="MJXp-Span-1303"><span id="MJXp-Span-1304"><span id="MJXp-Span-1305">ξ</span><span id="MJXp-Span-1306">t</span></span><span id="MJXp-Span-1307">=</span><span id="MJXp-Span-1308">{</span><span id="MJXp-Span-1309"><span id="MJXp-Span-1310">s</span><span id="MJXp-Span-1311">1</span></span><span id="MJXp-Span-1312">,</span><span id="MJXp-Span-1313"><span id="MJXp-Span-1314">a</span><span id="MJXp-Span-1315">1</span></span><span id="MJXp-Span-1316">,</span><span id="MJXp-Span-1317">…</span><span id="MJXp-Span-1318">,</span><span id="MJXp-Span-1319"><span id="MJXp-Span-1320">s</span><span id="MJXp-Span-1321">t</span></span><span id="MJXp-Span-1322">}</span></span></span><span id="MathJax-Element-104-Frame" tabindex="0"></span> be the trajectory history. We would like to reduce the entropy after taking a new action and observing the next state, which is to maximize the following:</p>

<p><span><span id="MJXp-Span-1323"><span id="MJXp-Span-1324"><span><span id="MJXp-Span-1325"><span id="MJXp-Span-1326"></span><span id="MJXp-Span-1327"><span id="MJXp-Span-1328"></span><span id="MJXp-Span-1329"><span><span id="MJXp-Span-1330"><span>∑</span></span></span><span><span id="MJXp-Span-1331">t</span></span></span><span id="MJXp-Span-1332">H</span><span id="MJXp-Span-1333">(</span><span id="MJXp-Span-1334">Θ</span><span id="MJXp-Span-1335">|</span><span id="MJXp-Span-1336"><span id="MJXp-Span-1337">ξ</span><span id="MJXp-Span-1338">t</span></span><span id="MJXp-Span-1339">,</span><span id="MJXp-Span-1340"><span id="MJXp-Span-1341">a</span><span id="MJXp-Span-1342">t</span></span><span id="MJXp-Span-1343">)</span><span id="MJXp-Span-1344">−</span><span id="MJXp-Span-1345">H</span><span id="MJXp-Span-1346">(</span><span id="MJXp-Span-1347">Θ</span><span id="MJXp-Span-1348">|</span><span id="MJXp-Span-1349"><span id="MJXp-Span-1350">S</span><span id="MJXp-Span-1351"><span id="MJXp-Span-1352">t</span><span id="MJXp-Span-1353">+</span><span id="MJXp-Span-1354">1</span></span></span><span id="MJXp-Span-1355">,</span><span id="MJXp-Span-1356"><span id="MJXp-Span-1357">ξ</span><span id="MJXp-Span-1358">t</span></span><span id="MJXp-Span-1359">,</span><span id="MJXp-Span-1360"><span id="MJXp-Span-1361">a</span><span id="MJXp-Span-1362">t</span></span><span id="MJXp-Span-1363">)</span></span></span><span id="MJXp-Span-1364"><span id="MJXp-Span-1365"><span id="MJXp-Span-1366">=</span></span><span id="MJXp-Span-1367"><span id="MJXp-Span-1368">I</span><span id="MJXp-Span-1369">(</span><span id="MJXp-Span-1370">Θ</span><span id="MJXp-Span-1371">;</span><span id="MJXp-Span-1372"><span id="MJXp-Span-1373">S</span><span id="MJXp-Span-1374"><span id="MJXp-Span-1375">t</span><span id="MJXp-Span-1376">+</span><span id="MJXp-Span-1377">1</span></span></span><span id="MJXp-Span-1378">|</span><span id="MJXp-Span-1379"><span id="MJXp-Span-1380">ξ</span><span id="MJXp-Span-1381">t</span></span><span id="MJXp-Span-1382">,</span><span id="MJXp-Span-1383"><span id="MJXp-Span-1384">a</span><span id="MJXp-Span-1385">t</span></span><span id="MJXp-Span-1386">)</span><span id="MJXp-Span-1387"></span><span id="MJXp-Span-1388"><span id="MJXp-Span-1389"><span id="MJXp-Span-1390">; because&nbsp;</span><span id="MJXp-Span-1391">I</span><span id="MJXp-Span-1392">(</span><span id="MJXp-Span-1393">X</span><span id="MJXp-Span-1394">;</span><span id="MJXp-Span-1395">Y</span><span id="MJXp-Span-1396">)</span><span id="MJXp-Span-1397">=</span><span id="MJXp-Span-1398">I</span><span id="MJXp-Span-1399">(</span><span id="MJXp-Span-1400">X</span><span id="MJXp-Span-1401">)</span><span id="MJXp-Span-1402">−</span><span id="MJXp-Span-1403">I</span><span id="MJXp-Span-1404">(</span><span id="MJXp-Span-1405">X</span><span id="MJXp-Span-1406">|</span><span id="MJXp-Span-1407">Y</span><span id="MJXp-Span-1408">)</span></span></span></span></span><span id="MJXp-Span-1409"><span id="MJXp-Span-1410"><span id="MJXp-Span-1411">=</span></span><span id="MJXp-Span-1412"><span id="MJXp-Span-1413"><span id="MJXp-Span-1414"><span id="MJXp-Span-1415">E</span></span><span id="MJXp-Span-1416"><span id="MJXp-Span-1417"><span id="MJXp-Span-1418">s</span><span id="MJXp-Span-1419"><span id="MJXp-Span-1420">t</span><span id="MJXp-Span-1421">+</span><span id="MJXp-Span-1422">1</span></span></span><span id="MJXp-Span-1423">∼</span><span id="MJXp-Span-1424"><span id="MJXp-Span-1425">P</span></span><span id="MJXp-Span-1426">(</span><span id="MJXp-Span-1427">.</span><span id="MJXp-Span-1428">|</span><span id="MJXp-Span-1429"><span id="MJXp-Span-1430">ξ</span><span id="MJXp-Span-1431">t</span></span><span id="MJXp-Span-1432">,</span><span id="MJXp-Span-1433"><span id="MJXp-Span-1434">a</span><span id="MJXp-Span-1435">t</span></span><span id="MJXp-Span-1436">)</span></span></span><span id="MJXp-Span-1437">[</span><span id="MJXp-Span-1438"><span id="MJXp-Span-1439">D</span><span id="MJXp-Span-1440">KL</span></span><span id="MJXp-Span-1441">(</span><span id="MJXp-Span-1442">p</span><span id="MJXp-Span-1443">(</span><span id="MJXp-Span-1444">θ</span><span id="MJXp-Span-1445">|</span><span id="MJXp-Span-1446"><span id="MJXp-Span-1447">ξ</span><span id="MJXp-Span-1448">t</span></span><span id="MJXp-Span-1449">,</span><span id="MJXp-Span-1450"><span id="MJXp-Span-1451">a</span><span id="MJXp-Span-1452">t</span></span><span id="MJXp-Span-1453">,</span><span id="MJXp-Span-1454"><span id="MJXp-Span-1455">s</span><span id="MJXp-Span-1456"><span id="MJXp-Span-1457">t</span><span id="MJXp-Span-1458">+</span><span id="MJXp-Span-1459">1</span></span></span><span id="MJXp-Span-1460">)</span><span id="MJXp-Span-1461">‖</span><span id="MJXp-Span-1462">p</span><span id="MJXp-Span-1463">(</span><span id="MJXp-Span-1464">θ</span><span id="MJXp-Span-1465">|</span><span id="MJXp-Span-1466"><span id="MJXp-Span-1467">ξ</span><span id="MJXp-Span-1468">t</span></span><span id="MJXp-Span-1469">,</span><span id="MJXp-Span-1470"><span id="MJXp-Span-1471">a</span><span id="MJXp-Span-1472">t</span></span><span id="MJXp-Span-1473">)</span><span id="MJXp-Span-1474">)</span><span id="MJXp-Span-1475">]</span><span id="MJXp-Span-1476"></span><span id="MJXp-Span-1477"><span id="MJXp-Span-1478"><span id="MJXp-Span-1479">; because&nbsp;</span><span id="MJXp-Span-1480">I</span><span id="MJXp-Span-1481">(</span><span id="MJXp-Span-1482">X</span><span id="MJXp-Span-1483">;</span><span id="MJXp-Span-1484">Y</span><span id="MJXp-Span-1485">)</span><span id="MJXp-Span-1486">=</span><span id="MJXp-Span-1487"><span id="MJXp-Span-1488"><span id="MJXp-Span-1489">E</span></span><span id="MJXp-Span-1490">Y</span></span><span id="MJXp-Span-1491">[</span><span id="MJXp-Span-1492"><span id="MJXp-Span-1493">D</span><span id="MJXp-Span-1494">KL</span></span><span id="MJXp-Span-1495">(</span><span id="MJXp-Span-1496"><span id="MJXp-Span-1497">p</span><span id="MJXp-Span-1498"><span id="MJXp-Span-1499">X</span><span id="MJXp-Span-1500">|</span><span id="MJXp-Span-1501">Y</span></span></span><span id="MJXp-Span-1502">‖</span><span id="MJXp-Span-1503"><span id="MJXp-Span-1504">p</span><span id="MJXp-Span-1505">X</span></span><span id="MJXp-Span-1506">)</span><span id="MJXp-Span-1507">]</span></span></span></span></span><span id="MJXp-Span-1508"><span id="MJXp-Span-1509"><span id="MJXp-Span-1510">=</span></span><span id="MJXp-Span-1511"><span id="MJXp-Span-1512"><span id="MJXp-Span-1513"><span id="MJXp-Span-1514">E</span></span><span id="MJXp-Span-1515"><span id="MJXp-Span-1516"><span id="MJXp-Span-1517">s</span><span id="MJXp-Span-1518"><span id="MJXp-Span-1519">t</span><span id="MJXp-Span-1520">+</span><span id="MJXp-Span-1521">1</span></span></span><span id="MJXp-Span-1522">∼</span><span id="MJXp-Span-1523"><span id="MJXp-Span-1524">P</span></span><span id="MJXp-Span-1525">(</span><span id="MJXp-Span-1526">.</span><span id="MJXp-Span-1527">|</span><span id="MJXp-Span-1528"><span id="MJXp-Span-1529">ξ</span><span id="MJXp-Span-1530">t</span></span><span id="MJXp-Span-1531">,</span><span id="MJXp-Span-1532"><span id="MJXp-Span-1533">a</span><span id="MJXp-Span-1534">t</span></span><span id="MJXp-Span-1535">)</span></span></span><span id="MJXp-Span-1536">[</span><span id="MJXp-Span-1537"><span id="MJXp-Span-1538">D</span><span id="MJXp-Span-1539">KL</span></span><span id="MJXp-Span-1540">(</span><span id="MJXp-Span-1541">p</span><span id="MJXp-Span-1542">(</span><span id="MJXp-Span-1543">θ</span><span id="MJXp-Span-1544">|</span><span id="MJXp-Span-1545"><span id="MJXp-Span-1546">ξ</span><span id="MJXp-Span-1547">t</span></span><span id="MJXp-Span-1548">,</span><span id="MJXp-Span-1549"><span id="MJXp-Span-1550">a</span><span id="MJXp-Span-1551">t</span></span><span id="MJXp-Span-1552">,</span><span id="MJXp-Span-1553"><span id="MJXp-Span-1554">s</span><span id="MJXp-Span-1555"><span id="MJXp-Span-1556">t</span><span id="MJXp-Span-1557">+</span><span id="MJXp-Span-1558">1</span></span></span><span id="MJXp-Span-1559">)</span><span id="MJXp-Span-1560">‖</span><span id="MJXp-Span-1561">p</span><span id="MJXp-Span-1562">(</span><span id="MJXp-Span-1563">θ</span><span id="MJXp-Span-1564">|</span><span id="MJXp-Span-1565"><span id="MJXp-Span-1566">ξ</span><span id="MJXp-Span-1567">t</span></span><span id="MJXp-Span-1568">)</span><span id="MJXp-Span-1569">)</span><span id="MJXp-Span-1570">]</span><span id="MJXp-Span-1571"></span><span id="MJXp-Span-1572"><span id="MJXp-Span-1573"><span id="MJXp-Span-1574">; because&nbsp;</span><span id="MJXp-Span-1575">θ</span><span id="MJXp-Span-1576">&nbsp;does not depend on&nbsp;</span><span id="MJXp-Span-1577"><span id="MJXp-Span-1578">a</span><span id="MJXp-Span-1579">t</span></span></span></span></span></span></span></span></span></span></p>

<p>While taking expectation over the new possible states, the agent is expected to take a new action to increase the KL divergence (<em>“information gain”</em>) between its new belief over the prediction model to the old one. This term can be added into the reward function as an intrinsic reward: <span><span id="MJXp-Span-1580"><span id="MJXp-Span-1581"><span id="MJXp-Span-1582">r</span><span><span><span><span><span id="MJXp-Span-1584">i</span></span></span></span><span><span><span><span id="MJXp-Span-1583">t</span></span></span></span></span></span><span id="MJXp-Span-1585">=</span><span id="MJXp-Span-1586"><span id="MJXp-Span-1587">D</span><span id="MJXp-Span-1588">KL</span></span><span id="MJXp-Span-1589">[</span><span id="MJXp-Span-1590">p</span><span id="MJXp-Span-1591">(</span><span id="MJXp-Span-1592">θ</span><span id="MJXp-Span-1593">|</span><span id="MJXp-Span-1594"><span id="MJXp-Span-1595">ξ</span><span id="MJXp-Span-1596">t</span></span><span id="MJXp-Span-1597">,</span><span id="MJXp-Span-1598"><span id="MJXp-Span-1599">a</span><span id="MJXp-Span-1600">t</span></span><span id="MJXp-Span-1601">,</span><span id="MJXp-Span-1602"><span id="MJXp-Span-1603">s</span><span id="MJXp-Span-1604"><span id="MJXp-Span-1605">t</span><span id="MJXp-Span-1606">+</span><span id="MJXp-Span-1607">1</span></span></span><span id="MJXp-Span-1608">)</span><span id="MJXp-Span-1609">‖</span><span id="MJXp-Span-1610">p</span><span id="MJXp-Span-1611">(</span><span id="MJXp-Span-1612">θ</span><span id="MJXp-Span-1613">|</span><span id="MJXp-Span-1614"><span id="MJXp-Span-1615">ξ</span><span id="MJXp-Span-1616">t</span></span><span id="MJXp-Span-1617">)</span><span id="MJXp-Span-1618">)</span><span id="MJXp-Span-1619">]</span></span></span><span id="MathJax-Element-106-Frame" tabindex="0"></span>.</p>

<p>However, computing the posterior <span><span id="MJXp-Span-1620"><span id="MJXp-Span-1621">p</span><span id="MJXp-Span-1622">(</span><span id="MJXp-Span-1623">θ</span><span id="MJXp-Span-1624">|</span><span id="MJXp-Span-1625"><span id="MJXp-Span-1626">ξ</span><span id="MJXp-Span-1627">t</span></span><span id="MJXp-Span-1628">,</span><span id="MJXp-Span-1629"><span id="MJXp-Span-1630">a</span><span id="MJXp-Span-1631">t</span></span><span id="MJXp-Span-1632">,</span><span id="MJXp-Span-1633"><span id="MJXp-Span-1634">s</span><span id="MJXp-Span-1635"><span id="MJXp-Span-1636">t</span><span id="MJXp-Span-1637">+</span><span id="MJXp-Span-1638">1</span></span></span><span id="MJXp-Span-1639">)</span></span></span><span id="MathJax-Element-107-Frame" tabindex="0"></span> is generally intractable.</p>

<p><span><span id="MJXp-Span-1640"><span id="MJXp-Span-1641"><span><span id="MJXp-Span-1642"><span id="MJXp-Span-1643"><span id="MJXp-Span-1644">p</span><span id="MJXp-Span-1645">(</span><span id="MJXp-Span-1646">θ</span><span id="MJXp-Span-1647">|</span><span id="MJXp-Span-1648"><span id="MJXp-Span-1649">ξ</span><span id="MJXp-Span-1650">t</span></span><span id="MJXp-Span-1651">,</span><span id="MJXp-Span-1652"><span id="MJXp-Span-1653">a</span><span id="MJXp-Span-1654">t</span></span><span id="MJXp-Span-1655">,</span><span id="MJXp-Span-1656"><span id="MJXp-Span-1657">s</span><span id="MJXp-Span-1658"><span id="MJXp-Span-1659">t</span><span id="MJXp-Span-1660">+</span><span id="MJXp-Span-1661">1</span></span></span><span id="MJXp-Span-1662">)</span></span><span id="MJXp-Span-1663"><span id="MJXp-Span-1664"></span><span id="MJXp-Span-1665">=</span><span id="MJXp-Span-1666"><span><span id="MJXp-Span-1667">p</span><span id="MJXp-Span-1668">(</span><span id="MJXp-Span-1669">θ</span><span id="MJXp-Span-1670">|</span><span id="MJXp-Span-1671"><span id="MJXp-Span-1672">ξ</span><span id="MJXp-Span-1673">t</span></span><span id="MJXp-Span-1674">,</span><span id="MJXp-Span-1675"><span id="MJXp-Span-1676">a</span><span id="MJXp-Span-1677">t</span></span><span id="MJXp-Span-1678">)</span><span id="MJXp-Span-1679">p</span><span id="MJXp-Span-1680">(</span><span id="MJXp-Span-1681"><span id="MJXp-Span-1682">s</span><span id="MJXp-Span-1683"><span id="MJXp-Span-1684">t</span><span id="MJXp-Span-1685">+</span><span id="MJXp-Span-1686">1</span></span></span><span id="MJXp-Span-1687">|</span><span id="MJXp-Span-1688"><span id="MJXp-Span-1689">ξ</span><span id="MJXp-Span-1690">t</span></span><span id="MJXp-Span-1691">,</span><span id="MJXp-Span-1692"><span id="MJXp-Span-1693">a</span><span id="MJXp-Span-1694">t</span></span><span id="MJXp-Span-1695">;</span><span id="MJXp-Span-1696">θ</span><span id="MJXp-Span-1697">)</span></span><span><span><span><span></span></span><span><span><span id="MJXp-Span-1698">p</span><span id="MJXp-Span-1699">(</span><span id="MJXp-Span-1700"><span id="MJXp-Span-1701">s</span><span id="MJXp-Span-1702"><span id="MJXp-Span-1703">t</span><span id="MJXp-Span-1704">+</span><span id="MJXp-Span-1705">1</span></span></span><span id="MJXp-Span-1706">|</span><span id="MJXp-Span-1707"><span id="MJXp-Span-1708">ξ</span><span id="MJXp-Span-1709">t</span></span><span id="MJXp-Span-1710">,</span><span id="MJXp-Span-1711"><span id="MJXp-Span-1712">a</span><span id="MJXp-Span-1713">t</span></span><span id="MJXp-Span-1714">)</span></span></span></span></span></span></span></span><span id="MJXp-Span-1715"><span id="MJXp-Span-1716"></span><span id="MJXp-Span-1717"><span id="MJXp-Span-1718"></span><span id="MJXp-Span-1719">=</span><span id="MJXp-Span-1720"><span><span id="MJXp-Span-1721">p</span><span id="MJXp-Span-1722">(</span><span id="MJXp-Span-1723">θ</span><span id="MJXp-Span-1724">|</span><span id="MJXp-Span-1725"><span id="MJXp-Span-1726">ξ</span><span id="MJXp-Span-1727">t</span></span><span id="MJXp-Span-1728">)</span><span id="MJXp-Span-1729">p</span><span id="MJXp-Span-1730">(</span><span id="MJXp-Span-1731"><span id="MJXp-Span-1732">s</span><span id="MJXp-Span-1733"><span id="MJXp-Span-1734">t</span><span id="MJXp-Span-1735">+</span><span id="MJXp-Span-1736">1</span></span></span><span id="MJXp-Span-1737">|</span><span id="MJXp-Span-1738"><span id="MJXp-Span-1739">ξ</span><span id="MJXp-Span-1740">t</span></span><span id="MJXp-Span-1741">,</span><span id="MJXp-Span-1742"><span id="MJXp-Span-1743">a</span><span id="MJXp-Span-1744">t</span></span><span id="MJXp-Span-1745">;</span><span id="MJXp-Span-1746">θ</span><span id="MJXp-Span-1747">)</span></span><span><span><span><span></span></span><span><span><span id="MJXp-Span-1748">p</span><span id="MJXp-Span-1749">(</span><span id="MJXp-Span-1750"><span id="MJXp-Span-1751">s</span><span id="MJXp-Span-1752"><span id="MJXp-Span-1753">t</span><span id="MJXp-Span-1754">+</span><span id="MJXp-Span-1755">1</span></span></span><span id="MJXp-Span-1756">|</span><span id="MJXp-Span-1757"><span id="MJXp-Span-1758">ξ</span><span id="MJXp-Span-1759">t</span></span><span id="MJXp-Span-1760">,</span><span id="MJXp-Span-1761"><span id="MJXp-Span-1762">a</span><span id="MJXp-Span-1763">t</span></span><span id="MJXp-Span-1764">)</span></span></span></span></span></span></span><span id="MJXp-Span-1765"><span id="MJXp-Span-1766"><span id="MJXp-Span-1767"><span id="MJXp-Span-1768">; because action doesn't affect the belief.</span></span></span></span></span><span id="MJXp-Span-1769"><span id="MJXp-Span-1770"></span><span id="MJXp-Span-1771"><span id="MJXp-Span-1772"></span><span id="MJXp-Span-1773">=</span><span id="MJXp-Span-1774"><span><span id="MJXp-Span-1775"><span id="MJXp-Span-1776">p</span><span id="MJXp-Span-1777">(</span><span id="MJXp-Span-1778">θ</span><span id="MJXp-Span-1779">|</span><span id="MJXp-Span-1780"><span id="MJXp-Span-1781">ξ</span><span id="MJXp-Span-1782">t</span></span><span id="MJXp-Span-1783">)</span></span><span id="MJXp-Span-1784">p</span><span id="MJXp-Span-1785">(</span><span id="MJXp-Span-1786"><span id="MJXp-Span-1787">s</span><span id="MJXp-Span-1788"><span id="MJXp-Span-1789">t</span><span id="MJXp-Span-1790">+</span><span id="MJXp-Span-1791">1</span></span></span><span id="MJXp-Span-1792">|</span><span id="MJXp-Span-1793"><span id="MJXp-Span-1794">ξ</span><span id="MJXp-Span-1795">t</span></span><span id="MJXp-Span-1796">,</span><span id="MJXp-Span-1797"><span id="MJXp-Span-1798">a</span><span id="MJXp-Span-1799">t</span></span><span id="MJXp-Span-1800">;</span><span id="MJXp-Span-1801">θ</span><span id="MJXp-Span-1802">)</span></span><span><span><span><span></span></span><span><span><span id="MJXp-Span-1803"><span id="MJXp-Span-1804">∫</span><span id="MJXp-Span-1805">Θ</span></span><span id="MJXp-Span-1806">p</span><span id="MJXp-Span-1807">(</span><span id="MJXp-Span-1808"><span id="MJXp-Span-1809">s</span><span id="MJXp-Span-1810"><span id="MJXp-Span-1811">t</span><span id="MJXp-Span-1812">+</span><span id="MJXp-Span-1813">1</span></span></span><span id="MJXp-Span-1814">|</span><span id="MJXp-Span-1815"><span id="MJXp-Span-1816">ξ</span><span id="MJXp-Span-1817">t</span></span><span id="MJXp-Span-1818">,</span><span id="MJXp-Span-1819"><span id="MJXp-Span-1820">a</span><span id="MJXp-Span-1821">t</span></span><span id="MJXp-Span-1822">;</span><span id="MJXp-Span-1823">θ</span><span id="MJXp-Span-1824">)</span><span id="MJXp-Span-1825"><span id="MJXp-Span-1826">p</span><span id="MJXp-Span-1827">(</span><span id="MJXp-Span-1828">θ</span><span id="MJXp-Span-1829">|</span><span id="MJXp-Span-1830"><span id="MJXp-Span-1831">ξ</span><span id="MJXp-Span-1832">t</span></span><span id="MJXp-Span-1833">)</span></span><span id="MJXp-Span-1834">d</span><span id="MJXp-Span-1835">θ</span></span></span></span></span></span></span><span id="MJXp-Span-1836"><span id="MJXp-Span-1837"><span id="MJXp-Span-1838"><span id="MJXp-Span-1839">; red part is hard to compute directly.</span></span></span></span></span></span></span></span></span></p>

<p>Since it is difficult to compute <span><span id="MJXp-Span-1840"><span id="MJXp-Span-1841">p</span><span id="MJXp-Span-1842">(</span><span id="MJXp-Span-1843">θ</span><span id="MJXp-Span-1844">|</span><span id="MJXp-Span-1845"><span id="MJXp-Span-1846">ξ</span><span id="MJXp-Span-1847">t</span></span><span id="MJXp-Span-1848">)</span></span></span><span id="MathJax-Element-109-Frame" tabindex="0"></span> directly, a natural choice is to approximate it with an alternative distribution <span><span id="MJXp-Span-1849"><span id="MJXp-Span-1850"><span id="MJXp-Span-1851">q</span><span id="MJXp-Span-1852">ϕ</span></span><span id="MJXp-Span-1853">(</span><span id="MJXp-Span-1854">θ</span><span id="MJXp-Span-1855">)</span></span></span><span id="MathJax-Element-110-Frame" tabindex="0"></span>. With variational lower bound, we know the maximization of <span><span id="MJXp-Span-1856"><span id="MJXp-Span-1857"><span id="MJXp-Span-1858">q</span><span id="MJXp-Span-1859">ϕ</span></span><span id="MJXp-Span-1860">(</span><span id="MJXp-Span-1861">θ</span><span id="MJXp-Span-1862">)</span></span></span><span id="MathJax-Element-111-Frame" tabindex="0"></span> is equivalent to maximizing <span><span id="MJXp-Span-1863"><span id="MJXp-Span-1864">p</span><span id="MJXp-Span-1865">(</span><span id="MJXp-Span-1866"><span id="MJXp-Span-1867">ξ</span><span id="MJXp-Span-1868">t</span></span><span id="MJXp-Span-1869">|</span><span id="MJXp-Span-1870">θ</span><span id="MJXp-Span-1871">)</span></span></span><span id="MathJax-Element-112-Frame" tabindex="0"></span> and minimizing <span><span id="MJXp-Span-1872"><span id="MJXp-Span-1873"><span id="MJXp-Span-1874">D</span><span id="MJXp-Span-1875">KL</span></span><span id="MJXp-Span-1876">[</span><span id="MJXp-Span-1877"><span id="MJXp-Span-1878">q</span><span id="MJXp-Span-1879">ϕ</span></span><span id="MJXp-Span-1880">(</span><span id="MJXp-Span-1881">θ</span><span id="MJXp-Span-1882">)</span><span id="MJXp-Span-1883">‖</span><span id="MJXp-Span-1884">p</span><span id="MJXp-Span-1885">(</span><span id="MJXp-Span-1886">θ</span><span id="MJXp-Span-1887">)</span><span id="MJXp-Span-1888">]</span></span></span><span id="MathJax-Element-113-Frame" tabindex="0"></span>.</p>

<p>Using the approximation distribution <span><span id="MJXp-Span-1889"><span id="MJXp-Span-1890">q</span></span></span><span id="MathJax-Element-114-Frame" tabindex="0"></span>, the intrinsic reward becomes:</p>

<p><span><span id="MJXp-Span-1891"><span id="MJXp-Span-1892"><span id="MJXp-Span-1893">r</span><span><span><span><span><span id="MJXp-Span-1895">i</span></span></span></span><span><span><span><span id="MJXp-Span-1894">t</span></span></span></span></span></span><span id="MJXp-Span-1896">=</span><span id="MJXp-Span-1897"><span id="MJXp-Span-1898">D</span><span id="MJXp-Span-1899">KL</span></span><span id="MJXp-Span-1900">[</span><span id="MJXp-Span-1901"><span id="MJXp-Span-1902">q</span><span id="MJXp-Span-1903"><span id="MJXp-Span-1904"><span id="MJXp-Span-1905">ϕ</span><span id="MJXp-Span-1906"><span id="MJXp-Span-1907">t</span><span id="MJXp-Span-1908">+</span><span id="MJXp-Span-1909">1</span></span></span></span></span><span id="MJXp-Span-1910">(</span><span id="MJXp-Span-1911">θ</span><span id="MJXp-Span-1912">)</span><span id="MJXp-Span-1913">‖</span><span id="MJXp-Span-1914"><span id="MJXp-Span-1915">q</span><span id="MJXp-Span-1916"><span id="MJXp-Span-1917"><span id="MJXp-Span-1918">ϕ</span><span id="MJXp-Span-1919">t</span></span></span></span><span id="MJXp-Span-1920">(</span><span id="MJXp-Span-1921">θ</span><span id="MJXp-Span-1922">)</span><span id="MJXp-Span-1923">)</span><span id="MJXp-Span-1924">]</span></span></span></p>

<p>where <span><span id="MJXp-Span-1925"><span id="MJXp-Span-1926"><span id="MJXp-Span-1927">ϕ</span><span id="MJXp-Span-1928"><span id="MJXp-Span-1929">t</span><span id="MJXp-Span-1930">+</span><span id="MJXp-Span-1931">1</span></span></span></span></span><span id="MathJax-Element-116-Frame" tabindex="0"></span> represents <span><span id="MJXp-Span-1932"><span id="MJXp-Span-1933">q</span></span></span><span id="MathJax-Element-117-Frame" tabindex="0"></span>’s parameters associated with the new relief after seeing <span><span id="MJXp-Span-1934"><span id="MJXp-Span-1935"><span id="MJXp-Span-1936">a</span><span id="MJXp-Span-1937">t</span></span></span></span><span id="MathJax-Element-118-Frame" tabindex="0"></span> and <span><span id="MJXp-Span-1938"><span id="MJXp-Span-1939"><span id="MJXp-Span-1940">s</span><span id="MJXp-Span-1941"><span id="MJXp-Span-1942">t</span><span id="MJXp-Span-1943">+</span><span id="MJXp-Span-1944">1</span></span></span></span></span><span id="MathJax-Element-119-Frame" tabindex="0"></span>. When used as an exploration bonus, it is normalized by division by the moving median of this KL divergence value.</p>

<p>Here the dynamics model is parameterized as a <a href="https://link.springer.com/book/10.1007/978-1-4612-0745-0">Bayesian neural network</a> (BNN), as it maintains a distribution over its weights. The BNN weight distribution <span><span id="MJXp-Span-1945"><span id="MJXp-Span-1946"><span id="MJXp-Span-1947">q</span><span id="MJXp-Span-1948">ϕ</span></span><span id="MJXp-Span-1949">(</span><span id="MJXp-Span-1950">θ</span><span id="MJXp-Span-1951">)</span></span></span><span id="MathJax-Element-120-Frame" tabindex="0"></span> is modeled as a fully <em>factorized</em> Gaussian with <span><span id="MJXp-Span-1952"><span id="MJXp-Span-1953">ϕ</span><span id="MJXp-Span-1954">=</span><span id="MJXp-Span-1955">{</span><span id="MJXp-Span-1956">μ</span><span id="MJXp-Span-1957">,</span><span id="MJXp-Span-1958">σ</span><span id="MJXp-Span-1959">}</span></span></span><span id="MathJax-Element-121-Frame" tabindex="0"></span> and we can easily sample <span><span id="MJXp-Span-1960"><span id="MJXp-Span-1961">θ</span><span id="MJXp-Span-1962">∼</span><span id="MJXp-Span-1963"><span id="MJXp-Span-1964">q</span><span id="MJXp-Span-1965">ϕ</span></span><span id="MJXp-Span-1966">(</span><span id="MJXp-Span-1967">.</span><span id="MJXp-Span-1968">)</span></span></span><span id="MathJax-Element-122-Frame" tabindex="0"></span>. After applying a second-order Taylor expansion, the KL term <span><span id="MJXp-Span-1969"><span id="MJXp-Span-1970"><span id="MJXp-Span-1971">D</span><span id="MJXp-Span-1972">KL</span></span><span id="MJXp-Span-1973">[</span><span id="MJXp-Span-1974"><span id="MJXp-Span-1975">q</span><span id="MJXp-Span-1976"><span id="MJXp-Span-1977">ϕ</span><span id="MJXp-Span-1978">+</span><span id="MJXp-Span-1979">λ</span><span id="MJXp-Span-1980">Δ</span><span id="MJXp-Span-1981">ϕ</span></span></span><span id="MJXp-Span-1982">(</span><span id="MJXp-Span-1983">θ</span><span id="MJXp-Span-1984">)</span><span id="MJXp-Span-1985">‖</span><span id="MJXp-Span-1986"><span id="MJXp-Span-1987">q</span><span id="MJXp-Span-1988"><span id="MJXp-Span-1989">ϕ</span></span></span><span id="MJXp-Span-1990">(</span><span id="MJXp-Span-1991">θ</span><span id="MJXp-Span-1992">)</span><span id="MJXp-Span-1993">]</span></span></span><span id="MathJax-Element-123-Frame" tabindex="0"></span> can be estimated using <a href="https://lilianweng.github.io/lil-log/2019/09/05/evolution-strategies.html#estimation-using-fisher-information-matrix">Fisher Information Matrix</a> <span><span id="MJXp-Span-1994"><span id="MJXp-Span-1995"><span id="MJXp-Span-1996"><span id="MJXp-Span-1997">F</span></span><span id="MJXp-Span-1998">ϕ</span></span></span></span><span id="MathJax-Element-124-Frame" tabindex="0"></span>, which is easy to compute, because <span><span id="MJXp-Span-1999"><span id="MJXp-Span-2000"><span id="MJXp-Span-2001">q</span><span id="MJXp-Span-2002">ϕ</span></span></span></span><span id="MathJax-Element-125-Frame" tabindex="0"></span> is factorized Gaussian and thus the covariance matrix is only a diagonal matrix. See more details in <a href="https://arxiv.org/abs/1605.09674">the paper</a>, especially section 2.3-2.5.</p>

<h4 id="random-networks">Random Networks</h4>

<p>But, what if the prediction task is not about the environment dynamics at all? It turns out when the prediction is for a random task, it still can help exploration.</p>

<p><strong>DORA</strong> (short for <em>“Directed Outreaching Reinforcement Action-Selection”</em>; <a href="https://arxiv.org/abs/1804.04012">Fox &amp; Choshen, et al. 2018</a>) is a novel framework that injects exploration signals based on a newly introduced, <strong>task-independent</strong> MDP. The idea of DORA depends on two parallel MDPs:</p>
<ul>
  <li>One is the original task MDP;</li>
  <li>The other is an identical MDP but with <em>no reward attached</em>: Rather, every state-action pair is designed to have value 0. The Q-value learned for the second MDP is called <em>E-value</em>. If the model cannot perfectly predict E-value to be zero, it is still missing information.</li>
</ul>

<p>Initially E-value is assigned with value 1. Such positive initialization can encourage directed exploration for better E-value prediction. State-action pairs with high E-value estimation don’t have enough information gathered yet, at least not enough to exclude their high E-values. To some extent, the logarithm of E-values can be considered as a generalization of <em>visit counters</em>.</p>

<p>When using a neural network to do function approximation for E-value, another value head is added to predict E-value and it is simply expected to predict zero. Given a predicted E-value <span><span id="MJXp-Span-2003"><span id="MJXp-Span-2004">E</span><span id="MJXp-Span-2005">(</span><span id="MJXp-Span-2006"><span id="MJXp-Span-2007">s</span><span id="MJXp-Span-2008">t</span></span><span id="MJXp-Span-2009">,</span><span id="MJXp-Span-2010"><span id="MJXp-Span-2011">a</span><span id="MJXp-Span-2012">t</span></span><span id="MJXp-Span-2013">)</span></span></span><span id="MathJax-Element-126-Frame" tabindex="0"></span>, the exploration bonus is <span><span id="MJXp-Span-2014"><span id="MJXp-Span-2015"><span id="MJXp-Span-2016">r</span><span><span><span><span><span id="MJXp-Span-2018">i</span></span></span></span><span><span><span><span id="MJXp-Span-2017">t</span></span></span></span></span></span><span id="MJXp-Span-2019">=</span><span id="MJXp-Span-2020"><span><span id="MJXp-Span-2021">1</span></span><span><span><span><span></span></span><span><span><span id="MJXp-Span-2022"><span><span>√</span></span><span><span></span><span><span id="MJXp-Span-2023">−</span><span id="MJXp-Span-2024">log</span><span id="MJXp-Span-2025"></span><span id="MJXp-Span-2026">E</span><span id="MJXp-Span-2027">(</span><span id="MJXp-Span-2028"><span id="MJXp-Span-2029">s</span><span id="MJXp-Span-2030">t</span></span><span id="MJXp-Span-2031">,</span><span id="MJXp-Span-2032"><span id="MJXp-Span-2033">a</span><span id="MJXp-Span-2034">t</span></span><span id="MJXp-Span-2035">)</span></span></span></span></span></span></span></span></span></span></span><span id="MathJax-Element-127-Frame" tabindex="0"></span>.</p>

<p><a name="RND"></a>Similar to DORA, <strong>Random Network Distillation</strong> (<strong>RND</strong>; <a href="https://arxiv.org/abs/1810.12894">Burda, et al. 2018</a>) introduces a prediction task <em>independent of the main task</em>. The RND exploration bonus is defined as the error of a neural network <span><span id="MJXp-Span-2036"><span id="MJXp-Span-2037"><span id="MJXp-Span-2038"><span><span><span><span id="MJXp-Span-2040">ˆ</span></span><span><span id="MJXp-Span-2039">f</span></span></span></span></span></span><span id="MJXp-Span-2041">(</span><span id="MJXp-Span-2042"><span id="MJXp-Span-2043">s</span><span id="MJXp-Span-2044">t</span></span><span id="MJXp-Span-2045">)</span></span></span><span id="MathJax-Element-128-Frame" tabindex="0"></span> predicting features of the observations given by a <em>fixed randomly initialized</em> neural network <span><span id="MJXp-Span-2046"><span id="MJXp-Span-2047">f</span><span id="MJXp-Span-2048">(</span><span id="MJXp-Span-2049"><span id="MJXp-Span-2050">s</span><span id="MJXp-Span-2051">t</span></span><span id="MJXp-Span-2052">)</span></span></span><span id="MathJax-Element-129-Frame" tabindex="0"></span>. The motivation is that given a new state, if similar states have been visited many times in the past, the prediction should be easier and thus has lower error. The exploration bonus is <span><span id="MJXp-Span-2053"><span id="MJXp-Span-2054"><span id="MJXp-Span-2055">r</span><span id="MJXp-Span-2056">i</span></span><span id="MJXp-Span-2057">(</span><span id="MJXp-Span-2058"><span id="MJXp-Span-2059">s</span><span id="MJXp-Span-2060">t</span></span><span id="MJXp-Span-2061">)</span><span id="MJXp-Span-2062">=</span><span id="MJXp-Span-2063">‖</span><span id="MJXp-Span-2064"><span id="MJXp-Span-2065"><span><span><span><span id="MJXp-Span-2067">ˆ</span></span><span><span id="MJXp-Span-2066">f</span></span></span></span></span></span><span id="MJXp-Span-2068">(</span><span id="MJXp-Span-2069"><span id="MJXp-Span-2070">s</span><span id="MJXp-Span-2071">t</span></span><span id="MJXp-Span-2072">;</span><span id="MJXp-Span-2073">θ</span><span id="MJXp-Span-2074">)</span><span id="MJXp-Span-2075">−</span><span id="MJXp-Span-2076">f</span><span id="MJXp-Span-2077">(</span><span id="MJXp-Span-2078"><span id="MJXp-Span-2079">s</span><span id="MJXp-Span-2080">t</span></span><span id="MJXp-Span-2081">)</span><span id="MJXp-Span-2082"><span id="MJXp-Span-2083">‖</span><span><span><span><span><span id="MJXp-Span-2085">2</span></span></span></span><span><span><span><span id="MJXp-Span-2084">2</span></span></span></span></span></span></span></span><span id="MathJax-Element-130-Frame" tabindex="0"></span>.</p>

<p><img src="https://lilianweng.github.io/lil-log/assets/images/RND.png" alt="RND"></p>
<p><em>Fig. 7. How RND (Random Network Distillation) works for providing an intrinsic reward. The features <span><span id="MJXp-Span-2086"><span id="MJXp-Span-2087"><span id="MJXp-Span-2088">O</span><span id="MJXp-Span-2089"><span id="MJXp-Span-2090">i</span><span id="MJXp-Span-2091">+</span><span id="MJXp-Span-2092">1</span></span></span><span id="MJXp-Span-2093">↦</span><span id="MJXp-Span-2094"><span id="MJXp-Span-2095">f</span><span id="MJXp-Span-2096"><span id="MJXp-Span-2097">i</span><span id="MJXp-Span-2098">+</span><span id="MJXp-Span-2099">1</span></span></span></span></span><span id="MathJax-Element-131-Frame" tabindex="0"></span> are generated by a fixed random neural network. (Image source: <a href="https://openai.com/blog/reinforcement-learning-with-prediction-based-rewards/">OpenAI Blog: “Reinforcement Learning with Prediction-Based Rewards”</a>)</em></p>

<p>Two factors are important in RND experiments:</p>
<ol>
  <li>Non-episodic setting results in better exploration, especially when not using any extrinsic rewards. It means that the return is not truncated at “Game over” and intrinsic return can spread across multiple episodes.</li>
  <li>Normalization is important since the scale of the reward is tricky to adjust given a random neural network as a prediction target. The intrinsic reward is normalized by division by a running estimate of the standard deviations of the intrinsic return.</li>
</ol>

<p>The RND setup works well for resolving the hard-exploration problem. For example, maximizing the RND exploration bonus consistently finds more than half of the rooms in Montezuma’s Revenge.</p>

<h4 id="physical-properties">Physical Properties</h4>

<p>Different from games in simulators, some RL applications like Robotics need to understand objects and intuitive reasoning in the physical world. Some prediction tasks require the agent to perform a sequence of interactions with the environment and to observe the corresponding consequences, such as estimating some hidden properties in physics (e.g. mass, friction, etc).</p>

<p>Motivated by such ideas, <a href="https://arxiv.org/abs/1611.01843">Denil, et al. (2017)</a> found that DRL agents can learn to perform necessary exploration to discover such hidden properties. Precisely they considered two experiments:</p>
<ol>
  <li><em>“Which is heavier?”</em> — The agent has to interact with the blocks and infer which one is heavier.</li>
  <li><em>“Towers”</em> — The agent needs to infer how many rigid bodies a tower is composed of by knocking it down.</li>
</ol>

<p>The agent in the experiments first goes through an exploration phase to interact with the environment and to collect information. Once the exploration phase ends, the agent is asked to output a <em>labeling</em> action to answer the question. Then a positive reward is assigned to the agent if the answer is correct; otherwise a negative one is assigned. Because the answer requires a decent amount of interactions with items in the scene, the agent has to learn to efficiently play around so as to figure out the physics and the correct answer. The exploration naturally happens.</p>

<p>In their experiments, the agent is able to learn in both tasks with performance varied by the difficulty of the task. Although the paper didn’t use the physics prediction task to provide intrinsic reward bonus along with extrinsic reward associated with another learning task, rather it focused on the exploration tasks themselves. I do enjoy the idea of encouraging sophisticated exploration behavior by predicting hidden physics properties in the environment.</p>

<h2 id="memory-based-exploration">Memory-based Exploration</h2>

<p>Reward-based exploration suffers from several drawbacks:</p>
<ul>
  <li>Function approximation is slow to catch up.</li>
  <li>Exploration bonus is non-stationary.</li>
  <li>Knowledge fading, meaning that states cease to be novel and cannot provide intrinsic reward signals in time.</li>
</ul>

<p>Methods in this section rely on external memory to resolve disadvantages of reward bonus-based exploration.</p>

<h3 id="episodic-memory">Episodic Memory</h3>

<p>As mentioned above, <a href="#RND">RND</a> is better running in an non-episodic setting, meaning the prediction knowledge is accumulated across multiple episodes. The exploration strategy, <strong>Never Give Up</strong> (<strong>NGU</strong>; <a href="https://arxiv.org/abs/2002.06038">Badia, et al. 2020a</a>), combines an episodic novelty module that can rapidly adapt within one episode with RND as a lifelong novelty module.</p>

<p>Precisely, the intrinsic reward in NGU consists of two exploration bonuses from two modules,  <em>within one episode</em> and <em>across multiple episodes</em>, respectively.</p>

<p>The short-term per-episode reward is provided by an <em>episodic novelty module</em>. It contains an episodic memory <span><span id="MJXp-Span-2100"><span id="MJXp-Span-2101">M</span></span></span><span id="MathJax-Element-132-Frame" tabindex="0"></span>, a dynamically-sized slot-based memory, and an IDF (inverse dynamics features) embedding function <span><span id="MJXp-Span-2102"><span id="MJXp-Span-2103">ϕ</span></span></span><span id="MathJax-Element-133-Frame" tabindex="0"></span>, same as the feature encoding in <a href="#ICM">ICM</a></p>
<ol>
  <li>At every step the current state embedding <span><span id="MJXp-Span-2104"><span id="MJXp-Span-2105">ϕ</span><span id="MJXp-Span-2106">(</span><span id="MJXp-Span-2107"><span id="MJXp-Span-2108">s</span><span id="MJXp-Span-2109">t</span></span><span id="MJXp-Span-2110">)</span></span></span><span id="MathJax-Element-134-Frame" tabindex="0"></span> is added into <span><span id="MJXp-Span-2111"><span id="MJXp-Span-2112">M</span></span></span><span id="MathJax-Element-135-Frame" tabindex="0"></span>.</li>
  <li>The intrinsic bonus is determined by comparing how similar the current observation is to the content of <span><span id="MJXp-Span-2113"><span id="MJXp-Span-2114">M</span></span></span><span id="MathJax-Element-136-Frame" tabindex="0"></span>. A larger difference results in a larger bonus.
<br>
<span><span id="MJXp-Span-2115"><span id="MJXp-Span-2116"><span id="MJXp-Span-2117">r</span><span><span><span><span><span id="MJXp-Span-2119">episodic</span></span></span></span><span><span><span><span id="MJXp-Span-2118">t</span></span></span></span></span></span><span id="MJXp-Span-2120">≈</span><span id="MJXp-Span-2121"><span><span id="MJXp-Span-2122">1</span></span><span><span><span><span></span></span><span><span><span id="MJXp-Span-2123"><span><span>√</span></span><span><span></span><span><span id="MJXp-Span-2124"><span id="MJXp-Span-2125">∑</span><span id="MJXp-Span-2126"><span id="MJXp-Span-2127"><span id="MJXp-Span-2128">ϕ</span><span id="MJXp-Span-2129">i</span></span><span id="MJXp-Span-2130">∈</span><span id="MJXp-Span-2131"><span id="MJXp-Span-2132">N</span><span id="MJXp-Span-2133">k</span></span></span></span><span id="MJXp-Span-2134">K</span><span id="MJXp-Span-2135">(</span><span id="MJXp-Span-2136">ϕ</span><span id="MJXp-Span-2137">(</span><span id="MJXp-Span-2138"><span id="MJXp-Span-2139">x</span><span id="MJXp-Span-2140">t</span></span><span id="MJXp-Span-2141">)</span><span id="MJXp-Span-2142">,</span><span id="MJXp-Span-2143"><span id="MJXp-Span-2144">ϕ</span><span id="MJXp-Span-2145">i</span></span><span id="MJXp-Span-2146">)</span></span></span></span><span id="MJXp-Span-2147">+</span><span id="MJXp-Span-2148">c</span></span></span></span></span></span></span></span><span id="MathJax-Element-137-Frame" tabindex="0"></span>
<br>
where <span><span id="MJXp-Span-2149"><span id="MJXp-Span-2150">K</span><span id="MJXp-Span-2151">(</span><span id="MJXp-Span-2152">x</span><span id="MJXp-Span-2153">,</span><span id="MJXp-Span-2154">y</span><span id="MJXp-Span-2155">)</span></span></span><span id="MathJax-Element-138-Frame" tabindex="0"></span> is a kernel function for measuring the distance between two samples. <span><span id="MJXp-Span-2156"><span id="MJXp-Span-2157"><span id="MJXp-Span-2158">N</span><span id="MJXp-Span-2159">k</span></span></span></span><span id="MathJax-Element-139-Frame" tabindex="0"></span> is a set of <span><span id="MJXp-Span-2160"><span id="MJXp-Span-2161">k</span></span></span><span id="MathJax-Element-140-Frame" tabindex="0"></span> nearest neighbors in <span><span id="MJXp-Span-2162"><span id="MJXp-Span-2163">M</span></span></span><span id="MathJax-Element-141-Frame" tabindex="0"></span> according to <span><span id="MJXp-Span-2164"><span id="MJXp-Span-2165">K</span><span id="MJXp-Span-2166">(</span><span id="MJXp-Span-2167">.</span><span id="MJXp-Span-2168">,</span><span id="MJXp-Span-2169">.</span><span id="MJXp-Span-2170">)</span></span></span><span id="MathJax-Element-142-Frame" tabindex="0"></span>.  <span><span id="MJXp-Span-2171"><span id="MJXp-Span-2172">c</span></span></span><span id="MathJax-Element-143-Frame" tabindex="0"></span> is a small constant to keep the denominator non-zero. In the paper, <span><span id="MJXp-Span-2173"><span id="MJXp-Span-2174">K</span><span id="MJXp-Span-2175">(</span><span id="MJXp-Span-2176">x</span><span id="MJXp-Span-2177">,</span><span id="MJXp-Span-2178">y</span><span id="MJXp-Span-2179">)</span></span></span><span id="MathJax-Element-144-Frame" tabindex="0"></span> is configured to be the inverse kernel:
<br>
<span><span id="MJXp-Span-2180"><span id="MJXp-Span-2181">K</span><span id="MJXp-Span-2182">(</span><span id="MJXp-Span-2183">x</span><span id="MJXp-Span-2184">,</span><span id="MJXp-Span-2185">y</span><span id="MJXp-Span-2186">)</span><span id="MJXp-Span-2187">=</span><span id="MJXp-Span-2188"><span><span id="MJXp-Span-2189">ϵ</span></span><span><span><span><span></span></span><span><span><span id="MJXp-Span-2190"><span><span id="MJXp-Span-2191"><span id="MJXp-Span-2192">d</span><span id="MJXp-Span-2193">2</span></span><span id="MJXp-Span-2194">(</span><span id="MJXp-Span-2195">x</span><span id="MJXp-Span-2196">,</span><span id="MJXp-Span-2197">y</span><span id="MJXp-Span-2198">)</span></span><span><span><span><span></span></span><span><span><span id="MJXp-Span-2199"><span id="MJXp-Span-2200">d</span><span><span><span><span><span id="MJXp-Span-2202">2</span></span></span></span><span><span><span><span id="MJXp-Span-2201">m</span></span></span></span></span></span></span></span></span></span></span><span id="MJXp-Span-2203">+</span><span id="MJXp-Span-2204">ϵ</span></span></span></span></span></span></span></span><span id="MathJax-Element-145-Frame" tabindex="0"></span>
<br>
where <span><span id="MJXp-Span-2205"><span id="MJXp-Span-2206">d</span><span id="MJXp-Span-2207">(</span><span id="MJXp-Span-2208">.</span><span id="MJXp-Span-2209">,</span><span id="MJXp-Span-2210">.</span><span id="MJXp-Span-2211">)</span></span></span><span id="MathJax-Element-146-Frame" tabindex="0"></span> is Euclidean distance between two samples and <span><span id="MJXp-Span-2212"><span id="MJXp-Span-2213"><span id="MJXp-Span-2214">d</span><span id="MJXp-Span-2215">m</span></span></span></span><span id="MathJax-Element-147-Frame" tabindex="0"></span> is a running average of the squared Euclidean distance of the k-th nearest neighbors for better robustness. <span><span id="MJXp-Span-2216"><span id="MJXp-Span-2217">ϵ</span></span></span><span id="MathJax-Element-148-Frame" tabindex="0"></span> is a small constant.</li>
</ol>

<p><img src="https://lilianweng.github.io/lil-log/assets/images/NGU.png" alt="RND"></p>
<p><em>Fig. 8. The architecture of NGU’s embedding function (left) and reward generator (right). (Image source: <a href="https://arxiv.org/abs/2002.06038">Badia, et al. 2020a</a>)</em></p>

<p>The long-term across-episode novelty relies on RND prediction error in <em>life-long novelty module</em>. The exploration bonus is <span><span id="MJXp-Span-2218"><span id="MJXp-Span-2219"><span id="MJXp-Span-2220">α</span><span id="MJXp-Span-2221">t</span></span><span id="MJXp-Span-2222">=</span><span id="MJXp-Span-2223">1</span><span id="MJXp-Span-2224">+</span><span id="MJXp-Span-2225"><span><span id="MJXp-Span-2226"><span id="MJXp-Span-2227">e</span><span id="MJXp-Span-2228">RND</span></span><span id="MJXp-Span-2229">(</span><span id="MJXp-Span-2230"><span id="MJXp-Span-2231">s</span><span id="MJXp-Span-2232">t</span></span><span id="MJXp-Span-2233">)</span><span id="MJXp-Span-2234">−</span><span id="MJXp-Span-2235"><span id="MJXp-Span-2236">μ</span><span id="MJXp-Span-2237">e</span></span></span><span><span><span><span></span></span><span><span><span id="MJXp-Span-2238"><span id="MJXp-Span-2239">σ</span><span id="MJXp-Span-2240">e</span></span></span></span></span></span></span></span></span><span id="MathJax-Element-149-Frame" tabindex="0"></span> where <span><span id="MJXp-Span-2241"><span id="MJXp-Span-2242"><span id="MJXp-Span-2243">μ</span><span id="MJXp-Span-2244">e</span></span></span></span><span id="MathJax-Element-150-Frame" tabindex="0"></span> and <span><span id="MJXp-Span-2245"><span id="MJXp-Span-2246"><span id="MJXp-Span-2247">σ</span><span id="MJXp-Span-2248">e</span></span></span></span><span id="MathJax-Element-151-Frame" tabindex="0"></span> are running mean and std dev for RND error <span><span id="MJXp-Span-2249"><span id="MJXp-Span-2250"><span id="MJXp-Span-2251">e</span><span id="MJXp-Span-2252">RND</span></span><span id="MJXp-Span-2253">(</span><span id="MJXp-Span-2254"><span id="MJXp-Span-2255">s</span><span id="MJXp-Span-2256">t</span></span><span id="MJXp-Span-2257">)</span></span></span><span id="MathJax-Element-152-Frame" tabindex="0"></span>.</p>

<blockquote>
  <p>However in the conclusion section of the <a href="https://arxiv.org/abs/1810.12894">RND paper</a>, I noticed the following statement:</p>

  <p>“We find that the RND exploration bonus is sufficient to deal with local exploration, i.e. exploring the consequences of short-term decisions, like whether to interact with a particular object, or avoid it. However global exploration that involves coordinated decisions over long time horizons is beyond the reach of our method. “</p>

  <p>And this confuses me a bit how RND can be used as a good life-long novelty bonus provider. If you know why, feel free to leave a comment below.</p>
</blockquote>

<p>The final combined intrinsic reward is <span><span id="MJXp-Span-2258"><span id="MJXp-Span-2259"><span id="MJXp-Span-2260">r</span><span><span><span><span><span id="MJXp-Span-2262">i</span></span></span></span><span><span><span><span id="MJXp-Span-2261">t</span></span></span></span></span></span><span id="MJXp-Span-2263">=</span><span id="MJXp-Span-2264"><span id="MJXp-Span-2265">r</span><span><span><span><span><span id="MJXp-Span-2267">episodic</span></span></span></span><span><span><span><span id="MJXp-Span-2266">t</span></span></span></span></span></span><span id="MJXp-Span-2268">⋅</span><span id="MJXp-Span-2269">clip</span><span id="MJXp-Span-2270">(</span><span id="MJXp-Span-2271"><span id="MJXp-Span-2272">α</span><span id="MJXp-Span-2273">t</span></span><span id="MJXp-Span-2274">,</span><span id="MJXp-Span-2275">1</span><span id="MJXp-Span-2276">,</span><span id="MJXp-Span-2277">L</span><span id="MJXp-Span-2278">)</span></span></span><span id="MathJax-Element-153-Frame" tabindex="0"></span>, where <span><span id="MJXp-Span-2279"><span id="MJXp-Span-2280">L</span></span></span><span id="MathJax-Element-154-Frame" tabindex="0"></span> is a constant maximum reward scalar.</p>

<p>The design of NGU enables it to have two nice properties:</p>
<ol>
  <li><em>Rapidly discourages</em> revisiting the same state <em>within</em> the same episode;</li>
  <li><em>Slowly discourages</em> revisiting states that have been visited many times <em>across</em> episodes.</li>
</ol>

<p>Later, built on top of NGU, DeepMind proposed “Agent57” (<a href="https://arxiv.org/abs/2003.13350">Badia, et al. 2020b</a>), the first deep RL agent that outperforms the standard human benchmark on <em>all</em> 57 Atari games. Two major improvements in Agent57 over NGU are:</p>
<ol>
  <li>A <em>population</em> of policies are trained in Agent57, each equipped with a different exploration parameter pair <span><span id="MJXp-Span-2281"><span id="MJXp-Span-2282">{</span><span id="MJXp-Span-2283">(</span><span id="MJXp-Span-2284"><span id="MJXp-Span-2285">β</span><span id="MJXp-Span-2286">j</span></span><span id="MJXp-Span-2287">,</span><span id="MJXp-Span-2288"><span id="MJXp-Span-2289">γ</span><span id="MJXp-Span-2290">j</span></span><span id="MJXp-Span-2291">)</span><span id="MJXp-Span-2292"><span id="MJXp-Span-2293">}</span><span><span><span><span><span id="MJXp-Span-2298">N</span></span></span></span><span><span><span><span id="MJXp-Span-2294"><span id="MJXp-Span-2295">j</span><span id="MJXp-Span-2296">=</span><span id="MJXp-Span-2297">1</span></span></span></span></span></span></span></span></span><span id="MathJax-Element-155-Frame" tabindex="0"></span>. Recall that given <span><span id="MJXp-Span-2299"><span id="MJXp-Span-2300"><span id="MJXp-Span-2301">β</span><span id="MJXp-Span-2302">j</span></span></span></span><span id="MathJax-Element-156-Frame" tabindex="0"></span>, the reward is constructed as <span><span id="MJXp-Span-2303"><span id="MJXp-Span-2304"><span id="MJXp-Span-2305">r</span><span id="MJXp-Span-2306"><span id="MJXp-Span-2307">j</span><span id="MJXp-Span-2308">,</span><span id="MJXp-Span-2309">t</span></span></span><span id="MJXp-Span-2310">=</span><span id="MJXp-Span-2311"><span id="MJXp-Span-2312">r</span><span><span><span><span><span id="MJXp-Span-2314">e</span></span></span></span><span><span><span><span id="MJXp-Span-2313">t</span></span></span></span></span></span><span id="MJXp-Span-2315">+</span><span id="MJXp-Span-2316"><span id="MJXp-Span-2317">β</span><span id="MJXp-Span-2318">j</span></span><span id="MJXp-Span-2319"><span id="MJXp-Span-2320">r</span><span><span><span><span><span id="MJXp-Span-2322">i</span></span></span></span><span><span><span><span id="MJXp-Span-2321">t</span></span></span></span></span></span></span></span><span id="MathJax-Element-157-Frame" tabindex="0"></span> and <span><span id="MJXp-Span-2323"><span id="MJXp-Span-2324"><span id="MJXp-Span-2325">γ</span><span id="MJXp-Span-2326">j</span></span></span></span><span id="MathJax-Element-158-Frame" tabindex="0"></span> is the reward discounting factor. It is natural to expect policies with higher <span><span id="MJXp-Span-2327"><span id="MJXp-Span-2328"><span id="MJXp-Span-2329">β</span><span id="MJXp-Span-2330">j</span></span></span></span><span id="MathJax-Element-159-Frame" tabindex="0"></span> and lower <span><span id="MJXp-Span-2331"><span id="MJXp-Span-2332"><span id="MJXp-Span-2333">γ</span><span id="MJXp-Span-2334">j</span></span></span></span><span id="MathJax-Element-160-Frame" tabindex="0"></span> to make more progress early in training, while the opposite would be expected as training progresses. A meta-controller (<a href="https://arxiv.org/pdf/0805.3415.pdf">sliding-window UCB bandit algorithm</a>) is trained to select which policies should be prioritized.</li>
  <li>The second improvement is a new parameterization of Q-value function that decomposes the contributions of the intrinsic and extrinsic rewards in a similar form as the bundled reward: <span><span id="MJXp-Span-2335"><span id="MJXp-Span-2336">Q</span><span id="MJXp-Span-2337">(</span><span id="MJXp-Span-2338">s</span><span id="MJXp-Span-2339">,</span><span id="MJXp-Span-2340">a</span><span id="MJXp-Span-2341">;</span><span id="MJXp-Span-2342"><span id="MJXp-Span-2343">θ</span><span id="MJXp-Span-2344">j</span></span><span id="MJXp-Span-2345">)</span><span id="MJXp-Span-2346">=</span><span id="MJXp-Span-2347">Q</span><span id="MJXp-Span-2348">(</span><span id="MJXp-Span-2349">s</span><span id="MJXp-Span-2350">,</span><span id="MJXp-Span-2351">a</span><span id="MJXp-Span-2352">;</span><span id="MJXp-Span-2353"><span id="MJXp-Span-2354">θ</span><span><span><span><span><span id="MJXp-Span-2356">e</span></span></span></span><span><span><span><span id="MJXp-Span-2355">j</span></span></span></span></span></span><span id="MJXp-Span-2357">)</span><span id="MJXp-Span-2358">+</span><span id="MJXp-Span-2359"><span id="MJXp-Span-2360">β</span><span id="MJXp-Span-2361">j</span></span><span id="MJXp-Span-2362">Q</span><span id="MJXp-Span-2363">(</span><span id="MJXp-Span-2364">s</span><span id="MJXp-Span-2365">,</span><span id="MJXp-Span-2366">a</span><span id="MJXp-Span-2367">;</span><span id="MJXp-Span-2368"><span id="MJXp-Span-2369">θ</span><span><span><span><span><span id="MJXp-Span-2371">i</span></span></span></span><span><span><span><span id="MJXp-Span-2370">j</span></span></span></span></span></span><span id="MJXp-Span-2372">)</span></span></span><span id="MathJax-Element-161-Frame" tabindex="0"></span>. During training, <span><span id="MJXp-Span-2373"><span id="MJXp-Span-2374">Q</span><span id="MJXp-Span-2375">(</span><span id="MJXp-Span-2376">s</span><span id="MJXp-Span-2377">,</span><span id="MJXp-Span-2378">a</span><span id="MJXp-Span-2379">;</span><span id="MJXp-Span-2380"><span id="MJXp-Span-2381">θ</span><span><span><span><span><span id="MJXp-Span-2383">e</span></span></span></span><span><span><span><span id="MJXp-Span-2382">j</span></span></span></span></span></span><span id="MJXp-Span-2384">)</span></span></span><span id="MathJax-Element-162-Frame" tabindex="0"></span> and <span><span id="MJXp-Span-2385"><span id="MJXp-Span-2386">Q</span><span id="MJXp-Span-2387">(</span><span id="MJXp-Span-2388">s</span><span id="MJXp-Span-2389">,</span><span id="MJXp-Span-2390">a</span><span id="MJXp-Span-2391">;</span><span id="MJXp-Span-2392"><span id="MJXp-Span-2393">θ</span><span><span><span><span><span id="MJXp-Span-2395">i</span></span></span></span><span><span><span><span id="MJXp-Span-2394">j</span></span></span></span></span></span><span id="MJXp-Span-2396">)</span></span></span><span id="MathJax-Element-163-Frame" tabindex="0"></span> are optimized separately with rewards <span><span id="MJXp-Span-2397"><span id="MJXp-Span-2398"><span id="MJXp-Span-2399">r</span><span><span><span><span><span id="MJXp-Span-2401">e</span></span></span></span><span><span><span><span id="MJXp-Span-2400">j</span></span></span></span></span></span></span></span><span id="MathJax-Element-164-Frame" tabindex="0"></span> and <span><span id="MJXp-Span-2402"><span id="MJXp-Span-2403"><span id="MJXp-Span-2404">r</span><span><span><span><span><span id="MJXp-Span-2406">i</span></span></span></span><span><span><span><span id="MJXp-Span-2405">j</span></span></span></span></span></span></span></span><span id="MathJax-Element-165-Frame" tabindex="0"></span>, respectively.</li>
</ol>

<p><img src="https://lilianweng.github.io/lil-log/assets/images/agent57.png" alt="Agent57"></p>
<p><em>Fig. 9. A pretty cool illustration of techniques developed in time since DQN in 2015, eventually leading to Agent57. (Image source: <a href="https://deepmind.com/blog/article/Agent57-Outperforming-the-human-Atari-benchmark">DeepMind Blog: “Agent57: Outperforming the human Atari benchmark”</a>)</em></p>

<p>Instead of using the Euclidean distance to measure closeness of states in episodic memory, <a href="https://arxiv.org/abs/1810.02274">Savinov, et al. (2019)</a> took the transition between states into consideration and proposed a method to measure the number of steps needed to visit one state from other states in memory, named <strong>Episodic Curiosity (EC)</strong> module. The novelty bonus depends on reachability between states.</p>

<ol>
  <li>At the beginning of each episode, the agent starts with an empty episodic memory <span><span id="MJXp-Span-2407"><span id="MJXp-Span-2408">M</span></span></span><span id="MathJax-Element-166-Frame" tabindex="0"></span>.</li>
  <li>At every step, the agent compares the current state with saved states in memory to determine novelty bonus: If the current state is novel (i.e., takes more steps to reach from observations in memory than a threshold), the agent gets a bonus.</li>
  <li>The current state is added into the episodic memory if the novelty bonus is high enough. (Imagine that if all the states were added into memory, any new state could be added within 1 step.)</li>
  <li>Repeat 1-3 until the end of this episode.</li>
</ol>

<p><img src="https://lilianweng.github.io/lil-log/assets/images/transition-graph.png" alt="Transition graph"></p>
<p><em>Fig. 10. The nodes in the graph are states, the edges are possible transitions. The blue nodes are states in memory. The green nodes are reachable from the memory within <span><span id="MJXp-Span-2409"><span id="MJXp-Span-2410">k</span><span id="MJXp-Span-2411">=</span><span id="MJXp-Span-2412">2</span></span></span><span id="MathJax-Element-167-Frame" tabindex="0"></span> steps (not novel). The orange nodes are further away, so they are considered as novel states. (Image source: <a href="https://arxiv.org/abs/1810.02274">Savinov, et al. 2019</a>)</em></p>

<p>In order to estimate reachability between states, we need to access the transition graph, which is unfortunately not entirely known. Thus, <a href="https://arxiv.org/abs/1810.02274">Savinov, et al. (2019)</a> trained a <a href="https://lilianweng.github.io/lil-log/2018/11/30/meta-learning.html#convolutional-siamese-neural-network">siamese</a> neural network to predict how many steps separate two states. It contains one embedding network <span><span id="MJXp-Span-2413"><span id="MJXp-Span-2414">ϕ</span><span id="MJXp-Span-2415">:</span><span id="MJXp-Span-2416"><span id="MJXp-Span-2417">S</span></span><span id="MJXp-Span-2418">↦</span><span id="MJXp-Span-2419"><span id="MJXp-Span-2420"><span id="MJXp-Span-2421">R</span></span><span id="MJXp-Span-2422">n</span></span></span></span><span id="MathJax-Element-168-Frame" tabindex="0"></span> to first encode the states to feature vectors and then one comparator network <span><span id="MJXp-Span-2423"><span id="MJXp-Span-2424">C</span><span id="MJXp-Span-2425">:</span><span id="MJXp-Span-2426"><span id="MJXp-Span-2427"><span id="MJXp-Span-2428">R</span></span><span id="MJXp-Span-2429">n</span></span><span id="MJXp-Span-2430">×</span><span id="MJXp-Span-2431"><span id="MJXp-Span-2432"><span id="MJXp-Span-2433">R</span></span><span id="MJXp-Span-2434">n</span></span><span id="MJXp-Span-2435">↦</span><span id="MJXp-Span-2436">[</span><span id="MJXp-Span-2437">0</span><span id="MJXp-Span-2438">,</span><span id="MJXp-Span-2439">1</span><span id="MJXp-Span-2440">]</span></span></span><span id="MathJax-Element-169-Frame" tabindex="0"></span> to output a binary label on whether two states are close enough (i.e., reachable within <span><span id="MJXp-Span-2441"><span id="MJXp-Span-2442">k</span></span></span><span id="MathJax-Element-170-Frame" tabindex="0"></span> steps) in the transition graph, <span><span id="MJXp-Span-2443"><span id="MJXp-Span-2444">C</span><span id="MJXp-Span-2445">(</span><span id="MJXp-Span-2446">ϕ</span><span id="MJXp-Span-2447">(</span><span id="MJXp-Span-2448"><span id="MJXp-Span-2449">s</span><span id="MJXp-Span-2450">i</span></span><span id="MJXp-Span-2451">)</span><span id="MJXp-Span-2452">,</span><span id="MJXp-Span-2453">ϕ</span><span id="MJXp-Span-2454">(</span><span id="MJXp-Span-2455"><span id="MJXp-Span-2456">s</span><span id="MJXp-Span-2457">j</span></span><span id="MJXp-Span-2458">)</span><span id="MJXp-Span-2459">)</span><span id="MJXp-Span-2460">↦</span><span id="MJXp-Span-2461">[</span><span id="MJXp-Span-2462">0</span><span id="MJXp-Span-2463">,</span><span id="MJXp-Span-2464">1</span><span id="MJXp-Span-2465">]</span></span></span><span id="MathJax-Element-171-Frame" tabindex="0"></span>.</p>

<p>An episodic memory buffer <span><span id="MJXp-Span-2466"><span id="MJXp-Span-2467">M</span></span></span><span id="MathJax-Element-172-Frame" tabindex="0"></span> stores embeddings of some past observations within the same episode. A new observation will be compared with existing state embeddings via <span><span id="MJXp-Span-2468"><span id="MJXp-Span-2469">C</span></span></span><span id="MathJax-Element-173-Frame" tabindex="0"></span> and the results are aggregated (e.g. max, 90th percentile) to provide a reachability score <span><span id="MJXp-Span-2470"><span id="MJXp-Span-2471"><span id="MJXp-Span-2472">C</span><span id="MJXp-Span-2473">M</span></span><span id="MJXp-Span-2474">(</span><span id="MJXp-Span-2475">ϕ</span><span id="MJXp-Span-2476">(</span><span id="MJXp-Span-2477"><span id="MJXp-Span-2478">s</span><span id="MJXp-Span-2479">t</span></span><span id="MJXp-Span-2480">)</span><span id="MJXp-Span-2481">)</span></span></span><span id="MathJax-Element-174-Frame" tabindex="0"></span>. The exploration bonus is <span><span id="MJXp-Span-2482"><span id="MJXp-Span-2483"><span id="MJXp-Span-2484">r</span><span><span><span><span><span id="MJXp-Span-2486">i</span></span></span></span><span><span><span><span id="MJXp-Span-2485">t</span></span></span></span></span></span><span id="MJXp-Span-2487">=</span><span id="MJXp-Span-2488"><span id="MJXp-Span-2489"><span>(</span></span></span><span id="MJXp-Span-2490"><span id="MJXp-Span-2491">C</span><span id="MJXp-Span-2492">′</span></span><span id="MJXp-Span-2493">−</span><span id="MJXp-Span-2494"><span id="MJXp-Span-2495">C</span><span id="MJXp-Span-2496">M</span></span><span id="MJXp-Span-2497">(</span><span id="MJXp-Span-2498">f</span><span id="MJXp-Span-2499">(</span><span id="MJXp-Span-2500"><span id="MJXp-Span-2501">s</span><span id="MJXp-Span-2502">t</span></span><span id="MJXp-Span-2503">)</span><span id="MJXp-Span-2504">)</span><span id="MJXp-Span-2505"><span id="MJXp-Span-2506"><span>)</span></span></span></span></span><span id="MathJax-Element-175-Frame" tabindex="0"></span>, where <span><span id="MJXp-Span-2507"><span id="MJXp-Span-2508"><span id="MJXp-Span-2509">C</span><span id="MJXp-Span-2510">′</span></span></span></span><span id="MathJax-Element-176-Frame" tabindex="0"></span> is a predefined threshold for determining the sign of the reward (e.g. <span><span id="MJXp-Span-2511"><span id="MJXp-Span-2512"><span id="MJXp-Span-2513">C</span><span id="MJXp-Span-2514">′</span></span><span id="MJXp-Span-2515">=</span><span id="MJXp-Span-2516">0.5</span></span></span><span id="MathJax-Element-177-Frame" tabindex="0"></span> works well for fixed-duration episodes). High bonus is awarded to new states when they are not easily reachable from states in the memory buffer.</p>

<p>They claimed that the EC module can overcome the <a href="#the-noisy-tv-problem">noisy-TV</a> problem.</p>

<p><img src="https://lilianweng.github.io/lil-log/assets/images/episodic-memory-overview.png" alt="EC module"></p>
<p><em>Fig. 11. The architecture of episodic curiosity (EC) module for intrinsic reward generation.  (Image source: <a href="https://arxiv.org/abs/1810.02274">Savinov, et al. 2019</a>)</em></p>

<h3 id="direct-exploration">Direct Exploration</h3>

<p><strong>Go-Explore</strong> (<a href="https://arxiv.org/abs/1901.10995">Ecoffet, et al., 2019</a>) is an algorithm aiming to solve the “hard-exploration” problem. It is composed of the following two phases.</p>

<p><strong>Phase 1 (“Explore until solved”)</strong> feels quite like <a href="https://en.wikipedia.org/wiki/Dijkstra%27s_algorithm">Dijkstra’s algorithm</a> for finding shortest paths in a graph. Indeed, no neural network is involved in phase 1. By maintaining a memory of interesting states as well as trajectories leading to them, the agent can go back (given a simulator is <em>deterministic</em>) to promising states and continue doing <em>random</em> exploration from there. The state is mapped into a short discretized code (named “cell”) in order to be memorized. The memory is updated if a new state appears or a better/shorter trajectory is found. When selecting which past states to return to, the agent might select one in the memory uniformly or according to heuristics like recency, visit count, count of neighbors in the memory, etc. This process is repeated until the task is solved and at least one solution trajectory is found.</p>

<p>The above found high-performance trajectories would not work well on evaluation envs with any stochasticity. Thus, <strong>Phase 2 (“Robustification”)</strong> is needed to robustify the solution via imitation learning. They adopted <a href="https://arxiv.org/abs/1812.03381">Backward Algorithm</a>, in which the agent is started near the last state in the trajectory and then runs RL optimization from there.</p>

<p>One important note in phase 1 is: In order to go back to a state deterministically without exploration, Go-Explore depends on a resettable and deterministic simulator, which is a big disadvantage.</p>

<p>To make the algorithm more generally useful to environments with stochasticity, an enhanced version of Go-Explore (<a href="https://arxiv.org/abs/2004.12919">Ecoffet, et al., 2020</a>), named <strong>policy-based Go-Explore</strong> was proposed later.</p>
<ul>
  <li>Instead of resetting the simulator state effortlessly, the policy-based Go-Explore learns a <em>goal-conditioned policy</em> and uses that to access a known state in memory repeatedly. The goal-conditioned policy is trained to follow the best trajectory that previously led to the selected states in memory. They include a <strong>Self-Imitation Learning</strong> (<strong>SIL</strong>; <a href="https://arxiv.org/abs/1806.05635">Oh, et al. 2018</a>) loss to help extract as much information as possible from successful trajectories.</li>
  <li>Also, they found sampling from policy works better than random actions when the agent returns to promising states to continue exploration.</li>
  <li>Another improvement in policy-based Go-Explore is to make the downscaling function of images to cells adjustable. It is optimized so that there would be neither too many nor too few cells in the memory.</li>
</ul>

<p><img src="https://lilianweng.github.io/lil-log/assets/images/policy-based-Go-Explore.png" alt="Policy-based Go-Explore"></p>
<p><em>Fig. 12. An overview of the Go-Explore algorithm. (Image source: <a href="https://arxiv.org/abs/2004.12919">Ecoffet, et al., 2020</a>)</em></p>

<p>After vanilla Go-Explore, <a href="https://arxiv.org/abs/1907.10247">Yijie Guo, et al. (2019)</a> proposed <strong>DTSIL</strong> (Diverse Trajectory-conditioned Self-Imitation Learning), which shared a similar idea as policy-based Go-Explore above. DTSIL maintains a memory of diverse demonstrations collected during training and uses them to train a trajectory-conditioned policy via <a href="https://arxiv.org/abs/1806.05635">SIL</a>. They prioritize trajectories that end with a rare state during sampling.</p>

<p><img src="https://lilianweng.github.io/lil-log/assets/images/DTSIL-algo.png" alt="DTSIL"></p>
<p><em>Fig. 13. Algorithm of DTSIL (Diverse Trajectory-conditioned Self-Imitation Learning). (Image source: <a href="https://arxiv.org/abs/1907.10247">Yijie Guo, et al. 2019</a>)</em></p>

<p>The similar approach is also seen in <a href="https://arxiv.org/abs/1906.07805">Guo, et al. (2019)</a>. The main idea is to store goals with <em>high uncertainty</em> in memory so that later the agent can revisit these goal states with a goal-conditioned policy repeatedly. In each episode, the agent flips a coin (probability 0.5) to decide whether it will act greedily w.r.t. the policy or do directed exploration by sampling goals from the memory.</p>

<p><img src="https://lilianweng.github.io/lil-log/assets/images/directed-exploration.png" alt="Directed exploration"></p>
<p><em>Fig. 14. Different components in directed exploration with function approximation. (Image source: <a href="https://arxiv.org/abs/1906.07805">Guo, et al. 2019</a>)</em></p>

<p>The uncertainty measure of a state can be something simple like count-based bonuses or something complex like density or bayesian models. The paper trained a forward dynamics model and took its prediction error as the uncertainty metric.</p>

<h2 id="q-value-exploration">Q-Value Exploration</h2>

<p>Inspired by <a href="https://lilianweng.github.io/lil-log/2018/01/23/the-multi-armed-bandit-problem-and-its-solutions.html#thompson-sampling">Thompson sampling</a>, <strong>Bootstrapped DQN</strong> (<a href="https://arxiv.org/abs/1602.04621">Osband, et al. 2016</a>) introduces a notion of uncertainty in Q-value approximation in classic <a href="https://lilianweng.github.io/lil-log/2018/02/19/a-long-peek-into-reinforcement-learning.html#deep-q-network">DQN</a> by using the <a href="https://en.wikipedia.org/wiki/Bootstrapping_(statistics)">bootstrapping</a> method. Bootstrapping is to approximate a distribution by sampling with replacement from the same population multiple times and then aggregate the results.</p>

<p>Multiple Q-value heads are trained in parallel but each only consumes a bootstrapped sub-sampled set of data and each has its own corresponding target network. All the Q-value heads share the same backbone network.</p>

<p><img src="https://lilianweng.github.io/lil-log/assets/images/bootstrapped-DQN-algo.png" alt="Bootstrapped DQN"></p>
<p><em>Fig. 15. The algorithm of Bootstrapped DQN. (Image source: <a href="https://arxiv.org/abs/1602.04621">Osband, et al. 2016</a>)</em></p>

<p>At the beginning of one episode, one Q-value head is sampled uniformly and acts for collecting experience data in this episode. Then a binary mask is sampled from the masking distribution <span><span id="MJXp-Span-2517"><span id="MJXp-Span-2518">m</span><span id="MJXp-Span-2519">∼</span><span id="MJXp-Span-2520"><span id="MJXp-Span-2521">M</span></span></span></span><span id="MathJax-Element-178-Frame" tabindex="0"></span> and decides which heads can use this data for training. The choice of masking distribution <span><span id="MJXp-Span-2522"><span id="MJXp-Span-2523"><span id="MJXp-Span-2524">M</span></span></span></span><span id="MathJax-Element-179-Frame" tabindex="0"></span> determines how bootstrapped samples are generated; For example,</p>
<ul>
  <li>If <span><span id="MJXp-Span-2525"><span id="MJXp-Span-2526"><span id="MJXp-Span-2527">M</span></span></span></span><span id="MathJax-Element-180-Frame" tabindex="0"></span> is an independent Bernoulli distribution with <span><span id="MJXp-Span-2528"><span id="MJXp-Span-2529">p</span><span id="MJXp-Span-2530">=</span><span id="MJXp-Span-2531">0.5</span></span></span><span id="MathJax-Element-181-Frame" tabindex="0"></span>, this corresponds to the double-or-nothing bootstrap.</li>
  <li>If <span><span id="MJXp-Span-2532"><span id="MJXp-Span-2533"><span id="MJXp-Span-2534">M</span></span></span></span><span id="MathJax-Element-182-Frame" tabindex="0"></span> always returns an all-one mask, the algorithm reduces to an ensemble method.</li>
</ul>

<p>However, this kind of exploration is still restricted, because uncertainty introduced by bootstrapping fully relies on the training data. It is better to inject some prior information independent of the data. This “noisy” prior is expected to drive the agent to keep exploring when the reward is sparse. The algorithm of adding random prior into bootstrapped DQN for better exploration (<a href="https://arxiv.org/abs/1806.03335">Osband, et al. 2018</a>) depends on Bayesian linear regression. The core idea of Bayesian regression is: We can <em>“generate posterior samples by training on noisy versions of the data, together with some random regularization”</em>.</p>

<p>Let <span><span id="MJXp-Span-2535"><span id="MJXp-Span-2536">θ</span></span></span><span id="MathJax-Element-183-Frame" tabindex="0"></span> be the Q function parameter and <span><span id="MJXp-Span-2537"><span id="MJXp-Span-2538"><span id="MJXp-Span-2539">θ</span><span id="MJXp-Span-2540">−</span></span></span></span><span id="MathJax-Element-184-Frame" tabindex="0"></span> for the target Q, the loss function using a randomized prior function <span><span id="MJXp-Span-2541"><span id="MJXp-Span-2542">p</span></span></span><span id="MathJax-Element-185-Frame" tabindex="0"></span> is:</p>

<p><span><span id="MJXp-Span-2543"><span id="MJXp-Span-2544"><span id="MJXp-Span-2545">L</span></span><span id="MJXp-Span-2546">(</span><span id="MJXp-Span-2547">θ</span><span id="MJXp-Span-2548">,</span><span id="MJXp-Span-2549"><span id="MJXp-Span-2550">θ</span><span id="MJXp-Span-2551"><span id="MJXp-Span-2552">−</span></span></span><span id="MJXp-Span-2553">,</span><span id="MJXp-Span-2554">p</span><span id="MJXp-Span-2555">,</span><span id="MJXp-Span-2556"><span id="MJXp-Span-2557">D</span></span><span id="MJXp-Span-2558">;</span><span id="MJXp-Span-2559">γ</span><span id="MJXp-Span-2560">)</span><span id="MJXp-Span-2561">=</span><span id="MJXp-Span-2562"><span><span id="MJXp-Span-2563"><span>∑</span></span></span><span><span id="MJXp-Span-2564"><span id="MJXp-Span-2565">t</span><span id="MJXp-Span-2566">∈</span><span id="MJXp-Span-2567"><span id="MJXp-Span-2568">D</span></span></span></span></span><span id="MJXp-Span-2569"><span id="MJXp-Span-2570"><span>(</span></span></span><span id="MJXp-Span-2571"><span id="MJXp-Span-2572">r</span><span id="MJXp-Span-2573">t</span></span><span id="MJXp-Span-2574">+</span><span id="MJXp-Span-2575">γ</span><span id="MJXp-Span-2576"><span><span id="MJXp-Span-2577">max</span></span><span><span id="MJXp-Span-2578"><span id="MJXp-Span-2579"><span id="MJXp-Span-2580">a</span><span id="MJXp-Span-2581">′</span></span><span id="MJXp-Span-2582">∈</span><span id="MJXp-Span-2583"><span id="MJXp-Span-2584">A</span></span></span></span></span><span id="MJXp-Span-2585">(</span><span id="MJXp-Span-2586"><span><span id="MJXp-Span-2587"><span id="MJXp-Span-2588"><span><span id="MJXp-Span-2589"><span id="MJXp-Span-2590">Q</span><span id="MJXp-Span-2591"><span id="MJXp-Span-2592"><span id="MJXp-Span-2593">θ</span><span id="MJXp-Span-2594">−</span></span></span></span><span id="MJXp-Span-2595">+</span><span id="MJXp-Span-2596">p</span><span id="MJXp-Span-2597">)</span></span><span><span id="MJXp-Span-2598">⏟</span></span></span></span></span><span><span id="MJXp-Span-2599">target Q</span></span></span><span id="MJXp-Span-2600">(</span><span id="MJXp-Span-2601"><span id="MJXp-Span-2602">s</span><span><span><span><span><span id="MJXp-Span-2604">′</span></span></span></span><span><span><span><span id="MJXp-Span-2603">t</span></span></span></span></span></span><span id="MJXp-Span-2605">,</span><span id="MJXp-Span-2606"><span id="MJXp-Span-2607">a</span><span id="MJXp-Span-2608">′</span></span><span id="MJXp-Span-2609">)</span><span id="MJXp-Span-2610">−</span><span id="MJXp-Span-2611"><span><span id="MJXp-Span-2612"><span id="MJXp-Span-2613"><span><span id="MJXp-Span-2614">(</span><span id="MJXp-Span-2615"><span id="MJXp-Span-2616">Q</span><span id="MJXp-Span-2617">θ</span></span><span id="MJXp-Span-2618">+</span><span id="MJXp-Span-2619">p</span><span id="MJXp-Span-2620">)</span></span><span><span id="MJXp-Span-2621">⏟</span></span></span></span></span><span><span id="MJXp-Span-2622">Q to optimize</span></span></span><span id="MJXp-Span-2623">(</span><span id="MJXp-Span-2624"><span id="MJXp-Span-2625">s</span><span id="MJXp-Span-2626">t</span></span><span id="MJXp-Span-2627">,</span><span id="MJXp-Span-2628"><span id="MJXp-Span-2629">a</span><span id="MJXp-Span-2630">t</span></span><span id="MJXp-Span-2631">)</span><span id="MJXp-Span-2632"><span id="MJXp-Span-2633"><span id="MJXp-Span-2634"><span>)</span></span></span><span id="MJXp-Span-2635">2</span></span></span></span></p>

<h2 id="varitional-options">Varitional Options</h2>

<p>Options are policies with termination conditions. There are a large set of options available in the search space and they are independent of an agent’s intentions. By explicitly including intrinsic options into modeling, the agent can obtain intrinsic rewards for exploration.</p>

<p><strong>VIC</strong> (short for <em>“Variational Intrinsic Control”</em>; <a href="https://arxiv.org/abs/1611.07507">Gregor, et al. 2017</a>) is such a framework for providing the agent with intrinsic exploration bonuses based on modeling options and learning policies conditioned on options. Let <span><span id="MJXp-Span-2636"><span id="MJXp-Span-2637">Ω</span></span></span><span id="MathJax-Element-187-Frame" tabindex="0"></span> represent an option which starts from <span><span id="MJXp-Span-2638"><span id="MJXp-Span-2639"><span id="MJXp-Span-2640">s</span><span id="MJXp-Span-2641">0</span></span></span></span><span id="MathJax-Element-188-Frame" tabindex="0"></span> and ends at <span><span id="MJXp-Span-2642"><span id="MJXp-Span-2643"><span id="MJXp-Span-2644">s</span><span id="MJXp-Span-2645">f</span></span></span></span><span id="MathJax-Element-189-Frame" tabindex="0"></span>. An environment probability distribution <span><span id="MJXp-Span-2646"><span id="MJXp-Span-2647"><span id="MJXp-Span-2648">p</span><span id="MJXp-Span-2649">J</span></span><span id="MJXp-Span-2650">(</span><span id="MJXp-Span-2651"><span id="MJXp-Span-2652">s</span><span id="MJXp-Span-2653">f</span></span><span id="MJXp-Span-2654">|</span><span id="MJXp-Span-2655"><span id="MJXp-Span-2656">s</span><span id="MJXp-Span-2657">0</span></span><span id="MJXp-Span-2658">,</span><span id="MJXp-Span-2659">Ω</span><span id="MJXp-Span-2660">)</span></span></span><span id="MathJax-Element-190-Frame" tabindex="0"></span> defines where an option <span><span id="MJXp-Span-2661"><span id="MJXp-Span-2662">Ω</span></span></span><span id="MathJax-Element-191-Frame" tabindex="0"></span> terminates given a starting state <span><span id="MJXp-Span-2663"><span id="MJXp-Span-2664"><span id="MJXp-Span-2665">s</span><span id="MJXp-Span-2666">0</span></span></span></span><span id="MathJax-Element-192-Frame" tabindex="0"></span>. A controllability distribution <span><span id="MJXp-Span-2667"><span id="MJXp-Span-2668"><span id="MJXp-Span-2669">p</span><span id="MJXp-Span-2670">C</span></span><span id="MJXp-Span-2671">(</span><span id="MJXp-Span-2672">Ω</span><span id="MJXp-Span-2673">|</span><span id="MJXp-Span-2674"><span id="MJXp-Span-2675">s</span><span id="MJXp-Span-2676">0</span></span><span id="MJXp-Span-2677">)</span></span></span><span id="MathJax-Element-193-Frame" tabindex="0"></span> defines the probability distribution of options we can sample from. And by definition we have <span><span id="MJXp-Span-2678"><span id="MJXp-Span-2679">p</span><span id="MJXp-Span-2680">(</span><span id="MJXp-Span-2681"><span id="MJXp-Span-2682">s</span><span id="MJXp-Span-2683">f</span></span><span id="MJXp-Span-2684">,</span><span id="MJXp-Span-2685">Ω</span><span id="MJXp-Span-2686">|</span><span id="MJXp-Span-2687"><span id="MJXp-Span-2688">s</span><span id="MJXp-Span-2689">0</span></span><span id="MJXp-Span-2690">)</span><span id="MJXp-Span-2691">=</span><span id="MJXp-Span-2692"><span id="MJXp-Span-2693">p</span><span id="MJXp-Span-2694">J</span></span><span id="MJXp-Span-2695">(</span><span id="MJXp-Span-2696"><span id="MJXp-Span-2697">s</span><span id="MJXp-Span-2698">f</span></span><span id="MJXp-Span-2699">|</span><span id="MJXp-Span-2700"><span id="MJXp-Span-2701">s</span><span id="MJXp-Span-2702">0</span></span><span id="MJXp-Span-2703">,</span><span id="MJXp-Span-2704">Ω</span><span id="MJXp-Span-2705">)</span><span id="MJXp-Span-2706"><span id="MJXp-Span-2707">p</span><span id="MJXp-Span-2708">C</span></span><span id="MJXp-Span-2709">(</span><span id="MJXp-Span-2710">Ω</span><span id="MJXp-Span-2711">|</span><span id="MJXp-Span-2712"><span id="MJXp-Span-2713">s</span><span id="MJXp-Span-2714">0</span></span><span id="MJXp-Span-2715">)</span></span></span><span id="MathJax-Element-194-Frame" tabindex="0"></span>.</p>

<p>While choosing options, we would like to achieve two goals:</p>
<ul>
  <li>Achieve a diverse set of the final states from <span><span id="MJXp-Span-2716"><span id="MJXp-Span-2717"><span id="MJXp-Span-2718">s</span><span id="MJXp-Span-2719">0</span></span></span></span><span id="MathJax-Element-195-Frame" tabindex="0"></span> ⇨ Maximization of <span><span id="MJXp-Span-2720"><span id="MJXp-Span-2721">H</span><span id="MJXp-Span-2722">(</span><span id="MJXp-Span-2723"><span id="MJXp-Span-2724">s</span><span id="MJXp-Span-2725">f</span></span><span id="MJXp-Span-2726">|</span><span id="MJXp-Span-2727"><span id="MJXp-Span-2728">s</span><span id="MJXp-Span-2729">0</span></span><span id="MJXp-Span-2730">)</span></span></span><span id="MathJax-Element-196-Frame" tabindex="0"></span>.</li>
  <li>Know precisely which state a given option <span><span id="MJXp-Span-2731"><span id="MJXp-Span-2732">Ω</span></span></span><span id="MathJax-Element-197-Frame" tabindex="0"></span> can end with ⇨ Minimization of <span><span id="MJXp-Span-2733"><span id="MJXp-Span-2734">H</span><span id="MJXp-Span-2735">(</span><span id="MJXp-Span-2736"><span id="MJXp-Span-2737">s</span><span id="MJXp-Span-2738">f</span></span><span id="MJXp-Span-2739">|</span><span id="MJXp-Span-2740"><span id="MJXp-Span-2741">s</span><span id="MJXp-Span-2742">0</span></span><span id="MJXp-Span-2743">,</span><span id="MJXp-Span-2744">Ω</span><span id="MJXp-Span-2745">)</span></span></span><span id="MathJax-Element-198-Frame" tabindex="0"></span>.</li>
</ul>

<p>Combining them, we get mutual information <span><span id="MJXp-Span-2746"><span id="MJXp-Span-2747">I</span><span id="MJXp-Span-2748">(</span><span id="MJXp-Span-2749">Ω</span><span id="MJXp-Span-2750">;</span><span id="MJXp-Span-2751"><span id="MJXp-Span-2752">s</span><span id="MJXp-Span-2753">f</span></span><span id="MJXp-Span-2754">|</span><span id="MJXp-Span-2755"><span id="MJXp-Span-2756">s</span><span id="MJXp-Span-2757">0</span></span><span id="MJXp-Span-2758">)</span></span></span><span id="MathJax-Element-199-Frame" tabindex="0"></span> to maximize:</p>

<p><span><span id="MJXp-Span-2759"><span id="MJXp-Span-2760"><span><span id="MJXp-Span-2761"><span id="MJXp-Span-2762"><span id="MJXp-Span-2763">I</span><span id="MJXp-Span-2764">(</span><span id="MJXp-Span-2765">Ω</span><span id="MJXp-Span-2766">;</span><span id="MJXp-Span-2767"><span id="MJXp-Span-2768">s</span><span id="MJXp-Span-2769">f</span></span><span id="MJXp-Span-2770">|</span><span id="MJXp-Span-2771"><span id="MJXp-Span-2772">s</span><span id="MJXp-Span-2773">0</span></span><span id="MJXp-Span-2774">)</span></span><span id="MJXp-Span-2775"><span id="MJXp-Span-2776"></span><span id="MJXp-Span-2777">=</span><span id="MJXp-Span-2778">H</span><span id="MJXp-Span-2779">(</span><span id="MJXp-Span-2780"><span id="MJXp-Span-2781">s</span><span id="MJXp-Span-2782">f</span></span><span id="MJXp-Span-2783">|</span><span id="MJXp-Span-2784"><span id="MJXp-Span-2785">s</span><span id="MJXp-Span-2786">0</span></span><span id="MJXp-Span-2787">)</span><span id="MJXp-Span-2788">−</span><span id="MJXp-Span-2789">H</span><span id="MJXp-Span-2790">(</span><span id="MJXp-Span-2791"><span id="MJXp-Span-2792">s</span><span id="MJXp-Span-2793">f</span></span><span id="MJXp-Span-2794">|</span><span id="MJXp-Span-2795"><span id="MJXp-Span-2796">s</span><span id="MJXp-Span-2797">0</span></span><span id="MJXp-Span-2798">,</span><span id="MJXp-Span-2799">Ω</span><span id="MJXp-Span-2800">)</span></span></span><span id="MJXp-Span-2801"><span id="MJXp-Span-2802"></span><span id="MJXp-Span-2803"><span id="MJXp-Span-2804"></span><span id="MJXp-Span-2805">=</span><span id="MJXp-Span-2806">−</span><span id="MJXp-Span-2807"><span><span id="MJXp-Span-2808"><span>∑</span></span></span><span><span id="MJXp-Span-2809"><span id="MJXp-Span-2810"><span id="MJXp-Span-2811">s</span><span id="MJXp-Span-2812">f</span></span></span></span></span><span id="MJXp-Span-2813">p</span><span id="MJXp-Span-2814">(</span><span id="MJXp-Span-2815"><span id="MJXp-Span-2816">s</span><span id="MJXp-Span-2817">f</span></span><span id="MJXp-Span-2818">|</span><span id="MJXp-Span-2819"><span id="MJXp-Span-2820">s</span><span id="MJXp-Span-2821">0</span></span><span id="MJXp-Span-2822">)</span><span id="MJXp-Span-2823">log</span><span id="MJXp-Span-2824"></span><span id="MJXp-Span-2825">p</span><span id="MJXp-Span-2826">(</span><span id="MJXp-Span-2827"><span id="MJXp-Span-2828">s</span><span id="MJXp-Span-2829">f</span></span><span id="MJXp-Span-2830">|</span><span id="MJXp-Span-2831"><span id="MJXp-Span-2832">s</span><span id="MJXp-Span-2833">0</span></span><span id="MJXp-Span-2834">)</span><span id="MJXp-Span-2835">+</span><span id="MJXp-Span-2836"><span><span id="MJXp-Span-2837"><span>∑</span></span></span><span><span id="MJXp-Span-2838"><span id="MJXp-Span-2839"><span id="MJXp-Span-2840">s</span><span id="MJXp-Span-2841">f</span></span><span id="MJXp-Span-2842">,</span><span id="MJXp-Span-2843">Ω</span></span></span></span><span id="MJXp-Span-2844">p</span><span id="MJXp-Span-2845">(</span><span id="MJXp-Span-2846"><span id="MJXp-Span-2847">s</span><span id="MJXp-Span-2848">f</span></span><span id="MJXp-Span-2849">,</span><span id="MJXp-Span-2850">Ω</span><span id="MJXp-Span-2851">|</span><span id="MJXp-Span-2852"><span id="MJXp-Span-2853">s</span><span id="MJXp-Span-2854">0</span></span><span id="MJXp-Span-2855">)</span><span id="MJXp-Span-2856">log</span><span id="MJXp-Span-2857"></span><span id="MJXp-Span-2858"><span><span id="MJXp-Span-2859">p</span><span id="MJXp-Span-2860">(</span><span id="MJXp-Span-2861"><span id="MJXp-Span-2862">s</span><span id="MJXp-Span-2863">f</span></span><span id="MJXp-Span-2864">,</span><span id="MJXp-Span-2865">Ω</span><span id="MJXp-Span-2866">|</span><span id="MJXp-Span-2867"><span id="MJXp-Span-2868">s</span><span id="MJXp-Span-2869">0</span></span><span id="MJXp-Span-2870">)</span></span><span><span><span><span></span></span><span><span><span id="MJXp-Span-2871"><span id="MJXp-Span-2872">p</span><span id="MJXp-Span-2873">C</span></span><span id="MJXp-Span-2874">(</span><span id="MJXp-Span-2875">Ω</span><span id="MJXp-Span-2876">|</span><span id="MJXp-Span-2877"><span id="MJXp-Span-2878">s</span><span id="MJXp-Span-2879">0</span></span><span id="MJXp-Span-2880">)</span></span></span></span></span></span></span></span><span id="MJXp-Span-2881"><span id="MJXp-Span-2882"></span><span id="MJXp-Span-2883"><span id="MJXp-Span-2884"></span><span id="MJXp-Span-2885">=</span><span id="MJXp-Span-2886">−</span><span id="MJXp-Span-2887"><span><span id="MJXp-Span-2888"><span>∑</span></span></span><span><span id="MJXp-Span-2889"><span id="MJXp-Span-2890"><span id="MJXp-Span-2891">s</span><span id="MJXp-Span-2892">f</span></span></span></span></span><span id="MJXp-Span-2893">p</span><span id="MJXp-Span-2894">(</span><span id="MJXp-Span-2895"><span id="MJXp-Span-2896">s</span><span id="MJXp-Span-2897">f</span></span><span id="MJXp-Span-2898">|</span><span id="MJXp-Span-2899"><span id="MJXp-Span-2900">s</span><span id="MJXp-Span-2901">0</span></span><span id="MJXp-Span-2902">)</span><span id="MJXp-Span-2903">log</span><span id="MJXp-Span-2904"></span><span id="MJXp-Span-2905">p</span><span id="MJXp-Span-2906">(</span><span id="MJXp-Span-2907"><span id="MJXp-Span-2908">s</span><span id="MJXp-Span-2909">f</span></span><span id="MJXp-Span-2910">|</span><span id="MJXp-Span-2911"><span id="MJXp-Span-2912">s</span><span id="MJXp-Span-2913">0</span></span><span id="MJXp-Span-2914">)</span><span id="MJXp-Span-2915">+</span><span id="MJXp-Span-2916"><span><span id="MJXp-Span-2917"><span>∑</span></span></span><span><span id="MJXp-Span-2918"><span id="MJXp-Span-2919"><span id="MJXp-Span-2920">s</span><span id="MJXp-Span-2921">f</span></span><span id="MJXp-Span-2922">,</span><span id="MJXp-Span-2923">Ω</span></span></span></span><span id="MJXp-Span-2924"><span id="MJXp-Span-2925">p</span><span id="MJXp-Span-2926">J</span></span><span id="MJXp-Span-2927">(</span><span id="MJXp-Span-2928"><span id="MJXp-Span-2929">s</span><span id="MJXp-Span-2930">f</span></span><span id="MJXp-Span-2931">|</span><span id="MJXp-Span-2932"><span id="MJXp-Span-2933">s</span><span id="MJXp-Span-2934">0</span></span><span id="MJXp-Span-2935">,</span><span id="MJXp-Span-2936">Ω</span><span id="MJXp-Span-2937">)</span><span id="MJXp-Span-2938"><span id="MJXp-Span-2939">p</span><span id="MJXp-Span-2940">C</span></span><span id="MJXp-Span-2941">(</span><span id="MJXp-Span-2942">Ω</span><span id="MJXp-Span-2943">|</span><span id="MJXp-Span-2944"><span id="MJXp-Span-2945">s</span><span id="MJXp-Span-2946">0</span></span><span id="MJXp-Span-2947">)</span><span id="MJXp-Span-2948">log</span><span id="MJXp-Span-2949"></span><span id="MJXp-Span-2950"><span id="MJXp-Span-2951">p</span><span id="MJXp-Span-2952">J</span></span><span id="MJXp-Span-2953">(</span><span id="MJXp-Span-2954"><span id="MJXp-Span-2955">s</span><span id="MJXp-Span-2956">f</span></span><span id="MJXp-Span-2957">|</span><span id="MJXp-Span-2958"><span id="MJXp-Span-2959">s</span><span id="MJXp-Span-2960">0</span></span><span id="MJXp-Span-2961">,</span><span id="MJXp-Span-2962">Ω</span><span id="MJXp-Span-2963">)</span></span></span></span></span></span></span></p>

<p>Because mutual information is symmetric, we can switch <span><span id="MJXp-Span-2964"><span id="MJXp-Span-2965"><span id="MJXp-Span-2966">s</span><span id="MJXp-Span-2967">f</span></span></span></span><span id="MathJax-Element-201-Frame" tabindex="0"></span> and <span><span id="MJXp-Span-2968"><span id="MJXp-Span-2969">Ω</span></span></span><span id="MathJax-Element-202-Frame" tabindex="0"></span> in several places without breaking the equivalence. Also because <span><span id="MJXp-Span-2970"><span id="MJXp-Span-2971">p</span><span id="MJXp-Span-2972">(</span><span id="MJXp-Span-2973">Ω</span><span id="MJXp-Span-2974">|</span><span id="MJXp-Span-2975"><span id="MJXp-Span-2976">s</span><span id="MJXp-Span-2977">0</span></span><span id="MJXp-Span-2978">,</span><span id="MJXp-Span-2979"><span id="MJXp-Span-2980">s</span><span id="MJXp-Span-2981">f</span></span><span id="MJXp-Span-2982">)</span></span></span><span id="MathJax-Element-203-Frame" tabindex="0"></span> is difficult to observe, let us replace it with an approximation distribution <span><span id="MJXp-Span-2983"><span id="MJXp-Span-2984">q</span></span></span><span id="MathJax-Element-204-Frame" tabindex="0"></span>. According to the variational lower bound, we would have <span><span id="MJXp-Span-2985"><span id="MJXp-Span-2986">I</span><span id="MJXp-Span-2987">(</span><span id="MJXp-Span-2988">Ω</span><span id="MJXp-Span-2989">;</span><span id="MJXp-Span-2990"><span id="MJXp-Span-2991">s</span><span id="MJXp-Span-2992">f</span></span><span id="MJXp-Span-2993">|</span><span id="MJXp-Span-2994"><span id="MJXp-Span-2995">s</span><span id="MJXp-Span-2996">0</span></span><span id="MJXp-Span-2997">)</span><span id="MJXp-Span-2998">≥</span><span id="MJXp-Span-2999"><span id="MJXp-Span-3000">I</span><span id="MJXp-Span-3001"><span id="MJXp-Span-3002">V</span><span id="MJXp-Span-3003">B</span></span></span><span id="MJXp-Span-3004">(</span><span id="MJXp-Span-3005">Ω</span><span id="MJXp-Span-3006">;</span><span id="MJXp-Span-3007"><span id="MJXp-Span-3008">s</span><span id="MJXp-Span-3009">f</span></span><span id="MJXp-Span-3010">|</span><span id="MJXp-Span-3011"><span id="MJXp-Span-3012">s</span><span id="MJXp-Span-3013">0</span></span><span id="MJXp-Span-3014">)</span></span></span><span id="MathJax-Element-205-Frame" tabindex="0"></span>.</p>

<p><span><span id="MJXp-Span-3015"><span id="MJXp-Span-3016"><span><span id="MJXp-Span-3017"><span id="MJXp-Span-3018"><span id="MJXp-Span-3019">I</span><span id="MJXp-Span-3020">(</span><span id="MJXp-Span-3021">Ω</span><span id="MJXp-Span-3022">;</span><span id="MJXp-Span-3023"><span id="MJXp-Span-3024">s</span><span id="MJXp-Span-3025">f</span></span><span id="MJXp-Span-3026">|</span><span id="MJXp-Span-3027"><span id="MJXp-Span-3028">s</span><span id="MJXp-Span-3029">0</span></span><span id="MJXp-Span-3030">)</span></span><span id="MJXp-Span-3031"><span id="MJXp-Span-3032"></span><span id="MJXp-Span-3033">=</span><span id="MJXp-Span-3034">I</span><span id="MJXp-Span-3035">(</span><span id="MJXp-Span-3036"><span id="MJXp-Span-3037">s</span><span id="MJXp-Span-3038">f</span></span><span id="MJXp-Span-3039">;</span><span id="MJXp-Span-3040">Ω</span><span id="MJXp-Span-3041">|</span><span id="MJXp-Span-3042"><span id="MJXp-Span-3043">s</span><span id="MJXp-Span-3044">0</span></span><span id="MJXp-Span-3045">)</span></span></span><span id="MJXp-Span-3046"><span id="MJXp-Span-3047"></span><span id="MJXp-Span-3048"><span id="MJXp-Span-3049"></span><span id="MJXp-Span-3050">=</span><span id="MJXp-Span-3051">−</span><span id="MJXp-Span-3052"><span><span id="MJXp-Span-3053"><span>∑</span></span></span><span><span id="MJXp-Span-3054"><span id="MJXp-Span-3055">Ω</span></span></span></span><span id="MJXp-Span-3056">p</span><span id="MJXp-Span-3057">(</span><span id="MJXp-Span-3058">Ω</span><span id="MJXp-Span-3059">|</span><span id="MJXp-Span-3060"><span id="MJXp-Span-3061">s</span><span id="MJXp-Span-3062">0</span></span><span id="MJXp-Span-3063">)</span><span id="MJXp-Span-3064">log</span><span id="MJXp-Span-3065"></span><span id="MJXp-Span-3066">p</span><span id="MJXp-Span-3067">(</span><span id="MJXp-Span-3068">Ω</span><span id="MJXp-Span-3069">|</span><span id="MJXp-Span-3070"><span id="MJXp-Span-3071">s</span><span id="MJXp-Span-3072">0</span></span><span id="MJXp-Span-3073">)</span><span id="MJXp-Span-3074">+</span><span id="MJXp-Span-3075"><span><span id="MJXp-Span-3076"><span>∑</span></span></span><span><span id="MJXp-Span-3077"><span id="MJXp-Span-3078"><span id="MJXp-Span-3079">s</span><span id="MJXp-Span-3080">f</span></span><span id="MJXp-Span-3081">,</span><span id="MJXp-Span-3082">Ω</span></span></span></span><span id="MJXp-Span-3083"><span id="MJXp-Span-3084">p</span><span id="MJXp-Span-3085">J</span></span><span id="MJXp-Span-3086">(</span><span id="MJXp-Span-3087"><span id="MJXp-Span-3088">s</span><span id="MJXp-Span-3089">f</span></span><span id="MJXp-Span-3090">|</span><span id="MJXp-Span-3091"><span id="MJXp-Span-3092">s</span><span id="MJXp-Span-3093">0</span></span><span id="MJXp-Span-3094">,</span><span id="MJXp-Span-3095">Ω</span><span id="MJXp-Span-3096">)</span><span id="MJXp-Span-3097"><span id="MJXp-Span-3098">p</span><span id="MJXp-Span-3099">C</span></span><span id="MJXp-Span-3100">(</span><span id="MJXp-Span-3101">Ω</span><span id="MJXp-Span-3102">|</span><span id="MJXp-Span-3103"><span id="MJXp-Span-3104">s</span><span id="MJXp-Span-3105">0</span></span><span id="MJXp-Span-3106">)</span><span id="MJXp-Span-3107">log</span><span id="MJXp-Span-3108"></span><span id="MJXp-Span-3109"><span id="MJXp-Span-3110">p</span><span id="MJXp-Span-3111">(</span><span id="MJXp-Span-3112">Ω</span><span id="MJXp-Span-3113">|</span><span id="MJXp-Span-3114"><span id="MJXp-Span-3115">s</span><span id="MJXp-Span-3116">0</span></span><span id="MJXp-Span-3117">,</span><span id="MJXp-Span-3118"><span id="MJXp-Span-3119">s</span><span id="MJXp-Span-3120">f</span></span><span id="MJXp-Span-3121">)</span></span></span></span><span id="MJXp-Span-3122"><span id="MJXp-Span-3123"><span id="MJXp-Span-3124"><span id="MJXp-Span-3125">I</span><span id="MJXp-Span-3126"><span id="MJXp-Span-3127">V</span><span id="MJXp-Span-3128">B</span></span></span><span id="MJXp-Span-3129">(</span><span id="MJXp-Span-3130">Ω</span><span id="MJXp-Span-3131">;</span><span id="MJXp-Span-3132"><span id="MJXp-Span-3133">s</span><span id="MJXp-Span-3134">f</span></span><span id="MJXp-Span-3135">|</span><span id="MJXp-Span-3136"><span id="MJXp-Span-3137">s</span><span id="MJXp-Span-3138">0</span></span><span id="MJXp-Span-3139">)</span></span><span id="MJXp-Span-3140"><span id="MJXp-Span-3141"></span><span id="MJXp-Span-3142">=</span><span id="MJXp-Span-3143">−</span><span id="MJXp-Span-3144"><span><span id="MJXp-Span-3145"><span>∑</span></span></span><span><span id="MJXp-Span-3146"><span id="MJXp-Span-3147">Ω</span></span></span></span><span id="MJXp-Span-3148">p</span><span id="MJXp-Span-3149">(</span><span id="MJXp-Span-3150">Ω</span><span id="MJXp-Span-3151">|</span><span id="MJXp-Span-3152"><span id="MJXp-Span-3153">s</span><span id="MJXp-Span-3154">0</span></span><span id="MJXp-Span-3155">)</span><span id="MJXp-Span-3156">log</span><span id="MJXp-Span-3157"></span><span id="MJXp-Span-3158">p</span><span id="MJXp-Span-3159">(</span><span id="MJXp-Span-3160">Ω</span><span id="MJXp-Span-3161">|</span><span id="MJXp-Span-3162"><span id="MJXp-Span-3163">s</span><span id="MJXp-Span-3164">0</span></span><span id="MJXp-Span-3165">)</span><span id="MJXp-Span-3166">+</span><span id="MJXp-Span-3167"><span><span id="MJXp-Span-3168"><span>∑</span></span></span><span><span id="MJXp-Span-3169"><span id="MJXp-Span-3170"><span id="MJXp-Span-3171">s</span><span id="MJXp-Span-3172">f</span></span><span id="MJXp-Span-3173">,</span><span id="MJXp-Span-3174">Ω</span></span></span></span><span id="MJXp-Span-3175"><span id="MJXp-Span-3176">p</span><span id="MJXp-Span-3177">J</span></span><span id="MJXp-Span-3178">(</span><span id="MJXp-Span-3179"><span id="MJXp-Span-3180">s</span><span id="MJXp-Span-3181">f</span></span><span id="MJXp-Span-3182">|</span><span id="MJXp-Span-3183"><span id="MJXp-Span-3184">s</span><span id="MJXp-Span-3185">0</span></span><span id="MJXp-Span-3186">,</span><span id="MJXp-Span-3187">Ω</span><span id="MJXp-Span-3188">)</span><span id="MJXp-Span-3189"><span id="MJXp-Span-3190">p</span><span id="MJXp-Span-3191">C</span></span><span id="MJXp-Span-3192">(</span><span id="MJXp-Span-3193">Ω</span><span id="MJXp-Span-3194">|</span><span id="MJXp-Span-3195"><span id="MJXp-Span-3196">s</span><span id="MJXp-Span-3197">0</span></span><span id="MJXp-Span-3198">)</span><span id="MJXp-Span-3199">log</span><span id="MJXp-Span-3200"></span><span id="MJXp-Span-3201"><span id="MJXp-Span-3202">q</span><span id="MJXp-Span-3203">(</span><span id="MJXp-Span-3204">Ω</span><span id="MJXp-Span-3205">|</span><span id="MJXp-Span-3206"><span id="MJXp-Span-3207">s</span><span id="MJXp-Span-3208">0</span></span><span id="MJXp-Span-3209">,</span><span id="MJXp-Span-3210"><span id="MJXp-Span-3211">s</span><span id="MJXp-Span-3212">f</span></span><span id="MJXp-Span-3213">)</span></span></span></span><span id="MJXp-Span-3214"><span id="MJXp-Span-3215"><span id="MJXp-Span-3216">I</span><span id="MJXp-Span-3217">(</span><span id="MJXp-Span-3218">Ω</span><span id="MJXp-Span-3219">;</span><span id="MJXp-Span-3220"><span id="MJXp-Span-3221">s</span><span id="MJXp-Span-3222">f</span></span><span id="MJXp-Span-3223">|</span><span id="MJXp-Span-3224"><span id="MJXp-Span-3225">s</span><span id="MJXp-Span-3226">0</span></span><span id="MJXp-Span-3227">)</span></span><span id="MJXp-Span-3228"><span id="MJXp-Span-3229"></span><span id="MJXp-Span-3230">≥</span><span id="MJXp-Span-3231"><span id="MJXp-Span-3232">I</span><span id="MJXp-Span-3233"><span id="MJXp-Span-3234">V</span><span id="MJXp-Span-3235">B</span></span></span><span id="MJXp-Span-3236">(</span><span id="MJXp-Span-3237">Ω</span><span id="MJXp-Span-3238">;</span><span id="MJXp-Span-3239"><span id="MJXp-Span-3240">s</span><span id="MJXp-Span-3241">f</span></span><span id="MJXp-Span-3242">|</span><span id="MJXp-Span-3243"><span id="MJXp-Span-3244">s</span><span id="MJXp-Span-3245">0</span></span><span id="MJXp-Span-3246">)</span></span></span></span></span></span></span></p>

<p><img src="https://lilianweng.github.io/lil-log/assets/images/VIC-explicit-options.png" alt="VIC"></p>
<p><em>Fig. 16. The algorithm for VIC (Variational Intrinsic Control). (Image source: <a href="https://arxiv.org/abs/1611.07507">Gregor, et al. 2017</a>)</em></p>

<p>Here <span><span id="MJXp-Span-3247"><span id="MJXp-Span-3248">π</span><span id="MJXp-Span-3249">(</span><span id="MJXp-Span-3250">a</span><span id="MJXp-Span-3251">|</span><span id="MJXp-Span-3252">Ω</span><span id="MJXp-Span-3253">,</span><span id="MJXp-Span-3254">s</span><span id="MJXp-Span-3255">)</span></span></span><span id="MathJax-Element-207-Frame" tabindex="0"></span> can be optimized with any RL algorithm. The option inference function <span><span id="MJXp-Span-3256"><span id="MJXp-Span-3257">q</span><span id="MJXp-Span-3258">(</span><span id="MJXp-Span-3259">Ω</span><span id="MJXp-Span-3260">|</span><span id="MJXp-Span-3261"><span id="MJXp-Span-3262">s</span><span id="MJXp-Span-3263">0</span></span><span id="MJXp-Span-3264">,</span><span id="MJXp-Span-3265"><span id="MJXp-Span-3266">s</span><span id="MJXp-Span-3267">f</span></span><span id="MJXp-Span-3268">)</span></span></span><span id="MathJax-Element-208-Frame" tabindex="0"></span> is doing supervised learning. The prior <span><span id="MJXp-Span-3269"><span id="MJXp-Span-3270"><span id="MJXp-Span-3271">p</span><span id="MJXp-Span-3272">C</span></span></span></span><span id="MathJax-Element-209-Frame" tabindex="0"></span> is updated so that it tends to choose <span><span id="MJXp-Span-3273"><span id="MJXp-Span-3274">Ω</span></span></span><span id="MathJax-Element-210-Frame" tabindex="0"></span> with higher rewards. Note that <span><span id="MJXp-Span-3275"><span id="MJXp-Span-3276"><span id="MJXp-Span-3277">p</span><span id="MJXp-Span-3278">C</span></span></span></span><span id="MathJax-Element-211-Frame" tabindex="0"></span> can also be fixed (e.g. a Gaussian). Various <span><span id="MJXp-Span-3279"><span id="MJXp-Span-3280">Ω</span></span></span><span id="MathJax-Element-212-Frame" tabindex="0"></span> will result in different behavior through learning. Additionally, <a href="https://arxiv.org/abs/1611.07507">Gregor, et al. (2017)</a> observed that it is difficult to make VIC with explicit options work in practice with function approximation and therefore they also proposed another version of VIC with implicit options.</p>

<p>Different from VIC which models <span><span id="MJXp-Span-3281"><span id="MJXp-Span-3282">Ω</span></span></span><span id="MathJax-Element-213-Frame" tabindex="0"></span> conditioned only on the start and end states, <strong>VALOR</strong> (short for <em>“Variational Auto-encoding Learning of Options by Reinforcement”</em>; <a href="https://arxiv.org/abs/1807.10299">Achiam, et al. 2018</a>) relies on the whole trajectory to extract the option context <span><span id="MJXp-Span-3283"><span id="MJXp-Span-3284">c</span></span></span><span id="MathJax-Element-214-Frame" tabindex="0"></span>, which is sampled from a fixed Gaussian distribution. In VALOR:</p>
<ul>
  <li>A policy acts as an encoder, translating contexts from a noise distribution into trajectories</li>
  <li>A decoder attempts to recover the contexts from the trajectories, and rewards the policies for making contexts easier to distinguish. The decoder never sees the actions during training, so the agent has to interact with the environment in a way that facilitates communication with the decoder for better prediction. Also, the decoder recurrently takes in a sequence of steps in one trajectory to better model the correlation between timesteps.</li>
</ul>

<p><img src="https://lilianweng.github.io/lil-log/assets/images/VALOR-decoder.png" alt="VALOR"></p>
<p><em>Fig. 17. The decoder of VALOR is a biLSTM which takes <span><span id="MJXp-Span-3285"><span id="MJXp-Span-3286">N</span><span id="MJXp-Span-3287">=</span><span id="MJXp-Span-3288">11</span></span></span><span id="MathJax-Element-215-Frame" tabindex="0"></span> equally spaced observations from one trajectory as inputs. (Image source: <a href="https://arxiv.org/abs/1807.10299">Achiam, et al. 2018</a>)</em></p>

<p>DIAYN (“Diversity is all you need”; <a href="https://arxiv.org/abs/1802.06070">Eysenbach, et al. 2018</a>) has the idea lying in the same direction, although with a different name — DIAYN models the policies conditioned on a latent <em>skill</em> variable. See my <a href="https://lilianweng.github.io/lil-log/2019/06/23/meta-reinforcement-learning.html#learning-with-random-rewards">previous post</a> for more details.</p>

<hr>
<p>Cited as:</p>
<div><div><pre><code>@article{weng2020exploration,
  title   = "Exploration Strategies in Deep Reinforcement Learning",
  author  = "Weng, Lilian",
  journal = "lilianweng.github.io/lil-log",
  year    = "2020",
  url     = "https://lilianweng.github.io/lil-log/2020/06/07/exploration-strategies-in-deep-reinforcement-learning.html"
}
</code></pre></div></div>

<h2 id="reference">Reference</h2>

<p>[1] Pierre-Yves Oudeyer &amp; Frederic Kaplan. <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.567.6524&amp;rep=rep1&amp;type=pdf">“How can we define intrinsic motivation?”</a> Conf. on Epigenetic Robotics, 2008.</p>

<p>[2] Marc G. Bellemare, et al. <a href="https://arxiv.org/abs/1606.01868">“Unifying Count-Based Exploration and Intrinsic Motivation”</a>. NIPS 2016.</p>

<p>[3] Georg Ostrovski, et al. <a href="https://arxiv.org/abs/1703.01310">“Count-Based Exploration with Neural Density Models”</a>. PMLR 2017.</p>

<p>[4] Rui Zhao &amp; Volker Tresp. <a href="https://arxiv.org/abs/1902.08039">“Curiosity-Driven Experience Prioritization via
Density Estimation”</a>. NIPS 2018.</p>

<p>[5] Haoran Tang, et al. <a href="https://arxiv.org/abs/1611.04717">“#Exploration: A Study of Count-Based Exploration for Deep Reinforcement Learning”</a>. NIPS 2017.</p>

<p>[6] Jürgen Schmidhuber. <a href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.45.957">“A possibility for implementing curiosity and boredom in model-building neural controllers”</a> 1991.</p>

<p>[7] Pierre-Yves Oudeyer, et al. <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.177.7661&amp;rep=rep1&amp;type=pdf">“Intrinsic Motivation Systems for Autonomous Mental Development”</a> IEEE Transactions on Evolutionary Computation, 2007.</p>

<p>[8] Bradly C. Stadie, et al. <a href="https://arxiv.org/abs/1507.00814">“Incentivizing Exploration In Reinforcement Learning With Deep Predictive Models”</a>. ICLR 2016.</p>

<p>[9] Deepak Pathak, et al. <a href="https://arxiv.org/abs/1705.05363">“Curiosity-driven Exploration by Self-supervised Prediction”</a>. CVPR 2017.</p>

<p>[10] Yuri Burda, Harri Edwards &amp; Deepak Pathak, et al. <a href="https://arxiv.org/abs/1808.04355">“Large-Scale Study of Curiosity-Driven Learning”</a>. arXiv 1808.04355 (2018).</p>

<p>[11] Joshua Achiam &amp; Shankar Sastry. <a href="https://arxiv.org/abs/1703.01732">“Surprise-Based Intrinsic Motivation for Deep Reinforcement Learning”</a> NIPS 2016 Deep RL Workshop.</p>

<p>[12] Rein Houthooft, et al. <a href="https://arxiv.org/abs/1605.09674">“VIME: Variational information maximizing exploration”</a>. NIPS 2016.</p>

<p>[13] Leshem Choshen, Lior Fox &amp; Yonatan Loewenstein. <a href="https://arxiv.org/abs/1804.04012">“DORA the explorer: Directed outreaching reinforcement action-selection”</a>. ICLR 2018</p>

<p>[14] Yuri Burda, et al. <a href="https://arxiv.org/abs/1810.12894">“Exploration by Random Network Distillation”</a> ICLR 2019.</p>

<p>[15] OpenAI Blog: <a href="https://openai.com/blog/reinforcement-learning-with-prediction-based-rewards/">“Reinforcement Learning with
Prediction-Based Rewards”</a> Oct, 2018.</p>

<p>[16] Misha Denil, et al. <a href="https://arxiv.org/abs/1611.01843">“Learning to Perform Physics Experiments via Deep Reinforcement Learning”</a>. ICLR 2017.</p>

<p>[17] Ian Osband, et al. <a href="https://arxiv.org/abs/1602.04621">“Deep Exploration via Bootstrapped DQN”</a>. NIPS 2016.</p>

<p>[18] Ian Osband, John Aslanides &amp; Albin Cassirer. <a href="https://arxiv.org/abs/1806.03335">“Randomized Prior Functions for Deep Reinforcement Learning”</a>. NIPS 2018.</p>

<p>[19] Karol Gregor, Danilo Jimenez Rezende &amp; Daan Wierstra. <a href="https://arxiv.org/abs/1611.07507">“Variational Intrinsic Control”</a>. ICLR 2017.</p>

<p>[20] Joshua Achiam, et al. <a href="https://arxiv.org/abs/1807.10299">“Variational Option Discovery Algorithms”</a>. arXiv 1807.10299 (2018).</p>

<p>[21] Benjamin Eysenbach, et al. <a href="https://arxiv.org/abs/1802.06070">“Diversity is all you need: Learning skills without a reward function.”</a>. ICLR 2019.</p>

<p>[22] Adrià Puigdomènech Badia, et al. <a href="https://arxiv.org/abs/2002.06038">“Never Give Up (NGU): Learning Directed Exploration Strategies”</a> ICLR 2020.</p>

<p>[23] Adrià Puigdomènech Badia, et al.  <a href="https://arxiv.org/abs/2003.13350">“Agent57: Outperforming the Atari Human Benchmark”</a>. arXiv 2003.13350 (2020).</p>

<p>[24] DeepMind Blog: <a href="https://deepmind.com/blog/article/Agent57-Outperforming-the-human-Atari-benchmark">“Agent57: Outperforming the human Atari benchmark”</a> Mar 2020.</p>

<p>[25] Nikolay Savinov, et al. <a href="https://arxiv.org/abs/1810.02274">“Episodic Curiosity through Reachability”</a> ICLR 2019.</p>

<p>[26] Adrien Ecoffet, et al. <a href="https://arxiv.org/abs/1901.10995">“Go-Explore: a New Approach for Hard-Exploration Problems”</a>. arXiv 1901.10995 (2019).</p>

<p>[27] Adrien Ecoffet, et al. <a href="https://arxiv.org/abs/2004.12919">“First return then explore”</a>. arXiv 2004.12919 (2020).</p>

<p>[28] Junhyuk Oh, et al. <a href="https://arxiv.org/abs/1806.05635">“Self-Imitation Learning”</a>. ICML 2018.</p>

<p>[29] Yijie Guo, et al. <a href="https://arxiv.org/abs/1907.10247">“Self-Imitation Learning via Trajectory-Conditioned Policy for Hard-Exploration Tasks”</a>. arXiv 1907.10247 (2019).</p>

<p>[30] Zhaohan Daniel Guo &amp; Emma Brunskill. <a href="https://arxiv.org/abs/1906.07805">“Directed Exploration for Reinforcement Learning”</a>. arXiv 1906.07805 (2019).</p>


  </div></div></div>
    </div>
    <footer>
        <div>created by buildstarted &copy; 2020 <a href="/about">about</a></div>
        <div>Share this page on social media: copy and paste this url https://linksfor.dev/</div>
        <div>If you prefer RSS: <a href="https://linksfor.dev/feed.xml">https://linksfor.dev/feed.xml</a></div>
		<div>Customer satisfaction guaranteed to be optional.</div>
    </footer>
    
    <script async defer>
        _dna = window._dna || {};
        _dna.siteId = "linksfor.devs";
        _dna.outlink = true;

        (function () {
            let dna = document.createElement('script');
            dna.type = 'text/javascript';
            dna.async = true;
            dna.src = '//dna.buildstarted.com/t.js';
            let s = document.getElementsByTagName('script')[0];
            s.parentNode.insertBefore(dna, s);
        })();
    </script>
    <noscript><img src="//dna.buildstarted.com/g?siteId=linksfor.devs"/></noscript>
</body>
</html>