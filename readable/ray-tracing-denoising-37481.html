<!DOCTYPE html>
<html lang="en">
<head>
    <title>
Ray Tracing Denoising - linksfor.dev(s)    </title>
    <meta charset="utf-8">
    <link rel="alternate" type="application/rss+xml" title="Linksfor.dev(s) feed" href="https://linksfor.dev/feed.rss" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta name="google" value="notranslate">
    <link rel="stylesheet" type="text/css" href="/style.min.css" />
    <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon">
    <link rel="icon" href="/favicon.ico" type="image/x-icon">
    <meta property="og:title" content="Ray Tracing Denoising - linksfor.dev(s)"/>
    <meta property="article:author" content="[object Object]"/>
    <meta property="og:description" content="An overview of the state of the art in ray denoising. We&#x27;ll go over sampling, noise, filtering, machine learning, and temporal techniques used in modern denoising systems."/>
    <meta property="og:type" content="website"/>
    <meta property="og:url" content="https://alain.xyz/blog/raytracing-denoising"/>

<meta property="og:site_name" content="linksfor.dev(s)" />
</head>
<body>
    <div class="devring" style="background: #222">
        <div style="text-align:center">Explore other dev related sites in this ring. If you would like to join this ring <a href="https://devring.club">click here</a>.</div>
        <div class="grid">
            <div style="display: grid; grid-template-columns: .5fr 1fr 1fr 1fr; text-align: center;">
                <span class="devring-title"><a href="https://devring.club/">devring.club</a></span>
                <a href="https://devring.club/sites/1/prev" class="devring-previous">Previous</a>
                <a href="https://devring.club/random" class="devring-random">Random</a>
                <a href="https://devring.club/sites/1/next" class="devring-next">Next</a>
            </div>
        </div>
    </div>
    <div class="grid">
        <h1 style="margin: unset">
<a href="/" style="color:inherit">linksfor.dev(s)</a>
        </h1>
        <title>linksfor.dev(s) - Ray Tracing Denoising</title>
<div class="readable">
        <h1>Ray Tracing Denoising</h1>
            <div>by [object Object]</div>
            <div>Reading time: 26-33 minutes</div>
        <div>Posted here: 26 Sep 2019</div>
        <p><a href="https://alain.xyz/blog/raytracing-denoising">https://alain.xyz/blog/raytracing-denoising</a></p>
        <hr/>
<div id="readability-page-1" class="page"><article><p><a href="https://alain.xyz/talks/ray-tracing-denoising">UPenn Talk</a></p>
<blockquote>
<p>There's an academic arms race to produce the best real-time denoised image from 1 sample per pixel (spp) path traced images. Every new paper like this brings us closer to an ideal solution that can be baked into silicon (mobile raytacing for the masses!) ~ <a href="https://twitter.com/ddiakopoulos/status/1129223581898985472">Dimitri Diakopoulos (@ddiakopoulos)</a></p>
</blockquote>
<p>Since the advent of monte-carlo ray tracing, there have been attempts to reduce the time it takes to converge rays.</p>
<p>This began with algorithms that focused on <strong>sampling techniques</strong>, using stochastic sampling with biased noise generated by quasi-random sequences, weighing different rays to approximate the best distribution of samples with multiple importance sampling (MIS), Next Event Estimation (NEE), Russian Roulette (RR) <a href="#ref_veach1998">[Veach 1998]</a>. Though not necessarily new, recently the use of low discrepancy sampling <a href="#ref_jarosz2019">[Jarosz et al. 2019]</a> and tillable blue noise <a href="#ref_benyoub2019">[Benyoub 2019]</a> has been used by Unity Technologies, and NVIDIA in real time ray tracers to great success.</p>
<blockquote>
<p>While this post reviews techniques on improving sampling via filters and noise algorithms, it does not touch improving sampling via modifications to your ray tracing routine with Environment MIS, Ray Sorting, etc. For more details on that visit my other post on <a href="https://alain.xyz/blog/real-time-ray-tracing">Real Time Ray Tracing</a>.</p>
</blockquote>
<p><strong>Encoding techniques</strong> have been used by recent literature such as a means of improving the performance of ray tracing applications by attempting to encode sparsely sampled outputs such as indirect global illumination as 2 levels of <em>spherical harmonics</em>  to help avoid salt/peppering, <em>upscaling and finding the best candidate</em> for a ray sample thanks to jittering the frame buffer to better match a given sample to it's closest hit point based on using feature buffers such as normals, depth <a href="#ref_baktash2018">[Abdollah-shamshir-saz 2018]</a>.</p>
<p><strong>Signal processing techniques</strong> such as <em>Gaussian</em>, <em>Median</em> <a href="#ref_mara2017">[Mara et al. 2017]</a>, <em>Bilateral</em>, <em>Ã€-Trous</em> <a href="#ref_dammertz2010">[Dammertz et al. 2010]</a>, and <em>Guided</em> <a href="#ref_he2012">[He et al. 2012]</a> filters have been used to average out and blend regions with low variance. In particular Guided filters driven by <em>feature buffers</em> such as common G-Buffer attachments like depth and normals have seen much success. Machine learning algorithms such as those employed by Intel's <a href="https://github.com/OpenImageDenoise/oidn">Open Image Denoise (OIDN)</a>, NVIDIA's Denoising Autoencoder <a href="#ref_chaitanya2017">[R. Alla Chaitanya et al. 2017]</a> were used for offline renders such as <a href="https://wiki.blender.org/wiki/Reference/Release_Notes/2.81/Cycles">Blender 2.81</a>, with NVIDIA <a href="#ref_kalantari2015">[Khademi Kalantari et al. 2015]</a> attempting to use <a href="https://twitter.com/NVBackchannel/status/1156995750477254656">trained neural networks in real-time</a>.</p>
<blockquote>
<p>Visit my other post on <a href="https://alain.xyz/blog/machine-learning-denoising">Machine Learning Denoising</a> for more details on implementing an AI denoiser, though these techniques could be used in conjunction with a neural network denoising pass.</p>
</blockquote>
<p><strong>Accumulation techniques</strong> have seen a resurgence in new literature. While a naive implementation would be to simply accumulate samples on an unchanging scene, it's possible to reuse samples in a moving scene. Examples include <a href="#ref_mara2017">[Mara et al. 2017]</a> Spatio-Temporal Filter, the Spatio-Temporal Variance Guided Filter (SVGF) <a href="#ref_schied2017">[Schied 2017]</a>, Spatial Denoising employed by <a href="#ref_baktash2018">[Abdollah-shamshir-saz 2018]</a>, Adaptive SVGF (A-SVGF) <a href="#ref_schied2018">[Schied 2018]</a>, Blockwise Multi-Order Feature Regression (BMFR) <a href="#ref_koskela2019">[Koskela et al. 2019]</a>), and Temporally Dense Ray Tracing <a href="#ref_andersson2019">[Andersson et al. 2019]</a>.</p>
<p><a href="https://morgan3d.github.io/articles/2019-04-01-ddgi/"><img src="https://alain.xyz/blog/raytracing-denoising/assets/rxgi.jpg" alt="RXGI fair use screenshot"></a></p>
<p>In addition, there's <strong>separation techniques</strong> that attempt to fine tune behavior for different aspects of a path tracer. The <em>RXGI</em> paper <a href="#ref_mjercik2019">[Majercik et al. 2019]</a> made famous by <a href="https://twitter.com/casualeffects">Morgan McGuire (@CasualEffects)</a> approximated ray traced global illumination (GI) with light probes which used ray tracing to better determine the proper radiance of each probe and to better position probes in the scene to avoid bleeding and inaccurate interiors. Specific techniques that can avoid monte carlo integration entirely also avoid the need for real time denoising routines.</p>
<p>We'll be reviewing techniques that can be used to converge to an image close to ground truth as quickly as possible with real-time ray-tracing renderers. Such denoising techniques can then be applied when rendering shadows, ambient occlusion, global illumination, reflections, refractions, volumetric lighting, and more in your applications.</p>
<hr>
<h2 id="execution-order">Execution Order</h2>
<p><img src="https://alain.xyz/blog/raytracing-denoising/assets/execution-order.svg" alt="Execution Order Diagram"></p>
<p>While there's a variety of ways one could write a ray tracing application and denoiser, the following execution order tends to be the case for the state of the art in real time ray tracing hybrid renderer applications in industry and research:</p>
<ol>
<li><p><strong>G-Buffer Pass</strong> - Write to feature buffers used commonly for deferred rendering such as view space depth, normals, velocity, or any other feature sets you intend on using when denoising.</p></li>
<li><p><strong>Ray Trace</strong> - Either with a unified path tracer or using dedicated passes for shadows/reflection/global illumination/ambient occlusion/subsurface scattering/refraction/translucency. Having each aspect of your ray tracer split however can allow for separate denoising schemes that work best for each type of output, though it could be slower.</p></li>
<li><p><strong>Accumulation</strong> - by interpolating between previous and current color data. Normally this involves reprojecting previous samples, though <a href="#ref_kristof2019">[Kristof 2019]</a> attempts to reproject previous samples first for reflections (so switching steps 2 and 3), then use that data to guide accumulation.</p></li>
<li><p><strong>Blit Accumulation History</strong> - write your accumulated buffers to be used by the next frame when spatially reprojecting samples. It's important to do this <em>before any filtering</em> to prevent a growing blur effect as every frame blurs the previous.</p></li>
<li><p><strong>Adaptive Heuristics</strong> - A-SVGF  introduces the <strong>Moment Buffer</strong> that encodes change in color variance/velocity to better avoid using old history on areas that have a lot of changes such as moving lights, mirrors, etc. By estimating the current radiance of your path traced image, you can also take advantage of that data to perform firefly rejection.</p></li>
<li><p><strong>Guided Filtering</strong> - Blur the weighting buffers using a guided filter such as Ã€-Trous, or an edge stopping Gaussian filter. Most denoisers do this 5 times with a 3x3 filter. This can also be guided by the change in velocity/luminance, history length, roughness, shadow penumbra distance, etc.</p></li>
<li><p><strong>Blit Prepass History</strong> - Write any important previous data sets (linear depth, normals, velocity), preferably by ping-ponging between two output frame buffers to save a copy operation. These will be used for accumulation and reprojection on the next frame.</p></li>
</ol>
<h2 id="sampling-techniques">Sampling Techniques</h2>
<p>One of the easiest and fastest ways to get an image to converge faster is to use low discrepancy noise when performing monte-carlo integration. This is better than using purely random noise due to it having a more uniform distribution. This means that the random samples cover the sample domain evenly, rather than cluster and form holes. <a href="#ref_cornel2014">[Cornel 2014]</a></p>
<p>The faster an image converges, the less variance your data will have when feeding it to a denoiser. However, it's important to bare in mind the <strong>Curse of Dimensionality</strong>:</p>
<blockquote>
<p>Once the integral is high dimensional, if the integrand is complex, <strong>Stratified Sampling doesn't help</strong>. ~ <a href="https://twitter.com/peter_shirley">Peter Shirley (@peter_shirley)</a></p>
</blockquote>
<p>Thus, try to keep the integrand of your ray trace as minimal or as full of constants as possible. <a href="#ref_shirley2018">[Shirley 2018]</a></p>
<h3 id="blue-noise">Blue Noise</h3>
<p><img src="https://alain.xyz/blog/raytracing-denoising/assets/bluenoise256.png" alt="Blue Noise Texture Courtesy of ShaderToy"></p>
<p><strong>Blue noise</strong> is uniformly distributed noise, that is noise with similar samples spread out as far away as possible over a given domain. This lends itself very well to denoising, as there are far less instances of high frequency error in blue noise, making it easier to blur and accumulate <a href="#ref_schied2019">[Schied et al. 2019]</a>.</p>
<p><a href="https://twitter.com/paniq">Leonard Ritter (@paniq)</a> wrote a highly performant blue noise texture generation ShaderToy here:</p>

<p>Once you have a uniform <strong>Blue Noise Look Up Texture (LUT)</strong> (sometimes called a <em>precomputed dither matrix</em> in literature) to work with, it's possible to map that texture to important regions to sample during ray tracing, such as the BRDF behavior of a given surface and/or the bright regions of an environment texture in the case of image based lighting.</p>
<p><img src="https://alain.xyz/blog/raytracing-denoising/assets/noise-blur-comparison.png" alt="Blue vs. White Noise Comparison"></p>
<p>Blurring blue noise even a little (in this case a 2x gaussian blur) results in a much more uniform result in comparison with white noise where high frequency noise present in monte-carlo rendering would be much more obvious.</p>
<p>Though it should be noted that it can be noticeable across edges, which will be important to note <a href="https://alain.xyz/blog/raytracing-denoising/#firefly-rejection">later</a></p>
<h3 id="sobol-sequences">Sobol Sequences</h3>
<p>Computing blue noise is a somewhat expensive and thus, is only suitable to compute in a preprocessing step. There do however exist functions that behave similarly to blue noise, known as <strong>quasi-random number sequence</strong> algorithms. Examples include the <em>Worley</em>, <em>Sobol</em>, or <em>Halton</em> Sequences, each of which have benefits and tradeoffs <a href="#ref_wolfe2017">[Wolfe 2017]</a>.</p>
<p><img src="https://alain.xyz/blog/raytracing-denoising/assets/sobolnoise256.png" alt="Sobol Noise 256x1 texture"></p>
<p><strong>Sobol</strong> is more computationally expensive but leads to far less discrepancy <a href="#ref_roberts2018">[Roberts 2018]</a>, while Halton is cheaper but has slightly more discrepancy <a href="#ref_wong1997">[Wong et al. 1997]</a>.</p>
<p><a href="https://twitter.com/auzaiffe">Anis Benyoub (Unity Technologies)</a> for instance has chosen to use a 256x1 Sobol texture and a 256x256 RG Blue Noise texture as a means of generating quasi-random monte-carlo (QMC) samples. <a href="#ref_benyoub2019">[Benyoub 2019]</a> <a href="https://twitter.com/eric_heitz">Eric Heitz (@eric_heitz)</a> et al. took this even further with his paper on distributing monte carlo errors over different frames by using a <code>uint sobolSequence[256*256]</code> to scramble blue noise tiles. <a href="#ref_heitz2019">[Heitz et al. 2019]</a></p>
<p>A C++ implementation of <a href="https://gist.github.com/alaingalvan/af92ddbaf3bb01f5ef29bc431bd37891">Sobol</a> can be found here, courtesy of Professor John Burkardt of Virginia Tech.</p>
<p>Having a low discrepancy domain from which to sample your scene can then be used in combination with other techniques such as <strong>Multiple Importance Sampling (MIS)</strong> of the BRDF (material) behavior and light source behavior (with environment maps and participating media being different cases to consider), <strong>Next Event Estimation</strong>, <strong>Russian Roulette</strong>, all with the goal of reducing the variance of a monte-carlo raytraced image.</p>
<blockquote>
<p>For more details regarding these techniques, visit my post on <a href="https://alain.xyz/blog/real-time-raytracing">Real Time Raytracing</a>.</p>
</blockquote>
<h2 id="encoding-techniques">Encoding Techniques</h2>
<h3 id="formats">Formats</h3>
<p>When managing a real time ray tracing application, the question of what formats to use with your trace output buffers will ultimately come into play. The answer to such a question ultimately boils down to:</p>
<ol>
<li><p>Are you targeting an HDR output such as with ACES linearization or an HDR 10 bit display, or a 16 bit or 32 bit output image.</p></li>
<li><p>How many samples you wish to accumulate, as there's a limit to where averaging out samples over time no longer contribute to the output image, and a small enough limit where they introduce bias to your image that can lead to color banding.</p></li>
</ol>
<p>This <em>sample count limit</em> can be defined as:</p>
<p><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mn>1</mn><mrow><mi>S</mi><mi>a</mi><mi>m</mi><mi>p</mi><mi>l</mi><mi>e</mi><mi>s</mi></mrow></mfrac><mo>â‰¥</mo><msub><mi>Ïµ</mi><mrow><mi>b</mi><mi>i</mi><mi>t</mi><mi>r</mi><mi>a</mi><mi>t</mi><mi>e</mi></mrow></msub></mrow><annotation encoding="application/x-tex"> \frac{1}{Samples} \geq  \epsilon_{bitrate} </annotation></semantics></math></span></span></span></p>
<p>The limit where the number of samples you've accumulated is less than the minimum <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Ïµ</mi></mrow><annotation encoding="application/x-tex"> \epsilon </annotation></semantics></math></span></span> of your floating point bit rate. For <code>RGBA16F</code> that number is about 2048 samples. <a href="#ref_wolfe2017b">[Wolfe 2017]</a></p>
<blockquote>
<p>On a side note, formats such as <code>R11G11B10</code> are not recommended due to any accumulation of samples ultimately having a bias towards turning yellow due to the extra precision on the R and G channels.</p>
</blockquote>
<h3 id="spherical-harmonics">Spherical Harmonics</h3>
<p>Rather than writing sparse samples as radiance in a 1 pixel location, it's possible to write such samples as spherical harmonics encodings across a larger radius that does not intersect with other samples, and use filtering to fill in the gaps.</p>
<h3 id="upscaling">Upscaling</h3>
<p>Raytracing is an expensive operation to be done at high resolutions such as Ultra HD 4K, so upscaling from a lower resolution can be employed to take advantage of raytracing at interactive frame rates.</p>
<h2 id="spatio-temporal-techniques">Spatio-Temporal Techniques</h2>
<p><strong>Spatiotemporal reprojection</strong> is simply reusing the data from previous frames, <em>spatially</em> reprojecting them to the current frame. Encoding radiance with spatial information has had its earliest literature in <strong>Irradiance Caching</strong> <a href="#ref_ward1988">[Ward et al. 1988]</a>, however real time raytracing takes this idea a step further by keeping data in view space, but attempting to reuse that data by translating it from one frame to the other by means of a motion buffer which calculates the change in a vertex's position between the current and previous frame.</p>
<p>This is <em>extremely effective</em> at reducing noise, but relying on temporal data inevitably introduces a slight lag with changes in a scene such as moving lights, objects, as new data needs to be accumulated over time to reflect those changes.</p>
<h3 id="motion-buffer">Motion Buffer</h3>
<p><img src="https://alain.xyz/blog/raytracing-denoising/assets/motion-buffer.jpg" alt="Motion Buffer Example"></p>
<p>A <strong>Motion Buffer</strong> (otherwise known as a <strong>Velocity Buffer</strong>) encodes the change in position of each vertex. This is at the core of spatio-temporal reprojection. This buffer can be calculated by determining the previous and current NDC space coordinate positions of each vertex being rendered, and taking the difference of the two. This can be encoded in a <code>RG16F</code> render target.</p>
<p><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover accent="true"><mi>V</mi><mo>âƒ—</mo></mover><mo>=</mo><msub><mover accent="true"><mrow><mi>N</mi><mi>D</mi><mi>C</mi></mrow><mo>âƒ—</mo></mover><mrow><mi>c</mi><mi>u</mi><mi>r</mi></mrow></msub><mo>âˆ’</mo><msub><mover accent="true"><mrow><mi>N</mi><mi>D</mi><mi>C</mi></mrow><mo>âƒ—</mo></mover><mrow><mi>p</mi><mi>r</mi><mi>e</mi><mi>v</mi></mrow></msub></mrow><annotation encoding="application/x-tex"> \vec{V} = \vec{NDC}</annotation></semantics></math><em>{cur} - \vec{NDC}</em>{prev} </span></span></span></p>
<p>Therefore, one would need the previous <code>modelViewProjection</code> matrix of an object, as well as that object's <em>animation vertex velocity</em>, the difference between the position between the current and previous animation sample.</p>
<pre><code>
<span>vec3</span> ndc = inPosition.xyz / inPosition.w;
<span>vec3</span> ndcPrev = inPositionPrev.xyz / inPositionPrev.w;
outVelocity = ndc.xy - ndcPrev.xy;
</code></pre>
<h3 id="history-buffer">History Buffer</h3>
<table>
<thead>
<tr><th><img src="https://alain.xyz/blog/raytracing-denoising/assets/history-length.jpg" alt="Normalized history buffer"> <strong>History (R16 Normalized [0, 212])</strong></th></tr>
</thead>
<tbody>
</tbody>
</table>
<p>When performing spatio-temporal reprojection, having a buffer describing the time for which a given sample had to accumulate is very valuable, a <strong>History Buffer</strong>. It can be used to drive a filter to blur stronger in regions with few accumulated samples or be used to estimate the variance of the current image (higher history would mean less variance).</p>
<p>This buffer would be read when accepting reprojected samples to determine the accumulation factor of a given sample with an exponentially moving average.</p>
<pre><code>outHistoryLength = successfulReprojection ? prevHistoryLength + <span>1.0</span> : <span>0.0</span>;
</code></pre>
<h3 id="exponentially-moving-average">Exponentially Moving Average</h3>
<p><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>C</mi><mrow><mi>f</mi><mi>i</mi><mi>n</mi><mi>a</mi><mi>l</mi></mrow></msub><mo>=</mo><mo stretchy="false">(</mo><mo stretchy="false">(</mo><mn>1</mn><mo>âˆ’</mo><mi>Î±</mi><mo stretchy="false">)</mo><mo>âˆ—</mo><msub><mi>C</mi><mrow><mi>p</mi><mi>r</mi><mi>e</mi><mi>v</mi><mi>i</mi><mi>o</mi><mi>u</mi><mi>s</mi></mrow></msub><mo stretchy="false">)</mo><mo>+</mo><mo stretchy="false">(</mo><mi>Î±</mi><mo>âˆ—</mo><msub><mi>C</mi><mrow><mi>c</mi><mi>u</mi><mi>r</mi><mi>r</mi><mi>e</mi><mi>n</mi><mi>t</mi></mrow></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex"> C_{final} = ( (1 - \alpha ) * C_{previous}  ) + ( \alpha * C_{current} )</annotation></semantics></math></span></span></span></p>
<pre><code>outColor = <span>mix</span>(colorPrevious, colorCurrent, accumulationFactor);
</code></pre>
<p>Having a history buffer available to you gives your algorithm a perspective of how many samples a given region of the screen have been accumulated, which you can feed to a shader to determine an accumulation factor <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Î±</mi></mrow><annotation encoding="application/x-tex"> \alpha </annotation></semantics></math></span></span>.</p>
<p>While this can encode spatial information thanks to the motion buffer allowing for the translation of previous samples, <em>A-SVGF</em> takes this a step further.</p>
<h3 id="moment-buffer">Moment Buffer</h3>
<p>By using the change in luminosity and velocity over correlated samples to help drive the accumulation factor of a spatio-temporal filter, it's possible to reduce ghosting significantly.</p>
<p>This difference in temporal variance and velocity is then used to determine the rate of change on a sample of the screen, encoding that information in what it calls a <strong>Moment Buffer</strong>. <a href="#ref_schied2019">[Schied et al. 2019]</a></p>
<p>We'll discuss an implementation of a moment buffer while discussing variance estimation later in the post.</p>
<h3 id="caveats">Caveats</h3>
<p>Temporal lag is a problem with this technique. There can be differences in brightness between regions with a high number of accumulated samples and new regions. This is especially evident in darker regions of a raytraced scene such as interiors. To mitigate this, rather than using 1 sample per pixel, it's best to use 2 or more samples per pixel in dark areas of a scene.</p>
<p>In addition, there can be significant differences between view positions and their ray traced outputs. While shadows, global illumination, ambient occlusion, subsurface scattering, and transucency can be reprojected spatially without much issue, ray traced outputs that are not lambertian in nature such as reflections and refraction cannot be reprojected (at least, without taking into account bounce velocity, which may be more computational trouble than its worth and not yield many reprojected samples). Thus, these buffers would rely much more on filtering techniques.</p>
<h2 id="signal-processing-techniques">Signal Processing Techniques</h2>
<p>Monte-carlo integration starts with a noisy image which gradually becomes clearer as time goes on. In an attempt to reduce the amount of time it takes to compute a clear final image, attempts have been made to use a variety of filtering techniques when rendering such as <strong>Gaussian</strong>, <strong>Median</strong>, <strong>Bilateral</strong>, and <strong>Ã€-Trous</strong> filters. These have been used in combination with variance estimation across neighboring samples to better reject fireflies and drive the accumulation of samples in regions that change in variance/velocity.</p>
<h3 id="bluring-kernels">Bluring Kernels</h3>
<p>Filtering is an expensive operation to do in a real-time renderer, as texture lookup operations introduces latency when executing a shader. <strong>Ã€-Trous (With holes)</strong> is a fast approximation of a bilateral filter currently used by <strong>Spatio-temporal Variance Guided Filtering (SVGF)</strong> and it's <strong>Adaptive (A-SVGF)</strong> counterpart <a href="#ref_schied2017">[Schied 2017]</a> <a href="#ref_schied2018">[Schied 2018]</a>, first introduced in <a href="#ref_dammertz2010">[Dammertz et al. 2010]</a>, in combination with spatio-temporal reprojection and variance estimation to perform a cheap, fast, and high quality edge avoiding blur.</p>
<table>
<thead>
<tr><th><img src="https://alain.xyz/blog/raytracing-denoising/assets/normal.jpg" alt="Ground Truth"> <strong>Normals (RG16F)</strong></th><th><img src="https://alain.xyz/blog/raytracing-denoising/assets/depth.jpg" alt="View Space Depth Buffer"> <strong>View Space Depth (R32F)</strong></th></tr>
</thead>
<tbody>
</tbody>
</table>
<p>A-Trous avoids sampling in a slightly dithered pattern to cover a wider radius than would normally be possible in a 3x3 or 5x5 guassian kernel while at the same time having the ability to be repeated multiple times, and avoid bluring across edges thanks number of different inputs <a href="#ref_he2012">[He et al. 2012]</a> <a href="#ref_li2012">[Li et al. 2012]</a> <a href="#ref_dammertz2010">[Dammertz et al. 2010]</a>, such as a <strong>Normals</strong>, <strong>Depth</strong>, <strong>Squared World Space Positions</strong>, derivatives of said attachments <a href="#ref_koskela2019">[Koskela et al. 2019]</a>, the change in variance of luminosity, and change in velocity of those buffers <a href="#ref_schied2018">[Schied 2018]</a>.</p>
<p>This can be done in combination with:</p>
<ul>
<li><p>Subsampling according to a dithering pattern, thus reducing the number of samples in your bluring kernel even more.</p></li>
<li><p>Drive your blur with more information such as surface roughness <a href="#ref_baktash2018">[Abdollah-shamshir-saz 2018]</a>, the aproximate Specular BRDF lobe <a href="#ref_tokuyoshi2015">[Tokuyoshi 2015]</a>, shadow penumbras <a href="#ref_liu2019">[Liu et al. 2019]</a>, etc.</p></li>
</ul>
<h3 id="caveats1">Caveats</h3>
<ul>
<li><p>Noise Dancing is more prominant and visible when it's blured, especially if your integrand is complex enough.</p></li>
<li><p>Balancing your blurring size with keeping high frequency details is a diffcult problem, with Machine Learning kernels doing better than variance guided filters.</p></li>
</ul>
<h3 id="variance-estimation">Variance Estimation</h3>
<p><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>Ïƒ</mi><mn>2</mn></msup><mo>=</mo><mfrac><mrow><mi mathvariant="normal">Î£</mi><mo stretchy="false">(</mo><mi>x</mi><mo>âˆ’</mo><mover accent="true"><mi>x</mi><mo>^</mo></mover><msup><mo stretchy="false">)</mo><mn>2</mn></msup></mrow><mi>n</mi></mfrac></mrow><annotation encoding="application/x-tex"> \sigma^2 = \frac{\Sigma(x - \hat{x})^2}{n} </annotation></semantics></math></span></span></span></p>
<p><strong>Variance</strong> is the squared difference of a signal's average (mean). One can take the average of the current signal and that signal squared with a 3x3 gaussian kernel, then taking the difference of the two.</p>
<p><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>Ïƒ</mi><mn>2</mn></msup><mo>=</mo><mfrac><mrow><mi mathvariant="normal">Î£</mi><msup><mi>x</mi><mn>2</mn></msup></mrow><mi>n</mi></mfrac><mo>âˆ’</mo><msup><mover accent="true"><mi>x</mi><mo>^</mo></mover><mn>2</mn></msup></mrow><annotation encoding="application/x-tex"> \sigma^2 = \frac{\Sigma x^2}{n} - \hat{x}^2 </annotation></semantics></math></span></span></span></p>
<pre><code><span>const</span> <span>float</span> radius = <span>2</span>; 
<span>vec2</span> sigmaVariancePair = <span>vec2</span>(<span>0.0</span>, <span>0.0</span>);
<span>float</span> sampCount = <span>0.0</span>;
<span>for</span> (<span>int</span> y = -radius; y &lt;= radius; ++y)
{
    <span>for</span> (<span>int</span> x = -radius; x &lt;= radius; ++x)
    {
        
        <span>if</span> (xx != <span>0</span> || yy != <span>0</span>) { <span>continue</span>; }

        
        <span>ivec2</span> p = ipos + <span>ivec2</span>(xx, yy);
        <span>vec4</span> curColor = <span>texelFetch</span>(tColor, p, <span>0</span>);

        
        
        <span>float</span> samp = luminance(curColor);
        <span>float</span> sampSquared = samp * samp;
        sigmaVariancePair += <span>vec2</span>(samp, sampSquared);

        sampCount += <span>1.0</span>;
    }
}
sigmaVariancePair /= sampCount;
<span>float</span> variance = <span>max</span>(<span>0.0</span>, sigmaVariancePair.y - sigmaVariancePair.x * sigmaVariancePair.x);
</code></pre>
<p>Christoph Schied does this in A-SVGF estimating the spatial variance as a combination of an edge avoiding guassian filter (just like in the a-trous guided filter) and using this in a feedback loop to drive the <code>accumulationFactor</code> during spatio-temporal reprojection:</p>
<pre><code>


<span>float</span> weightSum = <span>1.0</span>;
<span>int</span> radius = <span>3</span>; 
<span>vec2</span> moment = <span>texelFetch</span>(tMomentPrev, ipos, <span>0</span>).rg;
<span>vec4</span> c = <span>texelFetch</span>(tColor, ipos, <span>0</span>);
<span>float</span> histlen = <span>texelFetch</span>(tHistoryLength, ipos, <span>0</span>).r;

<span>for</span> (<span>int</span> yy = -radius; yy &lt;= radius; ++yy)
{
    <span>for</span> (<span>int</span> xx = -radius; xx &lt;= radius; ++xx)
    {
        
        <span>if</span> (xx != <span>0</span> || yy != <span>0</span>) { <span>continue</span>; }

        
        <span>ivec2</span> p = ipos + <span>ivec2</span>(xx, yy);
        <span>vec4</span> curColor = <span>texelFetch</span>(tColor, p, <span>0</span>);
        <span>float</span> curDepth = <span>texelFetch</span>(tDepth, p, <span>0</span>).x;
        <span>vec3</span> curNormal = <span>texelFetch</span>(tNormal, p, <span>0</span>).xyz;

        
        
        <span>float</span> l = luminance(curColor.rgb);

        <span>float</span> weightDepth = <span>abs</span>(curDepth - depth.x) / (depth.y * <span>length</span>(<span>vec2</span>(xx, yy)) + <span>1.0e-2</span>);
        <span>float</span> weightNormal = <span>pow</span>(<span>max</span>(<span>0</span>, <span>dot</span>(curNormal, normal)), <span>128.0</span>);

        <span>uint</span> curMeshID =  <span>floatBitsToUint</span>(<span>texelFetch</span>(tMeshID, p, <span>0</span>).r);

        <span>float</span> w = <span>exp</span>(-weightDepth) * weightNormal * (meshID == curMeshID ? <span>1.0</span> : <span>0.0</span>);

        <span>if</span> (<span>isnan</span>(w))
            w = <span>0.0</span>;

        weightSum += w;

        moment += <span>vec2</span>(l, l * l) * w;
        c.rgb += curColor.rgb * w;
    }
}

moment /= weightSum;
c.rgb /= weightSum;

varianceSpatial = (<span>1.0</span> + <span>2.0</span> * (<span>1.0</span> - histlen)) * <span>max</span>(<span>0.0</span>, moment.y - moment.x * moment.x);
outFragColor = <span>vec4</span>(c.rgb, (<span>1.0</span> + <span>3.0</span> * (<span>1.0</span> - histlen)) * <span>max</span>(<span>0.0</span>, moment.y - moment.x * moment.x));
</code></pre>
<h3 id="firefly-rejection">Firefly Rejection</h3>
<p>Firefly rejection can be done in a variety of ways, from adjusting how you're sampling during raytracing, to using filtering techniques or huristics about your output radiance.</p>
<h4>Increase Roughness Per Bounce</h4>
<pre><code>


<span>float</span> oldRoughness = payload.roughness;
payload.roughness = <span>min</span>(<span>1.0</span>, payload.roughness + roughnessBias);
roughnessBias += oldRoughness * <span>0.75</span>f;
</code></pre>
<h4>Clamp Rejection</h4>
<pre><code>
<span>vec3</span> fireflyRejectionClamp(<span>vec3</span> radiance, <span>vec3</span> maxRadiance)
{
    <span>return</span> <span>max</span>(radiance, maxRadiance);
}
</code></pre>
<h4>Variance Rejection</h4>
<pre><code>
<span>vec3</span> fireflyRejectionVariance(<span>vec3</span> radiance, <span>vec3</span> variance, <span>vec3</span> shortMean, <span>vec3</span> dev)
{
    <span>vec3</span> dev = <span>sqrt</span>(<span>max</span>(<span>1.0e-5</span>, variance));
    <span>vec3</span> highThreshold = <span>0.1</span> + shortMean + dev * <span>8.0</span>;
    <span>vec3</span> overflow = <span>max</span>(<span>0.0</span>, radiance - highThreshold);
    <span>return</span> radiance - overflow;
}
</code></pre>
<h2 id="conclusion">Conclusion</h2>
<p>Denoising results in a drastic difference in the quality of a given image. By using a combination of advanced sampling, encoding, accumulation, and filtering techniques, it's possible to achieve very high quality images for the small price of a few <code>ms</code>.</p>
<p>Aspects of this can also be driven by machine learning algorithms, which can serve as a smarter bluring tool than a bilateral filter, or trained to temporally accumulate samples faster. There's plenty of room for future work here!</p>
<p>Here's a few additional example implementations of real time ray tracing denoisers worth reviewing in no particular order:</p>
<ul>
<li><p><a href="https://www.linkedin.com/in/peterkristof/">Peter Kristof</a> of Microsoft made a really robust RTX Ambient Occlusion example with a robust implementation of SVGF <a href="https://github.com/microsoft/DirectX-Graphics-Samples/tree/master/Samples/Desktop/D3D12Raytracing/src/D3D12RaytracingRealTimeDenoisedAmbientOcclusion">here</a>.</p></li>
<li><p><a href="https://twitter.com/c_schied">Christof Shied</a> and Alexey Panteleev of NVIDIA wrote the denoiser for Quake 2 RTX (which is referenced heavily in this post) which is on Github <a href="https://github.com/NVIDIA/Q2RTX/blob/master/src/refresh/vkpt/shader/">here</a>.</p></li>
</ul>
<blockquote>
<p>Thanks for reading! Huge thanks to <a href="https://marmoset.co/">Marmoset</a> for being the perfect avenue to apply this research, love you guys ðŸ’–! Been reviewing the comments on <a href="https://twitter.com/KostasAAA/status/1176950866143862784">Twitter</a>/<a href="https://news.ycombinator.com/item?id=21074843">Hacker News</a>/<a href="https://www.reddit.com/r/raytracing/comments/d9eyak/ray_tracing_denoising/">Reddit</a>, feel free to DM me if you see any erata or want more details on any part of denoising!</p>
<p>~ Alain Galvan</p>
</blockquote>
<table>
<tbody><tr>
<td id="ref_veach1998">
[Veach 1998]<br>
<em><strong>Robust Monte Carlo Methods for Light Transport Simulation</strong></em><br>
Eric Veach<br>
Stanford University 1998<br>
<a href="http://graphics.stanford.edu/papers/veach_thesis/">graphics.stanford.edu</a>
</td>
</tr>

<tr>
<td id="ref_jarosz2019">
[Jarosz et al. 2019]<br>
<em><strong>Orthogonal array sampling for Monte Carlo rendering</strong></em><br>
Wojciech Jarosz, Afnan Enayet, Andrew Kensler, Charlie Kilpatrick and Per Christensen<br>
EGSR 2019<br>
<a href="https://cs.dartmouth.edu/~wjarosz/publications/jarosz19orthogonal.html">cs.dartmouth.edu</a>
</td>
</tr>

<tr>
<td id="ref_benyoub2019">
[Benyoub 2019]<br>
<em><strong>Leveraging Ray Tracing Hardware Acceleration In Unity</strong></em><br>
Anis Benyoub<br>
Digital Dragons 2019<br>
 <a href="https://auzaiffe.files.wordpress.com/2019/05/digital-dragons-leveraging-ray-tracing-hardware-acceleration-in-unity.pdf">auzaiffe.files.wordpress.com</a>
</td>
</tr>

<tr>
<td id="ref_baktash2018">
[Abdollah-shamshir-saz 2018]<br>
<em><strong>Voxel Based Hybrid Path Tracing with Spatial Denoising</strong></em><br>
Baktash Abdollah-shamshir-saz<br>
i3D 2018<br>
 <a href="http://toomuchvoltage.com/pub/vbhptwstd/abstract.pdf">toomuchvoltage.com</a>
</td>
</tr>

<tr>
<td id="ref_mara2017">
[Mara et al. 2017]<br>
<em><strong>An Efficient Denoising Algorithm for Global Illumination</strong></em><br>
Michael Mara, Morgan McGuire, Benedikt Bitterli and Wojciech Jarosz<br>
ACM, High Performance Graphics 2017<br>
 <a href="https://cs.dartmouth.edu/~wjarosz/publications/mara17towards.html">cs.dartmouth.edu</a>
</td>
</tr>

<tr>
<td id="ref_dammertz2010">
[Dammertz et al. 2010]<br>
<em><strong>Edge-Avoiding Ã€-Trous Wavelet Transform for fast Global Illumination Filtering</strong></em><br>
Holger Dammertz, Daniel Sewtz, Johannes Hanika and Hendrik P.A. Lensch<br>
High Performance Graphics 2010<br>
 <a href="https://jo.dreggn.org/home/2010_atrous.pdf">jo.dreggn.org</a>
</td>
</tr>

<tr>
<td id="ref_he2012">
[He et al. 2012]<br>
<em><strong>Guided Image Filtering</strong></em><br>
Kaiming He, Jian Sun and Xiaoou Tang<br>
2012<br>
 <a href="http://kaiminghe.com/publications/pami12guidedfilter.pdf">kaiminghe.com</a>
</td>
</tr>

<tr>
<td id="ref_chaitanya2017">
[R. Alla Chaitanya et al. 2017]<br>
<em><strong>Interactive Reconstruction of Monte Carlo Image Sequences using a Recurrent Denoising Autoencoder</strong></em><br>
Chakravarty R. Alla Chaitanya, Anton S. Kaplanyan, Christoph Schied, Marco Salvi, Aaron Lefohn, Derek Nowrouzezahrai and Timo Aila<br>
ACM 2017<br>
 <a href="https://research.nvidia.com/sites/default/files/publications/dnn_denoise_author.pdf">research.nvidia.com</a>
</td>
</tr>

<tr>
<td id="ref_kalantari2015">
[Khademi Kalantari et al. 2015]<br>
<em><strong>A Machine Learning Approach for Filtering Monte Carlo Noise</strong></em><br>
Nima Khademi Kalantari, Steve Bako and Pradeep Sen<br>
ACM Transactions on Graphics (TOG) 2015<br>
 <a href="http://cvc.ucsb.edu/graphics/Papers/SIGGRAPH2015_LBF/">cvc.ucsb.edu</a>
</td>
</tr>

<tr>
<td id="ref_mara2017">
[Mara et al. 2017]<br>
<em><strong>An Efficient Denoising Algorithm for Global Illumination</strong></em><br>
Michael Mara, Morgan McGuire, Benedikt Bitterli and Wojciech Jarosz<br>
ACM, High Performance Graphics 2017<br>
 <a href="https://cs.dartmouth.edu/~wjarosz/publications/mara17towards.html">cs.dartmouth.edu</a>
</td>
</tr>

<tr>
<td id="ref_schied2017">
[Schied 2017]<br>
<em><strong>Spatiotemporal Variance-Guided Filtering</strong></em><br>
Christoph Schied<br>
2017<br>
 <a href="http://research.nvidia.com/publication/2017-07_Spatiotemporal-Variance-Guided-Filtering%3A">research.nvidia.com</a>
</td>
</tr>

<tr>
<td id="ref_baktash2018">
[Abdollah-shamshir-saz 2018]<br>
<em><strong>Voxel Based Hybrid Path Tracing with Spatial Denoising</strong></em><br>
Baktash Abdollah-shamshir-saz<br>
i3D 2018<br>
 <a href="http://toomuchvoltage.com/pub/vbhptwstd/abstract.pdf">toomuchvoltage.com</a>
</td>
</tr>

<tr>
<td id="ref_schied2018">
[Schied 2018]<br>
<em><strong>Gradient Estimation for Real-Time Adaptive Temporal Filtering</strong></em><br>
Christoph Schied<br>
2018<br>
 <a href="https://cg.ivd.kit.edu/atf.php">cg.ivd.kit.edu</a>
</td>
</tr>

<tr>
<td id="ref_koskela2019">
[Koskela et al. 2019]<br>
<em><strong>Blockwise Multi-Order Feature Regression for Real-Time Path Tracing Reconstruction</strong></em><br>
Matias Koskela and Kalle Immonen<br>
ACM Transactions on Graphics (TOG) 2019<br>
 <a href="http://www.tut.fi/vga/publications/Blockwise_Multi-Order_Feature_Regression_for_Real-Time_Path_Tracing_Reconstruction.html">tut.fi</a>
</td>
</tr>

<tr>
<td id="ref_andersson2019">
[Andersson et al. 2019]<br>
<em><strong>Temporally Dense Ray Tracing</strong></em><br>
P. Andersson, J. Nilsson, M. Salvi, J. Spjut and T. Akenine-MÃ¶ller<br>
High Performance Graphics 2019<br>
 <a href="https://research.nvidia.com/sites/default/files/pubs/2019-07_Temporally-Dense-Ray//temporally-dense-ray-tracing.pdf">research.nvidia.com</a>
</td>
</tr>

<tr>
<td id="ref_mjercik2019">
[Majercik et al. 2019]<br>
<em><strong>Dynamic Diffuse Global Illumination with Ray-Traced Irradiance Fields</strong></em><br>
Zander Majercik, Jean-Philippe Guertin, Derek Nowrouzezahrai and Morgan McGuire<br>
2019<br>
 <a href="http://jcgt.org/published/0008/02/01/">jcgt.org</a>
</td>
</tr>

<tr>
<td id="ref_kristof2019">
[Kristof 2019]<br>
<em><strong>Real-Time Denoised Raytraced Ambient Occlusion</strong></em><br>
Peter Kristof<br>
Microsoft 2019<br>
 <a href="https://github.com/microsoft/DirectX-Graphics-Samples/tree/master/Samples/Desktop/D3D12Raytracing/src/D3D12RaytracingRealTimeDenoisedAmbientOcclusion">Github</a>
</td>
</tr>

<tr>
<td id="ref_cornel2014">
[Cornel 2014]<br>
<em><strong>Analysis of Forced Random Sampling</strong></em><br>
Daniel Cornel<br>
2014<br>
 <a href="http://drivenbynostalgia.com/files/DA.pdf">drivenbynostalgia.com</a>
</td>
</tr>

<tr>
<td id="ref_shirley2018">
[Shirley 2018]<br>
<em><strong>Flavors of Sampling in Ray Tracing</strong></em><br>
Pete Shirley<br>
BlogSpot 2018<br>
 <a href="http://psgraphics.blogspot.com/2018/10/flavors-of-sampling-in-ray-tracing.html">psgraphics.blogspot.com</a>
</td>
</tr>

<tr>
<td id="ref_schied2019">
[Schied et al. 2019]<br>
<em><strong>Real-Time Path Tracing and Denoising in 'Quake 2'</strong></em><br>
Christoph Schied and Alexey Panteleev<br>
GDC 2019<br>
 <a href="https://youtu.be/FewqoJjHR0A">youtu.be</a>
</td>
</tr>

<tr>
<td id="ref_wolfe2017">
[Wolfe 2017]<br>
<em><strong>When Random Numbers Are Too Random: Low Discrepancy Sequences</strong></em><br>
Alan Wolfe<br>
The Blog at the Bottom of the Sea 2017<br>
 <a href="https://blog.demofox.org/2017/05/29/when-random-numbers-are-too-random-low-discrepancy-sequences/">blog.demofox.org</a>
</td>
</tr>

<tr>
<td id="ref_roberts2018">
[Roberts 2018]<br>
<em><strong>The Unreasonable Effectiveness of Quasirandom Sequences</strong></em><br>
Martin Roberts<br>
2018<br>
 <a href="http://extremelearning.com.au/unreasonable-effectiveness-of-quasirandom-sequences/">extremelearning.com.au</a>
</td>
</tr>

<tr>
<td id="ref_wong1997">
[Wong et al. 1997]<br>
<em><strong>Sampling with Hammersley and Halton Points</strong></em><br>
Tien-Tsin Wong, Wai-Shing Luk and Pheng-Ann Heng<br>
1997<br>
 <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.552.864&amp;rep=rep1&amp;type=pdf">citeseerx.ist.psu.edu</a>
</td>
</tr>

<tr>
<td id="ref_benyoub2019">
[Benyoub 2019]<br>
<em><strong>Leveraging Ray Tracing Hardware Acceleration In Unity</strong></em><br>
Anis Benyoub<br>
Digital Dragons 2019<br>
 <a href="https://auzaiffe.files.wordpress.com/2019/05/digital-dragons-leveraging-ray-tracing-hardware-acceleration-in-unity.pdf">auzaiffe.files.wordpress.com</a>
</td>
</tr>

<tr>
<td id="ref_heitz2019">
[Heitz et al. 2019]<br>
<em><strong>Distributing Monte Carlo Errors as a Blue Noise in Screen Space by Permuting Pixel Seeds Between Frames</strong></em><br>
Eric Heitz and Laurent Belcour<br>
EGSR 2019<br>
 <a href="https://eheitzresearch.wordpress.com/772-2/">eheitzresearch.wordpress.com</a>
</td>
</tr>

<tr>
<td id="ref_wolfe2017b">
[Wolfe 2017]<br>
<em><strong>Demystifying Floating Point Precision</strong></em><br>
Alan Wolfe<br>
The Blog at the Bottom of the Sea 2017<br>
 <a href="https://blog.demofox.org/2017/11/21/floating-point-precision/">blog.demofox.org</a>
</td>
</tr>

<tr>
<td id="ref_ward1988">
[Ward et al. 1988]<br>
<em><strong>A Ray Tracing Solution for Diffuse Interreflection</strong></em><br>
Greg Ward, Francis Rubinstein and Robert Clear<br>
Siggraph 1988<br>
 <a href="https://floyd.lbl.gov/radiance/papers/sg88/paper.html">floyd.lbl.gov</a>
</td>
</tr>

<tr>
<td id="ref_schied2019">
[Schied et al. 2019]<br>
<em><strong>Real-Time Path Tracing and Denoising in 'Quake 2'</strong></em><br>
Christoph Schied and Alexey Panteleev<br>
GDC 2019<br>
 <a href="https://youtu.be/FewqoJjHR0A">youtu.be</a>
</td>
</tr>

<tr>
<td id="ref_schied2017">
[Schied 2017]<br>
<em><strong>Spatiotemporal Variance-Guided Filtering</strong></em><br>
Christoph Schied<br>
2017<br>
 <a href="http://research.nvidia.com/publication/2017-07_Spatiotemporal-Variance-Guided-Filtering%3A">research.nvidia.com</a>
</td>
</tr>

<tr>
<td id="ref_schied2018">
[Schied 2018]<br>
<em><strong>Gradient Estimation for Real-Time Adaptive Temporal Filtering</strong></em><br>
Christoph Schied<br>
2018<br>
 <a href="https://cg.ivd.kit.edu/atf.php">cg.ivd.kit.edu</a>
</td>
</tr>

<tr>
<td id="ref_dammertz2010">
[Dammertz et al. 2010]<br>
<em><strong>Edge-Avoiding Ã€-Trous Wavelet Transform for fast Global Illumination Filtering</strong></em><br>
Holger Dammertz, Daniel Sewtz, Johannes Hanika and Hendrik P.A. Lensch<br>
High Performance Graphics 2010<br>
 <a href="https://jo.dreggn.org/home/2010_atrous.pdf">jo.dreggn.org</a>
</td>
</tr>

<tr>
<td id="ref_he2012">
[He et al. 2012]<br>
<em><strong>Guided Image Filtering</strong></em><br>
Kaiming He, Jian Sun and Xiaoou Tang<br>
2012<br>
 <a href="http://kaiminghe.com/publications/pami12guidedfilter.pdf">kaiminghe.com</a>
</td>
</tr>

<tr>
<td id="ref_li2012">
[Li et al. 2012]<br>
<em><strong>SURE-based Optimization for Adaptive Sampling and Reconstruction</strong></em><br>
Tzu-Mao Li, Yu-Ting Wu and Yung-Yu Chang<br>
Siggraph Asia 2012<br>
 <a href="http://cseweb.ucsd.edu/~ravir/274/15/papers/a194-li.pdf">cseweb.ucsd.edu</a>
</td>
</tr>

<tr>
<td id="ref_dammertz2010">
[Dammertz et al. 2010]<br>
<em><strong>Edge-Avoiding Ã€-Trous Wavelet Transform for fast Global Illumination Filtering</strong></em><br>
Holger Dammertz, Daniel Sewtz, Johannes Hanika and Hendrik P.A. Lensch<br>
High Performance Graphics 2010<br>
 <a href="https://jo.dreggn.org/home/2010_atrous.pdf">jo.dreggn.org</a>
</td>
</tr>

<tr>
<td id="ref_koskela2019">
[Koskela et al. 2019]<br>
<em><strong>Blockwise Multi-Order Feature Regression for Real-Time Path Tracing Reconstruction</strong></em><br>
Matias Koskela and Kalle Immonen<br>
ACM Transactions on Graphics (TOG) 2019<br>
 <a href="http://www.tut.fi/vga/publications/Blockwise_Multi-Order_Feature_Regression_for_Real-Time_Path_Tracing_Reconstruction.html">tut.fi</a>
</td>
</tr>

<tr>
<td id="ref_schied2018">
[Schied 2018]<br>
<em><strong>Gradient Estimation for Real-Time Adaptive Temporal Filtering</strong></em><br>
Christoph Schied<br>
2018<br>
 <a href="https://cg.ivd.kit.edu/atf.php">cg.ivd.kit.edu</a>
</td>
</tr>

<tr>
<td id="ref_baktash2018">
[Abdollah-shamshir-saz 2018]<br>
<em><strong>Voxel Based Hybrid Path Tracing with Spatial Denoising</strong></em><br>
Baktash Abdollah-shamshir-saz<br>
i3D 2018<br>
 <a href="http://toomuchvoltage.com/pub/vbhptwstd/abstract.pdf">toomuchvoltage.com</a>
</td>
</tr>

<tr>
<td id="ref_tokuyoshi2015">
[Tokuyoshi 2015]<br>
<em><strong>Specular Lobe-Aware Filtering and Upsampling for Interactive Indirect Illumination</strong></em><br>
Yusuke Tokuyoshi<br>
Eurographics 2015<br>
 <a href="http://www.jp.square-enix.com/tech/library/pdf/Specular%20Lobe-Aware%20Filtering%20and%20Upsampling%20for%20Interactive%20Indirect%20Illumination.pdf">jp.square-enix.com</a>
</td>
</tr>

<tr>
<td id="ref_liu2019">
[Liu et al. 2019]<br>
<em><strong>Cinematic Rendering in UE4 with Real-Time Ray Tracing and Denoising</strong></em><br>
Edward Liu, Ignacio Llamas, Juan CaÃ±ada and Patrick Kelly<br>
Ray Tracing Gems 2019<br>
 <a href="http://www.realtimerendering.com/raytracinggems/">realtimerendering.com</a>
</td>
</tr>
</tbody></table></article></div></div>
    </div>
    <footer>
        <div>created by <a href="https://buildstarted.com">buildstarted</a> &copy; 2020 <a href="/about">about</a></div>
        <div>Share this page on social media: copy and paste this url https://linksfor.dev/</div>
        <div>If you prefer RSS: <a href="https://linksfor.dev/feed.xml">https://linksfor.dev/feed.xml</a></div>
        <div>Customer satisfaction guaranteed to be optional.</div>
    </footer>
    
    <script async defer>
        _dna = window._dna || {};
        _dna.siteId = "linksfor.devs";
        _dna.outlink = true;

        (function() {
            let dna = document.createElement('script');
            dna.type = 'text/javascript';
            dna.async = true;
            dna.src = '//dna.buildstarted.com/t.js';
            let s = document.getElementsByTagName('script')[0];
            s.parentNode.insertBefore(dna, s);
        })();
    </script>
    <noscript><img src="//dna.buildstarted.com/g?siteId=linksfor.devs"/></noscript>
</body>
</html>