<!DOCTYPE html>
<html lang="en">
<head>
    <title>
Benchmarks are hard &#xB7; jbd.dev - linksfor.dev(s)    </title>
    <meta charset="utf-8">
    <link rel="alternate" type="application/rss+xml" title="Linksfor.dev(s) feed" href="https://linksfor.dev/feed.rss" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta name="google" value="notranslate">
    <link rel="stylesheet" type="text/css" href="/style.css" />
    <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon">
    <link rel="icon" href="/favicon.ico" type="image/x-icon">
    <meta property="og:title" content="Benchmarks are hard &#xB7; jbd.dev - linksfor.dev(s)"/>
    <meta property="og:description" content="Benchmarking generally mean producing some measurements&#xA;from a specific program or workload to typically understand&#xA;and compare the performance characteristics of the&#xA;benchmarked workload."/>
    <meta property="og:type" content="website"/>
    <meta property="og:url" content="https://jbd.dev/benchmarks-are-hard/"/>

<meta property="og:site_name" content="linksfor.dev(s)" />
</head>
<body>
    <div class="grid">
        <h1>
<a href="/" style="color:inherit">linksfor.dev(s)</a>
        </h1>
        <title>linksfor.dev(s) - Benchmarks are hard &#xB7; jbd.dev</title>
<div class="readable">
        <h1>Benchmarks are hard &#xB7; jbd.dev</h1>
            <div>Reading time: 3-4 minutes</div>
        <div>Posted here: 11 Jul 2019</div>
        <p><a href="https://jbd.dev/benchmarks-are-hard/">https://jbd.dev/benchmarks-are-hard/</a></p>
        <hr/>
<div id="readability-page-1" class="page"><div><p>Benchmarking generally mean producing some measurements
from a specific program or workload to typically understand
and compare the performance characteristics of the
benchmarked workload.</p>
<p>Benchmarks can be useful for:</p>
<ul>
<li>Optimizing costly workloads.</li>
<li>Understanding how the underlying platform impact the cost.</li>
</ul>
<p>Benchmarking can give you insights
about your workload on various dimensions such as CPU cycles
spent or memory allocation done for a given task.
These measurements (even though they might be coming
from idealized environments) might give you some hints
about the cost. They may help you to pick the right algorithm
or optimize an overwhelmingly expensive workload.</p>
<p>Benchmarking can also give you insights about the underlying
runtime, operating system, and hardware. And you might find
insights to compare how each of these elements impact your
performance even if you don't change the code. You might want
to run the same suite of benchmarks on a new hardware to
estimate the impact of the new hardware on certain calls.</p>
<p>Benchmarking requires a deep understanding of all layers
you depend on. Consider CPU benchmarking. Any aspect below
can significantly impact your results:</p>
<ul>
<li>Whether the data is available in the CPU cache or not.</li>
<li>Whether you hit a garbage collection or not.</li>
<li>Whether compiler has optimized some cases or not.</li>
<li>Whether there is concurrency or not.</li>
<li>Whether you are sharing the cores with anything else.</li>
<li>Whether your cores are virtual or physical.</li>
<li>Whether the branch detector will do the same thing or not.</li>
<li>Whether the code you are benchmarking has any side effects
or not. Compilers are really good optimizing cases if it
has no impact on the global state.</li>
</ul>
<p>This is why when we talk about benchmarking, it is not
good practice to limit ourselves to the user-space code.
It can sometimes turn into a detective's job to design and evaluate benchmarks.
This is why complicated and long workloads
are harder to benchmark reliably. It becomes harder to
do the right accounting and figure out whether it is the
contention, context switches, caching, garbage collector,
compiler optimizations or the user-space code.</p>
<p>Even though microbenchmarks can give some insights, they
cannot replicate the situations how the workload is going
to perform in production. Replicating an average production
environment for microbenchmarking purposes is almost an
impossible task. This is why microbenchmarks don't always
serve  a <em>good starting point</em> if you want to evaluate your
production performance.</p>
<p>To conclude, replicating production-like situations is not
trivial.
Understanding the impact of underlying stack requires
a lot of experience and expertise.
Designing and evaluating benchmarks are not easy.
Examining the production services and understanding
the call patterns and pointing out the hot calls
may provide better insights about your production.
In the future articles, I will cover how
we think about performance evaluation in production.</p>
</div></div></div>
    </div>
    <footer>
        <div>created by buildstarted &copy; 2020 <a href="/about">about</a></div>
        <div>Share this page on social media: copy and paste this url https://linksfor.dev/</div>
        <div>If you prefer RSS: <a href="https://linksfor.dev/feed.xml">https://linksfor.dev/feed.xml</a></div>
		<div>Customer satisfaction guaranteed to be optional.</div>
    </footer>
    
    <script async defer>
        _dna = window._dna || {};
        _dna.siteId = "linksfor.devs";
        _dna.outlink = true;

        (function () {
            let dna = document.createElement('script');
            dna.type = 'text/javascript';
            dna.async = true;
            dna.src = '//dna.buildstarted.com/t.js';
            let s = document.getElementsByTagName('script')[0];
            s.parentNode.insertBefore(dna, s);
        })();
    </script>
    <noscript><img src="//dna.buildstarted.com/g?siteId=linksfor.devs"/></noscript>
</body>
</html>