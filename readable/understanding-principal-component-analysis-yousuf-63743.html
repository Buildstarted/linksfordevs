<!DOCTYPE html>
<html lang="en">
<head>
    <title>
Understanding principal component analysis | Yousuf - linksfor.dev(s)    </title>
    <meta charset="utf-8">
    <link rel="alternate" type="application/rss+xml" title="Linksfor.dev(s) feed" href="https://linksfor.dev/feed.rss" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta name="google" value="notranslate">
    <link rel="stylesheet" type="text/css" href="/style.css" />
    <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon">
    <link rel="icon" href="/favicon.ico" type="image/x-icon">
    <meta property="og:title" content="Understanding principal component analysis | Yousuf - linksfor.dev(s)"/>
    <meta property="og:description" content="In this post, we&#x2019;ll take a deep dive into PCA, from both a mathematical and implementation perspective. We&#x2019;ll derive the equation from the ground up, look at how we can compute it and finally end with what it can be used for. This post is primarily targeted at those with a basic understanding of PCA but want to know the assumptions it relies on, its properties and derive how it can be computed."/>
    <meta property="og:type" content="website"/>
    <meta property="og:url" content="https://ymohamedahmed.github.io/post/pca/"/>

<meta property="og:site_name" content="linksfor.dev(s)" />
</head>
<body>
    <div class="devring" style="background: #222">
        <div style="text-align:center">Explore other dev related sites in this ring. If you would like to join this ring <a href="https://devring.club">click here</a>.</div>
        <div class="grid">
            <div style="display: grid; grid-template-columns: .5fr 1fr 1fr 1fr; text-align: center;">
                <span class="devring-title"><a href="https://devring.club/">devring.club</a></span>
                <a href="https://devring.club/sites/1/prev" class="devring-previous">Previous</a>
                <a href="https://devring.club/random" class="devring-random">Random</a>
                <a href="https://devring.club/sites/1/next" class="devring-next">Next</a>
            </div>
        </div>
    </div>
    <div class="grid">
        <h1>
<a href="/" style="color:inherit">linksfor.dev(s)</a>
        </h1>
        <title>linksfor.dev(s) - Understanding principal component analysis | Yousuf</title>
<div class="readable">
        <h1>Understanding principal component analysis | Yousuf</h1>
            <div>Reading time: 18-23 minutes</div>
        <div>Posted here: 03 Jul 2020</div>
        <p><a href="https://ymohamedahmed.github.io/post/pca/">https://ymohamedahmed.github.io/post/pca/</a></p>
        <hr/>
<div id="readability-page-1" class="page"><div>
      <p>In this post, we’ll take a deep dive into PCA, from both a mathematical and implementation perspective.
We’ll derive the equation from the ground up, look at how we can compute it and finally end with what it can be used for. This post is primarily targeted at those with a basic understanding of PCA but want to know the assumptions it relies on, its properties and derive how it can be computed.</p>
<p>If you’re interested in the repository containing the Jupyter notebook from which this is derived, it’s 
<a href="https://github.com/ymohamedahmed/blog-posts/tree/master/pca" target="_blank" rel="noopener">here</a>.</p>
<h2 id="the-optimal-coding-perspective">The optimal coding perspective</h2>
<h3 id="finding-a-low-dimensional-representation">Finding a low dimensional representation</h3>
<p>PCA can be thought of as finding a low-dimensional representation of a set of vectors. Given points in an <em>n</em>-dimensional space, we might wish to find some new <em>k</em>-dimensional space (with <em>k</em> &lt; <em>n</em>) which captures as much of the <em>essence</em> of the original space as possible. The exact definition of capturing the ‘<em>essence</em>’ is subject to design, however, we can consider it from multiple perspectives.</p>


<p><img src="https://ymohamedahmed.github.io/post/pca/images/pca.svg"></p><h3 id="the-notion-of-reconstruction-error">The notion of reconstruction error</h3>
<p>If we take our low-dimensional representation and attempt to recover the original <em>n</em>-dimensional vector of each point, we could measure how much each point varies from its reconstruction. The difference between each reconstruction and the original, is one way of measuring the effectiveness of our new <em>k</em>-dimensional space and is the approach taken by PCA.  Naturally, this requires a definition of a <em>similarity</em> between two matrices. If we have a matrix  of our original points and our reconstruction  then we can define the difference between them as a sum of the square of errors .</p>
<p>This quantity is known as the <em>Frobenius</em> norm of the matrix  and is essentially an extension of the L2 () norm from vectors to matrices. It is just a fancy name for squaring every element in a matrix and taking their sum. Crucially, however, we can see that the Frobenius norm of a matrix, , is precisely equivalent to  (see the illustration below).</p>


<figure>
    <img src="https://ymohamedahmed.github.io/post/pca/images/output_smaller.gif">
    <figcaption>Showing the relationship between the Frobenius norm and trace operator: </figcaption>
</figure> 

<p>As a result, our error of interest, can be computed as , this will come in handy since the trace operator comes with a bunch of neat tricks for manipulating the matrices involved.</p>
<h3 id="pca-assumptions">PCA Assumptions</h3>
<p>We’ve defined how we’re going to evaluate this reconstruction, but not at all the means of performing the coding or its inverse.</p>
<p>PCA chooses to implement both the encoding and decoding as a matrix multiplication. So we can think of PCA as finding a matrix  that will transform our input,  to our coded version  by a matrix multiplication (i.e. ).
The matrix, , being our original data of  rows and  columns.</p>
<p>We might choose, however, to use some matrix,  which reduces the number of dimensions of our data from  to . To make it easier, we can tag the matrix with the new number of dimensions. If this is the case, then  is of size  and so that the resulting coding, , is of size .</p>
<p>One of the most crucial assumptions made by PCA is that the transformation matrix, , has <strong>orthonormal</strong> columns. Criticially, this does not, technically, make it an <strong>orthogonal</strong> matrix since it may not be square and the rows may not be orthonormal.</p>
<p>This assumption is useful since it simplifies the reconstruction process, in fact,  can be computed as , i.e. we can use the transpose of the encoding matrix to perform the decoding.</p>
<p>To understand exactly why the reconstruction can be performed by the transpose of the matrix let’s consider a code  which we’ve generated as  and are decoding using .</p>
<p>The reconstruction error is .</p>
<p>We can take the derivative with respect to the code , , which given the definition of  is equal to .</p>
<p>We defined our matrix to have orthonormal columns so we know that  because the dot product of each column with itself will be one. Hence, our gradient is  . This tells us that our encoding/decoding system is at a stationary point in the reconstruction error. Note that this only tells us for an individual point that was encoded with a matrix that has orthonormal columns, decoding with the transpose is a good idea. It doesn’t tell us anything about how to pick  or what happens if you use it for multiple points.</p>
<p>Given these assumptions, we can reframe our problem as finding the coding matrix which minimises the reconstruction error.</p>
<p>This means that formally, for some given value , we wish to discover the matrix .</p>

<h3 id="discovering-the-coding-function">Discovering the coding function</h3>
<p>Given that we wish to minimise the reconstruction error, let us attempt to discover the transformation precisely capable of this.</p>
<p>We’ll need a few tricks to get us there.</p>
<ul>
<li></li>
<li></li>
<li></li>
</ul>
<p>From the visual illustration, recall that we can write the reconstruction error as .</p>
<p>By using the rules  this can be expanded into </p>
<p>We are, however, only interested in the effect of the matrix  so we’ll axe the first term </p>
<p>Crucially, however, the trace operator has two useful properties for us:</p>
<ul>
<li>it’s insensitive to cyclic permutations (i.e. )</li>
<li>the trace of a sum of matrices is the sum of their traces (i.e )</li>
</ul>
<p>Given this, we can write the equation of interest as: .</p>
<p>Furthermore, since we define the columns of  as being orthonormal,  and so we can rewrite the second term as  which matches the form of the first term (from permuting it) and therefore can be written as the following maximisation.</p>

<p>This is sets up exactly the quantity that PCA is attempting to maximise.</p>
<h4 id="relationship-with-eigendecomposition">Relationship with eigendecomposition</h4>
<p>Having happily derived the maximisation problem, I’ll do my best to convince you that this is, in fact, maximised by having the k-columns of  as the k eigenvectors of  with largest eigenvalues. This will roughly take the form of an inductive proof. So let’s start with considering , that is if we had the choice of using a single vector, what would we go with?</p>
<h5 id="base-case">Base case</h5>
<p>We’ll use the lowercase, , to emphasise that it’s a vector rather than matrix. So let’s try and tackle:</p>

<p>The trace of a scalar, is defined as itself, so we wish to find </p>
<p>However, recall that we mandated as our first assumption that the columns of  are orthonormal and hence we know that .</p>
<p>This allows us to reframe this as a constrained optimisation problem and use Lagrange multipliers to discover which vector minimises the quantity of interest whilst satisfying the norm constraint. For convenience, we’ll use .</p>
<p>Hence we can write this as:
</p>
<blockquote>
<p>TL;DR Lagrange multipliers are just a technique for optimising a function over a specific region, in our case we’re not looking for all vectors but only ones with a norm of 1. We rewrite the function to include the constraint and take the derivative of the new function, known as the <em>Lagrangian</em>.</p>
</blockquote>
<p>By taking the derivative of  we can find the solution.</p>

<p>The derivative is precisely zero when , that is when  is an eigenvector of the matrix .
Furthermore, we can see that the quantity in fact takes on the associated eigenvalue since , where  is the associated eigenvalue. This is always possible since we can mandate that the eigenvector has a unit norm.</p>
<p>Hence, we can see here that for the base case, the best vector is the eigenvector with the largest eigenvalue.</p>
<h5 id="inductive-step">Inductive step</h5>
<p>Having showcased our hypothesis for , let’s see what happens for other values of . Precisely, let us show that if our hypothesis is true for some arbitrary  then it is also true for some . That is the best encoding matrix with  columns, consists of the  unit eigenvectors with the largest associated eigenvalues.</p>
<p>We are now interested in the following problem.</p>

<p>The key insight comes from thinking of the  as the previous matrix,, for which we have assumed the property to hold, and a new column which we would like to show corresponds the eigenvector with the -th largest eigenvalue.</p>


<p><img src="https://ymohamedahmed.github.io/post/pca/images/pca-proof.svg"></p><p>The matrix of interest is split into two new matrices,  and  as shown in the diagram. So we can rewrite the maximisation and expand it.</p>


<p>We’ll slowly get rid of three of these four terms and show our initial hypothesis holds.</p>
<ul>
<li>First term: is a constant from our perspective and so we don’t have to consider it</li>
<li>Second and third term: I’ll try to convince you from the diagram that both of these have zero entries along the diagonal. The diagram shows why it’s true for the second term and by symmetry we can see it’s also true for the third term. Since they have zeroes along the diagonal they will have a trace of zero and hence by splitting the trace we end up with  as the final maximisation.</li>
</ul>


<p><img src="https://ymohamedahmed.github.io/post/pca/images/pca-proof-2.svg"></p><p>However, if we look carefully enough we can see that the fourth term has zeroes everywhere other than in the bottom right corner, which I denote by .</p>


<p><img src="https://ymohamedahmed.github.io/post/pca/images/pca-proof-3.svg"></p><p>Furthermore, from the diagram we can see that  and that .</p>
<p>Now we can, finally, reframe our inductive case as </p>
<p>However, recall that we mandated that all the columns of the encoding matrix are orthonormal and hence, this condition is maximised, precisely, by the eigenvector with (k+1)-th largest eigenvalue. It is important to note that the eigenvectors (corresponding to different eigenvalues) are orthogonal here, since  is real and symmetric. Given that we chose our base case as an eigenvector, we know that if we pick the  vector as a different eigenvector, it will necessarily be orthogonal to all the other columns.</p>
<p>Finally, this induction cannot continue indefinitely, since we cannot have  orthogonal -dimensional vectors. This is intuitive since PCA is designed to find a lower-dimensional representation not increase the number of dimensions.
I hope I have convinced you that the we can find the principal components by eigendecomposition of .</p>
<p>That is, if we write it as , with the matrix  containing the eigenvectors as columns and with eigenvalues in the vector  then we can pick our encoding matrix as the  eigenvectors with largest eigenvalues.</p>
<h3 id="relationship-with-svd">Relationship with SVD</h3>
<p>Recall that singular value decomposition is defined as rewriting a matrix, , as a product, , of orthogonal matrices,  and  and a diagonal matrix .</p>
<p>Crucially, however,  is defined as the eigenvectors of the matrix . Sound familiar? That’s precisely the eigenvectors that we need for the PCA encoding.</p>
<p>Hence, we know that if , then we could also write , where  is our encoding matrix.</p>
<p>This now leaves us with two separate but equivalent ways of computing our encoding matrix — SVD or eigendecomposition.</p>
<h2 id="the-decorrelation-perspective">The decorrelation perspective</h2>
<p>There are two key properties of PCA to understand:</p>
<ul>
<li>Minimisation of the reconstruction error</li>
<li>‘Decorrelation’ of the features of data</li>
</ul>
<p>We’ve just finished showing the first one so let’s take a quick look at the second (much easier to show!).</p>
<p>Recall the unbiased estimator of the covariance of a centered matrix, , is  where  is the number of rows of the matrix. The covariance matrix is such that , that is it tells us the covariance between random variables.</p>
<p>If we can show that the covariance of PCA-encoded data,  is a diagonal matrix, then we will know that covariance between each feature derived by PCA is zero.
We know that , if  is our original data and  is the encoding matrix.</p>
<p>Therefore .</p>
<p>However, recall that from the SVD approach we know that  for a diagonal matrix .</p>
<p>From this we can rewrite  as , but since SVD gives us orthogonal matrices we know that . Hence, </p>
<p>At this point, we’re very close and you might be tempted to suggest that  since  is a diagonal matrix, and the transpose of a square, diagonal matrix is itself. However,  is not necessarily square! In fact, from the properties of SVD it has the same dimensions as . We do, however, know that the matrix  is necessarily diagonal since it is a product of two diagonal matrices, and so the point still stands.</p>
<p>Hence, we have shown that the resulting encoding has a diagonal covariance matrix, which means that the features have no linear dependence between each other. We’ll be able to visualise this later!</p>
<p>It’s crucial to understand that a zero covariance does not imply that no relationship exists between the features of the transformed space but rather that no linear relationship can exist. The stronger condition of having no relationship, would require a different algorithm and is the approach taken by Independent Component Analysis.</p>
<p>One of the beauties of PCA is that there are lots of ways to think about it. I’ve only presented one, but you can also think about it as maximising variance, or transforming the coordinate axes amongst many others.</p>
<h3 id="implementation">Implementation</h3>
<p>I didn’t say much about what PCA can be used for, but now that we have an understanding of where the computation comes from, let’s take a quick look at what it can be used for. We’ll look at it as a technique for compression and for visualisation of high-dimensional data.</p>
<p>We can implement it in two-lines both via eigendecomposition and SVD. Although, we are essentially cheating since NumPy is doing all the heavy-lifting by computing the eigenvectors for us. That being said, computing eigenvectors should be itself the topic of a separate blog post.</p>
<p>The only nuance is that we need to <em>center</em> the matrix. This means that each column is supposed to have a mean of zero. It is rather rare to see a description of why we need to do this, in fact, it will also very frequently work without doing it. However, recall that when we proved that PCA decorrelates data we made the assumption that the data was centered. Without this assumption we wouldn’t have been able to use  as an estimate of the covariance matrix.</p>
<div><div>
<table><tbody><tr><td>
<pre><code><span> <span>1</span>
</span><span> <span>2</span>
</span><span> <span>3</span>
</span><span> <span>4</span>
</span><span> <span>5</span>
</span><span> <span>6</span>
</span><span> <span>7</span>
</span><span> <span>8</span>
</span><span> <span>9</span>
</span><span><span>10</span>
</span><span><span>11</span>
</span><span><span>12</span>
</span></code></pre></td>
<td>
<pre><code data-lang="python"><span><span><span>def</span></span></span><span> </span><span><span><span>pca_eig</span></span></span><span><span><span>(</span></span></span><span><span>X</span></span><span><span><span>)</span>:</span></span>
    X <span>=</span> center<span>(</span>X<span>)</span>
    eigenvals<span>,</span> eigenvectors <span>=</span> np<span>.</span>linalg<span>.</span>eig<span>(</span>X<span>.</span>T <span>@</span> X<span>)</span>
    <span><span>return</span></span> eigenvals<span>,</span> eigenvectors

<span><span><span>def</span></span></span><span> </span><span><span><span>pca_svd</span></span></span><span><span><span>(</span></span></span><span><span>X</span></span><span><span><span>)</span>:</span></span>
    X <span>=</span> center<span>(</span>X<span>)</span>
    _<span>,</span>root_eigenvalues<span>,</span>x <span>=</span> np<span>.</span>linalg<span>.</span>svd<span>(</span>X<span>)</span>
    <span><span>return</span></span> root_eigenvalues<span>**</span><span><span>2</span></span><span>,</span> x

<span><span><span>def</span></span></span><span> </span><span><span><span>center</span></span></span><span><span><span>(</span></span></span><span><span>X</span></span><span><span><span>)</span>:</span></span>
    <span><span>return</span></span> X<span>-</span>np<span>.</span>mean<span>(</span>X<span>,</span> axis<span>=</span><span><span>0</span></span><span>)</span>
</code></pre></td></tr></tbody></table>
</div>
</div><h4 id="toy-example">Toy example</h4>
<p>We can generate a noisy straight line and take a look at the effect of PCA on it.</p>
<div><div>
<table><tbody><tr><td>
<pre><code><span><span>1</span>
</span><span><span>2</span>
</span><span><span>3</span>
</span><span><span>4</span>
</span><span><span>5</span>
</span></code></pre></td>
<td>
<pre><code data-lang="python"><span><span><span>def</span></span></span><span> </span><span><span><span>generate_noisy_line</span></span></span><span><span><span>(</span></span></span><span><span>gradient</span></span><span><span><span>,</span></span></span><span><span> N</span></span><span><span><span>=</span></span></span><span><span><span><span>100</span></span></span></span><span><span><span>)</span>:</span></span>
    X <span>=</span> np<span>.</span>zeros<span>(</span>shape<span>=</span><span>(</span>N<span>,</span> <span><span>2</span></span><span>))</span>
    X<span>[:,</span><span><span>0</span></span><span>]</span> <span>=</span> np<span>.</span>random<span>.</span>uniform<span>(</span><span><span>-</span></span><span><span>100</span></span><span>,</span> <span><span>100</span></span><span>,</span> size<span>=</span>N<span>)</span>
    X<span>[:,</span><span><span>1</span></span><span>]</span> <span>=</span> <span>(</span>X<span>[:,</span><span><span>0</span></span><span>]</span><span>*</span>gradient<span>)</span><span>+</span> np<span>.</span>random<span>.</span>uniform<span>(</span><span><span>-</span></span><span><span>100</span></span><span>,</span> <span><span>100</span></span><span>,</span> size<span>=</span>N<span>)</span>
    <span><span>return</span></span> X
</code></pre></td></tr></tbody></table>
</div>
</div><p>Notice the direction of the eigenvectors plotted. This points out a property of PCA that was only briefly mentioned — the PCA components are in the directions of maximum variance. PCA is trying to use its first component to capture as much of what distinguishes points as possible.
We can also see from the projected version the property of decorrelating data, which we just proved.</p>
<div><div>
<table><tbody><tr><td>
<pre><code><span><span>1</span>
</span><span><span>2</span>
</span><span><span>3</span>
</span><span><span>4</span>
</span><span><span>5</span>
</span></code></pre></td>
<td>
<pre><code data-lang="python">X <span>=</span> generate_noisy_line<span>(</span>gradient<span>=</span><span><span>4</span></span><span>)</span>
_<span>,</span>eigenvectors <span>=</span> pca_eig<span>(</span>X<span>)</span>

<span></span>
Z <span>=</span> X<span>@eig_dec</span>
</code></pre></td></tr></tbody></table>
</div>
</div><p><img src="https://ymohamedahmed.github.io/post/pca/images/pca_35_0.png" alt=""></p>
<p>Let’s compare our simple two-line implementations to the sklearn implementation.</p>
<div><div>
<table><tbody><tr><td>
<pre><code><span><span>1</span>
</span><span><span>2</span>
</span><span><span>3</span>
</span><span><span>4</span>
</span><span><span>5</span>
</span><span><span>6</span>
</span></code></pre></td>
<td>
<pre><code data-lang="python"><span></span>
<span><span>from</span></span> <span>sklearn.decomposition</span> <span><span>import</span></span> PCA
fitted <span>=</span> PCA<span>()</span><span>.</span>fit<span>(</span>center<span>(</span>X<span>))</span>
<span>print</span><span>(</span>fitted<span>.</span>components_<span>)</span>
<span>print</span><span>(</span>pca_eig<span>(</span>X<span>)[</span><span><span>1</span></span><span>])</span>
<span>print</span><span>(</span>pca_svd<span>(</span>X<span>)[</span><span><span>1</span></span><span>])</span>
</code></pre></td></tr></tbody></table>
</div>
</div><pre><code>[[ <span>0.22883214</span>  <span>0.9734659</span> ]
 [ <span>0.9734659</span>  <span>-0.22883214</span>]]
[[<span>-0.9734659</span>  <span>-0.22883214</span>]
 [ <span>0.22883214</span> <span>-0.9734659</span> ]]
[[<span>-0.22883214</span> <span>-0.9734659</span> ]
 [ <span>0.9734659</span>  <span>-0.22883214</span>]]
</code></pre>
<p>They match (considering that eigenvectors may be arbitrarily multiplied by -1), but I still wouldn’t rely on ours too much! SkLearn is doing a bunch of stuff behind the scenes to make sure it is always accurate!</p>
<h4 id="country-data">Country data</h4>
<p>Another common use-case of PCA is for visualising high-dimensional data. We can use the earlier derivation to select a matrix which projects our data down into two-dimensions. For example, I’ve taken some 7-dimensional data from 
<a href="https://en.wikipedia.org/wiki/World_Happiness_Report" target="_blank" rel="noopener">Wikipedia</a> on the happiness of populations around the world.
If we wished to plot the countries in relation to each other we could use the two eigenvectors with largest eigenvalues (as we showed earlier). This new matrix where each row only consists of two features, can be plotted and understood by mere mortals without the ability to visualise 7-dimensions.</p>
<div><div>
<table><tbody><tr><td>
<pre><code><span><span>1</span>
</span><span><span>2</span>
</span></code></pre></td>
<td>
<pre><code data-lang="python">df <span>=</span> pd<span>.</span>read_csv<span>(</span><span><span>"data/country-data.csv"</span></span><span>)</span>
df<span>.</span>head<span>()</span>
</code></pre></td></tr></tbody></table>
</div>
</div>

<div>

<table>
  <thead>
    <tr>
      <th></th>
      <th>Overall rank</th>
      <th>Country or region</th>
      <th>Score</th>
      <th>GDP per capita</th>
      <th>Social support</th>
      <th>Healthy life expectancy</th>
      <th>Freedom to make life choices</th>
      <th>Generosity</th>
      <th>Perceptions of corruption</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>Finland</td>
      <td>7.769</td>
      <td>1.340</td>
      <td>1.587</td>
      <td>0.986</td>
      <td>0.596</td>
      <td>0.153</td>
      <td>0.393</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2</td>
      <td>Denmark</td>
      <td>7.600</td>
      <td>1.383</td>
      <td>1.573</td>
      <td>0.996</td>
      <td>0.592</td>
      <td>0.252</td>
      <td>0.410</td>
    </tr>
    <tr>
      <th>2</th>
      <td>3</td>
      <td>Norway</td>
      <td>7.554</td>
      <td>1.488</td>
      <td>1.582</td>
      <td>1.028</td>
      <td>0.603</td>
      <td>0.271</td>
      <td>0.341</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4</td>
      <td>Iceland</td>
      <td>7.494</td>
      <td>1.380</td>
      <td>1.624</td>
      <td>1.026</td>
      <td>0.591</td>
      <td>0.354</td>
      <td>0.118</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5</td>
      <td>Netherlands</td>
      <td>7.488</td>
      <td>1.396</td>
      <td>1.522</td>
      <td>0.999</td>
      <td>0.557</td>
      <td>0.322</td>
      <td>0.298</td>
    </tr>
  </tbody>
</table>
</div>

<p>Let’s take the numeric columns (and exclude the score) which we will pass to the PCA algorithm. Given this, we take the two first columns of the returned eigenvector matrix (it’s returned sorted by eigenvalue so this is ok!) and project down by taking the matrix multiplication with the encoding matrix.</p>
<div><div>
<table><tbody><tr><td>
<pre><code><span><span>1</span>
</span><span><span>2</span>
</span><span><span>3</span>
</span><span><span>4</span>
</span><span><span>5</span>
</span><span><span>6</span>
</span><span><span>7</span>
</span><span><span>8</span>
</span><span><span>9</span>
</span></code></pre></td>
<td>
<pre><code data-lang="python">X <span>=</span> df<span>[</span>df<span>.</span>columns<span>[</span><span><span>3</span></span><span>:]]</span><span>.</span>to_numpy<span>()</span>
eigenvals<span>,</span> eigenvectors <span>=</span> pca_svd<span>(</span>X<span>)</span>
encoding_matrix <span>=</span> eigenvectors<span>[:,:</span><span><span>2</span></span><span>]</span>

<span></span>
sorted_eigenvals <span>=</span> np<span>.</span>argsort<span>(</span><span><span>-</span></span><span><span>1</span></span><span>*</span>eigenvals<span>)</span>
<span>print</span><span>(</span>sorted_eigenvals<span>)</span>

projected <span>=</span> X<span>@encoding_matrix</span>
</code></pre></td></tr></tbody></table>
</div>
</div><pre><code><span>[0</span> <span>1</span> <span>2</span> <span>3</span> <span>4</span> <span>5</span> <span>6</span> <span>7</span><span>]</span>
</code></pre>
<p>We can now plot the projected version as a two-dimensional plot. Try zooming in and out to see how ‘similar’ countries tend to clump together in the resulting plot. Hover your mouse over a point to see additional information about the country. We’ve managed to preserve a lot of the information of the original 7 dimensions.</p>





<h4 id="eigenfaces">Eigenfaces</h4>
<p>A common recent application of PCA is the so-called ‘eigenfaces’. That is, if we apply PCA to a dataset of images of faces we can attempt to generate a compact code for faces. For example, suppose we use 100x100 face images, this means that we effectively have a 10000-dimensional space in which all faces must exist. Obviously this is rather unwieldy and so, say, if we could generate say a code of 400 real numbers for a face, it could be useful for compressive purposes or indeed for face recognition.</p>
<p><em>Eigenfaces</em> specifically refers to the projection vectors which applying PCA to faces generates. These essentially represent the directions of most variance in our original 10000-dimensional space and help to point out some interesting things that PCA is doing.</p>
<p>To test our implementation out on faces, I used the following Kaggle 
<a href="https://www.kaggle.com/abhikjha/utk-face-cropped" target="_blank" rel="noopener">dataset</a>, which you can download to path in the code to reproduce the notebook.</p>
<p>Interestingly we can take a look at the ‘mean’ (in the sense of average) face in the dataset — pretty generic.</p>
<p><img src="https://ymohamedahmed.github.io/post/pca/images/pca_51_1.png" alt="png"></p>
<p>We can use PCA to reduce the number of components of our image data from 2400 to some new reduced number of dimensions. Given this new low-dimensional representation we’ll try to reconstruct the original and take a look at the reconstructed image.
As we can see, people frequently become recognisable well before 2400 dimensions. Some people are even recognisable with as little as a few hundred components.</p>
<p>If you rerun the cell, it will pick a person at random from the dataset and show this grid of images again.</p>
<div><div>
<table><tbody><tr><td>
<pre><code><span><span>1</span>
</span><span><span>2</span>
</span><span><span>3</span>
</span><span><span>4</span>
</span><span><span>5</span>
</span></code></pre></td>
<td>
<pre><code data-lang="python"><span><span><span>def</span></span></span><span> </span><span><span><span>reconstruct</span></span></span><span><span><span>(</span></span></span><span><span>num_components</span></span><span><span><span>,</span></span></span><span><span> encoding_matrix</span></span><span><span><span>,</span></span></span><span><span> test_image</span></span><span><span><span>)</span>:</span></span>
    proj <span>=</span> encoding_matrix<span>[:,:</span>num_components<span>]</span>
    rec <span>=</span> <span>(</span>test_image<span>@proj@proj.T</span><span>)</span><span>.</span>reshape<span>(</span>reversed_dims<span>)</span>
    <span><span>return</span></span> Image<span>.</span>fromarray<span>(</span>np<span>.</span>uint8<span>(</span>rec<span>))</span>
    
</code></pre></td></tr></tbody></table>
</div>
</div><p><img src="https://ymohamedahmed.github.io/post/pca/images/pca_55_0.png" alt="png"></p>
<p>PCA, in some sense, is quite interpretable. We can take a look at exactly what it’s considering, that is, the directions of the eigenvectors it’s using.</p>
<p>In this specific case, we can look at and interpret them as images which each test image is being projected onto.</p>
<p><img src="https://ymohamedahmed.github.io/post/pca/images/pca_57_0.png" alt="png"></p>
<p>If you thought that looked weird, let’s look at some of the other components. They get more and more bizarre at a first glance. However, we know that most faces roughly look the same, so later eigenvectors will be distinguishing less and less, hence why they become less obvious.</p>
<p><img src="https://ymohamedahmed.github.io/post/pca/images/pca_59_0.png" alt="png"></p>
<p>Thanks for reading I hope it was useful or at least mildly interesting.</p>

    </div></div></div>
    </div>
    <footer>
        <div>created by buildstarted &copy; 2020 <a href="/about">about</a></div>
        <div>Share this page on social media: copy and paste this url https://linksfor.dev/</div>
        <div>If you prefer RSS: <a href="https://linksfor.dev/feed.xml">https://linksfor.dev/feed.xml</a></div>
        <div>Customer satisfaction guaranteed to be optional.</div>
    </footer>
    
    <script async defer>
        _dna = window._dna || {};
        _dna.siteId = "linksfor.devs";
        _dna.outlink = true;

        (function() {
            let dna = document.createElement('script');
            dna.type = 'text/javascript';
            dna.async = true;
            dna.src = '//dna.buildstarted.com/t.js';
            let s = document.getElementsByTagName('script')[0];
            s.parentNode.insertBefore(dna, s);
        })();
    </script>
    <noscript><img src="//dna.buildstarted.com/g?siteId=linksfor.devs"/></noscript>
</body>
</html>