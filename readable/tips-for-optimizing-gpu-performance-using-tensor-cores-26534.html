<!DOCTYPE html>
<html lang="en">
<head>
    <title>
Tips for Optimizing GPU Performance Using Tensor Cores | NVIDIA Developer Blog - linksfor.dev(s)    </title>
    <meta charset="utf-8">
    <link rel="alternate" type="application/rss+xml" title="Linksfor.dev(s) feed" href="https://linksfor.dev/feed.rss" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta name="google" value="notranslate">
    <link rel="stylesheet" type="text/css" href="/style.css" />
    <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon">
    <link rel="icon" href="/favicon.ico" type="image/x-icon">
    <meta property="og:title" content="Tips for Optimizing GPU Performance Using Tensor Cores | NVIDIA Developer Blog - linksfor.dev(s)"/>
    <meta property="article:author" content="View all posts by Valerie Sarge"/>
    <meta property="og:description" content="The guide explains how GPUs process data and gives tips on how to design networks for better performance, including optimizing for Tensor Cores."/>
    <meta property="og:type" content="website"/>
    <meta property="og:url" content="https://devblogs.nvidia.com/optimizing-gpu-performance-tensor-cores/"/>

<meta property="og:site_name" content="linksfor.dev(s)" />
</head>
<body>
    <div class="devring" style="background: #222">
        <div style="text-align:center">Explore other dev related sites in this ring. If you would like to join this ring <a href="https://devring.club">click here</a>.</div>
        <div class="grid">
            <div style="display: grid; grid-template-columns: .5fr 1fr 1fr 1fr; text-align: center;">
                <span class="devring-title"><a href="https://devring.club/">devring.club</a></span>
                <a href="https://devring.club/sites/1/prev" class="devring-previous">Previous</a>
                <a href="https://devring.club/random" class="devring-random">Random</a>
                <a href="https://devring.club/sites/1/next" class="devring-next">Next</a>
            </div>
        </div>
    </div>
    <div class="grid">
        <h1>
<a href="/" style="color:inherit">linksfor.dev(s)</a>
        </h1>
        <title>linksfor.dev(s) - Tips for Optimizing GPU Performance Using Tensor Cores | NVIDIA Developer Blog</title>
<div class="readable">
        <h1>Tips for Optimizing GPU Performance Using Tensor Cores | NVIDIA Developer Blog</h1>
            <div>by View all posts by Valerie Sarge</div>
            <div>Reading time: 13-16 minutes</div>
        <div>Posted here: 14 Jun 2019</div>
        <p><a href="https://devblogs.nvidia.com/optimizing-gpu-performance-tensor-cores/">https://devblogs.nvidia.com/optimizing-gpu-performance-tensor-cores/</a></p>
        <hr/>
<div id="readability-page-1" class="page"><div id="pf-post-view">
    <div>

                  <article id="post-14687">
                

                <div>
                  <p>Our most popular question is “What can I do to get great GPU performance for deep learning?”&nbsp;We’ve recently published a detailed <a href="https://docs.nvidia.com/deeplearning/sdk/dl-performance-guide/index.html" target="_blank" rel="noopener">Deep Learning Performance Guide</a>&nbsp;to help answer this question.&nbsp;The guide explains how GPUs process data and gives tips on how to design networks for better performance. We also take a close look at Tensor Core optimization to help improve performance.</p>
<p>This post takes a closer look at some of the most important recommendations from the guide.&nbsp;We’ll give a general guideline and explanation for each tip, apply the guideline to an example layer, and compare performance before and after.&nbsp;</p>
<p>This post can be read standalone. However, we suggest you refer&nbsp;to the Deep Learning Performance Guide for a better understanding of why deep learning tasks perform the way they do on GPUs and how to improve that performance.</p>
<h2 id="h.3x7mf460q9aw">Tip 1: Activating Tensor Cores</h2>
<p>Tensor Cores, available on Volta and subsequent GPU architectures, accelerate common deep learning operations—specifically computationally-intensive tasks such as fully-connected and convolutional layers.</p>
<p>Workloads must use mixed precision to take advantage of Tensor Cores. Check out our post on <a href="https://developer.nvidia.com/automatic-mixed-precision" target="_blank" rel="noopener">Automatic Mixed Precision</a>&nbsp;for quick setup and our <a href="https://docs.nvidia.com/deeplearning/sdk/mixed-precision-training/index.html" target="_blank" rel="noopener">Training With Mixed Precision Guide</a>&nbsp;for more details. &nbsp;<strong>Additionally, Tensor Cores are activated when certain parameters of a layer are divisible by 8 (for FP16 data) or 16 (for INT8 data). &nbsp;</strong>A fully-connected layer with a batch size and number of inputs and outputs that follow this rule will use Tensor Cores, as will a convolutional layer with a number of input and output channels that do the same<strong>.</strong>&nbsp;</p>
<p>This is due to how GPUs store and access data. Layers that don’t meet this requirement are still accelerated on the GPU. However, these layers use 32-bit&nbsp;CUDA cores instead of Tensor Cores as a fallback option.</p>
<p>Note: There are cases where we relax the requirements. However, following these guidelines is the easiest way to ensure&nbsp;enabling&nbsp;Tensor Cores. For details, see sections on <a href="https://docs.nvidia.com/deeplearning/sdk/dl-performance-guide/index.html#requirements-tc" target="_blank" rel="noopener">Tensor Core Requirements</a>&nbsp;for matrix multiplies and <a href="https://docs.nvidia.com/deeplearning/sdk/dl-performance-guide/index.html#channels" target="_blank" rel="noopener">Channels In and Out</a>&nbsp;of convolutions from the Deep Learning Performance Guide.</p>
<p>Let’s look at two examples from the popular Transformer neural network to illustrate the kind of speedup you can expect from activating Tensor Cores . Transformers, described in <em>Attention Is All You Need</em>&nbsp;[Vaswani 2017], are currently state-of-the-art networks for language translation and other sequence tasks. Much of a Transformer network consists of fully-connected layers. We’ll discuss ways to optimize a few for Tensor Cores.</p>
<h3 id="h.9yili3t5wcy5">Padding Vocabulary Size – Projection&nbsp;Layer Example</h3>
<p>Figure 1 shows a simplified representation of a Transformer network. The&nbsp;network outputs a vector containing a probability for each token in the vocabulary. This vector of probabilities is produced using the softmax&nbsp;function over the outputs from a fully-connected layer, which we’ll call the projection layer. The number of outputs of this layer is equal to the vocabulary size, often in excess of 30,000. Given the heavyweight computation involved, it’s important to ensure effective Tensor Core use.</p>
<figure id="attachment_14694" aria-labelledby="figcaption_attachment_14694"><a href="https://devblogs.nvidia.com/wp-content/uploads/2019/06/Figure1.png"><img src="https://devblogs.nvidia.com/wp-content/uploads/2019/06/Figure1.png" alt="Softmax conversion diagram" width="624" height="162" srcset="https://devblogs.nvidia.com/wp-content/uploads/2019/06/Figure1.png 624w, https://devblogs.nvidia.com/wp-content/uploads/2019/06/Figure1-300x78.png 300w, https://devblogs.nvidia.com/wp-content/uploads/2019/06/Figure1-500x130.png 500w, https://devblogs.nvidia.com/wp-content/uploads/2019/06/Figure1-160x42.png 160w, https://devblogs.nvidia.com/wp-content/uploads/2019/06/Figure1-362x94.png 362w, https://devblogs.nvidia.com/wp-content/uploads/2019/06/Figure1-424x110.png 424w" sizes="(max-width: 624px) 100vw, 624px"></a><figcaption id="figcaption_attachment_14694">Figure 1. The projection layer (labeled FC) produces a vector of output values, one per word in the vocabulary. Softmax converts these values to a probability distribution; the word with the highest probability is the predicted output for this step. Three steps are shown here in different colors.</figcaption></figure>
<p><img alt="">Figure 2 shows the performance of one such projection layer, with 1024 inputs and a batch size of 5120, training on FP16 data on a Volta Tesla V100. Suppose we are using the combined English-German training datasets for the <a href="http://statmt.org/wmt14/translation-task.html" target="_blank" rel="noopener">WMT14 task</a>, which have&nbsp;a vocabulary size of 33708. Simply padding&nbsp;the vocabulary size to the next multiple of 8 activates Tensor Cores and improves throughput significantly.</p>
<figure id="attachment_14682" aria-labelledby="figcaption_attachment_14682"><a href="https://devblogs.nvidia.com/wp-content/uploads/2019/06/fig_num_tests1_tfactor100_invalid-tensorformat_Depthwise-Separable_GEMM__fillr_nin1024_nbatch5120.png"><img src="https://devblogs.nvidia.com/wp-content/uploads/2019/06/fig_num_tests1_tfactor100_invalid-tensorformat_Depthwise-Separable_GEMM__fillr_nin1024_nbatch5120-625x431.png" alt="Projection layer performance chart" width="625" height="431" srcset="https://devblogs.nvidia.com/wp-content/uploads/2019/06/fig_num_tests1_tfactor100_invalid-tensorformat_Depthwise-Separable_GEMM__fillr_nin1024_nbatch5120-625x431.png 625w, https://devblogs.nvidia.com/wp-content/uploads/2019/06/fig_num_tests1_tfactor100_invalid-tensorformat_Depthwise-Separable_GEMM__fillr_nin1024_nbatch5120-300x207.png 300w, https://devblogs.nvidia.com/wp-content/uploads/2019/06/fig_num_tests1_tfactor100_invalid-tensorformat_Depthwise-Separable_GEMM__fillr_nin1024_nbatch5120-768x530.png 768w, https://devblogs.nvidia.com/wp-content/uploads/2019/06/fig_num_tests1_tfactor100_invalid-tensorformat_Depthwise-Separable_GEMM__fillr_nin1024_nbatch5120-435x300.png 435w, https://devblogs.nvidia.com/wp-content/uploads/2019/06/fig_num_tests1_tfactor100_invalid-tensorformat_Depthwise-Separable_GEMM__fillr_nin1024_nbatch5120-131x90.png 131w, https://devblogs.nvidia.com/wp-content/uploads/2019/06/fig_num_tests1_tfactor100_invalid-tensorformat_Depthwise-Separable_GEMM__fillr_nin1024_nbatch5120-362x250.png 362w, https://devblogs.nvidia.com/wp-content/uploads/2019/06/fig_num_tests1_tfactor100_invalid-tensorformat_Depthwise-Separable_GEMM__fillr_nin1024_nbatch5120-160x110.png 160w, https://devblogs.nvidia.com/wp-content/uploads/2019/06/fig_num_tests1_tfactor100_invalid-tensorformat_Depthwise-Separable_GEMM__fillr_nin1024_nbatch5120.png 931w" sizes="(max-width: 625px) 100vw, 625px"></a><figcaption id="figcaption_attachment_14682">Figure 2. Activating Tensor Cores by choosing&nbsp;the vocabulary size to be a multiple of 8 substantially benefits performance of the projection&nbsp;layer. For all data shown, the layer uses 1024 inputs and a batch size of 5120. (Measured using FP16 data, Tesla V100 GPU, cuBLAS 10.1.)</figcaption></figure>
<h3 id="h.l1b3p7b632a5">Choosing Batch Size for Tensor Cores – Feed-Forward Layer Example</h3>
<p>The Transformer architecture also contains fully-connected layers as part of self-attention and feed-forward blocks. Let’s consider the first layer in a feed-forward block, a fully-connected layer&nbsp;with 1024 inputs and 4096 outputs. This layer’s batch size depends on batch assembly, which splits inputs to the network into batches, up to some maximum batch size. When assembly doesn’t consider Tensor Cores, irregularly-sized batches may be created.</p>
<p>Performance of this layer’s training steps with several batch sizes is shown in figure 3. This is an example where Tensor Core requirements are relaxed. Both forward and activation gradient passes perform the same with and without padding. The weight gradient pass, on the other hand, shows the same dramatic performance difference we saw in figure 2. CUDA cores are used as a fallback for weight gradient computation with batch sizes of 4084 or 4095 tokens, using 4088 or 4096 tokens per batch instead enables Tensor Core acceleration.</p>
<figure id="attachment_14686" aria-labelledby="figcaption_attachment_14686"><a href="https://devblogs.nvidia.com/wp-content/uploads/2019/06/fig_num_tests1_tfactor100_invalid-tensorformat_Depthwise-Separable_GEMM__fillr_nin1024_nout4096-1.png"><img src="https://devblogs.nvidia.com/wp-content/uploads/2019/06/fig_num_tests1_tfactor100_invalid-tensorformat_Depthwise-Separable_GEMM__fillr_nin1024_nout4096-1-625x423.png" alt="Feedforward layer performance chart" width="625" height="423" srcset="https://devblogs.nvidia.com/wp-content/uploads/2019/06/fig_num_tests1_tfactor100_invalid-tensorformat_Depthwise-Separable_GEMM__fillr_nin1024_nout4096-1-625x423.png 625w, https://devblogs.nvidia.com/wp-content/uploads/2019/06/fig_num_tests1_tfactor100_invalid-tensorformat_Depthwise-Separable_GEMM__fillr_nin1024_nout4096-1-300x203.png 300w, https://devblogs.nvidia.com/wp-content/uploads/2019/06/fig_num_tests1_tfactor100_invalid-tensorformat_Depthwise-Separable_GEMM__fillr_nin1024_nout4096-1-768x520.png 768w, https://devblogs.nvidia.com/wp-content/uploads/2019/06/fig_num_tests1_tfactor100_invalid-tensorformat_Depthwise-Separable_GEMM__fillr_nin1024_nout4096-1-443x300.png 443w, https://devblogs.nvidia.com/wp-content/uploads/2019/06/fig_num_tests1_tfactor100_invalid-tensorformat_Depthwise-Separable_GEMM__fillr_nin1024_nout4096-1-133x90.png 133w, https://devblogs.nvidia.com/wp-content/uploads/2019/06/fig_num_tests1_tfactor100_invalid-tensorformat_Depthwise-Separable_GEMM__fillr_nin1024_nout4096-1-362x245.png 362w, https://devblogs.nvidia.com/wp-content/uploads/2019/06/fig_num_tests1_tfactor100_invalid-tensorformat_Depthwise-Separable_GEMM__fillr_nin1024_nout4096-1-163x110.png 163w, https://devblogs.nvidia.com/wp-content/uploads/2019/06/fig_num_tests1_tfactor100_invalid-tensorformat_Depthwise-Separable_GEMM__fillr_nin1024_nout4096-1.png 949w" sizes="(max-width: 625px) 100vw, 625px"></a><figcaption id="figcaption_attachment_14686">Figure 3. Activating Tensor Cores by choosing batch size to be a multiple of 8 benefits performance of the first fully-connected layer in the feed-forward block (1024 inputs, 4096 outputs). The weight gradient pass shows significant improvement with Tensor Cores over CUDA cores; forward and activation gradient passes demonstrate that Tensor Cores may activate for some parts of training even when a parameter is indivisible by 8. (Measured using FP16 data, Tesla V100 GPU, cuBLAS 10.1.)</figcaption></figure>
<p>At least one of the forward, activation gradient, and weight gradient passes will not be accelerated by Tensor Cores when any relevant parameter is not optimally sized. We recommend ensuring all such parameters are multiples of 8 when training with FP16 and multiples of 16 when training with INT8. These include batch size and number of inputs and outputs, for a fully-connected layer and channels in and out, for a convolutional layer. This is the easiest way to guarantee Tensor Cores will accelerate your task!</p>
<h3 id="h.g2god3ge3ch">Checking for Tensor Core Usage</h3>
<p>You can use <a href="https://devblogs.nvidia.com/using-nsight-compute-nvprof-mixed-precision-deep-learning-models/">NVIDIA’s profiling tools</a>&nbsp;to check if Tensor Cores have been activated. More information about these tools is available in the <a href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html" target="_blank" rel="noopener">CUDA documentation</a>.</p>
<p>Note: although we focus on Tensor Cores in this post, deep learning operations not accelerated by Tensor Cores also contribute to overall network performance. You can read about these operations in the <a href="https://docs.nvidia.com/deeplearning/sdk/dl-performance-guide/index.html#mem-limited" target="_blank" rel="noopener">Memory-Limited Layers</a>&nbsp;section of the Deep Learning Performance Guide, and about further optimizations and <a href="https://docs.nvidia.com/deeplearning/sdk/mixed-precision-training/index.html#decrease-nontc" target="_blank" rel="noopener">decreasing non-Tensor-Core work</a>&nbsp;in the Training With Mixed Precision documentation.</p>
<h2 id="h.5ewahopem94a">Tip 2: Considering Quantization Effects</h2>
<p>We’ve focused so far on how to ensure&nbsp;Tensor Cores are accelerating your task. Now let’s discuss efficiency on the GPU and a few parameter tweaks that can help you get the most out of Tensor Cores.</p>
<p>GPUs perform many computations concurrently; we refer to these parallel computations as threads. Conceptually, threads are grouped into thread blocks, each of which is responsible for a subset of the calculations being done. When the GPU executes a task, it is split into equally-sized thread blocks.</p>
<p>Now&nbsp;consider a fully-connected layer. During training, forward propagation, activation gradient calculation, and weight gradient calculation are each represented as a matrix multiply. The GPU divides the output matrix into uniformly-sized, rectangular tiles. Each tile is computed by a thread block; figure 4 illustrates the process for one such tile. You can find cases where multiple thread blocks contribute to one tile, but for simplicity, we’ll assume one thread block per tile in this post. More detail can be found in the Deep Learning Performance Guide, in the sections discussing <a href="https://docs.nvidia.com/deeplearning/sdk/dl-performance-guide/index.html#gpu-perf" target="_blank" rel="noopener">GPU efficiency</a>&nbsp;and <a href="https://docs.nvidia.com/deeplearning/sdk/dl-performance-guide/index.html#cublas-tile-dim" target="_blank" rel="noopener">tiling</a>.</p>
<figure id="attachment_14684" aria-labelledby="figcaption_attachment_14684"><img src="https://devblogs.nvidia.com/wp-content/uploads/2019/06/pasted-image-0-1.png" alt="Matrix multiply layout figure" width="401" height="356" srcset="https://devblogs.nvidia.com/wp-content/uploads/2019/06/pasted-image-0-1.png 401w, https://devblogs.nvidia.com/wp-content/uploads/2019/06/pasted-image-0-1-300x266.png 300w, https://devblogs.nvidia.com/wp-content/uploads/2019/06/pasted-image-0-1-338x300.png 338w, https://devblogs.nvidia.com/wp-content/uploads/2019/06/pasted-image-0-1-101x90.png 101w, https://devblogs.nvidia.com/wp-content/uploads/2019/06/pasted-image-0-1-362x321.png 362w, https://devblogs.nvidia.com/wp-content/uploads/2019/06/pasted-image-0-1-124x110.png 124w" sizes="(max-width: 401px) 100vw, 401px"><figcaption id="figcaption_attachment_14684">Figure 4. A matrix multiply (A ⋅ B = C) is computed by dividing the output (C) matrix into tiles of size Mtile x Ntile and stepping through the K dimension.</figcaption></figure>
<p>However, not all output matrices divide evenly into an available tile size. Further, the thread blocks created may not divide evenly among the multiprocessors on the GPU. These effects, called <em>tile quantization</em>&nbsp;and <em>wave quantization</em>&nbsp;respectively, can lead to wasted cycles and inefficiency.</p>
<p>Tile quantization occurs when one dimension of the output matrix is not evenly divisible by the corresponding tile dimension. The thread blocks for the final row or column of tiles created for the remainder then perform the same amount of math as any other column, but produce a smaller amount of useful output data. While the cuBLAS library tries to choose the best tile size available, most tile sizes are powers of 2. To avoid tile quantization, <strong>choose</strong> <strong>parameters that are divisible by powers of 2</strong> (at least 64 and ideally 256, to account for the most common tile sizes).</p>
<p>We also consider the number of thread blocks that can run concurrently on the GPU for wave quantization. Take the example of a Tesla V100 GPU, which has 80 multiprocessors and a tile size of 256×128, where the V100 GPU can execute one thread block per multiprocessor. In this case, a wave of 80 thread blocks fully occupies the GPU. Suppose a task creates 96 thread blocks. The first 80 will be computed efficiently as a ‘full wave’ while the 16 leftover thread blocks will make up an inefficient ‘tail wave’ during which the GPU is underutilized. Figure 5 illustrates a simple version of this situation.</p>
<figure id="attachment_14683" aria-labelledby="figcaption_attachment_14683"><img src="https://devblogs.nvidia.com/wp-content/uploads/2019/06/pasted-image-0.png" alt="Threadblock split example diagram" width="305" height="158" srcset="https://devblogs.nvidia.com/wp-content/uploads/2019/06/pasted-image-0.png 305w, https://devblogs.nvidia.com/wp-content/uploads/2019/06/pasted-image-0-300x155.png 300w, https://devblogs.nvidia.com/wp-content/uploads/2019/06/pasted-image-0-160x83.png 160w, https://devblogs.nvidia.com/wp-content/uploads/2019/06/pasted-image-0-212x110.png 212w" sizes="(max-width: 305px) 100vw, 305px"><figcaption id="figcaption_attachment_14683">Figure 5. On a GPU with 8 SMs, a task with 12 thread blocks will be split into a wave of 8 thread blocks (occupying all SMs) and a tail wave of 4 thread blocks (occupying only 50% of SMs). Thus, overall GPU utilization for the task will be 75% at best.</figcaption></figure>
<p>Absent information about what tile size will be used, <strong>choose parameters so that the total number of tiles/thread blocks is divisible by the number of multiprocessors</strong>&nbsp;to avoid wave quantization effects.</p>
<p>Now let’s look at how this maps back to parameters of a fully-connected layer. Figure 6 shows the dimensions of equivalent matrix multiplies for forward, activation gradient, and weight gradient passes.</p>
<figure id="attachment_14713" aria-labelledby="figcaption_attachment_14713"><a href="https://devblogs.nvidia.com/wp-content/uploads/2019/06/figure6.jpg"><img src="https://devblogs.nvidia.com/wp-content/uploads/2019/06/figure6.jpg" alt="Equivalent matrix multiplies for different calculation types diagram" width="727" height="282" srcset="https://devblogs.nvidia.com/wp-content/uploads/2019/06/figure6.jpg 727w, https://devblogs.nvidia.com/wp-content/uploads/2019/06/figure6-300x116.jpg 300w, https://devblogs.nvidia.com/wp-content/uploads/2019/06/figure6-625x242.jpg 625w, https://devblogs.nvidia.com/wp-content/uploads/2019/06/figure6-500x194.jpg 500w, https://devblogs.nvidia.com/wp-content/uploads/2019/06/figure6-160x62.jpg 160w, https://devblogs.nvidia.com/wp-content/uploads/2019/06/figure6-362x140.jpg 362w, https://devblogs.nvidia.com/wp-content/uploads/2019/06/figure6-284x110.jpg 284w" sizes="(max-width: 727px) 100vw, 727px"></a><figcaption id="figcaption_attachment_14713">Figure 6. Equivalent matrix multiplies for (a) forward propagation, (b) activation gradient calculation, and (c) weight gradient calculation of a fully-connected layer. The Deep Learning Performance Guide contains <a href="https://docs.nvidia.com/deeplearning/sdk/dl-performance-guide/index.html#fullyconnected-layer" target="_blank" rel="noopener">details</a>&nbsp;as well as the <a href="https://docs.nvidia.com/deeplearning/sdk/dl-performance-guide/index.html#imp-gemm-dim" target="_blank" rel="noopener">equivalent dimensions for convolutional layers</a>.</figcaption></figure>
<p>Batch size directly controls the width of the output matrix during both forward and activation gradient passes. Consider again our previous example of the first layer in a Transformer feed-forward block (a fully-connected layer with 1024 inputs and 4096 outputs). During forward propagation, the output matrix is of shape 4096 x batch size. Assuming a tile size of 256×128, this matrix divides into 4096/256 = 16 rows and (batch size) / 128 columns of tiles.</p>
<p>Avoiding tile quantization is straightforward: batch size should be divisible by 128.&nbsp;Wave quantization is more complex. For some integer n, we want n*80 total tiles and already know that there will be 16 rows of tiles. Therefore, our task should create n*5 columns of tiles. Given a tile width of 128, this corresponds to an output matrix width (and batch size) of n*5*128 = n*640. Thus, choosing batch size to be divisible by 640 avoids wave quantization effects.</p>
<p>The Deep Learning Performance Guide goes into more detail about <a href="https://docs.nvidia.com/deeplearning/sdk/dl-performance-guide/index.html#dim-quantization" target="_blank" rel="noopener">both types of quantization effects</a>,&nbsp;<a href="https://docs.nvidia.com/deeplearning/sdk/dl-performance-guide/index.html#quant-effects" target="_blank" rel="noopener">as well as how this applies to convolutions</a>, with examples.</p>
<h3 id="h.de96g34t3ue7">Choosing Batch Size for Quantization – Feed-Forward Layer Example</h3>
<p>Figure 7 shows the performance&nbsp;of our example feed-forward layer for several different batch sizes. Choosing a quantization-free batch size (2560 instead of 2048, 5120 instead of 4096) considerably improves performance. Notice that a batch size of 2560 (resulting in 4 waves of 80 thread blocks) <em>achieves higher throughput than the larger batch size of 4096</em>&nbsp;(a total of 512 tiles, resulting in 6 waves of 80 thread blocks and a tail wave remainder of 32 thread blocks). The weight gradient pass doesn’t show this drastic change. Batch size maps to the ‘K’ dimension of the matrix multiply during this pass and thus does not directly control the size of the output matrix or the number of tiles and thread blocks created.</p>
<figure id="attachment_14681" aria-labelledby="figcaption_attachment_14681"><a href="https://devblogs.nvidia.com/wp-content/uploads/2019/06/fig_num_tests1_tfactor100_invalid-tensorformat_Depthwise-Separable_GEMM__fillr_nin1024_nout4096.png"><img src="https://devblogs.nvidia.com/wp-content/uploads/2019/06/fig_num_tests1_tfactor100_invalid-tensorformat_Depthwise-Separable_GEMM__fillr_nin1024_nout4096-625x423.png" alt="Feedforward performance with differing batches chart" width="625" height="423" srcset="https://devblogs.nvidia.com/wp-content/uploads/2019/06/fig_num_tests1_tfactor100_invalid-tensorformat_Depthwise-Separable_GEMM__fillr_nin1024_nout4096-625x423.png 625w, https://devblogs.nvidia.com/wp-content/uploads/2019/06/fig_num_tests1_tfactor100_invalid-tensorformat_Depthwise-Separable_GEMM__fillr_nin1024_nout4096-300x203.png 300w, https://devblogs.nvidia.com/wp-content/uploads/2019/06/fig_num_tests1_tfactor100_invalid-tensorformat_Depthwise-Separable_GEMM__fillr_nin1024_nout4096-768x520.png 768w, https://devblogs.nvidia.com/wp-content/uploads/2019/06/fig_num_tests1_tfactor100_invalid-tensorformat_Depthwise-Separable_GEMM__fillr_nin1024_nout4096-443x300.png 443w, https://devblogs.nvidia.com/wp-content/uploads/2019/06/fig_num_tests1_tfactor100_invalid-tensorformat_Depthwise-Separable_GEMM__fillr_nin1024_nout4096-133x90.png 133w, https://devblogs.nvidia.com/wp-content/uploads/2019/06/fig_num_tests1_tfactor100_invalid-tensorformat_Depthwise-Separable_GEMM__fillr_nin1024_nout4096-362x245.png 362w, https://devblogs.nvidia.com/wp-content/uploads/2019/06/fig_num_tests1_tfactor100_invalid-tensorformat_Depthwise-Separable_GEMM__fillr_nin1024_nout4096-163x110.png 163w, https://devblogs.nvidia.com/wp-content/uploads/2019/06/fig_num_tests1_tfactor100_invalid-tensorformat_Depthwise-Separable_GEMM__fillr_nin1024_nout4096.png 949w" sizes="(max-width: 625px) 100vw, 625px"></a><figcaption id="figcaption_attachment_14681">Figure 7. Choosing batch size to avoid wave quantization effects improves performance of the first fully-connected layer in the feed-forward block (1024 inputs, 4096 outputs) during the forward and activation gradient passes. Wave quantization does not occur over batch size for the weight gradient pass. (Measured using FP16 data, Tesla V100 GPU, cuBLAS 10.1.)</figcaption></figure>
<h2 id="h.v9dy7afbwy8a">Learning&nbsp;More</h2>
<p>Learn more about how to ensure&nbsp;your network is taking advantage of Tensor Cores from the <a href="https://docs.nvidia.com/deeplearning/sdk/dl-performance-guide/index.html" target="_blank" rel="noopener">Deep Learning Performance Guide</a>. To get started, read <a href="https://docs.nvidia.com/deeplearning/sdk/dl-performance-guide/index.html#perf-guidelines" target="_blank" rel="noopener">our summary of performance guidelines</a>, which offers quick rundown of the most important information about Tensor Core performance and includes tips that you can apply to your network in a few minutes! &nbsp;Each part of the summary links to other sections in the guide where you can find more detail about the topic.</p>
<p>Also, check out the recording of GTC Silicon Valley 2019 session S9926: <a href="https://developer.nvidia.com/gtc/2019/video/S9926" target="_blank" rel="noopener">Tensor Core Performance: The Ultimate Guide</a>&nbsp;and S9143: <a href="https://developer.nvidia.com/gtc/2019/video/S9143" target="_blank" rel="noopener">Mixed Precision Training of Deep Neural Networks</a>. &nbsp;Additional information about how to train using mixed precision can be found in the <a href="https://arxiv.org/abs/1710.03740" target="_blank" rel="noopener">Mixed Precision Training paper</a>&nbsp;and <a href="https://docs.nvidia.com/deeplearning/sdk/mixed-precision-training/index.html" target="_blank" rel="noopener">Training With Mixed Precision documentation</a>.</p>
<h3 id="h.oku6rqmulm89">References</h3>
<p>[Vaswani 2017] Ashish Vaswani, <a href="https://arxiv.org/abs/1706.03762" target="_blank" rel="noopener">Attention Is All You Need</a>, arXiv:1706.03762, 2017.&nbsp;</p>





                    

                    
                </div>
            </article>
        
    </div>
</div></div></div>
    </div>
    <footer>
        <div>created by <a href="https://buildstarted.com">buildstarted</a> &copy; 2020 <a href="/about">about</a></div>
        <div>Share this page on social media: copy and paste this url https://linksfor.dev/</div>
        <div>If you prefer RSS: <a href="https://linksfor.dev/feed.xml">https://linksfor.dev/feed.xml</a></div>
        <div>Customer satisfaction guaranteed to be optional.</div>
    </footer>
    
    <script async defer>
        _dna = window._dna || {};
        _dna.siteId = "linksfor.devs";
        _dna.outlink = true;

        (function() {
            let dna = document.createElement('script');
            dna.type = 'text/javascript';
            dna.async = true;
            dna.src = '//dna.buildstarted.com/t.js';
            let s = document.getElementsByTagName('script')[0];
            s.parentNode.insertBefore(dna, s);
        })();
    </script>
    <noscript><img src="//dna.buildstarted.com/g?siteId=linksfor.devs"/></noscript>
</body>
</html>