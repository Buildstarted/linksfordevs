<!DOCTYPE html>
<html lang="en">
<head>
    <title>
Learning to Drive Smoothly in Minutes -
linksfor.dev(s)
    </title>
    <link rel="alternate" type="application/rss+xml" title="Linksfor.dev(s) feed" href="https://linksfor.dev/feed.rss" />
    <meta charset="utf-8">
    <meta name="Description" content="A curated source of links that devs might find interesting. Updated around the clock." />
    <meta name="google" value="notranslate">
    <link rel="stylesheet" type="text/css" href="/style.css" />
    <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon">
    <link rel="icon" href="/favicon.ico" type="image/x-icon">
</head>
<body>
    <div class="grid">
        
<div class="readable">
    <div id="readOverlay" class="style-ebook"><div id="readInner" class="margin-medium size-medium"><h1>Learning to Drive Smoothly in Minutes</h1><div><div class="ac ae af ag ah ec aj ak"><h2 id="ef78" class="iv iw ef at as ix iy iz ja jb jc jd je jf jg jh ji">Learning to Drive in Minutes — The Updated Approach</h2><p id="99a7" class="if ig ef at ih b ey kh fa ki ik kj im kk io kl iq dx">Although Wayve.ai technique may work in principle, it has some issues that needs to be addressed to apply it to a self-driving RC car.</p><p id="b570" class="if ig ef at ih b ey ii fa ij ik il im in io ip iq dx">First, because the feature extractor (VAE) is trained after each episode, the distribution of features is not stationary. That is to say, <em class="kt">the features are changing over time</em> and can lead to instabilities in the policy training. Also, training a VAE on a laptop (without a GPU) is quite slow, so we would like to avoid retraining the VAE after each episode.</p><p id="709c" class="if ig ef at ih b ey ii fa ij ik il im in io ip iq dx">To address those two problems, I decided to <em class="kt">train a VAE beforehand</em> and used Google <a href="https://colab.research.google.com/drive/1mF2abRb_yi4UNqYXVBF-t4FuCy6fl1c1#scrollTo=9bIR_N7R11XI" class="dc by ir is it iu" target="_blank" rel="noopener nofollow">Colab notebook</a> to preserve my computer. That way, the policy is trained with a fixed feature extractor.</p><p id="0912" class="if ig ef at ih b ey ii fa ij ik il im in io ip iq dx">In the image below, we <a href="https://github.com/araffin/srl-zoo" class="dc by ir is it iu" target="_blank" rel="noopener nofollow">explore what the VAE has learned</a>. We navigate in its latent space (using the sliders) and observe the reconstructed image.</p><figure class="hi hj hk hl hm hh do dp paragraph-image"><figcaption class="ax fz ia ib ic dq do dp id ie as cx"><a href="https://github.com/araffin/srl-zoo" class="dc by ir is it iu" target="_blank" rel="noopener nofollow">Exploring the latent space</a> learned by the VAE</figcaption></figure><p id="37d4" class="if ig ef at ih b ey ii fa ij ik il im in io ip iq dx">Then, DDPG is known to be <a href="https://arxiv.org/abs/1709.06560" class="dc by ir is it iu" target="_blank" rel="noopener nofollow">unstable</a> (in the sense that its performance can drop catastrophically during training) and is quite hard to tune. Fortunately, a recent algorithm named <a href="https://stable-baselines.readthedocs.io/en/master/modules/sac.html" class="dc by ir is it iu" target="_blank" rel="noopener nofollow"><strong class="ih ky">Soft Actor-Critic</strong></a><strong class="ih ky"> (SAC) has equivalent performances and is much easier to tune</strong>*.</p><p id="5066" class="if ig ef at ih b ey ii fa ij ik il im in io ip iq dx">*during my experiments, I tried <a href="https://stable-baselines.readthedocs.io/en/master/modules/ppo2.html" class="dc by ir is it iu" target="_blank" rel="noopener nofollow">PPO</a>, SAC and DDPG. DDPG and SAC were giving the best results in few episodes but SAC was simpler to tune.</p><p id="58a1" class="if ig ef at ih b ey ii fa ij ik il im in io ip iq dx">For this project, I used the Soft Actor-Critic (SAC) implementation I wrote for <a href="https://github.com/hill-a/stable-baselines" class="dc by ir is it iu" target="_blank" rel="noopener nofollow">stable-baselines</a><strong class="ih ky"></strong>(if you are working with RL, I definitely recommend you to take a look ;) ), that has the <a href="https://bair.berkeley.edu/blog/2018/12/14/sac/" class="dc by ir is it iu" target="_blank" rel="noopener nofollow">latest improvements</a> of the algorithm in it.</p><p id="8221" class="if ig ef at ih b ey ii fa ij ik il im in io ip iq dx">Finally, I updated the reward function and the action space to smooth the control and maximize the speed.</p><h2 id="4a3b" class="iv iw ef at as ix iy iz ja jb jc jd je jf jg jh ji">Reward Function: Go Fast but Stay on the Track!</h2><p id="1d1f" class="if ig ef at ih b ey kh fa ki ik kj im kk io kl iq dx">The robot car does not have any odometry (nor speed sensor), so the number of meters traveled (nor speed) cannot be used as a reward.</p><p id="077d" class="if ig ef at ih b ey ii fa ij ik il im in io ip iq dx">Therefore, I decided to give a “life bonus” at each time-step (i.e., a +1 reward for staying on the track) and penalize the robot, using a <strong class="ih ky">“</strong>crash penalty<strong class="ih ky">”</strong> (-10 reward) for leaving the track. Additionally, I found it beneficial to also punish the car for getting off the road too fast: an additional negative reward proportional to the throttle is added to the crash penalty.</p><p id="38ef" class="if ig ef at ih b ey ii fa ij ik il im in io ip iq dx">Finally, because<strong class="ih ky"></strong>we want to go fast as it is a racing car, I added a “throttle bonus” proportional to the current throttle.<strong class="ih ky"></strong>That way, <strong class="ih ky">the robot will try to stay on the track and maximize its speed at the same time</strong>.</p><p id="d7bc" class="if ig ef at ih b ey ii fa ij ik il im in io ip iq dx">To sum it up:</p><figure class="hi hj hk hl hm hh do dp paragraph-image"><p id="19e3" class="if ig ef at ih b ey ii fa ij ik il im in io ip iq dx">where w1 and w2 are just constant that allows to balance the objectives (with w1 &lt;&lt; 10 and w2 &lt;&lt; 1 because they are secondary objectives)</p><h2 id="c2ab" class="iv iw ef at as ix iy iz ja jb jc jd je jf jg jh ji">Avoiding Shaky Control: Learning to Drive Smoothly</h2><blockquote class="lp lq lr"><p id="a09b" class="if ig ef kt ih b ey ii fa ij ik il im in io ip iq dx">The world is not really stochastic. If you’ve noticed — a robot doesn’t just spontaneously start shaking. Unless you hook up an RL algorithm to it. — Emo Todorov</p></blockquote><figure class="hi hj hk hl hm hh do dp paragraph-image"><figcaption class="ax fz ia ib ic dq do dp id ie as cx">Left: Shaky Control — Right: Smooth Control using the proposed technique</figcaption></figure><p id="f8ad" class="if ig ef at ih b ey ii fa ij ik il im in io ip iq dx">If you apply the presented approach so far, it will work: the car will stay on the track and will try to go fast. However, <strong class="ih ky">you will probably end up with a shaky control: </strong>the car will oscillate as shown in the above image, because it has no incentive not to do so, it just tries to maximize its reward.</p><p id="9a5a" class="if ig ef at ih b ey ii fa ij ik il im in io ip iq dx"><strong class="ih ky">The solution to smooth the control is to constrain the change in steering angle while augmenting the input with the history of previous commands </strong>(steering and throttle). That way, <em class="kt">you impose continuity</em> in the steering.</p><p id="4247" class="if ig ef at ih b ey ii fa ij ik il im in io ip iq dx">As an example, if the current car steering angle is 0° and it tries suddenly to steer at 90°, the continuity constrain will only allow it to steer at 40° for instance. Hence, the difference between two consecutive steering commands stays in a given range. This additional constrain comes at a cost of a little more training.</p><p id="97e2" class="if ig ef at ih b ey ii fa ij ik il im in io ip iq dx">I passed several days trying to solve that issue before finding a satisfying solution<strong class="ih ky">, so here is what I tried that did not work</strong>:</p><ul class=""><li id="b6a0" class="if ig ef at ih b ey ii fa ij ik il im in io ip iq lu lv lw">output relative steering instead of absolute steering: produces oscillations with lower frequency</li><li id="37a6" class="if ig ef at ih b ey lx fa ly ik lz im ma io mb iq lu lv lw">add a continuity penalty (penalize the robot for high changes in steering): the robot does not optimize the right thing, it works sometimes but then don’t stay on the track. If the cost on that penalty is too low, it just ignores it.</li><li id="1c2e" class="if ig ef at ih b ey lx fa ly ik lz im ma io mb iq lu lv lw">limit the maximum steering: the car cannot stay on the track anymore in the sharpest turns</li><li id="2fad" class="if ig ef at ih b ey lx fa ly ik lz im ma io mb iq lu lv lw"><a href="https://danieltakeshi.github.io/2016/11/25/frame-skipping-and-preprocessing-for-deep-q-networks-on-atari-2600-games/" class="dc by ir is it iu" target="_blank" rel="noopener nofollow">stack several frames </a>to give some velocity information: produces oscillation of lower frequency</li></ul><p id="9d09" class="if ig ef at ih b ey ii fa ij ik il im in io ip iq dx">Note: recently, <a href="http://robotics.sciencemag.org/content/4/26/eaau5872" class="dc by ir is it iu" target="_blank" rel="noopener nofollow">researchers from ETH Zurich</a> suggested to use curriculum learning to have continuous and energy-efficient control. This could be a second solution (although a bit harder to tune).</p><h2 id="5df6" class="iv iw ef at as ix iy iz ja jb jc jd je jf jg jh ji">Recap of the Approach</h2><p id="dfb3" class="if ig ef at ih b ey kh fa ki ik kj im kk io kl iq dx">In our approach, we decouple policy learning from feature extraction and add an additional constrain to smooth the control.</p><p id="4f61" class="if ig ef at ih b ey ii fa ij ik il im in io ip iq dx">First, a human collects data by driving manually the car (10k images in ~5 minutes of manual driving). Those images are used to train a VAE.</p><p id="59e4" class="if ig ef at ih b ey ii fa ij ik il im in io ip iq dx">Then, we alternate between exploration episodes (a stochastic policy is used) and policy training (done when the human puts the car back on the track to optimize the time spent).</p><p id="a2eb" class="if ig ef at ih b ey ii fa ij ik il im in io ip iq dx">For training the policy, the images are first encoded using a VAE (here with a latent space of dimension 64) and concatenated with an history of the last ten actions taken (throttle and steering) creating a 84D feature vector.</p><p id="3691" class="if ig ef at ih b ey ii fa ij ik il im in io ip iq dx">The control policy is represented by a neural network (2 fully-connected layers of 32 and 16 units, with ReLU or ELU activation function).</p><p id="5efc" class="if ig ef at ih b ey ii fa ij ik il im in io ip iq dx">This controller outputs steering angle and throttle. We constrain the throttle to be in a given range and also limits the difference between current and previous steering angle.</p><h2 id="d18f" class="iv iw ef at as ix iy iz ja jb jc jd je jf jg jh ji">Conclusion</h2><p id="0914" class="if ig ef at ih b ey kh fa ki ik kj im kk io kl iq dx">In this article, we have presented an approach to learn a smooth control policy for the Donkey Car in minutes, using only a camera.</p><p id="aeb4" class="if ig ef at ih b ey ii fa ij ik il im in io ip iq dx">As this method is designed to be applied in the real world, this is definitely my next step in this project: test the approach on a real RC car* (see below). This will require to shrink the VAE model (the policy network is already quite small) in order to make it run on a raspberry pi.</p><p id="f5af" class="if ig ef at ih b ey ii fa ij ik il im in io ip iq dx">That’s all for today, don’t hesitate to test the code, comment or ask questions, and remember, sharing is caring ;)!</p><p id="91a9" class="if ig ef at ih b ey ii fa ij ik il im in io ip iq dx">*the wayve.ai approach was <a href="https://www.youtube.com/watch?v=6JUjDw9tfD4" class="dc by ir is it iu" target="_blank" rel="noopener nofollow">reproduced by Roma Sokolkov on a real RC car</a>, however that does not include the latest improvements for smooth control</p><figure class="hi hj hk hl hm hh"><h2 id="793a" class="iv iw ef at as ix iy iz ja jb jc jd je jf jg jh ji">Acknowledgments</h2><p id="fecb" class="if ig ef at ih b ey kh fa ki ik kj im kk io kl iq dx">This work would not have been possible without <a href="https://github.com/r7vme/learning-to-drive-in-a-day" class="dc by ir is it iu" target="_blank" rel="noopener nofollow">Roma Sokolkov</a>’s re-implementation of Wayve.ai approach, <a href="https://github.com/tawnkramer" class="dc by ir is it iu" target="_blank" rel="noopener nofollow">Tawn Kramer</a>’s Donkey Car simulator, <a href="https://flyyufelix.github.io/2018/09/11/donkey-rl-simulation.html" class="dc by ir is it iu" target="_blank" rel="noopener nofollow">Felix Yu</a>’s blog post for inspiration, <a href="https://github.com/hardmaru/WorldModelsExperiments" class="dc by ir is it iu" target="_blank" rel="noopener nofollow">David Ha</a> for his VAE implementation, <a href="https://github.com/hill-a/stable-baselines" class="dc by ir is it iu" target="_blank" rel="noopener nofollow">Stable-Baselines</a> and its <a href="https://github.com/araffin/rl-baselines-zoo" class="dc by ir is it iu" target="_blank" rel="noopener nofollow">model zoo</a> for SAC implementation and training scripts, the <a href="https://github.com/sergionr2/RacingRobot" class="dc by ir is it iu" target="_blank" rel="noopener nofollow">Racing Robot project</a> for the teleoperation and the <a href="https://github.com/araffin/robotics-rl-srl" class="dc by ir is it iu" target="_blank" rel="noopener nofollow">S-RL Toolbox</a> for VAE debugging and training tools.</p><p id="ba86" class="if ig ef at ih b ey ii fa ij ik il im in io ip iq dx">I would like also to thanks Roma, Sebastian, Tawn, Florence, Johannes, Jonas, Gabriel, Alvaro, Arthur and Sergio for the feedbacks.</p><h2 id="d47b" class="iv iw ef at as ix iy iz ja jb jc jd je jf jg jh ji">Appendix: Learning a State Representation</h2><p id="f09f" class="if ig ef at ih b ey kh fa ki ik kj im kk io kl iq dx"><em class="kt">Influence of the latent space dimension and number of samples</em></p><p id="c032" class="if ig ef at ih b ey ii fa ij ik il im in io ip iq dx">The latent space dimension of the VAE just need to be big enough so the VAE manages to reconstruct the important part of the input image. For instance, there were no huge differences in the resulting control policy between a 64D and 512D VAE.</p><p id="d900" class="if ig ef at ih b ey ii fa ij ik il im in io ip iq dx">One important thing is not really the number of samples but rather the diversity and the representativeness of samples. If your training images does not cover all the environment diversity, then you need more samples.</p><p id="52de" class="if ig ef at ih b ey ii fa ij ik il im in io ip iq dx"><em class="kt">Can we learn a control policy from random features?</em></p><p id="e971" class="if ig ef at ih b ey ii fa ij ik il im in io ip iq dx">I tried to fix the weights of the VAE right after initialization and then learn a policy on those random features. However, that did not work.</p><p id="935a" class="if ig ef at ih b ey ii fa ij ik il im in io ip iq dx"><em class="kt">Comparison with learning from pixels</em></p><p id="9528" class="if ig ef at ih b ey ii fa ij ik il im in io ip iq dx">I did not have the time (because my laptop does not have a GPU) to compare the approach from learning the policy directly from pixels. However, I’ll be interested in the results if someone could do that using my codebase.</p><p id="0ae2" class="if ig ef at ih b ey ii fa ij ik il im in io ip iq dx"><em class="kt">What is the minimal policy that works?</em></p><p id="b636" class="if ig ef at ih b ey ii fa ij ik il im in io ip iq dx">A one-layer mlp works. I tried also with a linear policy, however, I did not succeed to obtain a good controller.</p></figure></figure></div></div></div></div>
</div>
    </div>
    <footer>
        <div>created by buildstarted &copy; 2020 <a href="/about">about</a></div>
        <div>Share this page on social media: copy and paste this url https://linksfor.dev/</div>
        <div>If you prefer RSS: <a href="https://linksfor.dev/feed.xml">https://linksfor.dev/feed.xml</a></div>
    </footer>
    
    <script>
        (function () {
            var COLLECT_URL = "https://dna.buildstarted.com/t";
            var SITE_ID = "linksfor.devs";
            var GLOBAL_VAR_NAME = "__DNA__";

            window[GLOBAL_VAR_NAME] = {};

            window[GLOBAL_VAR_NAME].sendPageView = function () {
                var path = location.pathname;
                var referrer = document.referrer;

                var url = COLLECT_URL + "?siteid=" + SITE_ID + "&p=" + encodeURIComponent(path) + "&r=" + encodeURIComponent(referrer);

                try { fetch(url, { method: "GET" }); } catch (e) { }
            };

            window[GLOBAL_VAR_NAME].sendPageView();
        })();
    </script>
</body>
</html>