<!DOCTYPE html>
<html lang="en">
<head>
    <title>
Binary Stochastic Neurons in Tensorflow -
linksfor.dev(s)
    </title>
    <link rel="alternate" type="application/rss+xml" title="Linksfor.dev(s) feed" href="https://linksfor.dev/feed.rss" />
    <meta charset="utf-8">
    <meta name="Description" content="A curated source of links that devs might find interesting. Updated around the clock." />
    <meta name="google" value="notranslate">
    <link rel="stylesheet" type="text/css" href="/style.css" />
    <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon">
    <link rel="icon" href="/favicon.ico" type="image/x-icon">
</head>
<body>
    <div class="grid">
        
<div class="readable">
    <div id="readOverlay" class="style-ebook"><div id="readInner" class="margin-medium size-medium"><h1>Binary Stochastic Neurons in Tensorflow</h1><div><div><div class="container"><section id="content" class="article content"><header><p class="text-muted">Sat 24 September 2016</p></header><div class="entry-content"><meta charset="utf-8"><meta name="generator" content="pandoc"><meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes"><title></title><p>In this post, I introduce and discuss binary stochastic neurons, implement trainable binary stochastic neurons in Tensorflow, and conduct several simple experiments on the MNIST dataset to get a feel for their behavior. Binary stochastic neurons offer two advantages over real-valued neurons: they can act as a regularizer and they enable conditional computation by enabling a network to make yes/no decisions. Conditional computation opens the door to new and exciting neural network architectures, such as the choice of experts architecture and heirarchical multiscale neural networks, which I plan to discuss in future posts.</p><h3 id="the-binary-stochastic-neuron">The binary stochastic neuron</h3><p>A binary stochastic neuron is a neuron with a noisy output: some proportion <span class="math inline">\(p\)</span> of the time it outputs 1, otherwise 0. An easy way to turn a real-valued input, <span class="math inline">\(a\)</span>, into this proportion, <span class="math inline">\(p\)</span>, is to set <span class="math inline">\(p = \text{sigm}(a)\)</span>, where <span class="math inline">\(\text{sigm}\)</span> is the logistic sigmoid, <span class="math inline">\(\text{sigm}(x) = \frac{1}{1 + \exp(-x)}\)</span>. Thus, we define the binary stochastic neuron, <span class="math inline">\(\text{BSN}\)</span>, as:</p><p><span class="math display">\[\text{BSN}(a) = \textbf{1}_{z\ \lt\ \text{sigm}(a)}\]</span></p><p>where <span class="math inline">\(\textbf{1}_{x}\)</span> is the <a href="https://en.wikipedia.org/wiki/Indicator_function">indicator function</a> on the truth value of <span class="math inline">\(x\)</span> and <span class="math inline">\(z \sim U[0,1]\)</span>.</p><h3 id="advantages-of-the-binary-stochastic-neuron">Advantages of the binary stochastic neuron</h3><ol type="1"><li><p>A binary stochastic neuron is a noisy modification of the logistic sigmoid: instead of outputting <span class="math inline">\(p\)</span>, it outputs 1 with probability <span class="math inline">\(p\)</span> and 0 otherwise. Noise generally serves as a regularizer (see, e.g., <a href="http://www.jmlr.org/papers/v15/srivastava14a.html">Srivastava et al. (2014)</a> and <a href="https://arxiv.org/abs/1511.06807">Neelakantan et al. (2015)</a>), and so we might expect the same from binary stochastic neurons as compared to the logistic neurons. Indeed, this is the claimed “unpublished result” from the end of <a href="https://www.youtube.com/watch?v=LN0xtUuJsEI&amp;list=PLoRl3Ht4JOcdU872GhiYWf6jwrk_SNhz9&amp;index=41">Hinton et al.’s Coursera Lecture 9c</a>, which I demonstrate empirically in this post.</p></li><li><p>Further, by enabling networks to make binary decisions, the binary stochastic neuron allows for conditional computation. This opens the door to some interesting new architectures. For example, instead of a mixture of experts architecture, which weights the outputs of several “expert” sub-networks and requires that all subnetworks be computed, we could use a <em>choice</em> of experts architecture, which conditionally uses expert sub-networks as needed. This architecture is implicitly proposed in <a href="https://arxiv.org/abs/1308.3432">Bengio et al. (2013)</a>, wherein the experiments use a choice of expert units architecture (i.e., a gated architecture where gates must be 1 or 0). Another example, proposed in <a href="https://arxiv.org/abs/1308.3432">Bengio et al. (2013)</a> and implemented by <a href="https://arxiv.org/abs/1609.01704">Chung et al. (2016)</a>, is the Heirarchical Multiscale Recurrent Neural Network (HM-RNN) architecture, which achieves great results on language modelling tasks.</p></li></ol><h3 id="training-the-binary-stochastic-neuron">Training the binary stochastic neuron</h3><p>For any single trial, the binary stochastic neuron generally has a derivative of 0 and cannot be trained by simple backpropagation. To see this, consider that if <span class="math inline">\(z \neq \text{sigm}(a)\)</span> in the <span class="math inline">\(\text{BSN}\)</span> function above, there exists a <a href="https://en.wikipedia.org/wiki/Neighbourhood_(mathematics)">neighborhood</a> around <span class="math inline">\(a\)</span> such that the output of <span class="math inline">\(\text{BSN}(a)\)</span> is unchanged (i.e., the derivative is 0). We get around this by <em>estimating</em> the derivative with respect to the <em>expected</em> loss, rather than calculating the derivative with respect to the outcome of a single trial. We can only estimate this derivative, because in any given trial, we only see the loss value with respect to the given noise – we don’t know what the loss would have been given another level of noise. We call a method that provides such an estimate an “estimator”. An estimator is <em>unbiased</em> if the expectation of its estimate equals the expectation of the derivative it is estimating; otherwise, it is <em>biased</em>.</p><p>In this post we implement the two estimators discussed in <a href="https://arxiv.org/abs/1308.3432">Bengio et al. (2013)</a>:</p><ol type="1"><li><p>The REINFORCE estimator, which is an unbiased estimator and a special case of the REINFORCE algorithm discussed in <a href="http://link.springer.com/article/10.1007/BF00992696">Williams (1992)</a>.</p><p>The REINFORCE estimator estimates the expectation of <span class="math inline">\(\frac{\partial L}{\partial a}\)</span> as <span class="math inline">\((\text{BSN}(a) - \text{sigm}(a))(L - c)\)</span>, where <span class="math inline">\(c\)</span> is a constant. <a href="https://arxiv.org/abs/1308.3432">Bengio et al. (2013)</a> proves that:</p><p><span class="math display">\[\mathbb{E}[(\text{BSN}(a) - \text{sigm}(a))(L - c)] = \mathbb{E}\big[\frac{\partial L}{\partial a}\big].\]</span></p><p><a href="https://arxiv.org/abs/1308.3432">Bengio et al. (2013)</a> further shows that to minimize the variance of the estimation, we choose:</p><p><span class="math display">\[c = \bar L = \frac{\mathbb{E}[\text{BSN}(a) - \text{sigm}(a))^2L]}{\mathbb{E}[\text{BSN}(a) - \text{sigm}(a))^2]}\]</span></p><p>which we can practically implement by keeping track of the numerator and denominator as a moving average. Interestingly, the REINFORCE estimator does not require any backpropagated loss gradient–it operates directly on the loss of the network.</p></li><li><p>The straight through (ST) estimator, which is a biased estimator that was first proposed by <a href="https://www.youtube.com/watch?v=LN0xtUuJsEI&amp;list=PLoRl3Ht4JOcdU872GhiYWf6jwrk_SNhz9&amp;index=41">Hinton et al.’s Coursera Lecture 9c</a>.</p><p>The ST estimator simply replaces the derivative factor used during backpropagation, <span class="math inline">\(\frac{d\text{BSN}(a)}{da} = 0\)</span>, with the identity function <span class="math inline">\(\frac{d\text{BSN}(a)}{da} = 1\)</span>.<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a> A variant of the ST estimator replaces the derivative factor with <span class="math inline">\(\frac{d\text{BSN}(a)}{da} = \frac{d\text{sigm}(a)}{da}\)</span>. Whereas <a href="https://arxiv.org/abs/1308.3432">Bengio et al. (2013)</a> found that the former is more effective, the latter variant was successfully used in <a href="https://arxiv.org/abs/1609.01704">Chung et al. (2016)</a> in combination with the <em>slope-annealing trick</em> and deterministic binary neurons (which we will see perform very similarly to, if not better than, stochastic binary neurons when used with slope-annealing). The slope-anealing trick modifies <span class="math inline">\(\text{BSN}(a)\)</span> by first multiplying the input <span class="math inline">\(a\)</span> by a slope <span class="math inline">\(m\)</span> as follows:</p><p><span class="math display">\[\text{BSN}_{\text{SL}(m)}(a) = \textbf{1}_{z \lt \text{sigm}(ma)}.\]</span></p><p>Then, we increase the slope as training progresses and use <span class="math inline">\(\frac{d\text{BSN}_{\text{SL}(m)}(a)}{da} = \frac{d\text{sigm}(ma)}{da}\)</span> when computing the gradient. The idea behind this is that as the slope increases, the logistic sigmoid approaches a step function, so that it’s derivative approaches the true derivative. All three variants are tested in this post.</p></li></ol><h3 id="implementing-the-binary-stochastic-neuron-in-tensorflow">Implementing the binary stochastic neuron in Tensorflow</h3><p>The tricky part of implementing a binary stochastic neuron in Tensorflow is not the forward computation, but the implementation of the REINFORCE and straight through estimators. Each requires replacing the gradient of one or more Tensorflow operations. The <a href="https://www.tensorflow.org/how_tos/adding_an_op/">official approach</a> to this is to write a new op in C++, which seems wholly unnecessary. There are, however, two workable unofficial approaches, one of which is <a href="http://stackoverflow.com/questions/36456436/how-can-i-define-only-the-gradient-for-a-tensorflow-subgraph/36480182">a trick credited to Sergey Ioffe</a>, and another that uses <code>gradient_override_map</code>, an experimental feature of Tensorflow that is documented <a href="https://www.tensorflow.org/api_docs/python/framework/core_graph_data_structures#Graph.gradient_override_map">here</a>. We will use <code>gradient_override_map</code>, which works well for our purposes.</p><h4 id="imports-and-utility-functions">Imports and Utility Functions</h4><div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="im">import</span> numpy <span class="im">as</span> np
<span class="im">import</span> tensorflow <span class="im">as</span> tf
<span class="im">from</span> tensorflow.examples.tutorials.mnist <span class="im">import</span> input_data
<span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt
<span class="op">%</span>matplotlib inline
mnist <span class="op">=</span> input_data.read_data_sets(<span class="st">'MNIST_data'</span>, one_hot<span class="op">=</span><span class="va">True</span>)
<span class="im">from</span> tensorflow.python.framework <span class="im">import</span> ops
<span class="im">from</span> enum <span class="im">import</span> Enum
<span class="im">import</span> seaborn <span class="im">as</span> sns
sns.<span class="bu">set</span>(color_codes<span class="op">=</span><span class="va">True</span>)

<span class="kw">def</span> reset_graph():
    <span class="cf">if</span><span class="st">'sess'</span><span class="kw">in</span><span class="bu">globals</span>() <span class="kw">and</span> sess:
        sess.close()
    tf.reset_default_graph()

<span class="kw">def</span> layer_linear(inputs, shape, scope<span class="op">=</span><span class="st">'linear_layer'</span>):
    <span class="cf">with</span> tf.variable_scope(scope):
        w <span class="op">=</span> tf.get_variable(<span class="st">'w'</span>,shape)
        b <span class="op">=</span> tf.get_variable(<span class="st">'b'</span>,shape[<span class="op">-</span><span class="dv">1</span>:])
    <span class="cf">return</span> tf.matmul(inputs,w) <span class="op">+</span> b

<span class="kw">def</span> layer_softmax(inputs, shape, scope<span class="op">=</span><span class="st">'softmax_layer'</span>):
    <span class="cf">with</span> tf.variable_scope(scope):
        w <span class="op">=</span> tf.get_variable(<span class="st">'w'</span>,shape)
        b <span class="op">=</span> tf.get_variable(<span class="st">'b'</span>,shape[<span class="op">-</span><span class="dv">1</span>:])
    <span class="cf">return</span> tf.nn.softmax(tf.matmul(inputs,w) <span class="op">+</span> b)

<span class="kw">def</span> accuracy(y, pred):
    correct <span class="op">=</span> tf.equal(tf.argmax(y,<span class="dv">1</span>), tf.argmax(pred,<span class="dv">1</span>))
    <span class="cf">return</span> tf.reduce_mean(tf.cast(correct, tf.float32))

<span class="kw">def</span> plot_n(data_and_labels, lower_y <span class="op">=</span><span class="fl">0.</span>, title<span class="op">=</span><span class="st">"Learning Curves"</span>):
    fig, ax <span class="op">=</span> plt.subplots()
    <span class="cf">for</span> data, label <span class="kw">in</span> data_and_labels:
        ax.plot(<span class="bu">range</span>(<span class="dv">0</span>,<span class="bu">len</span>(data)<span class="op">*</span><span class="dv">100</span>,<span class="dv">100</span>),data, label<span class="op">=</span>label)
    ax.set_xlabel(<span class="st">'Training steps'</span>)
    ax.set_ylabel(<span class="st">'Accuracy'</span>)
    ax.set_ylim([lower_y,<span class="dv">1</span>])
    ax.set_title(title)
    ax.legend(loc<span class="op">=</span><span class="dv">4</span>)
    plt.show()

<span class="kw">class</span> StochasticGradientEstimator(Enum):
    ST <span class="op">=</span><span class="dv">0</span>
    REINFORCE <span class="op">=</span><span class="dv">1</span></code></pre></div><pre><code>Extracting MNIST_data/train-images-idx3-ubyte.gz
Extracting MNIST_data/train-labels-idx1-ubyte.gz
Extracting MNIST_data/t10k-images-idx3-ubyte.gz
Extracting MNIST_data/t10k-labels-idx1-ubyte.gz</code></pre><h4 id="binary-stochastic-neuron-with-straight-through-estimator">Binary stochastic neuron with straight through estimator</h4><div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="kw">def</span> binaryRound(x):
    <span class="co">"""</span><span class="co">    Rounds a tensor whose values are in [0,1] to a tensor with values in {0, 1},</span><span class="co">    using the straight through estimator for the gradient.</span><span class="co">    """</span>
    g <span class="op">=</span> tf.get_default_graph()

    <span class="cf">with</span> ops.name_scope(<span class="st">"BinaryRound"</span>) <span class="im">as</span> name:
        <span class="cf">with</span> g.gradient_override_map({<span class="st">"Round"</span>: <span class="st">"Identity"</span>}):
            <span class="cf">return</span> tf.<span class="bu">round</span>(x, name<span class="op">=</span>name)

        <span class="co"># For Tensorflow v0.11 and below use:</span><span class="co">#with g.gradient_override_map({"Floor": "Identity"}):</span><span class="co">#    return tf.round(x, name=name)</span></code></pre></div><div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="kw">def</span> bernoulliSample(x):
    <span class="co">"""</span><span class="co">    Uses a tensor whose values are in [0,1] to sample a tensor with values in {0, 1},</span><span class="co">    using the straight through estimator for the gradient.</span><span class="co">    E.g.,:</span><span class="co">    if x is 0.6, bernoulliSample(x) will be 1 with probability 0.6, and 0 otherwise,</span><span class="co">    and the gradient will be pass-through (identity).</span><span class="co">    """</span>
    g <span class="op">=</span> tf.get_default_graph()

    <span class="cf">with</span> ops.name_scope(<span class="st">"BernoulliSample"</span>) <span class="im">as</span> name:
        <span class="cf">with</span> g.gradient_override_map({<span class="st">"Ceil"</span>: <span class="st">"Identity"</span>,<span class="st">"Sub"</span>: <span class="st">"BernoulliSample_ST"</span>}):
            <span class="cf">return</span> tf.ceil(x <span class="op">-</span> tf.random_uniform(tf.shape(x)), name<span class="op">=</span>name)

<span class="at">@ops.RegisterGradient</span>(<span class="st">"BernoulliSample_ST"</span>)
<span class="kw">def</span> bernoulliSample_ST(op, grad):
    <span class="cf">return</span> [grad, tf.zeros(tf.shape(op.inputs[<span class="dv">1</span>]))]</code></pre></div><div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="kw">def</span> passThroughSigmoid(x, slope<span class="op">=</span><span class="dv">1</span>):
    <span class="co">"""Sigmoid that uses identity function as its gradient"""</span>
    g <span class="op">=</span> tf.get_default_graph()
    <span class="cf">with</span> ops.name_scope(<span class="st">"PassThroughSigmoid"</span>) <span class="im">as</span> name:
        <span class="cf">with</span> g.gradient_override_map({<span class="st">"Sigmoid"</span>: <span class="st">"Identity"</span>}):
            <span class="cf">return</span> tf.sigmoid(x, name<span class="op">=</span>name)

<span class="kw">def</span> binaryStochastic_ST(x, slope_tensor<span class="op">=</span><span class="va">None</span>, pass_through<span class="op">=</span><span class="va">True</span>, stochastic<span class="op">=</span><span class="va">True</span>):
    <span class="co">"""</span><span class="co">    Sigmoid followed by either a random sample from a bernoulli distribution according</span><span class="co">    to the result (binary stochastic neuron) (default), or a sigmoid followed by a binary</span><span class="co">    step function (if stochastic == False). Uses the straight through estimator.</span><span class="co">    See https://arxiv.org/abs/1308.3432.</span><span class="co">    Arguments:</span><span class="co">    * x: the pre-activation / logit tensor</span><span class="co">    * slope_tensor: if passThrough==False, slope adjusts the slope of the sigmoid function</span><span class="co">        for purposes of the Slope Annealing Trick (see http://arxiv.org/abs/1609.01704)</span><span class="co">    * pass_through: if True (default), gradient of the entire function is 1 or 0;</span><span class="co">        if False, gradient of 1 is scaled by the gradient of the sigmoid (required if</span><span class="co">        Slope Annealing Trick is used)</span><span class="co">    * stochastic: binary stochastic neuron if True (default), or step function if False</span><span class="co">    """</span><span class="cf">if</span> slope_tensor <span class="kw">is</span><span class="va">None</span>:
        slope_tensor <span class="op">=</span> tf.constant(<span class="fl">1.0</span>)

    <span class="cf">if</span> pass_through:
        p <span class="op">=</span> passThroughSigmoid(x)
    <span class="cf">else</span>:
        p <span class="op">=</span> tf.sigmoid(slope_tensor<span class="op">*</span>x)

    <span class="cf">if</span> stochastic:
        <span class="cf">return</span> bernoulliSample(p)
    <span class="cf">else</span>:
        <span class="cf">return</span> binaryRound(p)</code></pre></div><h4 id="binary-stochastic-neuron-with-reinforce-estimator">Binary stochastic neuron with REINFORCE estimator</h4><div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="kw">def</span> binaryStochastic_REINFORCE(x, stochastic <span class="op">=</span><span class="va">True</span>, loss_op_name<span class="op">=</span><span class="st">"loss_by_example"</span>):
    <span class="co">"""</span><span class="co">    Sigmoid followed by a random sample from a bernoulli distribution according</span><span class="co">    to the result (binary stochastic neuron). Uses the REINFORCE estimator.</span><span class="co">    See https://arxiv.org/abs/1308.3432.</span><span class="co"></span><span class="al">NOTE</span><span class="co">: Requires a loss operation with name matching the argument for loss_op_name</span><span class="co">    in the graph. This loss operation should be broken out by example (i.e., not a</span><span class="co">    single number for the entire batch).</span><span class="co">    """</span>
    g <span class="op">=</span> tf.get_default_graph()

    <span class="cf">with</span> ops.name_scope(<span class="st">"BinaryStochasticREINFORCE"</span>):
        <span class="cf">with</span> g.gradient_override_map({<span class="st">"Sigmoid"</span>: <span class="st">"BinaryStochastic_REINFORCE"</span>,
                                      <span class="st">"Ceil"</span>: <span class="st">"Identity"</span>}):
            p <span class="op">=</span> tf.sigmoid(x)

            reinforce_collection <span class="op">=</span> g.get_collection(<span class="st">"REINFORCE"</span>)
            <span class="cf">if</span><span class="kw">not</span> reinforce_collection:
                g.add_to_collection(<span class="st">"REINFORCE"</span>, {})
                reinforce_collection <span class="op">=</span> g.get_collection(<span class="st">"REINFORCE"</span>)
            reinforce_collection[<span class="dv">0</span>][p.op.name] <span class="op">=</span> loss_op_name

            <span class="cf">return</span> tf.ceil(p <span class="op">-</span> tf.random_uniform(tf.shape(x)))


<span class="at">@ops.RegisterGradient</span>(<span class="st">"BinaryStochastic_REINFORCE"</span>)
<span class="kw">def</span> _binaryStochastic_REINFORCE(op, _):
    <span class="co">"""Unbiased estimator for binary stochastic function based on REINFORCE."""</span>
    loss_op_name <span class="op">=</span> op.graph.get_collection(<span class="st">"REINFORCE"</span>)[<span class="dv">0</span>][op.name]
    loss_tensor <span class="op">=</span> op.graph.get_operation_by_name(loss_op_name).outputs[<span class="dv">0</span>]

    sub_tensor <span class="op">=</span> op.outputs[<span class="dv">0</span>].consumers()[<span class="dv">0</span>].outputs[<span class="dv">0</span>] <span class="co">#subtraction tensor</span>
    ceil_tensor <span class="op">=</span> sub_tensor.consumers()[<span class="dv">0</span>].outputs[<span class="dv">0</span>] <span class="co">#ceiling tensor</span>

    outcome_diff <span class="op">=</span> (ceil_tensor <span class="op">-</span> op.outputs[<span class="dv">0</span>])

    <span class="co"># Provides an early out if we want to avoid variance adjustment for</span><span class="co"># whatever reason (e.g., to show that variance adjustment helps)</span><span class="cf">if</span> op.graph.get_collection(<span class="st">"REINFORCE"</span>)[<span class="dv">0</span>].get(<span class="st">"no_variance_adj"</span>):
        <span class="cf">return</span> outcome_diff <span class="op">*</span> tf.expand_dims(loss_tensor, <span class="dv">1</span>)

    outcome_diff_sq <span class="op">=</span> tf.square(outcome_diff)
    outcome_diff_sq_r <span class="op">=</span> tf.reduce_mean(outcome_diff_sq, reduction_indices<span class="op">=</span><span class="dv">0</span>)
    outcome_diff_sq_loss_r <span class="op">=</span> tf.reduce_mean(outcome_diff_sq <span class="op">*</span> tf.expand_dims(loss_tensor, <span class="dv">1</span>),
                                            reduction_indices<span class="op">=</span><span class="dv">0</span>)

    L_bar_num <span class="op">=</span> tf.Variable(tf.zeros(outcome_diff_sq_r.get_shape()), trainable<span class="op">=</span><span class="va">False</span>)
    L_bar_den <span class="op">=</span> tf.Variable(tf.ones(outcome_diff_sq_r.get_shape()), trainable<span class="op">=</span><span class="va">False</span>)

    <span class="co">#Note: we already get a decent estimate of the average from the minibatch</span>
    decay <span class="op">=</span><span class="fl">0.95</span>
    train_L_bar_num <span class="op">=</span> tf.assign(L_bar_num, L_bar_num<span class="op">*</span>decay <span class="op">+\</span>
                                            outcome_diff_sq_loss_r<span class="op">*</span>(<span class="dv">1</span><span class="op">-</span>decay))
    train_L_bar_den <span class="op">=</span> tf.assign(L_bar_den, L_bar_den<span class="op">*</span>decay <span class="op">+\</span>
                                            outcome_diff_sq_r<span class="op">*</span>(<span class="dv">1</span><span class="op">-</span>decay))


    <span class="cf">with</span> tf.control_dependencies([train_L_bar_num, train_L_bar_den]):
        L_bar <span class="op">=</span> train_L_bar_num<span class="op">/</span>(train_L_bar_den<span class="fl">+1e-4</span>)
        L <span class="op">=</span> tf.tile(tf.expand_dims(loss_tensor,<span class="dv">1</span>),
                    tf.constant([<span class="dv">1</span>,L_bar.get_shape().as_list()[<span class="dv">0</span>]]))
        <span class="cf">return</span> outcome_diff <span class="op">*</span> (L <span class="op">-</span> L_bar)</code></pre></div><h4 id="wrapper-to-create-layer-of-binary-stochastic-neurons">Wrapper to create layer of binary stochastic neurons</h4><div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="kw">def</span> binary_wrapper(<span class="op">\</span>
                pre_activations_tensor,
                estimator<span class="op">=</span>StochasticGradientEstimator.ST,
                stochastic_tensor<span class="op">=</span>tf.constant(<span class="va">True</span>),
                pass_through<span class="op">=</span><span class="va">True</span>,
                slope_tensor<span class="op">=</span>tf.constant(<span class="fl">1.0</span>)):
    <span class="co">"""</span><span class="co">    Turns a layer of pre-activations (logits) into a layer of binary stochastic neurons</span><span class="co">    Keyword arguments:</span><span class="co">    *estimator: either ST or REINFORCE</span><span class="co">    *stochastic_tensor: a boolean tensor indicating whether to sample from a bernoulli</span><span class="co">        distribution (True, default) or use a step_function (e.g., for inference)</span><span class="co">    *pass_through: for ST only - boolean as to whether to substitute identity derivative on the</span><span class="co">        backprop (True, default), or whether to use the derivative of the sigmoid</span><span class="co">    *slope_tensor: for ST only - tensor specifying the slope for purposes of slope annealing</span><span class="co">        trick</span><span class="co">    """</span><span class="cf">if</span> estimator <span class="op">==</span> StochasticGradientEstimator.ST:
        <span class="cf">if</span> pass_through:
            <span class="cf">return</span> tf.cond(stochastic_tensor,
                    <span class="kw">lambda</span>: binaryStochastic_ST(pre_activations_tensor),
                    <span class="kw">lambda</span>: binaryStochastic_ST(pre_activations_tensor, stochastic<span class="op">=</span><span class="va">False</span>))
        <span class="cf">else</span>:
            <span class="cf">return</span> tf.cond(stochastic_tensor,
                    <span class="kw">lambda</span>: binaryStochastic_ST(pre_activations_tensor, slope_tensor <span class="op">=</span> slope_tensor,
                                             pass_through<span class="op">=</span><span class="va">False</span>),
                    <span class="kw">lambda</span>: binaryStochastic_ST(pre_activations_tensor, slope_tensor <span class="op">=</span> slope_tensor,
                                             pass_through<span class="op">=</span><span class="va">False</span>, stochastic<span class="op">=</span><span class="va">False</span>))
    <span class="cf">elif</span> estimator <span class="op">==</span> StochasticGradientEstimator.REINFORCE:
        <span class="co"># binaryStochastic_REINFORCE was designed to only be stochastic, so using the ST version</span><span class="co"># for the step fn for purposes of using step fn at evaluation / not for training</span><span class="cf">return</span> tf.cond(stochastic_tensor,
                <span class="kw">lambda</span>: binaryStochastic_REINFORCE(pre_activations_tensor),
                <span class="kw">lambda</span>: binaryStochastic_ST(pre_activations_tensor, stochastic<span class="op">=</span><span class="va">False</span>))

    <span class="cf">else</span>:
        <span class="cf">raise</span><span class="pp">ValueError</span>(<span class="st">"Unrecognized estimator."</span>)</code></pre></div><h4 id="function-to-build-graph-for-mnist-classifier">Function to build graph for MNIST classifier</h4><div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="kw">def</span> build_classifier(hidden_dims<span class="op">=</span>[<span class="dv">100</span>],
                        lr <span class="op">=</span><span class="fl">0.5</span>,
                        pass_through <span class="op">=</span><span class="va">True</span>,
                        non_binary <span class="op">=</span><span class="va">False</span>,
                        estimator <span class="op">=</span> StochasticGradientEstimator.ST,
                        no_var_adj<span class="op">=</span><span class="va">False</span>):
    reset_graph()
    g <span class="op">=</span> {}

    <span class="cf">if</span> no_var_adj:
        tf.get_default_graph().add_to_collection(<span class="st">"REINFORCE"</span>, {<span class="st">"no_variance_adj"</span>: no_var_adj})

    g[<span class="st">'x'</span>] <span class="op">=</span> tf.placeholder(tf.float32, [<span class="va">None</span>, <span class="dv">784</span>], name<span class="op">=</span><span class="st">'x_placeholder'</span>)
    g[<span class="st">'y'</span>] <span class="op">=</span> tf.placeholder(tf.float32, [<span class="va">None</span>, <span class="dv">10</span>], name<span class="op">=</span><span class="st">'y_placeholder'</span>)
    g[<span class="st">'stochastic'</span>] <span class="op">=</span> tf.constant(<span class="va">True</span>)
    g[<span class="st">'slope'</span>] <span class="op">=</span> tf.constant(<span class="fl">1.0</span>)

    g[<span class="st">'layers'</span>] <span class="op">=</span> {<span class="dv">0</span>: g[<span class="st">'x'</span>]}
    hidden_layers <span class="op">=</span><span class="bu">len</span>(hidden_dims)
    dims <span class="op">=</span> [<span class="dv">784</span>] <span class="op">+</span> hidden_dims

    <span class="cf">for</span> i <span class="kw">in</span><span class="bu">range</span>(<span class="dv">1</span>, hidden_layers<span class="op">+</span><span class="dv">1</span>):
        <span class="cf">with</span> tf.variable_scope(<span class="st">"layer_"</span><span class="op">+</span><span class="bu">str</span>(i)):
            pre_activations <span class="op">=</span> layer_linear(g[<span class="st">'layers'</span>][i<span class="dv">-1</span>], dims[i<span class="dv">-1</span>:i<span class="op">+</span><span class="dv">1</span>], scope<span class="op">=</span><span class="st">'layer_'</span><span class="op">+</span><span class="bu">str</span>(i))
            <span class="cf">if</span> non_binary:
                g[<span class="st">'layers'</span>][i] <span class="op">=</span> tf.sigmoid(pre_activations)
            <span class="cf">else</span>:
                g[<span class="st">'layers'</span>][i] <span class="op">=</span> binary_wrapper(pre_activations,
                                              estimator <span class="op">=</span> estimator,
                                              pass_through <span class="op">=</span> pass_through,
                                              stochastic_tensor <span class="op">=</span> g[<span class="st">'stochastic'</span>],
                                              slope_tensor <span class="op">=</span> g[<span class="st">'slope'</span>])

    g[<span class="st">'pred'</span>] <span class="op">=</span> layer_softmax(g[<span class="st">'layers'</span>][hidden_layers], [dims[<span class="op">-</span><span class="dv">1</span>], <span class="dv">10</span>])

    g[<span class="st">'loss'</span>] <span class="op">=</span><span class="op">-</span>tf.reduce_mean(g[<span class="st">'y'</span>] <span class="op">*</span> tf.log(g[<span class="st">'pred'</span>]),reduction_indices<span class="op">=</span><span class="dv">1</span>)

    <span class="co"># named loss_by_example necessary for REINFORCE estimator</span>
    tf.identity(g[<span class="st">'loss'</span>], name<span class="op">=</span><span class="st">"loss_by_example"</span>)

    g[<span class="st">'ts'</span>] <span class="op">=</span> tf.train.GradientDescentOptimizer(lr).minimize(g[<span class="st">'loss'</span>])

    g[<span class="st">'accuracy'</span>] <span class="op">=</span> accuracy(g[<span class="st">'y'</span>], g[<span class="st">'pred'</span>])

    g[<span class="st">'init_op'</span>] <span class="op">=</span> tf.global_variables_initializer()
    <span class="cf">return</span> g</code></pre></div><h4 id="function-to-train-the-classifier">Function to train the classifier</h4><div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="kw">def</span> train_classifier(<span class="op">\</span>
        hidden_dims<span class="op">=</span>[<span class="dv">100</span>,<span class="dv">100</span>],
        estimator<span class="op">=</span>StochasticGradientEstimator.ST,
        stochastic_train<span class="op">=</span><span class="va">True</span>,
        stochastic_eval<span class="op">=</span><span class="va">True</span>,
        slope_annealing_rate<span class="op">=</span><span class="va">None</span>,
        epochs<span class="op">=</span><span class="dv">10</span>,
        lr<span class="op">=</span><span class="fl">0.5</span>,
        non_binary<span class="op">=</span><span class="va">False</span>,
        no_var_adj<span class="op">=</span><span class="va">False</span>,
        train_set <span class="op">=</span> mnist.train,
        val_set <span class="op">=</span> mnist.validation,
        verbose<span class="op">=</span><span class="va">False</span>,
        label<span class="op">=</span><span class="va">None</span>):
    <span class="cf">if</span> slope_annealing_rate <span class="kw">is</span><span class="va">None</span>:
        g <span class="op">=</span> build_classifier(hidden_dims<span class="op">=</span>hidden_dims, lr<span class="op">=</span>lr, pass_through<span class="op">=</span><span class="va">True</span>,
                                non_binary<span class="op">=</span>non_binary, estimator<span class="op">=</span>estimator, no_var_adj<span class="op">=</span>no_var_adj)
    <span class="cf">else</span>:
        g <span class="op">=</span> build_classifier(hidden_dims<span class="op">=</span>hidden_dims, lr<span class="op">=</span>lr, pass_through<span class="op">=</span><span class="va">False</span>,
                                non_binary<span class="op">=</span>non_binary, estimator<span class="op">=</span>estimator, no_var_adj<span class="op">=</span>no_var_adj)

    <span class="cf">with</span> tf.Session() <span class="im">as</span> sess:
        sess.run(g[<span class="st">'init_op'</span>])
        slope <span class="op">=</span><span class="dv">1</span>
        res_tr, res_val <span class="op">=</span> [], []
        <span class="cf">for</span> epoch <span class="kw">in</span><span class="bu">range</span>(epochs):
            feed_dict<span class="op">=</span>{g[<span class="st">'x'</span>]: val_set.images,
                       g[<span class="st">'y'</span>]: val_set.labels,
                       g[<span class="st">'stochastic'</span>]: stochastic_eval,
                       g[<span class="st">'slope'</span>]: slope}
            <span class="cf">if</span> verbose:
                <span class="bu">print</span>(<span class="st">"Epoch"</span>, epoch, sess.run(g[<span class="st">'accuracy'</span>], feed_dict<span class="op">=</span>feed_dict))

            accuracy <span class="op">=</span><span class="dv">0</span><span class="cf">for</span> i <span class="kw">in</span><span class="bu">range</span>(<span class="dv">1001</span>):
                x, y <span class="op">=</span> train_set.next_batch(<span class="dv">50</span>)
                feed_dict<span class="op">=</span>{g[<span class="st">'x'</span>]: x, g[<span class="st">'y'</span>]: y, g[<span class="st">'stochastic'</span>]: stochastic_train}
                acc, _ <span class="op">=</span> sess.run([g[<span class="st">'accuracy'</span>],g[<span class="st">'ts'</span>]], feed_dict<span class="op">=</span>feed_dict)
                accuracy <span class="op">+=</span> acc
                <span class="cf">if</span> i <span class="op">%</span><span class="dv">100</span><span class="op">==</span><span class="dv">0</span><span class="kw">and</span> i <span class="op">&gt;</span><span class="dv">0</span>:
                    res_tr.append(accuracy<span class="op">/</span><span class="dv">100</span>)
                    accuracy <span class="op">=</span><span class="dv">0</span>
                    feed_dict<span class="op">=</span>{g[<span class="st">'x'</span>]: val_set.images,
                               g[<span class="st">'y'</span>]: val_set.labels,
                               g[<span class="st">'stochastic'</span>]: stochastic_eval,
                               g[<span class="st">'slope'</span>]: slope}
                    res_val.append(sess.run(g[<span class="st">'accuracy'</span>], feed_dict<span class="op">=</span>feed_dict))

            <span class="cf">if</span> slope_annealing_rate <span class="kw">is</span><span class="kw">not</span><span class="va">None</span>:
                slope <span class="op">=</span> slope<span class="op">*</span>slope_annealing_rate
                <span class="cf">if</span> verbose:
                    <span class="bu">print</span>(<span class="st">"Sigmoid slope:"</span>, slope)

        feed_dict<span class="op">=</span>{g[<span class="st">'x'</span>]: val_set.images, g[<span class="st">'y'</span>]: val_set.labels,
                   g[<span class="st">'stochastic'</span>]: stochastic_eval, g[<span class="st">'slope'</span>]: slope}
        <span class="bu">print</span>(<span class="st">"Epoch"</span>, epoch<span class="op">+</span><span class="dv">1</span>, sess.run(g[<span class="st">'accuracy'</span>], feed_dict<span class="op">=</span>feed_dict))
        <span class="cf">if</span> label <span class="kw">is</span><span class="kw">not</span><span class="va">None</span>:
            <span class="cf">return</span> (res_tr, label <span class="op">+</span><span class="st">" - Training"</span>), (res_val, label <span class="op">+</span><span class="st">" - Validation"</span>)
        <span class="cf">else</span>:
            <span class="cf">return</span> [(res_tr, <span class="st">"Training"</span>), (res_val, <span class="st">"Validation"</span>)]</code></pre></div><h3 id="experiments">Experiments</h3><p>We’ve now set up a good foundation from which we can run a number of simple experiments. The experiments are as follows:</p><ul><li><strong>Experiment 0</strong>: A non-stochastic, non-binary baseline.</li><li><strong>Experiment 1</strong>: A comparison of variance-adjusted REINFORCE and non-variance adjusted REINFORCE, which shows that the variance adjustment allows for faster learning and higher learning rates.</li><li><strong>Experiment 2</strong>: A comparison of pass-through ST and sigmoid-adjusted ST, which shows that the sigmoid-adjusted ST estimator obtains better results, a result that does not agree with the findings of <a href="https://arxiv.org/abs/1308.3432">Bengio et al. (2013</a>.</li><li><strong>Experiment 3</strong>: A comparison of sigmoid-adjusted ST and slope-annealed sigmoid-adjusted ST, which shows that a well-tuned slope-annealed ST outperforms the base sigmoid-adjusted ST.</li><li><strong>Experiment 4</strong>: A direct comparison of variance-adjusted REINFORCE and slope-annealed ST, which shows that ST performs significantly better than REINFORCE.</li><li><strong>Experiment 5</strong>: A look at the deterministic step function, during training and evaluation, which shows that deterministic evaluation can provide a slight boost at inference, and that with slope annealing, deterministic training is just as effective, if not more effective than stochastic training.</li><li><strong>Experiment 6</strong>: A look at how network depth affects performance, which shows that deep stochastic networks can be difficult to train.</li><li><strong>Experiment 7</strong>: A look at using binary stochastic neurons as a regularizer, which validates Hinton’s claim that stochastic neurons can serve as effective regularizers.</li></ul><h4 id="experiment-0-a-non-stochastic-non-binary-baseline">Experiment 0: A non-stochastic, non-binary baseline</h4><div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">res <span class="op">=</span> train_classifier(hidden_dims<span class="op">=</span>[<span class="dv">100</span>], epochs<span class="op">=</span><span class="dv">20</span>, lr<span class="op">=</span><span class="fl">1.0</span>, non_binary<span class="op">=</span><span class="va">True</span>)
plot_n(res, lower_y<span class="op">=</span><span class="fl">0.8</span>, title<span class="op">=</span><span class="st">"Logistic Sigmoid Baseline"</span>)</code></pre></div><pre><code>Epoch 20 0.9698</code></pre><figure><img src="https://r2rt.com/static/images/BSN_output_16_1.png" alt="png"><figcaption>png</figcaption></figure><h4 id="experiment-1-variance-adjusted-vs.not-variance-adjusted-reinforce">Experiment 1: Variance-adjusted vs.&nbsp;not variance-adjusted REINFORCE</h4><p>Recall that the REINFORCE estimator estimates the expectation of <span class="math inline">\(\frac{\partial L}{\partial a}\)</span> as <span class="math inline">\((\text{BSN}(a) - \text{sigm}(a))(L - c)\)</span>, where <span class="math inline">\(c\)</span> is a constant. The non-variance-adjusted form of REINFORCE uses <span class="math inline">\(c = 0\)</span>, whereas the variance-adjusted form uses the variance minimizing result stated above. Naturally we should prefer the least variance, and the experimental results below agree.</p><p>It seems that both forms of REINFORCE often break down for learning rates greater than or equal to 0.3 (compare to the learning rate of 1.0 that used in Experiment 0). After a few trials, variance-adjusted REINFORCE appears to be more resistant to such failures.</p><div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="bu">print</span>(<span class="st">"Variance-adjusted:"</span>)
res1 <span class="op">=</span> train_classifier(hidden_dims<span class="op">=</span>[<span class="dv">100</span>], estimator<span class="op">=</span>StochasticGradientEstimator.REINFORCE, epochs<span class="op">=</span><span class="dv">3</span>,
                       lr<span class="op">=</span><span class="fl">0.3</span>, verbose<span class="op">=</span><span class="va">True</span>)
<span class="bu">print</span>(<span class="st">"Not variance-adjusted:"</span>)<span class="kw">and</span>
res2<span class="op">=</span> train_classifier(hidden_dims<span class="op">=</span>[<span class="dv">100</span>], estimator<span class="op">=</span>StochasticGradientEstimator.REINFORCE, epochs<span class="op">=</span><span class="dv">3</span>,
                       lr<span class="op">=</span><span class="fl">0.3</span>, no_var_adj<span class="op">=</span><span class="va">True</span>, verbose<span class="op">=</span><span class="va">True</span>)</code></pre></div><pre><code>Variance-adjusted:
Epoch 0 0.1026
Epoch 1 0.4466
Epoch 2 0.511
Epoch 3 0.575
Not variance-adjusted:
Epoch 0 0.0964
Epoch 1 0.0958
Epoch 2 0.0958
Epoch 3 0.0958</code></pre><p>In terms of performance at lower learning rates, a learning rate of about 0.05 provided the best results. The results show that the variance-adjusted REINFORCE learns faster, but that its non-variance adjusted eventually catches up. This result is consistent with the mathematical result that they are both unbiased estimators. Performance is predictably worse than it was for the plain logistic sigmoid in Experiment 0, although there is almost no generalization gap, consistent with the hypothesis that binary stochastic neurons can act as regularizers.</p><div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">res1 <span class="op">=</span> train_classifier(hidden_dims<span class="op">=</span>[<span class="dv">100</span>], estimator<span class="op">=</span>StochasticGradientEstimator.REINFORCE, epochs<span class="op">=</span><span class="dv">20</span>,
                       lr<span class="op">=</span><span class="fl">0.05</span>, label <span class="op">=</span><span class="st">"Variance-adjusted"</span>)
res2<span class="op">=</span> train_classifier(hidden_dims<span class="op">=</span>[<span class="dv">100</span>], estimator<span class="op">=</span>StochasticGradientEstimator.REINFORCE, epochs<span class="op">=</span><span class="dv">20</span>,
                       lr<span class="op">=</span><span class="fl">0.05</span>, no_var_adj<span class="op">=</span><span class="va">True</span>, label <span class="op">=</span><span class="st">"Not variance-adjusted"</span>)

plot_n(res1 <span class="op">+</span> res2, lower_y<span class="op">=</span><span class="fl">0.6</span>, title<span class="op">=</span><span class="st">"Experiment 1: REINFORCE variance adjustment"</span>)</code></pre></div><pre><code>Epoch 20 0.9274
Epoch 20 0.923</code></pre><figure><img src="https://r2rt.com/static/images/BSN_output_20_1.png" alt="png"><figcaption>png</figcaption></figure><h4 id="experiment-2-pass-through-vs.sigmoid-adjusted-st-estimation">Experiment 2: Pass-through vs.&nbsp;sigmoid-adjusted ST estimation</h4><p>Recall that one variant of the straight-through estimator uses the identity function as the backpropagated gradient (pass-through), and another variant multiplies that by the gradient of the logistic sigmoid that the neuron calculates (sigmoid-adjusted). In <a href="https://arxiv.org/abs/1308.3432">Bengio et al. (2013)</a>, it was remarked that, surprisingly, the former performs better. My results below disagree, and by a surprisingly wide margin.</p><div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">res1 <span class="op">=</span> train_classifier(hidden_dims<span class="op">=</span>[<span class="dv">100</span>], estimator<span class="op">=</span>StochasticGradientEstimator.ST, epochs<span class="op">=</span><span class="dv">20</span>,
                       lr<span class="op">=</span><span class="fl">0.1</span>, label <span class="op">=</span><span class="st">"Pass-through - 0.1"</span>)
res2 <span class="op">=</span> train_classifier(hidden_dims<span class="op">=</span>[<span class="dv">100</span>], estimator<span class="op">=</span>StochasticGradientEstimator.ST, epochs<span class="op">=</span><span class="dv">20</span>,
                       lr<span class="op">=</span><span class="fl">0.1</span>, slope_annealing_rate <span class="op">=</span><span class="fl">1.0</span>, label <span class="op">=</span><span class="st">"Sigmoid-adjusted - 0.1"</span>)

res3 <span class="op">=</span> train_classifier(hidden_dims<span class="op">=</span>[<span class="dv">100</span>], estimator<span class="op">=</span>StochasticGradientEstimator.ST, epochs<span class="op">=</span><span class="dv">20</span>,
                       lr<span class="op">=</span><span class="fl">0.3</span>, label <span class="op">=</span><span class="st">"Pass-through - 0.3"</span>)
res4 <span class="op">=</span> train_classifier(hidden_dims<span class="op">=</span>[<span class="dv">100</span>], estimator<span class="op">=</span>StochasticGradientEstimator.ST, epochs<span class="op">=</span><span class="dv">20</span>,
                       lr<span class="op">=</span><span class="fl">0.3</span>, slope_annealing_rate <span class="op">=</span><span class="fl">1.0</span>, label <span class="op">=</span><span class="st">"Sigmoid-adjusted - 0.3"</span>)

res5 <span class="op">=</span> train_classifier(hidden_dims<span class="op">=</span>[<span class="dv">100</span>], estimator<span class="op">=</span>StochasticGradientEstimator.ST, epochs<span class="op">=</span><span class="dv">20</span>,
                       lr<span class="op">=</span><span class="fl">1.0</span>, label <span class="op">=</span><span class="st">"Pass-through - 1.0"</span>)
res6 <span class="op">=</span> train_classifier(hidden_dims<span class="op">=</span>[<span class="dv">100</span>], estimator<span class="op">=</span>StochasticGradientEstimator.ST, epochs<span class="op">=</span><span class="dv">20</span>,
                       lr<span class="op">=</span><span class="fl">1.0</span>, slope_annealing_rate <span class="op">=</span><span class="fl">1.0</span>, label <span class="op">=</span><span class="st">"Sigmoid-adjusted - 1.0"</span>)

plot_n(res1[<span class="dv">1</span>:] <span class="op">+</span> res2[<span class="dv">1</span>:] <span class="op">+</span> res3[<span class="dv">1</span>:] <span class="op">+</span> res4[<span class="dv">1</span>:] <span class="op">+</span> res5[<span class="dv">1</span>:] <span class="op">+</span> res6[<span class="dv">1</span>:],
       lower_y<span class="op">=</span><span class="fl">0.4</span>, title<span class="op">=</span><span class="st">"Experiment 2: Pass-through vs sigmoid-adjusted ST"</span>)</code></pre></div><pre><code>Epoch 20 0.8334
Epoch 20 0.9566
Epoch 20 0.8828
Epoch 20 0.9668
Epoch 20 0.0958
Epoch 20 0.9572</code></pre><figure><img src="https://r2rt.com/static/images/BSN_output_22_1.png" alt="png"><figcaption>png</figcaption></figure><h4 id="experiment-3-pass-through-vs.slope-annealed-st-estimation">Experiment 3: Pass-through vs.&nbsp;slope-annealed ST estimation</h4><p>Recall that <a href="https://arxiv.org/abs/1609.01704">Chung et al. (2016)</a> improves upon the sigmoid-adjusted variant of the ST estimator by using the <em>slope-annealing trick</em>, which slowly increases the slope of the logistic sigmoid as training progresses. Using the slope-annealing trick with an annealing rate of 1.1 times per epoch (so the slope at epoch 20 is <span class="math inline">\(1.1^{19} \approx 6.1\)</span>), we’re able to improve upon the sigmoid-adjusted ST estimator, and even beat our non-stochastic, non-binary baseline! Note that the slope annealed neuron used here is not the same as the one used by <a href="https://arxiv.org/abs/1609.01704">Chung et al. (2016)</a>, who employ a deterministic step function and use a hard sigmoid in place of a sigmoid for the backpropagation.</p><div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">res1 <span class="op">=</span> train_classifier(hidden_dims<span class="op">=</span>[<span class="dv">100</span>], estimator<span class="op">=</span>StochasticGradientEstimator.ST, epochs<span class="op">=</span><span class="dv">20</span>,
                       lr<span class="op">=</span><span class="fl">0.1</span>, slope_annealing_rate <span class="op">=</span><span class="fl">1.0</span>, label <span class="op">=</span><span class="st">"Sigmoid-adjusted - 0.1"</span>)
res2 <span class="op">=</span> train_classifier(hidden_dims<span class="op">=</span>[<span class="dv">100</span>], estimator<span class="op">=</span>StochasticGradientEstimator.ST, epochs<span class="op">=</span><span class="dv">20</span>,
                       lr<span class="op">=</span><span class="fl">0.1</span>, slope_annealing_rate <span class="op">=</span><span class="fl">1.1</span>, label <span class="op">=</span><span class="st">"Slope-annealed - 0.1"</span>)

res3 <span class="op">=</span> train_classifier(hidden_dims<span class="op">=</span>[<span class="dv">100</span>], estimator<span class="op">=</span>StochasticGradientEstimator.ST, epochs<span class="op">=</span><span class="dv">20</span>,
                       lr<span class="op">=</span><span class="fl">0.3</span>, slope_annealing_rate <span class="op">=</span><span class="fl">1.0</span>, label <span class="op">=</span><span class="st">"Sigmoid-adjusted - 0.3"</span>)
res4 <span class="op">=</span> train_classifier(hidden_dims<span class="op">=</span>[<span class="dv">100</span>], estimator<span class="op">=</span>StochasticGradientEstimator.ST, epochs<span class="op">=</span><span class="dv">20</span>,
                       lr<span class="op">=</span><span class="fl">0.3</span>, slope_annealing_rate <span class="op">=</span><span class="fl">1.1</span>, label <span class="op">=</span><span class="st">"Slope-annealed - 0.3"</span>)

res5 <span class="op">=</span> train_classifier(hidden_dims<span class="op">=</span>[<span class="dv">100</span>], estimator<span class="op">=</span>StochasticGradientEstimator.ST, epochs<span class="op">=</span><span class="dv">20</span>,
                       lr<span class="op">=</span><span class="fl">1.0</span>, slope_annealing_rate <span class="op">=</span><span class="fl">1.0</span>, label <span class="op">=</span><span class="st">"Sigmoid-adjusted - 1.0"</span>)
res6 <span class="op">=</span> train_classifier(hidden_dims<span class="op">=</span>[<span class="dv">100</span>], estimator<span class="op">=</span>StochasticGradientEstimator.ST, epochs<span class="op">=</span><span class="dv">20</span>,
                       lr<span class="op">=</span><span class="fl">1.0</span>, slope_annealing_rate <span class="op">=</span><span class="fl">1.1</span>, label <span class="op">=</span><span class="st">"Slope-annealed - 1.0"</span>)

plot_n(res1[<span class="dv">1</span>:] <span class="op">+</span> res2[<span class="dv">1</span>:] <span class="op">+</span> res3[<span class="dv">1</span>:] <span class="op">+</span> res4[<span class="dv">1</span>:] <span class="op">+</span> res5[<span class="dv">1</span>:] <span class="op">+</span> res6[<span class="dv">1</span>:],
       lower_y<span class="op">=</span><span class="fl">0.6</span>, title<span class="op">=</span><span class="st">"Experiment 3: Sigmoid-adjusted vs slope-annealed ST"</span>)</code></pre></div><pre><code>Epoch 20 0.9548
Epoch 20 0.974
Epoch 20 0.9704
Epoch 20 0.9764
Epoch 20 0.9608
Epoch 20 0.9624</code></pre><figure><img src="https://r2rt.com/static/images/BSN_output_24_1.png" alt="png"><figcaption>png</figcaption></figure><h4 id="experiment-4-variance-adjusted-reinforce-vs-slope-annealed-st">Experiment 4: Variance-adjusted REINFORCE vs slope-annealed ST</h4><p>We now directly compare the variance-adjusted REINFORCE and slope-annealed ST, both at their best learning rates. In this setting, despite being a biased estimator, the straight-through estimator displays faster learning, less variance, and better overall results than the variance-adjusted REINFORCE estimator.</p><div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">res1 <span class="op">=</span> train_classifier(hidden_dims<span class="op">=</span>[<span class="dv">100</span>], estimator<span class="op">=</span>StochasticGradientEstimator.REINFORCE, epochs<span class="op">=</span><span class="dv">20</span>,
                       lr<span class="op">=</span><span class="fl">0.05</span>, label <span class="op">=</span><span class="st">"Variance-adjusted REINFORCE"</span>)

res2 <span class="op">=</span> train_classifier(hidden_dims<span class="op">=</span>[<span class="dv">100</span>], estimator<span class="op">=</span>StochasticGradientEstimator.ST, epochs<span class="op">=</span><span class="dv">20</span>,
                       lr<span class="op">=</span><span class="fl">0.3</span>, slope_annealing_rate <span class="op">=</span><span class="fl">1.1</span>, label <span class="op">=</span><span class="st">"Slope-annealed ST"</span>)

plot_n(res1[<span class="dv">1</span>:] <span class="op">+</span> res2[<span class="dv">1</span>:],
       lower_y<span class="op">=</span><span class="fl">0.6</span>, title<span class="op">=</span><span class="st">"Experiment 4: Variance-adjusted REINFORCE vs slope-annealed ST"</span>)</code></pre></div><pre><code>Epoch 20 0.926
Epoch 20 0.9782</code></pre><figure><img src="https://r2rt.com/static/images/BSN_output_26_1.png" alt="png"><figcaption>png</figcaption></figure><h4 id="experiment-5-a-look-at-the-deterministic-step-function-during-training-and-evaluation">Experiment 5: A look at the deterministic step function, during training and evaluation</h4><p>Similar to how dropout is not applied at inference when using dropout for training, it makes sense that we might replace the stochastic sigmoid with a deterministic step function at inference when using binary neurons. We might go even further than that, and use deterministic neurons during training, which is the approach taken by <a href="https://arxiv.org/abs/1609.01704">Chung et al. (2016)</a>. The following three combinations are compared below, using the slope-annealed straight through estimator, without slope annealing:</p><ul><li>stochastic during training, stochastic during test</li><li>stochastic during training, deterministic during test</li><li>deterministic during training, deterministic during test</li></ul><p>The results show that deterministic neurons train the fastest, but also display more overfitting and may not achieve the best final results. Stochastic inference and deterministic inference, when combined with stochastic training, are closely comparable. Similar results hold for the REINFORCE estimator.</p><div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">res1 <span class="op">=</span> train_classifier(hidden_dims<span class="op">=</span>[<span class="dv">100</span>], estimator<span class="op">=</span>StochasticGradientEstimator.ST, epochs<span class="op">=</span><span class="dv">20</span>,
                        lr<span class="op">=</span><span class="fl">0.3</span>, slope_annealing_rate <span class="op">=</span><span class="fl">1.1</span>, label <span class="op">=</span><span class="st">"Stochastic, Stochastic"</span>)
res2 <span class="op">=</span> train_classifier(hidden_dims<span class="op">=</span>[<span class="dv">100</span>], estimator<span class="op">=</span>StochasticGradientEstimator.ST, epochs<span class="op">=</span><span class="dv">20</span>,
                        lr<span class="op">=</span><span class="fl">0.3</span>, slope_annealing_rate <span class="op">=</span><span class="fl">1.1</span>, stochastic_eval<span class="op">=</span><span class="va">False</span>, label <span class="op">=</span><span class="st">"Stochastic, Deterministic"</span>)
res3 <span class="op">=</span> train_classifier(hidden_dims<span class="op">=</span>[<span class="dv">100</span>], estimator<span class="op">=</span>StochasticGradientEstimator.ST, epochs<span class="op">=</span><span class="dv">20</span>,
                        lr<span class="op">=</span><span class="fl">0.3</span>, slope_annealing_rate <span class="op">=</span><span class="fl">1.1</span>, stochastic_train<span class="op">=</span><span class="va">False</span>, stochastic_eval<span class="op">=</span><span class="va">False</span>,
                        label <span class="op">=</span><span class="st">"Deterministic, Deterministic"</span>)

plot_n(res1 <span class="op">+</span> res2 <span class="op">+</span> res3,
       lower_y<span class="op">=</span><span class="fl">0.6</span>, title<span class="op">=</span><span class="st">"Experiment 5: Stochastic vs Deterministic (Slope-annealed ST)"</span>)</code></pre></div><pre><code>Epoch 20 0.9776
Epoch 20 0.977
Epoch 20 0.9704</code></pre><figure><img src="https://r2rt.com/static/images/BSN_output_28_1.png" alt="png"><figcaption>png</figcaption></figure><h4 id="experiment-6-the-effect-of-depth-on-reinforce-and-st-estimators">Experiment 6: The effect of depth on REINFORCE and ST estimators</h4><p>Next, I look at how each estimator interacts with depth. From a theoretical perpective, there is reason to think the straight-through estimator will suffer from depth; as noted by <a href="https://arxiv.org/abs/1308.3432">Bengio et al. (2013)</a>, it is not even guaranteed to have the same sign as the expected gradient during backpropagation. It turns out that the slope-annealed straight-through estimator is resilient to depth, even at a reasonable learning rate. The REINFORCE estimator, on the other hand, starts to fail as depth is introduced. However, if we lower the learning rate dramatically (25x), we can start to get the deeper networks to train with the REINFORCE estimator.</p><div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">res1 <span class="op">=</span> train_classifier(hidden_dims<span class="op">=</span>[<span class="dv">100</span>], estimator<span class="op">=</span>StochasticGradientEstimator.ST, epochs<span class="op">=</span><span class="dv">20</span>,
                        lr<span class="op">=</span><span class="fl">0.3</span>, slope_annealing_rate<span class="op">=</span><span class="fl">1.1</span>, label <span class="op">=</span><span class="st">"1 hidden layer"</span>)
res2 <span class="op">=</span> train_classifier(hidden_dims<span class="op">=</span>[<span class="dv">100</span>, <span class="dv">100</span>], estimator<span class="op">=</span>StochasticGradientEstimator.ST, epochs<span class="op">=</span><span class="dv">20</span>,
                        lr<span class="op">=</span><span class="fl">0.3</span>, slope_annealing_rate<span class="op">=</span><span class="fl">1.1</span>, label <span class="op">=</span><span class="st">"2 hidden layers"</span>)
res3 <span class="op">=</span> train_classifier(hidden_dims<span class="op">=</span>[<span class="dv">100</span>, <span class="dv">100</span>, <span class="dv">100</span>], estimator<span class="op">=</span>StochasticGradientEstimator.ST, epochs<span class="op">=</span><span class="dv">20</span>,
                        lr<span class="op">=</span><span class="fl">0.3</span>, slope_annealing_rate<span class="op">=</span><span class="fl">1.1</span>, label <span class="op">=</span><span class="st">"3 hidden layers"</span>)

plot_n(res1[<span class="dv">1</span>:] <span class="op">+</span> res2[<span class="dv">1</span>:] <span class="op">+</span> res3[<span class="dv">1</span>:], title<span class="op">=</span><span class="st">"Experiment 6: The effect of depth (straight-through)"</span>)</code></pre></div><pre><code>Epoch 20 0.9774
Epoch 20 0.9738
Epoch 20 0.9728</code></pre><figure><img src="https://r2rt.com/static/images/BSN_output_30_1.png" alt="png"><figcaption>png</figcaption></figure><div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">res1 <span class="op">=</span> train_classifier(hidden_dims<span class="op">=</span>[<span class="dv">100</span>], estimator<span class="op">=</span>StochasticGradientEstimator.REINFORCE, epochs<span class="op">=</span><span class="dv">20</span>,
                       lr<span class="op">=</span><span class="fl">0.05</span>, label <span class="op">=</span><span class="st">"1 hidden layer"</span>)
res2 <span class="op">=</span> train_classifier(hidden_dims<span class="op">=</span>[<span class="dv">100</span>,<span class="dv">100</span>], estimator<span class="op">=</span>StochasticGradientEstimator.REINFORCE, epochs<span class="op">=</span><span class="dv">20</span>,
                       lr<span class="op">=</span><span class="fl">0.05</span>, label <span class="op">=</span><span class="st">"2 hidden layers"</span>)
res3 <span class="op">=</span> train_classifier(hidden_dims<span class="op">=</span>[<span class="dv">100</span>,<span class="dv">100</span>,<span class="dv">100</span>], estimator<span class="op">=</span>StochasticGradientEstimator.REINFORCE, epochs<span class="op">=</span><span class="dv">20</span>,
                       lr<span class="op">=</span><span class="fl">0.05</span>, label <span class="op">=</span><span class="st">"3 hidden layers"</span>)

plot_n(res1[<span class="dv">1</span>:] <span class="op">+</span> res2[<span class="dv">1</span>:] <span class="op">+</span> res3[<span class="dv">1</span>:], title<span class="op">=</span><span class="st">"Experiment 6: The effect of depth (REINFORCE)"</span>)</code></pre></div><pre><code>Epoch 20 0.9302
Epoch 20 0.8788
Epoch 20 0.2904</code></pre><figure><img src="https://r2rt.com/static/images/BSN_output_31_1.png" alt="png"><figcaption>png</figcaption></figure><div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">res1 <span class="op">=</span> train_classifier(hidden_dims<span class="op">=</span>[<span class="dv">100</span>], epochs<span class="op">=</span><span class="dv">50</span>, non_binary<span class="op">=</span><span class="va">True</span>,
                       lr<span class="op">=</span><span class="fl">0.002</span>, label <span class="op">=</span><span class="st">"1 hidden layer"</span>)
res2 <span class="op">=</span> train_classifier(hidden_dims<span class="op">=</span>[<span class="dv">100</span>,<span class="dv">100</span>], epochs<span class="op">=</span><span class="dv">50</span>, non_binary<span class="op">=</span><span class="va">True</span>,
                       lr<span class="op">=</span><span class="fl">0.002</span>, label <span class="op">=</span><span class="st">"2 hidden layers"</span>)
res3 <span class="op">=</span> train_classifier(hidden_dims<span class="op">=</span>[<span class="dv">100</span>,<span class="dv">100</span>,<span class="dv">100</span>], epochs<span class="op">=</span><span class="dv">50</span>, non_binary<span class="op">=</span><span class="va">True</span>,
                       lr<span class="op">=</span><span class="fl">0.002</span>, label <span class="op">=</span><span class="st">"3 hidden layers"</span>)

plot_n(res1[<span class="dv">1</span>:] <span class="op">+</span> res2[<span class="dv">1</span>:] <span class="op">+</span> res3[<span class="dv">1</span>:], title<span class="op">=</span><span class="st">"Experiment 6: The effect of depth (REINFORCE) (LR = 0.002)"</span>)</code></pre></div><pre><code>Epoch 50 0.931
Epoch 50 0.9294
Epoch 50 0.9096</code></pre><figure><img src="https://r2rt.com/static/images/BSN_output_32_1.png" alt="png"><figcaption>png</figcaption></figure><h4 id="experiment-7-using-binary-stochastic-neurons-as-a-regularizer.">Experiment 7: Using binary stochastic neurons as a regularizer.</h4><p>I now test the “unpublished result” put forth at the end of <a href="https://www.youtube.com/watch?v=LN0xtUuJsEI&amp;list=PLoRl3Ht4JOcdU872GhiYWf6jwrk_SNhz9&amp;index=41">Hinton et al.’s Coursera Lecture 9c</a>, which states that we can improve upon the performance of an overfitting multi-layer sigmoid net by turning its neurons binary stochastic neurons with a straight-through estimator.</p><p>To test the claim, we will need a dataset that is easier to overfit than MNIST, and so the following experiment uses the MNIST validation set for training (10x smaller than the MNIST training set and therefore much easier to overfit). The hidden layer size is also increased by a factor of 2 to increase overfitting.</p><p>We can see below that the stochastic net has a clear advantage in terms of both the generalization gap and training speed, ultimately resulting in a better final fit.</p><div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">res1 <span class="op">=</span> train_classifier(hidden_dims <span class="op">=</span> [<span class="dv">200</span>], epochs<span class="op">=</span><span class="dv">20</span>, train_set<span class="op">=</span>mnist.validation, val_set<span class="op">=</span>mnist.test,
                        lr <span class="op">=</span><span class="fl">0.03</span>, non_binary <span class="op">=</span><span class="va">True</span>, label <span class="op">=</span><span class="st">"Deterministic sigmoid net"</span>)

res2 <span class="op">=</span> train_classifier(hidden_dims <span class="op">=</span> [<span class="dv">200</span>], epochs<span class="op">=</span><span class="dv">20</span>, stochastic_eval<span class="op">=</span><span class="va">False</span>, train_set<span class="op">=</span>mnist.validation,
                        val_set<span class="op">=</span>mnist.test, slope_annealing_rate<span class="op">=</span><span class="fl">1.1</span>, estimator<span class="op">=</span>StochasticGradientEstimator.ST,
                        lr <span class="op">=</span><span class="fl">0.3</span>, label <span class="op">=</span><span class="st">"Binary stochastic net"</span>)

plot_n(res1 <span class="op">+</span> res2, lower_y<span class="op">=</span><span class="fl">0.8</span>, title<span class="op">=</span><span class="st">"Experiment 8: Using binary stochastic neurons as a regularizer"</span>)</code></pre></div><pre><code>Epoch 20 0.9276
Epoch 20 0.941</code></pre><figure><img src="https://r2rt.com/static/images/BSN_output_34_1.png" alt="png"><figcaption>png</figcaption></figure><h3 id="conclusion">Conclusion</h3><p>In this post we introduced, implemented and experimented with binary stochastic neurons in Tensorflow. We saw that the biased straight-through estimator generally outperforms the unbiased REINFORCE estimator, and can even outperform a non-stochastic, non-binary sigmoid net. We explored the variants of each estimator, and showed that the slope-annealed straight through estimator is better than other straight through variants, and that it is worth using the variance-adjusted REINFORCE estimator over the not variance-adjusted REINFORCE estimator. Finally, we explored the potential use for binary stochastic neurons as regularizers, and demonstrated that a stochastic binary network trained with the slope-annealed straight through estimator trains faster and generalizes better than an ordinary sigmoid net.</p></div></section></div></div></div></div></div>
</div>
    </div>
    <footer>
        <div>created by buildstarted &copy; 2020 <a href="/about">about</a></div>
        <div>Share this page on social media: copy and paste this url https://linksfor.dev/</div>
        <div>If you prefer RSS: <a href="https://linksfor.dev/feed.xml">https://linksfor.dev/feed.xml</a></div>
    </footer>
    
    <script>
        (function () {
            var COLLECT_URL = "https://dna.buildstarted.com/t";
            var SITE_ID = "linksfor.devs";
            var GLOBAL_VAR_NAME = "__DNA__";

            window[GLOBAL_VAR_NAME] = {};

            window[GLOBAL_VAR_NAME].sendPageView = function () {
                var path = location.pathname;
                var referrer = document.referrer;

                var url = COLLECT_URL + "?siteid=" + SITE_ID + "&p=" + encodeURIComponent(path) + "&r=" + encodeURIComponent(referrer);

                try { fetch(url, { method: "GET" }); } catch (e) { }
            };

            window[GLOBAL_VAR_NAME].sendPageView();
        })();
    </script>
</body>
</html>