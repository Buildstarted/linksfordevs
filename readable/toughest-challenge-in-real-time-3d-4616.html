<!DOCTYPE html>
<html lang="en">
<head>
    <title>
SSRTGI: Toughest Challenge in Real-Time 3D - linksfor.dev(s)    </title>
    <meta charset="utf-8">
    <link rel="alternate" type="application/rss+xml" title="Linksfor.dev(s) feed" href="https://linksfor.dev/feed.rss" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta name="google" value="notranslate">
    <link rel="stylesheet" type="text/css" href="/style.css" />
    <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon">
    <link rel="icon" href="/favicon.ico" type="image/x-icon">
    <meta property="og:title" content="SSRTGI: Toughest Challenge in Real-Time 3D - linksfor.dev(s)"/>
    <meta property="og:description" content="UNIGINE&#x27;s&#xA0;Davyd&#xA0;Vidiger&#xA0;talked about the way&#xA0;Screen Space Ray-Traced Global Illumination is used to improve the image quality in real-time graphics."/>
    <meta property="og:type" content="website"/>
    <meta property="og:url" content="https://80.lv/articles/ssrtgi-toughest-challenge-in-real-time-3d/"/>

<meta property="og:site_name" content="linksfor.dev(s)" />
</head>
<body>
    <div class="grid">
        <h1>
<a href="/" style="color:inherit">linksfor.dev(s)</a>
        </h1>
        <title>linksfor.dev(s) - SSRTGI: Toughest Challenge in Real-Time 3D</title>
<div class="readable">
        <h1>SSRTGI: Toughest Challenge in Real-Time 3D</h1>
            <div>Reading time: 11-14 minutes</div>
        <div>Posted here: 27 Feb 2019</div>
        <p><a href="https://80.lv/articles/ssrtgi-toughest-challenge-in-real-time-3d/">https://80.lv/articles/ssrtgi-toughest-challenge-in-real-time-3d/</a></p>
        <hr/>
<div id="readability-page-1" class="page"><div><div data-widget-size="full"><div><h3>Lead 3D Artist in&nbsp;<a href="http://unigine.com/?utm_source=80lv&amp;utm_medium=article&amp;utm_campaign=ssrtgi" target="_blank" rel="noopener noreferrer">UNIGINE</a>&nbsp;Davyd&nbsp;Vidiger&nbsp;talked about the way&nbsp;Screen Space Ray-Traced Global Illumination is used to improve the image quality in real-time graphics.</h3></div></div><div data-widget-size="full"><p><iframe src="https://www.youtube.com/embed/Ya4Z-obAkWA?feature=oembed&amp;enablejsapi=1&amp;origin=https://80.lv" frameborder="0" allow="encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></p></div><div data-widget-size="full"><div><p>My name is Davyd Vidiger, I have been working as a Lead 3D Artist in <a href="http://unigine.com/?utm_source=80lv&amp;utm_medium=article&amp;utm_campaign=ssrtgi" target="_blank" rel="noopener noreferrer">UNIGINE</a> for several years. Today I am going to tell you how Unigine managed to improve the image quality in real-time 3D graphics using Screen Space Ray-Traced Global Illumination, what challenges did we face and why this technology is so important.</p><h3>Introduction</h3><p>Everyone who worked with real-time 3D graphics knows the dilemma: either to choose realistic static (baked), or less realistic dynamic lighting. We’ve also faced this dilemma while working on the <a href="http://benchmark.unigine.com/superposition?utm_source=80lv&amp;utm_medium=article&amp;utm_campaign=ssrtgi" target="_blank" rel="noopener noreferrer">Superposition Benchmark</a> project. Unlike regular real-time applications, we have over 900 dynamic objects in a single room. All of these objects can be moved freely which significantly affects lighting. This scene unveiled a set of typical problems in modern 3D engines.</p><p>Here are the challenges we faced:</p><p><strong>1. With baked lighting we can’t move dynamic objects</strong></p><p>Suppose we pre-calculate lightmaps for each object offline, using ray tracing, and then bake them into textures. We would surely get a completely realistic and convincing lighting but, all objects would have to remain static – we couldn’t move them. Apart from looking plain and boring in VR, this would be considered as a technologically outdated solution.</p><p><strong>2. Mixing dynamic and static lighting gives us an inconsistent image</strong></p><p>It’s possible to bake lighting for static geometry (lightmaps or static AO), but this baked lighting won’t affect dynamic objects. As a result we can get too bright objects on dark shelves. Even if we try to improve the scene by content adjustment – inserting a dark environment probe into each bookcase, it still won’t look realistic. The fakeness of real-time graphics will still be evident.</p><p><strong>3. Using environment probes gives sharp transitions of ambient lighting on the adjacent surfaces with different normals</strong></p><p>We used environment probes for ambient lighting, applying it to objects’ surfaces according to the following scheme: if a surface normal points to the left, we use the lighting baked from the left side, if it points to the right – we use what was baked from the right. This is a classic method, which is now widely used in real-time for ambient lighting. However, it produces sharp transitions of ambient lighting on the adjacent surfaces with different normals which looks totally unnatural.</p></div></div><div data-widget-size="full"><div><div><p><img data="image" src="https://new-cdn.80.lv/upload/content/50/5d2c86321d5a7.gif" alt=""></p></div></div></div><div data-widget-size="full"><div><p><strong>4. Providing an equally </strong>high-quality<strong> lighting for both medium shots and close-ups is really a challenge</strong></p><p>Close-ups are still one of the most difficult challenges in real-time 3D graphics. It is close-ups, that’s where people notice even a slight imperfection, be it the content or renderer. The reason is that we deal with close-ups and human-sized objects every day. When the camera is close to an object, such deviations from reality as low-detailed content, unnatural shadows or reflections, become more noticeable than they are at a distance. Superdetailing is one of the most important conditions for close-ups, but at the same time, it is the main problem.</p></div></div><div data-widget-size="full"><div><div><p><img data="image" src="https://new-cdn.80.lv/upload/content/e7/5d2c863361466.gif" alt=""></p></div></div></div><div data-widget-size="full"><div><h3>Why offline renderers do not suffer from these problems?</h3><p>If we compare a ray-tracing renderer with a real-time one, we’ll see that the difference between the two consists in lighting, that makes the scene so realistic and plausible. The algorithms for diffuse lighting and specular reflections are best fitted by ray tracing, as it tries to mimic the behavior of photons bouncing from surfaces until they hit the eye. That’s natural, that’s how we see the world around us.</p><p>Shadows are areas hit by a low number of photons. A greenish sheet of paper lying next to a bright green wall is a result of reflection of photons from the wall before hitting the sheet (the Global Illumination effect). Multiple bouncing of photons from surfaces provides the required realistic smoothness of lighting.</p><p>Tracing algorithms are quite resource-consuming and even the most powerful of currently available GPU’s are not capable enough to process a FullHD picture at 60+ FPS properly without visible noise. It will take an extremely long time to calculate noiseless lighting for a very dark scene containing a small bright object (e.g. light shining through a small slit in the doorway into a dark room) since the number of rays will be low.</p><h3>Research and implementations</h3><p>While thinking about the problem of incorrect ambient lighting I came to an idea. If it is possible to bake static AO into a texture providing smooth transitions between different faces, why don’t we try to pre-calculate and bake normals. I was in touch with some guys from the <a href="https://3dcoat.com/home/" target="_blank" rel="noopener noreferrer">3D-Coat</a> crew, so I asked one of their programmers to bake such normal maps according to the algorithm that I had composed. The result was fantastic.</p></div></div><div data-widget-size="full"><div><div><div><div><div><div><div><p><img data="image" src="https://new-cdn.80.lv/upload/content/5a/5d2c863496675.gif" alt=""></p></div></div></div><div><div><div><p><img data="image" src="https://new-cdn.80.lv/upload/content/ed/5d2c863583dbd.gif" alt=""></p></div></div></div></div></div></div><p><span>1<!-- --> of <!-- -->2</span></p></div></div><div data-widget-size="full"><div><div><p><img data="image" src="https://new-cdn.80.lv/upload/content/3e/images/5d2c863688532/widen_920x0.jpg" srcset="https://new-cdn.80.lv/upload/content/3e/images/5d2c863688532/widen_1840x0.jpg 2x" alt=""></p></div></div></div><div data-widget-size="full"><div><p>Support for such a normal map indeed made some objects look more realistic to a certain extent. However, it was almost of no use for the scene as a whole, because a normal map works only for a single separate object (exactly as ambient occlusion baked into a texture). We couldn’t bake a single normal map for all objects, since objects had to be dynamic. So, this sort of implementation was no good for us, but it gave us a hint of using the very idea of such normals. After surfing the Web for a while we discovered that a similar technology had already existed, but no one really knew how to use it and what are its advantages. This was clear from the questions asked on different forums. The technology was called “Bent Normals” and wasn’t really popular due to the lack of the appropriate baking software. So, we had to make some modifications, which subsequently led us to the creation of our own technology.</p><p>After some time we were almost done with creating the content for our project. That was the time we faced the challenge of insufficient realism of lighting again. Objects in bookcases were still too bright, and when zooming in, we could see sharp transitions of ambient lighting between dynamic objects. That is why another attempt to improve lighting was made.</p><p>Our R&amp;D Team came to the following assumption: if the popular Screen Space Reflection is so good at dealing with unnecessary reflections from environment probes, why don’t we try to calculate SSAO using a similar algorithm. So, we tried and got an amazing result.</p></div></div><div data-widget-size="full"><div><div><p><img data="image" src="https://new-cdn.80.lv/upload/content/89/5d2c863761993.gif" alt=""></p></div></div></div><div data-widget-size="full"><p>Instantly we realized that everything we had before did not look like realistic lighting at all. We used the same approach to implement secondary diffuse light bounces, self-illumination of emission objects, and again Bent Normals, but this time without baking, since everything was calculated in real-time. Surprisingly, the whole thing worked perfectly. So, we called this technology Screen Space Ray-Traced Global Illumination (SSRTGI).</p></div><div data-widget-size="full"><div><div><div><div><div><div><div><p><img data="image" src="https://new-cdn.80.lv/upload/content/50/5d2c863893d6a.gif" alt=""></p></div></div></div><div><div><div><p><img data="image" src="https://new-cdn.80.lv/upload/content/e7/5d2c8639c13a0.gif" alt=""></p></div></div></div></div></div></div><p><span>1<!-- --> of <!-- -->2</span></p></div></div><div data-widget-size="full"><div><div><p><img data="image" src="https://new-cdn.80.lv/upload/content/e2/5d2c863aec952.gif" alt=""></p></div></div></div><div data-widget-size="full"><div><p>Ordinary screen-space effects, such as SSAO and SSGI, that are used to simulate global illumination, do not treat objects as obstacles for light rays. Roughly speaking, the light passes through objects, that’s why all popular approaches are unable to provide realistic lighting, offering just a rough imitation of interaction between photons and objects. Our approach is a real screen space ray tracing that takes obstacles into account.</p><h3>Technical details:</h3><ol><li><p><b>Basic settings</b></p></li></ol><p><i>а. Number of rays.</i> The higher is the value, the more rays are used for the effect, giving us a smoother picture;</p></div></div><div data-widget-size="full"><div><div><p><img data="image" src="https://new-cdn.80.lv/upload/content/8c/5d2c863c61e8d.gif" alt=""></p></div></div></div><div data-widget-size="full"><p><i>b. Number of steps.</i> The higher is the value, the further the tracing will be performed, resulting in wider radial gradients;</p></div><div data-widget-size="full"><div><div><p><img data="image" src="https://new-cdn.80.lv/upload/content/3b/5d2c863d03ce7.gif" alt=""></p></div></div></div><div data-widget-size="full"><p><i>c. Step size. </i>The higher is the value, the more precisely the obstacles would be determined. It results in smoother gradients on small objects;</p></div><div data-widget-size="full"><div><div><p><img data="image" src="https://new-cdn.80.lv/upload/content/3b/5d2c863d972c8.gif" alt=""></p></div></div></div><div data-widget-size="full"><div><div><p><img data="image" src="https://new-cdn.80.lv/upload/content/86/5d2c863e385bd.gif" alt=""></p></div></div></div><div data-widget-size="full"><div><p><b>2. Tracing</b></p><p>We used blue noise for sampling of the rays to be traced. Comparing the standard Photoshop pattern to blue noise, we can see that that the picture becomes less noisy, while the number of samples is the same.</p></div></div><div data-widget-size="full"><div><div><p><img data="image" src="https://new-cdn.80.lv/upload/content/86/5d2c863ee1d5f.gif" alt=""></p></div></div></div><div data-widget-size="full"><div><p><b>3. Ray tracing gives us the following information:</b></p><ol type="a"><li><p>Number of rays, that hit obstacles, to be used for Ambient Occlusion calculation;</p></li><li><p>Arithmetic mean of vectors, that didn’t hit any obstacles, to be used for Bent Normals calculation;</p></li><li><p>Final color of the picture (without post-effects) to be used to determine the secondary bounce color and the self-illumination of the objects.</p></li></ol><p><b>4. Denoising</b></p><p>This post-processing helps to remove noise, when the number of samples is insufficient and it is especially important for self-luminous objects, since they create very bright individual pixels, that immediately catch the eye.</p></div></div><div data-widget-size="full"><div><div><p><img data="image" src="https://new-cdn.80.lv/upload/content/31/5d2c863fbcdcb.gif" alt=""></p></div></div></div><div data-widget-size="full"><p><b>5. Upscaling</b><br>
 SSRTGI can be rendered in full, half or quarter resolution. By reducing the resolution you can get a noticeable performance gain, but at the cost of a drastic change of image quality. The main problem is that the edges of objects become blurred due to low resolution. To make contours sharper, we took depth and normals for both lower and full resolution, compared them, and then restored the non-matching pixels.</p></div><div data-widget-size="full"><div><div><p><img data="image" src="https://new-cdn.80.lv/upload/content/44/5d2c86409f848.gif" alt=""></p></div></div></div><div data-widget-size="full"><div><p><b>6. Temporal filter</b></p><p>Every frame we change noise pattern for ray sampling, then make a reprojection of the previous frame taking into account the velocity buffer and mix the result with the current frame.</p><p>Practical application</p><p>Today, SSAO, HBAO, and SSDO are unable to achieve the level of image quality and realism offered by SSRTGI, even if we increase their resolution. However, using SSRTGI at its full capacity requires more advanced hardware. The key point here is that you can configure it to take more time providing a more realistic image, or to be faster producing a result, that does not differ much visually from these techniques. Fortunately, thanks to the flexibility of SSRTGI settings, you can adjust the balance between quality and performance.</p></div></div><div data-widget-size="full"><div><div><p><img data="image" src="https://new-cdn.80.lv/upload/content/f3/5d2c8641bbf61.gif" alt=""></p></div></div></div><div data-widget-size="full"><div><p>Using a GTX 1070 we configured SSRTGI with the following parameters:</p><p>low = rays 4, steps 2, step size 2</p><p>medium = rays 16, steps 4, step size 2</p><p>high = rays 32, steps 8, step size 1</p><table><colgroup><col width="236"><col width="110"><col width="184"><col width="108"> </colgroup><tbody><tr><td>&nbsp;</td><td><p>low</p></td><td><p>medium</p></td><td><p>high</p></td></tr><tr><td><p>1280×720</p></td><td><p>0.3</p></td><td><p>0.6</p></td><td><p>1.3</p></td></tr><tr><td><p>1920×1080</p></td><td><p>0.85</p></td><td><p>1.3</p></td><td><p>3.2</p></td></tr><tr><td><p>2560×1440</p></td><td><p>1.47</p></td><td><p>3</p></td><td><p>6.2</p></td></tr><tr><td><p>3840×2160</p></td><td><p>3.4</p></td><td><p>7.8</p></td><td><p>16.5</p></td></tr></tbody></table><p>At the highest presets of the Superposition Benchmark we used SSRTGI at its full capacity and turned off baked lighting even for static objects. The result: we got them looking more consistent.</p></div></div><div data-widget-size="full"><div><div><p><img data="image" src="https://new-cdn.80.lv/upload/content/2a/5d2c8642d9be5.gif" alt=""></p></div></div></div><div data-widget-size="full"><div><h3>Significance of the technology</h3><p>With SSRTGI technology it is possible to make real-time lighting consistent and realistic. It is a powerful tool in the hands of an experienced artist, offering an exceptional level of flexibility. A realistic bounce of warm light (we used a lamp in the dark to highlight the effect), adding more realism to the whole scene, serves as a good illustration of what this technology is capable of.</p></div></div><div data-widget-size="full"><div><div><p><img data="image" src="https://new-cdn.80.lv/upload/content/f6/5d2c86462758d.gif" alt=""></p></div></div></div><div data-widget-size="full"><div><div><p><img data="image" src="https://new-cdn.80.lv/upload/content/41/5d2c86475b725.gif" alt=""></p></div></div></div><div data-widget-size="full"><div><h3>Summary</h3><ul><li><p>SSRTGI on a per-pixel basis gives us a consistent and realistic picture at any distance, from close-ups to outdoor locations;</p></li><li><p>SSRTGI is a perfect solution for large open-space worlds, where it is impossible to bake lighting;</p></li><li><p>With this technology, we bridge the gap that exists between real-time and offline renderers regarding image quality;</p></li><li><p>SSRTGI has a great potential: it can replace ordinary Screen Space Global Illumination effects providing a comparable performance, and moreover, it can drastically improve image quality at higher presets;</p></li><li><p>This effect is already included in <a href="http://unigine.com/en/products/engine?utm_source=80lv&amp;utm_medium=article&amp;utm_campaign=ssrtgi" target="_blank" rel="noopener noreferrer">Unigine 2 SDK</a>. It is used in the released projects based on Unigine 2 and will be used in future projects as well;</p></li><li><p>The technology was presented and highly appreciated at the last SIGGRAPH conference in Los Angeles (Real-Time Live! show).</p></li></ul><p>You can download the Superposition Benchmark, where we used SSRTGI, from the <a href="http://unigine.com/?utm_source=80lv&amp;utm_medium=article&amp;utm_campaign=ssrtgi" target="_blank" rel="noopener noreferrer">UNIGINE Benchmark</a> website absolutely for free! In addition to the desktop mode, it also works perfectly well in VR.</p><h3>Davyd Vidiger,&nbsp;<a href="http://unigine.com/?utm_source=80lv&amp;utm_medium=article&amp;utm_campaign=ssrtgi" target="_blank" rel="noopener noreferrer">UNIGINE</a></h3><h5>Interview conducted by&nbsp;<a href="https://ua.linkedin.com/in/kikatok" target="_blank" rel="noopener noreferrer">Kirill Tokarev</a>.&nbsp;</h5><h2>Follow 80.lv&nbsp;on&nbsp;<a href="https://www.facebook.com/LevelEighty/" target="_blank" rel="noopener noreferrer">Facebook</a>,&nbsp;<a href="https://twitter.com/EightyLevel" target="_blank" rel="noopener noreferrer">Twitter</a>&nbsp;and&nbsp;<a href="https://www.instagram.com/80.lv/" target="_blank" rel="noopener noreferrer">Instagram</a></h2></div></div></div></div></div>
    </div>
    <footer>
        <div>created by buildstarted &copy; 2020 <a href="/about">about</a></div>
        <div>Share this page on social media: copy and paste this url https://linksfor.dev/</div>
        <div>If you prefer RSS: <a href="https://linksfor.dev/feed.xml">https://linksfor.dev/feed.xml</a></div>
		<div>Customer satisfaction guaranteed to be optional.</div>
    </footer>
    
    <script async defer>
        _dna = window._dna || {};
        _dna.siteId = "linksfor.devs";
        _dna.outlink = true;

        (function () {
            let dna = document.createElement('script');
            dna.type = 'text/javascript';
            dna.async = true;
            dna.src = '//dna.buildstarted.com/t.js';
            let s = document.getElementsByTagName('script')[0];
            s.parentNode.insertBefore(dna, s);
        })();
    </script>
    <noscript><img src="//dna.buildstarted.com/g?siteId=linksfor.devs"/></noscript>
</body>
</html>