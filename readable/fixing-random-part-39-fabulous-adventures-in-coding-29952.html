<!DOCTYPE html>
<html lang="en">
<head>
    <title>
Fixing Random, part 39 - linksfor.dev(s)    </title>
    <meta charset="utf-8">
    <link rel="alternate" type="application/rss+xml" title="Linksfor.dev(s) feed" href="https://linksfor.dev/feed.rss" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta name="google" value="notranslate">
    <link rel="stylesheet" type="text/css" href="/style.css" />
    <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon">
    <link rel="icon" href="/favicon.ico" type="image/x-icon">
    <meta property="og:title" content="Fixing Random, part 39 - linksfor.dev(s)"/>
    <meta property="og:description" content="Let&#x2019;s sum up the last few episodes: Suppose we have a distribution of doubles, p, and a function f&#xA0;from double to double. We often want to answer the question &#x201C;what is the average value&#x2026;"/>
    <meta property="og:type" content="website"/>
    <meta property="og:url" content="https://ericlippert.com/2019/07/15/fixing-random-part-39/"/>

<meta property="og:site_name" content="linksfor.dev(s)" />
</head>
<body>
    <div class="devring" style="background: #222">
        <div style="text-align:center">Explore other dev related sites in this ring. If you would like to join this ring <a href="https://devring.club">click here</a>.</div>
        <div class="grid">
            <div style="display: grid; grid-template-columns: .5fr 1fr 1fr 1fr; text-align: center;">
                <span class="devring-title"><a href="https://devring.club/">devring.club</a></span>
                <a href="https://devring.club/sites/1/prev" class="devring-previous">Previous</a>
                <a href="https://devring.club/random" class="devring-random">Random</a>
                <a href="https://devring.club/sites/1/next" class="devring-next">Next</a>
            </div>
        </div>
    </div>
    <div class="grid">
        <h1>
<a href="/" style="color:inherit">linksfor.dev(s)</a>
        </h1>
        <title>linksfor.dev(s) - Fixing Random, part 39</title>
<div class="readable">
        <h1>Fixing Random, part 39</h1>
            <div>Reading time: 10-13 minutes</div>
        <div>Posted here: 15 Jul 2019</div>
        <p><a href="https://ericlippert.com/2019/07/15/fixing-random-part-39/">https://ericlippert.com/2019/07/15/fixing-random-part-39/</a></p>
        <hr/>
<div id="readability-page-1" class="page"><div>
		<p data-adtags-visited="true">Let’s sum up the last few episodes:</p>
<p data-adtags-visited="true">Suppose we have a distribution of doubles, <code>p</code>, and a function <code>f</code>&nbsp;from double to double. We often want to answer the question <em>“what is the average value of <code>f</code>&nbsp;when it is given samples from <code>p</code>?”</em> This quantity is called the <strong>expected value</strong>.</p>
<p data-adtags-visited="true">The obvious (or “naive”) way to do it is: take a bunch of samples, evaluate the function on those samples, take the average. Easy! However, this can lead to problems if there are “black swans”: values that are rarely sampled, but massively affect the value of the average when run through <code>f</code>. We would like to get a good estimate without having to massively increase the number of samples in our average.</p>
<p data-adtags-visited="true">We developed two techniques to estimate the expected value:</p>
<p data-adtags-visited="true">First, abandon sampling entirely and do numerical integral calculus:</p>
<ul>
<li>Use quadrature to compute two areas: the area under <code>f(x)*p.Weight(x)</code>&nbsp; and the area under <code>p.Weight(x)</code>&nbsp;(which is the normalization constant of <code>p</code>)</li>
<li>Their quotient is an extremely accurate estimate of the expected value</li>
<li>But we have to know what region to do quadrature over.</li>
</ul>
<p data-adtags-visited="true">Second, use importance sampling:</p>
<ul>
<li>Find a helper distribution <code>q</code> whose weight is large where <code>f(x)*p.Weight(x)</code>&nbsp;bounds a lot of area.</li>
<li>Use the naive algorithm to estimate the expected value of&nbsp;&nbsp;<code>x=&gt;f(x)*p.Weight(x)/q.Weight(x)</code>from samples of <code>q</code></li>
<li>That is <em>proportional</em> to the expected value of <code>f</code>&nbsp;with respect to <code>p</code></li>
<li>We gave a technique for estimating the proportionality constant by sampling from <code>q</code>&nbsp;also.</li>
</ul>
<p data-adtags-visited="true">The problem with importance sampling then is finding a good <code>q</code>. We discussed some techniques:</p>
<ul>
<li>If you know the range, just use a uniform distribution over that range.</li>
<li>Stretch and shift <code>p</code>&nbsp;so that the transformed PDF doesn’t have a “black swan”, but the normalization constant is the same.</li>
<li>Use the Metropolis algorithm to generate a helper PDF from <code>Abs(f*p)</code>, though in my experiments this worked poorly</li>
<li>If we know the range of interest, we can use the VEGAS algorithm. It makes cheap, naive estimates of the area of subranges, and then uses that information to gradually refine a piecewise-uniform helper PDF that targets spikes and avoid flat areas of <code>f*p</code>.</li>
<li>However, the VEGAS algorithm is complex, and I did not attempt to implement it for this series.</li>
</ul>
<p data-adtags-visited="true">The question you may have been asking yourself these past few episodes is:</p>
<p data-adtags-visited="true"><strong>If quadrature is an accurate and cheap way to estimate the expected value of <code>f</code>&nbsp;over samples from <code>p</code> then why are we even considering doing sampling at all? Surely we typically know at least approximately the range over which <code>f*p</code>&nbsp;has some area. What’s the point of all this?</strong></p>
<p data-adtags-visited="true">Quadrature just splits up the range into some number — say, a thousand — equally-sized pieces, evaluates&nbsp;<code>f*p</code>&nbsp;on each of them, and takes the average. That sure seems cheaper and easier than all this mucking around with sampling. Have I just been wasting your time these past few episodes? And why has there been so much research and effort put into finding techniques for estimating expected value?</p>
<p data-adtags-visited="true">This series is called “Fixing Random” because the built-in base class library tools we have in C# for representing probabilities are weak. I’ve approached everything in this series from the perspective of <em>“I want to have an object that represents probabilities in my business domain, and I want to use that object to solve my business problems”.</em></p>
<p data-adtags-visited="true"><em>“What is the expected value of this function given this distribution?”</em> is a very natural question to ask when solving business problems that involve probabilities, and as we’ve seen, you can answer that question by simulating integral calculus through quadrature.</p>
<p data-adtags-visited="true">But, as I keep on saying: <strong>things equal to the same are equal to each other.</strong> Flip the script. Suppose our business domain involves&nbsp;<em>solving integral&nbsp;calculus problems</em>. And suppose there is an integral calculus problem that we <em>cannot</em>&nbsp;efficiently solve with quadrature. What do we do?</p>
<ul>
<li>We can solve expected value problems with integral calculus techniques such as quadrature.</li>
<li>We can solve expected value problems with sampling techniques</li>
<li>Things equal to the same are equal to each other.</li>
<li>Therefore <strong>we can solve integral calculus problems with sampling techniques.&nbsp;</strong></li>
</ul>
<p data-adtags-visited="true">That is why there has been so much research into computing expected values: the expected value is the <em>area</em> under the function <code>f(x)*p.Weight(x)</code>&nbsp;so <strong>if we can compute the expected value by sampling, then we can compute that area</strong> and solve the integral calculus problem <em>without</em> doing quadrature!</p>
<p data-adtags-visited="true">I said above <em>“if quadrature is accurate and cheap”</em>, but there are<em> many</em> scenarios in which quadrature is not a cheap way to compute an area.</p>
<p data-adtags-visited="true">What’s an example? Well, let’s generalize. So far in this series I’ve assumed that <code>f</code>&nbsp;is a <code>Func&lt;double, double&gt;</code>&nbsp;. What if <code>f</code>&nbsp;is a <code>Func&lt;double, double, double&gt;</code>&nbsp;— a function from pairs of doubles to double. That is <code>f</code>&nbsp;is not a line in two dimensions, it is a surface in three.</p>
<p data-adtags-visited="true"><span>Let’s suppose we have </span><code>f</code><span>&nbsp;being such a function, and we would like to solve a calculus problem: what is the volume under </span><code>f</code><span>&nbsp;on the range (0,0) to (1, 1)?</span></p>
<p data-adtags-visited="true">We could do it by quadrature, but remember, in my example we split up the range 0-to-1 into a thousand points. If we do quadrature in two dimensions with the same granularity of 0.001, that’s a million points we have to evaluate and sum. If we only have computational resources to do a thousand points, then we have to have a granularity of around 0.03.</p>
<p data-adtags-visited="true">What if the function is zero at most of those points? We could then have a really crappy estimate of the total area because our granularity is so low.</p>
<p data-adtags-visited="true">We now reason as follows: take a two-dimensional probability distribution. Let’s say we have the <em>standard continuous uniform implementation</em> of&nbsp;&nbsp;<code>IWeightedDistribution&lt;(double, double)&gt;</code>&nbsp;.</p>
<p data-adtags-visited="true">All the techniques I have explored in this series work equally well in two dimensions as one! So we can use those techniques. Let’s do so:</p>
<ul>
<li>What is the estimated value of <code>f</code>&nbsp;when applied to samples from this distribution?</li>
<li>It is equal to the volume under <code>f(x,y)*p.Weight((x,y)).&nbsp;</code></li>
<li><code></code>But&nbsp;<code>p.Weight((x,y))</code><span>&nbsp;is always 1.0 on the region we care about; it’s the standard continuous uniform distribution, after all.</span></li>
<li>Therefore<strong> the estimated expected value of <code>f</code>&nbsp;when evaluated on samples from <code>p</code>&nbsp;is an estimate of the volume we care about.</strong></li>
</ul>
<p data-adtags-visited="true">How does that help?</p>
<p data-adtags-visited="true">It doesn’t.</p>
<p data-adtags-visited="true">If we’re taking a thousand points by quadrature or a thousand points by sampling from a uniform distribution over the same range, it doesn’t matter. We’re still computing a value at a thousand points and taking an average.</p>
<p data-adtags-visited="true">But now here’s the trick.</p>
<p data-adtags-visited="true">Suppose we can find a <em>helper</em> distribution <code>q</code> that is large where <code>f(x,y)</code>&nbsp;has a lot of volume and very small where it has little volume.</p>
<p data-adtags-visited="true">We can then use importance sampling to compute a more accurate estimate of the desired expected value, and therefore the desired volume, because most of the points we sample from <code>q</code>&nbsp;are in high-volume regions. Our thousand points from <code>q</code>&nbsp;will give us a better estimate!</p>
<p data-adtags-visited="true">Now, up the dimensionality further. Suppose we’ve got a function that takes <em>three</em> doubles and goes to double, and we wish to know its hypervolume over (0, 0, 0) to (1, 1, 1).</p>
<p data-adtags-visited="true">With quadrature, we’re either doing a billion computations at a granularity of 0.001, or, if we can only afford to do a thousand evaluations, that’s a granularity of 0.1.</p>
<p data-adtags-visited="true"><strong>Every time we add a dimension, either the cost of our quadrature goes up by a factor of a thousand, or the cost stays the same but the granularity is enormously coarsened.</strong></p>
<p data-adtags-visited="true">Oh, but it gets worse.</p>
<p data-adtags-visited="true">When you are evaluating the hypervolume of a 3-d surface embedded in 4 dimensions, there are a <em>lot</em> more points where the function can be zero! There is just so much <em>room</em> in high dimensions for stuff to be. <strong>The higher the dimensionality gets, the more important it is that you find the spikes and avoid the flats.&nbsp;</strong></p>
<hr>
<p data-adtags-visited="true"><strong>Exercise:</strong> Consider an n-dimensional cube of side 1. That thing always has a hypervolume of 1, no matter what n is.</p>
<p data-adtags-visited="true">Now consider a concentric n-dimensional cube inside it where the sides are 0.9 long.</p>
<ul>
<li>For a 1-dimensional cube — a line — the inner line is 90% of the length of the outer line, so we’ll say that 10% of the length of the outer line is “close to the surface”.</li>
<li>For a 2-dimensional cube — a square — the inner square has 81% of the area of the outer square, so 19% of the area of the outer square is “close to the surface”.</li>
</ul>
<p data-adtags-visited="true">At what dimensionality is more than 50% of the hypervolume of the outer hypercube “close to the surface”?</p>
<p data-adtags-visited="true"><strong>Exercise:</strong> Now consider an n-dimensional cube of side 1 again, and the concentric n-dimensional sphere. That is, a circle that exactly fits inside a square, a sphere that exactly fits inside a cube, and so on. The radius is 1/2.</p>
<ul>
<li>The area of the circle is pi/4 = 79% of the area of the square.</li>
<li>The volume of the sphere is pi/6 = 52% of the volume of the cube.</li>
<li>… and so on</li>
</ul>
<p data-adtags-visited="true">At what value for n does the volume of the hypersphere become 1% of the volume of the hypercube?</p>
<hr>
<p data-adtags-visited="true">In high dimensions, <em>any</em> shape that is <em>anywhere</em> on the <em>interior</em> of a hypercube is <em>tiny</em> when compared to the massive hypervolume near the cube’s <em>surface</em>!</p>
<p data-adtags-visited="true">That means: if you’re trying to determine the hypervolume bounded by a function that has large values somewhere <em>inside</em> a hypercube, the samples <em>must</em>&nbsp;frequently hit that important region where the values are big. If you spend time “near the edges” where the values are small, you’ll spend &gt;90% of your time sampling irrelevant values.</p>
<p data-adtags-visited="true">That’s why importance sampling is so useful, and why we spend so much effort studying how to find distributions that compute expected values.<strong> Importance sampling allows us to numerically solve multidimensional integral calculus problems with reasonable compute resources.</strong></p>
<hr>
<p data-adtags-visited="true"><strong>Aside</strong>: Now you know why I said earlier that I misled you when I said that the VEGAS algorithm was designed to find helpful distributions for importance sampling. The VEGAS algorithm absolutely does that, but that’s not what it was <em>designed</em> to do; <em>it was designed to solve multidimensional integral calculus problems.</em> Finding good helper distributions is how it does its job.</p>
<hr>
<p data-adtags-visited="true"><strong>Exercise:</strong> Perhaps you can see how we would extend the algorithms we’ve implemented on distributions of doubles to distributions of tuples of doubles; I’m not going to do that in this series; give it a shot and see how it goes!</p>
<hr>
<p data-adtags-visited="true"><strong>Next time on FAIC:</strong> This has been one of the longest blog series I’ve done, and looking back over the last sixteen years, I have never actually <em>completed</em> any of the really big projects I started: building a script engine, building a Zork implementation, explaining Milner’s paper, and so on. I’m going to complete this one!</p>
<p data-adtags-visited="true">There is so much more to say on this topic; people spend their careers studying this stuff. But I’m going to wrap it up in the next couple of episodes by giving some final thoughts, a summary of the work we’ve done, a list of some of the topics I did not cover that I’d hoped to, and a partial bibliography of the papers and other resources that I read when doing this series.</p>

			
			
						</div></div></div>
    </div>
    <footer>
        <div>created by buildstarted &copy; 2020 <a href="/about">about</a></div>
        <div>Share this page on social media: copy and paste this url https://linksfor.dev/</div>
        <div>If you prefer RSS: <a href="https://linksfor.dev/feed.xml">https://linksfor.dev/feed.xml</a></div>
        <div>Customer satisfaction guaranteed to be optional.</div>
    </footer>
    
    <script async defer>
        _dna = window._dna || {};
        _dna.siteId = "linksfor.devs";
        _dna.outlink = true;

        (function () {
            let dna = document.createElement('script');
            dna.type = 'text/javascript';
            dna.async = true;
            dna.src = '//dna.buildstarted.com/t.js';
            let s = document.getElementsByTagName('script')[0];
            s.parentNode.insertBefore(dna, s);
        })();
    </script>
    <noscript><img src="//dna.buildstarted.com/g?siteId=linksfor.devs" /></noscript>
</body>
</html>