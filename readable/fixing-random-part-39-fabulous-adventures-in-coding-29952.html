<!DOCTYPE html>
<html lang="en">
<head>
    <title>
Fixing Random, part 39 - linksfor.dev(s)    </title>
    <meta charset="utf-8">
    <link rel="alternate" type="application/rss+xml" title="Linksfor.dev(s) feed" href="https://linksfor.dev/feed.rss" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta name="google" value="notranslate">
    <link rel="stylesheet" type="text/css" href="/style.css" />
    <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon">
    <link rel="icon" href="/favicon.ico" type="image/x-icon">
    <meta property="og:title" content="Fixing Random, part 39 - linksfor.dev(s)"/>
    <meta property="og:description" content="Let&#x2019;s sum up the last few episodes: Suppose we have a distribution of doubles, p, and a function f&#xA0;from double to double. We often want to answer the question &#x201C;what is the average value&#x2026;"/>
    <meta property="og:type" content="website"/>
    <meta property="og:url" content="https://ericlippert.com/2019/07/15/fixing-random-part-39/"/>

<meta property="og:site_name" content="linksfor.dev(s)" />
</head>
<body>
    <div class="grid">
        <h1>
                <span style="cursor: default" title="linksfor.dev(s) has been running for 1 year! :partypopper:">üéâ</span>
<a href="/" style="color:inherit">linksfor.dev(s)</a>
        </h1>
        <title>linksfor.dev(s) - Fixing Random, part 39</title>
<div class="readable">
        <h1>Fixing Random, part 39</h1>
            <div>Reading time: 10-13 minutes</div>
        <div>Posted here: 15 Jul 2019</div>
        <p><a href="https://ericlippert.com/2019/07/15/fixing-random-part-39/">https://ericlippert.com/2019/07/15/fixing-random-part-39/</a></p>
        <hr/>
<div id="readability-page-1" class="page"><div>
		<p data-adtags-visited="true">Let‚Äôs sum up the last few episodes:</p>
<p data-adtags-visited="true">Suppose we have a distribution of doubles, <code>p</code>, and a function <code>f</code>&nbsp;from double to double. We often want to answer the question <em>‚Äúwhat is the average value of <code>f</code>&nbsp;when it is given samples from <code>p</code>?‚Äù</em> This quantity is called the <strong>expected value</strong>.</p>
<p data-adtags-visited="true">The obvious (or ‚Äúnaive‚Äù) way to do it is: take a bunch of samples, evaluate the function on those samples, take the average. Easy! However, this can lead to problems if there are ‚Äúblack swans‚Äù: values that are rarely sampled, but massively affect the value of the average when run through <code>f</code>. We would like to get a good estimate without having to massively increase the number of samples in our average.</p>
<p data-adtags-visited="true">We developed two techniques to estimate the expected value:</p>
<p data-adtags-visited="true">First, abandon sampling entirely and do numerical integral calculus:</p>
<ul>
<li>Use quadrature to compute two areas: the area under <code>f(x)*p.Weight(x)</code>&nbsp; and the area under <code>p.Weight(x)</code>&nbsp;(which is the normalization constant of <code>p</code>)</li>
<li>Their quotient is an extremely accurate estimate of the expected value</li>
<li>But we have to know what region to do quadrature over.</li>
</ul>
<p data-adtags-visited="true">Second, use importance sampling:</p>
<ul>
<li>Find a helper distribution <code>q</code> whose weight is large where <code>f(x)*p.Weight(x)</code>&nbsp;bounds a lot of area.</li>
<li>Use the naive algorithm to estimate the expected value of&nbsp;&nbsp;<code>x=&gt;f(x)*p.Weight(x)/q.Weight(x)</code>from samples of <code>q</code></li>
<li>That is <em>proportional</em> to the expected value of <code>f</code>&nbsp;with respect to <code>p</code></li>
<li>We gave a technique for estimating the proportionality constant by sampling from <code>q</code>&nbsp;also.</li>
</ul>
<p data-adtags-visited="true">The problem with importance sampling then is finding a good <code>q</code>. We discussed some techniques:</p>
<ul>
<li>If you know the range, just use a uniform distribution over that range.</li>
<li>Stretch and shift <code>p</code>&nbsp;so that the transformed PDF doesn‚Äôt have a ‚Äúblack swan‚Äù, but the normalization constant is the same.</li>
<li>Use the Metropolis algorithm to generate a helper PDF from <code>Abs(f*p)</code>, though in my experiments this worked poorly</li>
<li>If we know the range of interest, we can use the VEGAS algorithm. It makes cheap, naive estimates of the area of subranges, and then uses that information to gradually refine a piecewise-uniform helper PDF that targets spikes and avoid flat areas of <code>f*p</code>.</li>
<li>However, the VEGAS algorithm is complex, and I did not attempt to implement it for this series.</li>
</ul>
<p data-adtags-visited="true">The question you may have been asking yourself these past few episodes is:</p>
<p data-adtags-visited="true"><strong>If quadrature is an accurate and cheap way to estimate the expected value of <code>f</code>&nbsp;over samples from <code>p</code> then why are we even considering doing sampling at all? Surely we typically know at least approximately the range over which <code>f*p</code>&nbsp;has some area. What‚Äôs the point of all this?</strong></p>
<p data-adtags-visited="true">Quadrature just splits up the range into some number ‚Äî say, a thousand ‚Äî equally-sized pieces, evaluates&nbsp;<code>f*p</code>&nbsp;on each of them, and takes the average. That sure seems cheaper and easier than all this mucking around with sampling. Have I just been wasting your time these past few episodes? And why has there been so much research and effort put into finding techniques for estimating expected value?</p>
<p data-adtags-visited="true">This series is called ‚ÄúFixing Random‚Äù because the built-in base class library tools we have in C# for representing probabilities are weak. I‚Äôve approached everything in this series from the perspective of <em>‚ÄúI want to have an object that represents probabilities in my business domain, and I want to use that object to solve my business problems‚Äù.</em></p>
<p data-adtags-visited="true"><em>‚ÄúWhat is the expected value of this function given this distribution?‚Äù</em> is a very natural question to ask when solving business problems that involve probabilities, and as we‚Äôve seen, you can answer that question by simulating integral calculus through quadrature.</p>
<p data-adtags-visited="true">But, as I keep on saying: <strong>things equal to the same are equal to each other.</strong> Flip the script. Suppose our business domain involves&nbsp;<em>solving integral&nbsp;calculus problems</em>. And suppose there is an integral calculus problem that we <em>cannot</em>&nbsp;efficiently solve with quadrature. What do we do?</p>
<ul>
<li>We can solve expected value problems with integral calculus techniques such as quadrature.</li>
<li>We can solve expected value problems with sampling techniques</li>
<li>Things equal to the same are equal to each other.</li>
<li>Therefore <strong>we can solve integral calculus problems with sampling techniques.&nbsp;</strong></li>
</ul>
<p data-adtags-visited="true">That is why there has been so much research into computing expected values: the expected value is the <em>area</em> under the function <code>f(x)*p.Weight(x)</code>&nbsp;so <strong>if we can compute the expected value by sampling, then we can compute that area</strong> and solve the integral calculus problem <em>without</em> doing quadrature!</p>
<p data-adtags-visited="true">I said above <em>‚Äúif quadrature is accurate and cheap‚Äù</em>, but there are<em> many</em> scenarios in which quadrature is not a cheap way to compute an area.</p>
<p data-adtags-visited="true">What‚Äôs an example? Well, let‚Äôs generalize. So far in this series I‚Äôve assumed that <code>f</code>&nbsp;is a <code>Func&lt;double, double&gt;</code>&nbsp;. What if <code>f</code>&nbsp;is a <code>Func&lt;double, double, double&gt;</code>&nbsp;‚Äî a function from pairs of doubles to double. That is <code>f</code>&nbsp;is not a line in two dimensions, it is a surface in three.</p>
<p data-adtags-visited="true"><span>Let‚Äôs suppose we have </span><code>f</code><span>&nbsp;being such a function, and we would like to solve a calculus problem: what is the volume under </span><code>f</code><span>&nbsp;on the range (0,0) to (1, 1)?</span></p>
<p data-adtags-visited="true">We could do it by quadrature, but remember, in my example we split up the range 0-to-1 into a thousand points. If we do quadrature in two dimensions with the same granularity of 0.001, that‚Äôs a million points we have to evaluate and sum. If we only have computational resources to do a thousand points, then we have to have a granularity of around 0.03.</p>
<p data-adtags-visited="true">What if the function is zero at most of those points? We could then have a really crappy estimate of the total area because our granularity is so low.</p>
<p data-adtags-visited="true">We now reason as follows: take a two-dimensional probability distribution. Let‚Äôs say we have the <em>standard continuous uniform implementation</em> of&nbsp;&nbsp;<code>IWeightedDistribution&lt;(double, double)&gt;</code>&nbsp;.</p>
<p data-adtags-visited="true">All the techniques I have explored in this series work equally well in two dimensions as one! So we can use those techniques. Let‚Äôs do so:</p>
<ul>
<li>What is the estimated value of <code>f</code>&nbsp;when applied to samples from this distribution?</li>
<li>It is equal to the volume under <code>f(x,y)*p.Weight((x,y)).&nbsp;</code></li>
<li><code></code>But&nbsp;<code>p.Weight((x,y))</code><span>&nbsp;is always 1.0 on the region we care about; it‚Äôs the standard continuous uniform distribution, after all.</span></li>
<li>Therefore<strong> the estimated expected value of <code>f</code>&nbsp;when evaluated on samples from <code>p</code>&nbsp;is an estimate of the volume we care about.</strong></li>
</ul>
<p data-adtags-visited="true">How does that help?</p>
<p data-adtags-visited="true">It doesn‚Äôt.</p>
<p data-adtags-visited="true">If we‚Äôre taking a thousand points by quadrature or a thousand points by sampling from a uniform distribution over the same range, it doesn‚Äôt matter. We‚Äôre still computing a value at a thousand points and taking an average.</p>
<p data-adtags-visited="true">But now here‚Äôs the trick.</p>
<p data-adtags-visited="true">Suppose we can find a <em>helper</em> distribution <code>q</code> that is large where <code>f(x,y)</code>&nbsp;has a lot of volume and very small where it has little volume.</p>
<p data-adtags-visited="true">We can then use importance sampling to compute a more accurate estimate of the desired expected value, and therefore the desired volume, because most of the points we sample from <code>q</code>&nbsp;are in high-volume regions. Our thousand points from <code>q</code>&nbsp;will give us a better estimate!</p>
<p data-adtags-visited="true">Now, up the dimensionality further. Suppose we‚Äôve got a function that takes <em>three</em> doubles and goes to double, and we wish to know its hypervolume over (0, 0, 0) to (1, 1, 1).</p>
<p data-adtags-visited="true">With quadrature, we‚Äôre either doing a billion computations at a granularity of 0.001, or, if we can only afford to do a thousand evaluations, that‚Äôs a granularity of 0.1.</p>
<p data-adtags-visited="true"><strong>Every time we add a dimension, either the cost of our quadrature goes up by a factor of a thousand, or the cost stays the same but the granularity is enormously coarsened.</strong></p>
<p data-adtags-visited="true">Oh, but it gets worse.</p>
<p data-adtags-visited="true">When you are evaluating the hypervolume of a 3-d surface embedded in 4 dimensions, there are a <em>lot</em> more points where the function can be zero! There is just so much <em>room</em> in high dimensions for stuff to be. <strong>The higher the dimensionality gets, the more important it is that you find the spikes and avoid the flats.&nbsp;</strong></p>
<hr>
<p data-adtags-visited="true"><strong>Exercise:</strong> Consider an n-dimensional cube of side 1. That thing always has a hypervolume of 1, no matter what n is.</p>
<p data-adtags-visited="true">Now consider a concentric n-dimensional cube inside it where the sides are 0.9 long.</p>
<ul>
<li>For a 1-dimensional cube ‚Äî a line ‚Äî the inner line is 90% of the length of the outer line, so we‚Äôll say that 10% of the length of the outer line is ‚Äúclose to the surface‚Äù.</li>
<li>For a 2-dimensional cube ‚Äî a square ‚Äî the inner square has 81% of the area of the outer square, so 19% of the area of the outer square is ‚Äúclose to the surface‚Äù.</li>
</ul>
<p data-adtags-visited="true">At what dimensionality is more than 50% of the hypervolume of the outer hypercube ‚Äúclose to the surface‚Äù?</p>
<p data-adtags-visited="true"><strong>Exercise:</strong> Now consider an n-dimensional cube of side 1 again, and the concentric n-dimensional sphere. That is, a circle that exactly fits inside a square, a sphere that exactly fits inside a cube, and so on. The radius is 1/2.</p>
<ul>
<li>The area of the circle is pi/4 = 79% of the area of the square.</li>
<li>The volume of the sphere is pi/6 = 52% of the volume of the cube.</li>
<li>‚Ä¶ and so on</li>
</ul>
<p data-adtags-visited="true">At what value for n does the volume of the hypersphere become 1% of the volume of the hypercube?</p>
<hr>
<p data-adtags-visited="true">In high dimensions, <em>any</em> shape that is <em>anywhere</em> on the <em>interior</em> of a hypercube is <em>tiny</em> when compared to the massive hypervolume near the cube‚Äôs <em>surface</em>!</p>
<p data-adtags-visited="true">That means: if you‚Äôre trying to determine the hypervolume bounded by a function that has large values somewhere <em>inside</em> a hypercube, the samples <em>must</em>&nbsp;frequently hit that important region where the values are big. If you spend time ‚Äúnear the edges‚Äù where the values are small, you‚Äôll spend &gt;90% of your time sampling irrelevant values.</p>
<p data-adtags-visited="true">That‚Äôs why importance sampling is so useful, and why we spend so much effort studying how to find distributions that compute expected values.<strong> Importance sampling allows us to numerically solve multidimensional integral calculus problems with reasonable compute resources.</strong></p>
<hr>
<p data-adtags-visited="true"><strong>Aside</strong>: Now you know why I said earlier that I misled you when I said that the VEGAS algorithm was designed to find helpful distributions for importance sampling. The VEGAS algorithm absolutely does that, but that‚Äôs not what it was <em>designed</em> to do; <em>it was designed to solve multidimensional integral calculus problems.</em> Finding good helper distributions is how it does its job.</p>
<hr>
<p data-adtags-visited="true"><strong>Exercise:</strong> Perhaps you can see how we would extend the algorithms we‚Äôve implemented on distributions of doubles to distributions of tuples of doubles; I‚Äôm not going to do that in this series; give it a shot and see how it goes!</p>
<hr>
<p data-adtags-visited="true"><strong>Next time on FAIC:</strong> This has been one of the longest blog series I‚Äôve done, and looking back over the last sixteen years, I have never actually <em>completed</em> any of the really big projects I started: building a script engine, building a Zork implementation, explaining Milner‚Äôs paper, and so on. I‚Äôm going to complete this one!</p>
<p data-adtags-visited="true">There is so much more to say on this topic; people spend their careers studying this stuff. But I‚Äôm going to wrap it up in the next couple of episodes by giving some final thoughts, a summary of the work we‚Äôve done, a list of some of the topics I did not cover that I‚Äôd hoped to, and a partial bibliography of the papers and other resources that I read when doing this series.</p>

			
			
						</div></div></div>
    </div>
    <footer>
        <div>created by buildstarted &copy; 2020 <a href="/about">about</a></div>
        <div>Share this page on social media: copy and paste this url https://linksfor.dev/</div>
        <div>If you prefer RSS: <a href="https://linksfor.dev/feed.xml">https://linksfor.dev/feed.xml</a></div>
    </footer>
    
</body>
</html>