<!DOCTYPE html>
<html lang="en">
<head>
    <title>
Fixing Random, part&#xA0;38 -
linksfor.dev(s)
    </title>
    <meta charset="utf-8">
    <meta name="Description" content="A curated source of links that devs might find interesting. Updated around the clock." />
    <meta name="google" value="notranslate">
    <style type="text/css">
        html {
            font-family: sans-serif;
            line-height: 1.15;
            -webkit-text-size-adjust: 100%;
            -webkit-tap-highlight-color: transparent;
            height: 100%;
        }

        *, ::after, ::before {
            box-sizing: border-box;
        }

        body {
            margin: 0;
            font-family: -apple-system,BlinkMacSystemFont,"Segoe UI",Roboto,"Helvetica Neue",Arial,"Noto Sans",sans-serif,"Apple Color Emoji","Segoe UI Emoji","Segoe UI Symbol","Noto Color Emoji";
            font-size: 1rem;
            font-weight: 400;
            line-height: 1.5;
            color: #60656a;
            text-align: left;
            background-color: #323b44;
        }

        h1 {
            font-size: 6rem;
            font-weight: 300;
            line-height: 1.2;
            margin-top: 0;
            margin-bottom: 0.5rem;
            margin-bottom: 0.5rem
        }

        a {
            color: #007bff;
            color: #ccc;
            text-decoration: none;
            background-color: transparent;
            word-break: break-all;
        }

        .unseen a {
            font-weight: bold;
        }

        h3 {
            margin-top: 0;
            padding-top: 0;
            font-weight: normal;
        }

        .grid {
            -ms-flex-direction: column;
            flex-direction: column;
            width: 1024px;
            margin: 0 auto;
            flex: 1 0 auto;
        }

        .row {
            -ms-flex-direction: row;
            flex-direction: row;
            width: 100%;
            -ms-flex-wrap: wrap;
            flex-wrap: wrap;
            display: -ms-flexbox;
            display: flex;
        }

        .col {
            margin: 0 10px 0 10px;
            box-sizing: border-box;
            vertical-align: top;
        }

        .col-3-of-4, .col-6-of-8, .col-9-of-12 {
            width: calc(75% - 20px);
        }

        .col-1-of-4, .col-2-of-8, .col-3-of-12 {
            width: calc(25% - 20px);
        }

        @media (max-width:1023px) {
            /* big landscape tablets, laptops, and desktops */
            body {
                overflow-x: hidden;
            }

            main {
                width: 99%;
            }

            h1 {
                font-size: 50px;
            }
        }

        .text-right {
            text-align: right;
        }

        footer {
            left: 0;
            width: 100%;
            margin-top: 2em;
            padding: 50px 0;
            text-align: center;
            -moz-box-sizing: border-box;
            -webkit-box-sizing: border-box;
            box-sizing: border-box;
        }

        .readable {
            color: #949ba2;
        }

        svg:not(:root).svg-inline--fa {
            color: #60656a;
            overflow: visible;
        }

        .svg-inline--fa.fa-w-12 {
            width: 0.75em;
        }

        svg:not(:root) {
            overflow: hidden;
        }

        .svg-inline--fa {
            display: inline-block;
            font-size: inherit;
            height: 1em;
            overflow: visible;
            vertical-align: -0.125em;
        }

        img {
            max-width: 100%;
        }

        .text-center {
            text-align: center;
        }

        .readable h1 {
            font-size: 2em;
        }
    </style>
</head>
<body>
    <div class="grid">
        
<div class="readable">
    <h1>Fixing Random, part&#xA0;38</h1>
    <div class="entry-content"> <p><a href="https://ericlippert.com/2019/07/01/fixing-random-part-37/">Last time on FAIC</a> we were attacking our final problem in computing the expected value of a function <code>f</code>&#xA0;applied to a set of samples from a distribution <code>p</code>. We discovered that we could sometimes do a &#x201C;stretch and shift&#x201D; of <code>p</code>, and then run importance sampling on the stretched distribution; that way we are more likely to sample from &#x201C;black swan&#x201D; regions, and therefore the estimated expected value is more likely to be accurate.</p>
<p>However, determining what the parameters to <code>Stretch.Distribution</code>&#xA0;should be to get a good result is not obvious; it seems like we&#x2019;d want to do what we did: actually look at the graphs and play around with parameters until we get something that looks right.</p>
<p>It seems like there ought to be a way to automate this process to get an accurate estimate of the expected value. Let&#x2019;s take a step back and review what exactly it is we need from the helper distribution. Start with the things it must have:</p>
<ul>
<li>Obviously it <strong>must</strong> be a weighted distribution of doubles that we can sample from!</li>
<li>That means that it <strong>must</strong> have a weight function that is always non-negative.</li>
<li>And the area under its weight function <strong>must</strong> be finite, though not necessarily 1.0.</li>
</ul>
<p>And then the things we want:</p>
<ul>
<li>The support of the helper distribution does not have to be <em>exactly</em> support of the <code>p</code>, but it&#x2019;s nice if it is.</li>
<li>The helper&#x2019;s weight function should be large in ranges where <code>f(x) * p.Weight(x)</code>&#xA0;bounds a large area, positive or negative.</li>
<li>And conversely, it&#x2019;s helpful if the weight function is small in areas where the area is small.</li>
</ul>
<p>Well, where is the area likely to be large? Precisely in the places where <code>Abs(f(x)*p.Weight(x))</code>&#xA0;is large. Where is it likely to be small? Where that quantity is small&#x2026; so&#x2026;</p>
<p><em>why don&#x2019;t we use that as the weight function for the helper distribution?</em></p>
<p>Great Scott, why didn&#x2019;t we think of that before?</p> <p><strong>Aside:</strong> As I noted before in this series, all of these techniques require that the expected value actually <em>exist</em>. I&#x2019;m sure you can imagine functions where <code>f*p</code>&#xA0;bounds a finite area, so the expected value exists, but <code>abs(f*p)</code> does not bound a finite area, and therefore is not the weight function of a distribution. This technique will probably not work well in those weird cases.</p> <p>If only we had a way to turn an arbitrary function into a non-normalized distribution we could sample from&#x2026; oh wait, <strong>we do. </strong>(Code is <a href="https://github.com/ericlippert/probability/tree/episode38">here</a>.)</p>
<p><span><span>var</span><span>&#xA0;</span><span>p</span><span>&#xA0;</span><span>=</span><span>&#xA0;</span><span>Normal</span><span>.</span><span>Distribution</span><span>(</span><span>0.75</span><span>,</span><span>&#xA0;</span><span>0.09</span><span>);</span><br>
<span>double</span><span>&#xA0;</span><span>f</span><span>(</span><span>double</span><span>&#xA0;</span><span>x</span><span>)</span><span>&#xA0;</span><span>=&gt;</span><span>&#xA0;</span><span>Atan</span><span>(</span><span>1000</span><span>&#xA0;</span><span>*</span><span>&#xA0;</span><span>(</span><span>x</span><span>&#xA0;</span><span>&#x2013;</span><span>&#xA0;</span><span>.45</span><span>))</span><span>&#xA0;</span><span>*</span><span>&#xA0;</span><span>20</span><span>&#xA0;</span><span>&#x2013;</span><span>&#xA0;</span><span>31.2</span><span>;</span><br>
<span>var</span><span>&#xA0;</span><span>m</span><span>&#xA0;</span><span>=</span><span>&#xA0;</span><span>Metropolis</span><span>&lt;</span><span>double</span><span>&gt;</span><span>.</span><span>Distribution</span><span>(<br>
</span><span>&#xA0; x</span><span>&#xA0;</span><span>=&gt;</span><span>&#xA0;</span><span>Abs</span><span>(</span><span>f</span><span>(</span><span>x</span><span>)</span><span>&#xA0;</span><span>*</span><span>&#xA0;</span><span>p</span><span>.</span><span>Weight</span><span>(</span><span>x</span><span>)),</span><span><br>
</span><span>&#xA0; p</span><span>,</span><span><br>
</span><span>&#xA0; x</span><span>&#xA0;</span><span>=&gt;</span><span>&#xA0;</span><span>Normal</span><span>.</span><span>Distribution</span><span>(</span><span>x</span><span>,</span><span>&#xA0;</span><span>0.15</span><span>));</span></span></p>
<p>Let&#x2019;s take a look at it:</p>
<p><span><span>Console</span><span>.</span><span>WriteLine</span><span>(</span><span>m</span><span>.</span><span>Histogram</span><span>(</span><span>0.3</span><span>,</span><span>&#xA0;</span><span>1</span><span>));</span></span></p>
<pre>                         ***            
                         ****           
                        *****           
                       *******          
        *              *******          
       **             *********         
       **             *********         
       **             *********         
       **            ***********        
       **            ***********        
       **           *************       
       **           *************       
      ***           **************      
      ***          ***************      
      ***          ***************      
      ***         *****************     
     ****        *******************    
     ****        ********************   
    *****       *********************** 
----------------------------------------</pre>
<p>That sure looks like the distribution we want.</p>
<p>What happens if we try it as the helper distribution in importance sampling? Unfortunately, the results are not so good:</p>
<pre>0.11,&#xA0;0.14,&#xA0;0.137,&#xA0;0.126,&#xA0;0.153,&#xA0;0.094, ...</pre>
<p>Recall that again, the correct result is <code>0.113</code>. We&#x2019;re getting <em>worse</em> results with this helper distribution than we did with the original black-swan-susceptible distribution.</p>
<p><strong>I&#x2019;m not sure what has gone wrong here.</strong> I tried experimenting with different proposal distributions and couldn&#x2019;t find one that gave better results than just using the proposal distribution itself as the helper distribution.</p>
<p>So once again we&#x2019;ve discovered that there&#x2019;s some art here; this technique looks like it should work right out of the box, but there&#x2019;s something that needs tweaking here. <strong>Any experts in this area who want to comment on why this didn&#x2019;t work, please leave comments.</strong></p>
<p>And of course all we&#x2019;ve done here is pushed the problem off a level; our problem is to find a good helper distribution for this expected value problem, but to do that with Metropolis, we need to find a good proposal distribution for the Metropolis algorithm to consume, so it is not clear that we&#x2019;ve made much progress here. Sampling efficiently and accurately is hard!</p> <p>I&#x2019;ll finish up this topic with a sketch of a rather complicated algorithm called VEGAS; this is an algorithm for solving the problem&#xA0;<em>&#x201C;how do we generate a good helper distribution for importance sampling knowing only <code>p</code>&#xA0;and <code>f</code>?&#x201D;</em></p> <p><strong>Aside:</strong> The statement above is slightly misleading, but we&#x2019;ll say why in the next episode!</p> <p>This technique, like quadrature, does require us to have a &#x201C;range&#x201D; over which we know that the bulk of the area of <code>f(x)*p.Weight(x)</code>&#xA0;is found. Like our disappointing attempt above, the idea is to find a distribution whose weight function is large where it needs to be, and small where it is not.</p>
<p>I am not an expert on this algorithm by any means, but I can give you a quick sketch of the idea. The first thing we do is divide up our range of interest into some number of equally-sized subranges. On each of those subranges we make a uniform distribution and use it to make an estimate of the area of the function on that subrange.</p>
<p>How do we do that? Remember that the expected value of a function evaluated on samples drawn from a distribution is equal to the area of the function divided by the area of the distribution. We can construct a uniform distribution to have area of 1.0, so the expected value is equal to the area. But <em>we can estimate the expected value by sampling. </em>So we can estimate areas by sampling too! Again: things equal to the same are equal to each other; if we need to find an area, we can find it by sampling to determine an expected value.</p>
<p>So we estimate the expected value of a uniform distribution restricted to each sub-range. Again, here&#x2019;s the function of interest, <code>f(x)*p.Weight(x)</code></p>
<p><img class="alignnone size-full wp-image-6392" src="https://ericlippert.files.wordpress.com/2019/07/screen-shot-2019-05-24-at-10.42.32-am.png?w=509" alt="Screen Shot 2019-05-24 at 10.42.32 AM.png" srcset="https://ericlippert.files.wordpress.com/2019/07/screen-shot-2019-05-24-at-10.42.32-am.png 509w, https://ericlippert.files.wordpress.com/2019/07/screen-shot-2019-05-24-at-10.42.32-am.png?w=150 150w, https://ericlippert.files.wordpress.com/2019/07/screen-shot-2019-05-24-at-10.42.32-am.png?w=300 300w" sizes="(max-width: 509px) 100vw, 509px"></p>
<p>Ultimately we want to <em>accurately</em> find the area of this thing, but we need a black-swan-free distribution that samples a lot where the area of this thing is big.</p>
<p>Let&#x2019;s start by making some <em>cheap</em> estimates of the area of subranges. We&#x2019;ll split this thing up into ten sub-ranges, and do a super cheap estimate of the area of the subrange by sampling over a uniform distribution confined to that subrange.</p>
<p>Let&#x2019;s suppose our cheap estimate finds the area of each subrange as follows:</p>
<p><img class="alignnone size-full wp-image-6394" src="https://ericlippert.files.wordpress.com/2019/07/screen-shot-2019-05-24-at-10.47.02-am.png?w=516" alt="Screen Shot 2019-05-24 at 10.47.02 AM.png" srcset="https://ericlippert.files.wordpress.com/2019/07/screen-shot-2019-05-24-at-10.47.02-am.png 516w, https://ericlippert.files.wordpress.com/2019/07/screen-shot-2019-05-24-at-10.47.02-am.png?w=150 150w, https://ericlippert.files.wordpress.com/2019/07/screen-shot-2019-05-24-at-10.47.02-am.png?w=300 300w" sizes="(max-width: 516px) 100vw, 516px"></p>
<p>Now, you might say, hey, <em>the sum of all of these is an estimate of the area</em>, and that&#x2019;s what we&#x2019;re after; and sure, in this case it would be pretty good. But stay focussed: what we&#x2019;re after here with this technique is <em>a distribution that we can sample from that is likely to have high weight where the area is high</em>.</p>
<p>So what do we do? We now have an estimate of where the area of the function is big &#x2014; where the expected value of the sub-range is far from zero &#x2014; and where it is small.</p>
<p>We <em>could</em> just take the absolute value and stitch it all together:</p>
<p><img class="alignnone size-full wp-image-6395" src="https://ericlippert.files.wordpress.com/2019/07/screen-shot-2019-05-24-at-11.00.51-am.png?w=484" alt="Screen Shot 2019-05-24 at 11.00.51 AM.png" srcset="https://ericlippert.files.wordpress.com/2019/07/screen-shot-2019-05-24-at-11.00.51-am.png 484w, https://ericlippert.files.wordpress.com/2019/07/screen-shot-2019-05-24-at-11.00.51-am.png?w=150 150w, https://ericlippert.files.wordpress.com/2019/07/screen-shot-2019-05-24-at-11.00.51-am.png?w=300 300w" sizes="(max-width: 484px) 100vw, 484px"></p>
<p>And then use this as our helper distribution; as we prefer, it will be large when the area is likely to be large, and small where it is likely to be small. We&#x2019;ll spend almost no time sampling from 0.0 to 0.3 where the contribution to the expected value is very small, but lots of time sampling near both the big lumps.</p> <p><strong>Aside:</strong> This is an interesting distribution: it&#x2019;s a&#xA0;<em>piecewise uniform distribution</em>. We have not shown how to sample from such a distribution in this series, but if you&#x2019;ve been following along, I&#x2019;m sure you can see how to do it efficiently; after all, our &#x201C;unfair die&#x201D; distribution from way back is basically the same. You can efficiently implement distributions shaped like the above using similar techniques.</p> <p>This is already <em>pretty</em> good; we&#x2019;ve done ten <em>cheap</em> area estimates and generated a reasonably high-quality helper PDF that we can then use for importance sampling. But you&#x2019;ve probably noticed that it is far from perfect; it seems like the subranges on the right side are either way too big or way too small, and this might skew the results.</p>
<p>The insight of the VEGAS algorithm&#x2019;s designer was: don&#x2019;t stop now!&#xA0;<em>We have information to refine our helper PDF further</em>.</p>
<p>How?</p>
<p>We started with ten <em>equally-sized</em> subranges. Numbering them from the left, it sure looks like regions 1, 2, 3, 5 and 6 were useless in terms of providing area, and regions 5 and 9 were awesome, so <strong>let&#x2019;s start over with ten <em>unequally</em> sized ranges</strong>. We&#x2019;ll make regions 1, 2, and 3 into one big subrange, and also regions 5 and 6 into one big subrange, and then split up regions 4, 7, 8, 9 and 10 into eight smaller regions and <em>do it again</em>.</p> <p><strong>Aside:</strong>&#xA0;The exact details of how we rebalance the subranges involve a lot of fiddly bookkeeping, and that&#x2019;s why I don&#x2019;t want to go there in this series; getting the alias algorithm right was enough work, and this would be more. Maybe in a later series I&#x2019;ll investigate this algorithm in more detail.</p> <p>We can then keep on repeating that process until we have a helper PDF that is fine-grained where it needs to be: in the places where the area is large and changing rapidly. And it is then coarse-grained where there is not much change in area and the area is small.</p>
<p>Or, put another way: <em>VEGAS looks for the spikes and the flats, and refines its estimate to be more accurate at the spikes because that&#x2019;s where the area is at.</em></p>
<p>And bonus, the helper PDF is always piecewise continuous uniform, which as I noted above, is relatively easy to implement and very cheap to sample from.</p>
<p>This technique really does generate a high-quality helper PDF for importance sampling when given a probability distribution and a function. But, it sounds insanely complicated; <em>why would we bother?</em></p> <p><strong>Next time on FAIC:</strong> I&#x2019;ll wrap up this topic with some thoughts on why we have so many techniques for computing expected value.</p> </div>
</div>
    </div>
    <footer>
        <div>created by buildstarted &copy; 2019 <a href="/about">about</a></div>
        <div>Share this page on social media: copy and paste this url https://linksfor.dev/</div>
    </footer>
    
</body>
</html>