<!DOCTYPE html>
<html lang="en">
<head>
    <title>
Fixing Random, part 34 - linksfor.dev(s)    </title>
    <meta charset="utf-8">
    <link rel="alternate" type="application/rss+xml" title="Linksfor.dev(s) feed" href="https://linksfor.dev/feed.rss" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta name="google" value="notranslate">
    <link rel="stylesheet" type="text/css" href="/style.css" />
    <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon">
    <link rel="icon" href="/favicon.ico" type="image/x-icon">
    <meta property="og:title" content="Fixing Random, part 34 - linksfor.dev(s)"/>
    <meta property="og:description" content="Last time on FAIC we implemented a better technique for estimating the expected value of a function f applied to samples from a distribution p: Compute the total area (including negative areas) und&#x2026;"/>
    <meta property="og:type" content="website"/>
    <meta property="og:url" content="https://ericlippert.com/2019/06/10/fixing-random-part-34/"/>

<meta property="og:site_name" content="linksfor.dev(s)" />
</head>
<body>
    <div class="devring" style="background: #222">
        <div style="text-align:center">Explore other dev related sites in this ring. If you would like to join this ring <a href="https://devring.club">click here</a>.</div>
        <div class="grid">
            <div style="display: grid; grid-template-columns: .5fr 1fr 1fr 1fr; text-align: center;">
                <span class="devring-title"><a href="https://devring.club/">devring.club</a></span>
                <a href="https://devring.club/sites/1/prev" class="devring-previous">Previous</a>
                <a href="https://devring.club/random" class="devring-random">Random</a>
                <a href="https://devring.club/sites/1/next" class="devring-next">Next</a>
            </div>
        </div>
    </div>
    <div class="grid">
        <h1>
<a href="/" style="color:inherit">linksfor.dev(s)</a>
        </h1>
        <title>linksfor.dev(s) - Fixing Random, part 34</title>
<div class="readable">
        <h1>Fixing Random, part 34</h1>
            <div>Reading time: 11-13 minutes</div>
        <div>Posted here: 10 Jun 2019</div>
        <p><a href="https://ericlippert.com/2019/06/10/fixing-random-part-34/">https://ericlippert.com/2019/06/10/fixing-random-part-34/</a></p>
        <hr/>
<div id="readability-page-1" class="page"><div>
		<p data-adtags-visited="true"><a href="https://ericlippert.com/2019/06/03/fixing-random-part-33/">Last time on FAIC</a> we implemented a better technique for estimating the expected value of a function <code>f</code> applied to samples from a distribution <code>p</code>:</p>
<ul>
<li>Compute the total area (including negative areas) under the function <code>x =&gt; f(x) * p.Weight(x)</code></li>
<li>Compute the total area under <code>x =&gt; p.Weight(x)</code>
<ul>
<li>This is 1.0 for a normalized PDF, or the normalizing constant of a non-normalized PDF; if we already know it, we don’t have to compute it.</li>
</ul>
</li>
<li>The quotient of these areas is the expected value</li>
</ul>
<p data-adtags-visited="true">Essentially our technique was to use quadrature to get an approximate numerical solution to an integral calculus problem.</p>
<p data-adtags-visited="true">However, we also noted that it seems like there might still be room for improvement, in two main areas:</p>
<ul>
<li>This technique only works when we have a good bound on the support of the distribution; for my contrived example I chose a “profit function” and a distribution where I said that I was only interested in the region from 0.0 to 1.0.</li>
<li>Our initial intuition that implementing an estimate of “average of many samples” by, you know, <em>averaging many samples,</em> seems like it was on the right track; can we get back there?</li>
</ul>
<p data-adtags-visited="true">In this episode I’m going to stick to the restriction to distributions with support over 0.0 to 1.0 for pedagogic reasons, but our aim is to find a technique that gets us back to sampling over arbitrary distributions.</p>
<p data-adtags-visited="true">The argument that I’m going to make here (several times!) is: <em>two things that are both equal to the same third thing are also equal to each other.</em></p>
<p data-adtags-visited="true">Recall that we arrived at our quadrature implementation by estimating that our continuous distribution’s expected value is close to the expected value of a very similar discrete distribution. I’m going to make my argument a little bit more general here by removing the assumption that <code>p</code> is a normalized distribution. That means that we’ll need to know the normalizing factor <code>np</code>, which as we’ve noted is <code>Area(p.Weight)</code>.</p>
<p data-adtags-visited="true">We said that we could estimate the expected value like this:</p>
<ul>
<li>Imagine that we create a 1000 sided “unfair die” discrete distribution.</li>
<li>Each side corresponds to a 0.001 wide slice from the range 0.0 to 1.0; let’s say that we have a variable <code>x</code> that takes on values 0.000, 0.001, 0.002, and so on, corresponding to the 1000 sides.</li>
<li>The <em>weight</em> of each side is the probability of choosing this slice: <code>p.Weight(x) / 1000 / np</code></li>
<li>The <em>value</em> of each side is the “profit function” <code>f(x)</code></li>
<li>The expected value of “rolling this die” is <em>the sum of (value times weight)</em>: the sum of <code>f(x) * (p.Weight(x) / 1000 / np)</code>over our thousand values of <code>x</code></li>
</ul>
<p data-adtags-visited="true">Here’s the trick:</p>
<ul>
<li>Consider the standard continuous uniform distribution <code>u</code>. That’s a perfectly good distribution with support 0.0 to 1.0.</li>
<li>Consider the function <code>w(x)</code> which is <code>x =&gt; f(x) * p.Weight(x) / np</code>.&nbsp; That’s a perfectly good function from double to double.</li>
<li>Question: <em>What is an estimate of the expected value of <code>w</code><span> over samples from </span><code>u</code><span>?</span></em></li>
</ul>
<p data-adtags-visited="true">We can use the same technique:</p>
<ul>
<li>Imagine we create a 1000-sided “unfair die” discrete distribution</li>
<li><code>x</code> is as before</li>
<li>The weight of each side is the probability of choosing that slice, but since this is a uniform distribution, every weight is the same — so, turns out, it is not an unfair die! The <em>weight</em> of each side is 0.001.</li>
<li>&nbsp;The <em>value</em> of each side is our function <code>w(x)</code></li>
<li>The expected value of rolling this die is <em>the sum of (value times weight):</em> the sum of <code>(f(x) * p.Weight(x) / np) * 0.001</code> over our thousand values of x</li>
</ul>
<p data-adtags-visited="true">But compare those two expressions; we are computing exactly the same sum both times. These two expected values must be the same value.</p>
<p data-adtags-visited="true">Things equal to the same are equal to each other, which implies this conclusion:</p>
<p data-adtags-visited="true"><strong>If we can compute an estimate of the expected value of <code>w</code> applied to samples from&nbsp;<code>u</code> by any technique then we have also computed an estimate of the expected value of <code>f</code>&nbsp;applied to samples from&nbsp;<code>p</code>.</strong></p>
<p data-adtags-visited="true">Why is this important?</p>
<p data-adtags-visited="true">The problem with the naïve algorithm in our original case was that there was a “black swan” — a region of large (negative) area that is sampled only one time in <em>1400</em> samples. But that region is sampled one time in about <em>14</em> samples when we sample from a uniform distribution, so we will get a much better and more consistent estimate of the expected value if we use the naïve technique over the uniform distribution.</p>
<p data-adtags-visited="true"><strong>In order to get 100x times as many samples in the black swan region, we do not have to do 100x times as many samples overall. We can just sample from a helper distribution that targets that important region more often.</strong></p>
<p data-adtags-visited="true">Let’s try it! (Code can be found <a href="https://github.com/ericlippert/probability/tree/episode34">here</a>.)</p>
<p data-adtags-visited="true">In order to not get confused here, I’m going to rename some of our methods so that they’re not all called <code>ExpectedValue</code>. The one that just takes any distribution and averages a bunch of samples is now <code>ExpectedValueBySampling</code>&nbsp;and the one that computes two areas and takes their quotient is <code>ExpectedValueByQuadrature</code>.</p>
<p data-adtags-visited="true"><span><span>var</span><span>&nbsp;</span><span>p</span><span>&nbsp;</span><span>=</span><span>&nbsp;</span><span>Normal</span><span>.</span><span>Distribution</span><span>(</span><span>0.75</span><span>,</span><span>&nbsp;</span><span>0.09</span><span>);</span><br>
<span>double</span><span>&nbsp;</span><span>f</span><span>(</span><span>double</span><span>&nbsp;</span><span>x</span><span>)</span><span>&nbsp;</span><span>=&gt;</span><span>&nbsp;</span><span>Atan</span><span>(</span><span>1000</span><span>&nbsp;</span><span>*</span><span>&nbsp;</span><span>(</span><span>x</span><span>&nbsp;</span><span>–</span><span>&nbsp;</span><span>.45</span><span>))</span><span>&nbsp;</span><span>*</span><span>&nbsp;</span><span>20</span><span>&nbsp;</span><span>–</span><span>&nbsp;</span><span>31.2</span><span>;</span><br>
<span>var</span><span>&nbsp;</span><span>u</span><span>&nbsp;</span><span>=</span><span>&nbsp;</span><span>StandardContinuousUniform</span><span>.</span><span>Distribution</span><span>;</span><br>
<span>double</span><span>&nbsp;</span><span>np</span><span>&nbsp;</span><span>=</span><span>&nbsp;</span><span>1.0</span><span>;</span><span>&nbsp;</span><span>//&nbsp;p&nbsp;is&nbsp;normalized</span><br>
<span>double</span><span>&nbsp;w</span><span>(</span><span>double</span><span>&nbsp;</span><span>x</span><span>)</span><span>&nbsp;</span><span>=&gt;</span><span>&nbsp;</span><span>f</span><span>(</span><span>x</span><span>)</span><span>&nbsp;</span><span>*</span><span>&nbsp;</span><span>p</span><span>.</span><span>Weight</span><span>(</span><span>x</span><span>)</span><span>&nbsp;</span><span>/</span><span>&nbsp;</span><span>np</span><span>;</span><br>
<span>for</span><span>&nbsp;</span><span>(</span><span>int</span><span>&nbsp;</span><span>i</span><span>&nbsp;</span><span>=</span><span>&nbsp;</span><span>0</span><span>;</span><span>&nbsp;</span><span>i</span><span>&nbsp;</span><span>&lt;</span><span>&nbsp;</span><span>100</span><span>;</span><span>&nbsp;</span><span>++</span><span>i</span><span>)</span><br>
<span>&nbsp; </span><span>Console</span><span>.</span><span>WriteLine</span><span>(</span><span>$”</span><span>{</span><span>u</span><span>.</span><span>ExpectedValueBySampling</span><span>(w</span><span>):</span><span>0.###</span><span>}</span><span>“</span><span>);</span></span></p>
<p data-adtags-visited="true">Remember, the correct answer that we computed by quadrature is <code>0.113</code>. When sampling <code>p</code>&nbsp;directly we got values ranging from <code>0.7</code> to <code>0.14</code>. But now we get:</p>
<pre>0.114, 0.109, 0.109, 0.118, 0.111, 0.107, 0.113, 0.112, ...</pre>
<p data-adtags-visited="true">So much better!</p>
<p data-adtags-visited="true">This is awesome, but wait, <em>it gets more awesome.</em> What is so special about the uniform distribution? Nothing, that’s what. I’m going to do this argument one more time:</p>
<p data-adtags-visited="true">Suppose I have distribution <code>q</code>, any distribution whatsoever, so long as its support is the same as <code>p</code> — in this case, 0.0 to 1.0. In particular, suppose that <code>q</code> is not necessarily a normalized distribution, but that again, <em>we know its normalization factor.</em> Call it <code>nq</code>.&nbsp; Recall that the normalization factor can be computed by <code>nq = Area(q.Weight)</code>.</p>
<p data-adtags-visited="true">Our special function <code>g(x)</code> is this oddity:</p>
<p data-adtags-visited="true"><code>x =&gt; (f(x) * (p.Weight(x) / q.Weight(x)) * (nq / np)</code></p>
<p data-adtags-visited="true">What is the expected value of <code>g</code> over distribution <code>q</code>?&nbsp; One more time:</p>
<ul>
<li>Create a 1000-sided unfair die, <code>x</code> as before.</li>
<li>The <strong>weight</strong> of each side is the probability of choosing that side, which is <code>(q.Weight(x) / 1000) / nq</code></li>
<li>The <strong>value</strong> of each side is <code>g(x)</code>.</li>
<li>The expected value is the sum of <code>g(x) * (q.Weight(x) / 1000) / nq</code> but if you work that out, of course that is the sum of <code>f(x) * p.Weight(x) / np / 1000</code></li>
</ul>
<p data-adtags-visited="true">And once again, we’ve gotten back to the same sum by clever choice of function. <strong>If we can compute the expected value of <code>g</code>evaluated on samples from&nbsp;<code>q</code>, then we know the expected value of <code>f</code> evaluated on samples from&nbsp;<code>p</code>!</strong></p>
<p data-adtags-visited="true">This means that we can choose our helper distribution <code>q</code> so that it is highly likely to pick values in regions we consider important. Let’s look at our graph of <code>p.Weight</code>&nbsp;and&nbsp;<code>f*p.Weight</code>again:</p>
<p data-adtags-visited="true"><img data-attachment-id="6303" data-permalink="https://ericlippert.com/2019/06/03/fixing-random-part-33/screen-shot-2019-05-21-at-9-17-06-am/" data-orig-file="https://ericlippert.files.wordpress.com/2019/06/screen-shot-2019-05-21-at-9.17.06-am.png" data-orig-size="502,443" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Screen Shot 2019-05-21 at 9.17.06 AM" data-image-description="" data-medium-file="https://ericlippert.files.wordpress.com/2019/06/screen-shot-2019-05-21-at-9.17.06-am.png?w=300" data-large-file="https://ericlippert.files.wordpress.com/2019/06/screen-shot-2019-05-21-at-9.17.06-am.png?w=502" src="https://ericlippert.files.wordpress.com/2019/06/screen-shot-2019-05-21-at-9.17.06-am.png?w=584" alt="Screen Shot 2019-05-21 at 9.17.06 AM" srcset="https://ericlippert.files.wordpress.com/2019/06/screen-shot-2019-05-21-at-9.17.06-am.png 502w, https://ericlippert.files.wordpress.com/2019/06/screen-shot-2019-05-21-at-9.17.06-am.png?w=150 150w, https://ericlippert.files.wordpress.com/2019/06/screen-shot-2019-05-21-at-9.17.06-am.png?w=300 300w" sizes="(max-width: 502px) 100vw, 502px"></p>
<p data-adtags-visited="true">There are segments of the graph where the area under the blue line is very small but the area under the orange line is large, and that’s our black swan; <strong>what we want is a distribution that samples from regions where the orange area is large, and if possible skips regions where it is small.</strong> That is, we consider the large-area regions <em>important</em> contributors to the expected value, and the small-area regions <em>unimportant</em> contributors; we wish to target our samples so that no important regions are ignored. That’s why<strong> this technique for computing expected value is called “importance sampling”.</strong></p>
<hr>
<p data-adtags-visited="true"><strong>Exercise:&nbsp;</strong>The uniform distribution is pretty good on the key requirement that it never be small when the area under the orange line is large, because it is always the same size from 0.0 to 1.0; that’s why it is the <em>uniform</em> distribution, after all. It’s not great on our second measure; it spends around 30% of its time sampling from regions where the orange line has essentially no area under it.</p>
<p data-adtags-visited="true">Write some code that tries different distributions. For example, implement the distribution that has weight function <code>x =&gt; (0 &lt;= x &amp;&amp; x &lt;= 1) ? x : 0</code></p>
<p data-adtags-visited="true">(Remember that this is not a normalized distribution, so you’ll have to compute <code>nq</code>.)</p>
<p data-adtags-visited="true">Does that give us an even better estimate of the expected value of <code>f</code>?</p>
<p data-adtags-visited="true">Something to ponder while you’re working on that: <em>what would be the ideal choice of distribution?</em></p>
<hr>
<p data-adtags-visited="true"><strong>Summing up:</strong></p>
<ul>
<li>Suppose we have a weighted distribution of doubles <code>p</code> and a function from double to double <code>f</code>.</li>
<li>We wish to accurately estimate the average value of <code>f</code> when it is applied to a large set of samples from <code>p</code>; this is the <em>expected value.</em></li>
<li>However, there may be “black swan” regions where the value of <code>f</code> is important to the average, but the probability of sampling from that region is low, so our average could require a huge number of samples to get an accurate average.</li>
<li>We can fix the problem by choosing any weighted distribution <code>q</code> that has the same support as <code>p</code>but is more likely to sample from important regions.</li>
<li>The expected value of <code>g</code>&nbsp;(given above) over samples drawn from <code>q</code> is the same as the expected value of <code>f</code>over samples from&nbsp;<code>p</code>.</li>
</ul>
<p data-adtags-visited="true">This is great, and I don’t know if you noticed, but I removed any restriction there that <code>p</code> or <code>q</code> be distributions only on 0.0 to 1.0; this technique works for weighted distributions of doubles over any support!</p>
<hr>
<p data-adtags-visited="true"><strong>Aside:</strong> We can weaken our restriction that <code>q</code> have the same support as <code>p</code>; if we decide that <code>q</code> can have zero weight for particularly unimportant regions, where, say, we know that <code>f(x)*p.Weight(x)</code>&nbsp;is very small, then that’s still going to produce a good estimate.</p>
<p data-adtags-visited="true"><strong>Aside:</strong> Something I probably should have mentioned before is that all of the techniques I’m describing in this series for estimating expected values require that the expected value <em>exists</em>! Not all functions applied to probability distributions have an expected value because the average value of the function computed on a group of samples might not converge as the size of the group gets larger. An easy example is, suppose we have a standard normal distribution as our <code>p</code>​ and <code>x =&gt; 1.0 / p.Weight(x)</code>&nbsp;as our <code>f</code>. The more samples from <code>p</code> we take, the more likely it is that the average value of <code>f</code> gets <em>larger</em>!</p>
<hr>
<p data-adtags-visited="true">However, it’s not all sunshine and roses. We still have two problems, and they’re pretty big ones:</p>
<ul>
<li>How do we find a good-quality <code>q</code> distribution?</li>
<li><strong>We need to know the normalization constants for both distributions.</strong> If we do not know them ahead of time (because, say, we have special knowledge that the continuous uniform distribution is normalized) then how are we going to compute them?&nbsp; <code>Area(p.Weight)</code>&nbsp;or <code>Area(q.Weight)</code> might be expensive or difficult to compute.It seems like in the general case we still have to solve the calculus problem. 😦</li>
</ul>
<hr>
<p data-adtags-visited="true"><strong>Aside:</strong> The boldface sentence in my last bullet point contains a small but important error. What is it? Leave your guesses in the comments; the answer will be in an upcoming episode.</p>
<hr>
<p data-adtags-visited="true">I’m not going to implement a general-purpose importance sampling algorithm until we’ve made at least some headway on these remaining problems.</p>
<p data-adtags-visited="true"><strong>Next time on FAIC:&nbsp; </strong>It’s Groundhog Day! I’m going to do this entire episode over again; we’re going to make a similar argument — things equal to the same are equal to each other — but starting from a different place. We’ll end up with the same result, and deduce that importance sampling works.</p>

			
			
						</div></div></div>
    </div>
    <footer>
        <div>created by buildstarted &copy; 2020 <a href="/about">about</a></div>
        <div>Share this page on social media: copy and paste this url https://linksfor.dev/</div>
        <div>If you prefer RSS: <a href="https://linksfor.dev/feed.xml">https://linksfor.dev/feed.xml</a></div>
        <div>Customer satisfaction guaranteed to be optional.</div>
    </footer>
    
    <script async defer>
        _dna = window._dna || {};
        _dna.siteId = "linksfor.devs";
        _dna.outlink = true;

        (function () {
            let dna = document.createElement('script');
            dna.type = 'text/javascript';
            dna.async = true;
            dna.src = '//dna.buildstarted.com/t.js';
            let s = document.getElementsByTagName('script')[0];
            s.parentNode.insertBefore(dna, s);
        })();
    </script>
    <noscript><img src="//dna.buildstarted.com/g?siteId=linksfor.devs" /></noscript>
</body>
</html>