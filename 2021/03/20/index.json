[
  {
    "Title": "mathiasbynens/dotfiles",
    "Url": "https://github.com/mathiasbynens/dotfiles/blob/master/.macos",
    "Timestamp": "2021-03-20T03:12:50",
    "Domain": "github.com",
    "Description": ":wrench: .files, including ~/.macos — sensible hacker defaults for macOS - mathiasbynens/dotfiles"
  },
  {
    "Title": "A Universe of Sorts",
    "Url": "http://bollu.github.io/a-hackers-guide-to-numerical-analysis.html",
    "Timestamp": "2021-03-20T02:11:24",
    "Domain": "bollu.github.io",
    "Description": "If xx is a number and x^\\hat x is its approximation, then the are two notions of\nerror:\n absolute errror: ∣x−x^∣|x - \\hat x|. relative error: ∣x−x^∣/∣x∣|x - \\hat x|/|x|. \nSince the relative error is invariant under scaling (x↦αx)(x \\mapsto \\alpha x), we\nwill mostly be interested in relative error.\n §  Significant digits\nThe significant digits in a number are the first nonzero digit and all\nsucceeding digits. Thus 1.7320 has five significant digits. 0.0491 has\nonly three significant digits.\nIt is not transparent to me why this definition is sensible.\n §  Correct Significant digits --- a first stab\nWe can naively define x^\\hat x agrees to xx upto pp significant digits\nif x^\\hat x and xx round to the same number upto pp significant digits.\nThis definition is seriously problematic. Consider the numbers:\n x=0.9949x = 0.9949, x1=1.0x_1 = 1.0, x2=0.99x_2 = 0.99, x3=0.9950x_3 = 0.9950 y=0.9951y = 0.9951, y1=1.0y_1 = 1.0, y2=1.0y_2 = 1.0, y3=0.9950y_3 = 0.9950\nHere, yy has correct one and three significant digits relative to xx,\nbut incorrect 2 significant digits, since the truncation at x2x_2 and y2y_2 \ndo not agree even to the first significant digit.\n §  Correct Significant digits --- the correct definition\nWe say that x^\\hat x agress to xx upto pp significant digits if ∣x−x^∣|x - \\hat x|\nis less than half a unit in the pth significant digit of xx.\n §  Accuracy v/s precision\n Accuracy: absolute or relative error of a quantity. Precision: accuracy with which basic arithmetic +, -, *, / are performed.for floating point, measured by unit round-off (we have not met this yet).\nAccuracy is not limited by precision: By using fixed precision arithmetic,\nwe can emulate arbitrary precision arithmetic. The problem is that often\nthis emulation is too expensive to be useful.\n §  Backward, Forward errors\nLet y=f(x)y = f(x), where f:R→Rf: \\mathbb R \\rightarrow \\mathbb R. Let us compute y^\\hat y as an approximation to \nyy, in an arithmetic of precision uu. How do we measure the quality of y^\\hat y?\n In many cases, we maybe happy with an y^\\hat y such that the relative error betweenyy and y^\\hat y is equal to uu: we did the best we can with the precisionthat was given. This is the forward error.  An alternative question we can ask is, for what δx\\delta x do we have that y^=f(x+δx)\\hat y = f(x + \\delta x). That is, how far away from the input do we need to stray, to get a matching output? There maybe many such δx\\delta x,so we ask for min⁡∣δx∣\\min |\\delta x|. We can divide this error by xx as a normalization factor. This is the backward error.\n\nThere are two reasons we prefer backward error.\n It converts error analysis into \"data analysis\". The data itself tendsto be uncertain. If the error given by the backward analysis is smallerthan the data uncertainty, then we can write off our error as beingtoo small. Since for all we know, we have 'fixed' the uncertain datawith our small error. It reduces the question of error analysis into perturbation theory,which is very well understood for a large class of problems.\n §  Backward stable\nA method for computing y=f(x)y = f(x) is called backward stable\nif it produces a y^\\hat y with small backward error. That is, we need a \nsmall δx\\delta x such that y^=f(x+δx)\\hat y = f(x + \\delta x).\n §  Mixed forward-backward error\nWe assume that addition and subtraction are backward stable, where uu\nis the number of significant digits to which our arithmetic operations\ncan be performed:\nx±y=x(1+Δ)±y(1+Δ)∀∣Δ∣≤u\nx \\pm y = x(1 + \\Delta) \\pm y(1 + \\Delta) \\forall |\\Delta| \\leq u\n\nAnother type of error we can consider is that of the form:\ny^+δy=f(x+Δx)                                                                              \n\\hat y + \\delta y = f(x + \\Delta x)\n\nThat is, for a small perturbation in the output (δy)(\\delta y), we can get a\nbackward error of δx\\delta x. This is called as mixed forward backward error.\n\nWe can say that an algorithm with mixed-forward-backward-error is stable iff:\ny^+δy=f(x+Δx)∣Δy∣/∣y^∣<ϵ∣Δx∣/∣x^∣<ηϵ,η are small\n\\begin{aligned}\n&\\hat y + \\delta y = f(x + \\Delta x) \\\\\n&|\\Delta y|/|\\hat y| < \\epsilon \\\\\n&|\\Delta x|/|\\hat x| < \\eta \\\\\n&\\text{$\\epsilon, \\eta$ are small}\n\\end{aligned}\n\nThis definition of stability is useful when rounding errors are the dominant\nform of errors.\n §  Conditioning\nRelationship between forward and backward error is govered by conditioning:\nthe sensitivity of solutions to perturbations of data. Let us have an approximate\nsolution y^=f(x+δx)\\hat y = f(x + \\delta x). Then:\ny^−y=f(x+δx)−f(x)=f′(x)δx+O((δx)2)(y^−y)/y=(xf′(x)/f(x))(Δx/x)+O((Δx)2)(y^−y)/y=c(x)(Δx/x)+O((Δx)2)c(x)≡∣xf′(x)/f(x)∣\n\\begin{aligned}\n&\\hat y - y = f(x + \\delta x) - f(x) = f'(x) \\delta x + O((\\delta x)^2) \\\\\n&(\\hat y - y)/y = (x f'(x)/f(x)) (\\Delta x/x) + O((\\Delta x)^2) \\\\\n&(\\hat y - y)/y = c(x) (\\Delta x/x) + O((\\Delta x)^2)\\\\\n&c(x) \\equiv |x f'(x)/f(x)|\n\\end{aligned}\n\nThe quantity c(x)c(x) measures the scaling factor to go from the relative\nchange in output to the relative change in input. Note that this is a property\nof the function ff, not any particular algorithm.\n §  Example: log⁡x\\log x\nIf f(x)=log⁡xf(x) = \\log x, then c(x)=∣(x(log⁡x)′)/log⁡x∣=∣1/log⁡x∣c(x) = |(x (\\log x)') / \\log x| = |1/\\log x|. This\nquantity is very large for x≃1x \\simeq 1. So, a small change in xx can \nproduce a drastic change in log⁡x\\log x around 11.\n Note the the absolute change is quite small: log⁡(x+δx)≃log⁡x+δx/x\\log(x + \\delta x) \\simeq \\log x + \\delta x/x.However, relative to log⁡x\\log x, this change of δx/x\\delta x/x is quite large.\n §  Rule of thumb\nWe now gain access to the useful rule:\nforward error≲condition number×backward error\n\\text{forward error} \\lesssim \\text{condition number} \\times \\text{backward error}\n\n Glass half empty: Ill-conditioned problems can have large forward error. Glass half full: Well-conditioned problems do not amplify error in data.\n §  Forward stable\nIf a method produces answers with forward errors of similar magnitude to those\nproduced by a backward stable method, then it is called forward stable. \nBackward stability implies forward stability, but not vice-versa (TODO: why?)\n §  Cancellation\nConsider the following program:\n#include <cmath>\n#include <stdio.h>\n\nint main() {\n    double x = 12e-9;\n    double c = cos(x);\n    double one_sub_c = 1 - c;\n    double denom = x*x;\n    double yhat = one_sub_c / denom;\n\n    printf(\"x:         %20.16f\\n\"\n           \"cx:        %20.16f\\n\"\n           \"one_sub_c: %20.16f\\n\"\n           \"denom:     %20.16f\\n\"\n           \"yhat:      %20.16f\\n\",\n            x, c, one_sub_c, denom, yhat); \n}\n\nwhich produces the output:\nx:           0.0000000120000000\ncx:          0.9999999999999999\none_sub_c:   0.0000000000000001\ndenom:       0.0000000000000001\nyhat:        0.7709882115452477\n\nThis is clearly wrong, because we know that (1−cos⁡x)/x2)≤1/2(1-\\cos x)/x^2) \\leq 1/2.\nThe reason for this terrible result is that:\n we know cos⁡x\\cos x to high accuracy, since xx was some fixed quantity. 1−cos⁡x1 - \\cos x converted the error in cos⁡x\\cos x into its value. 1−cos⁡x1 - \\cos x has only one significant figure. This makes it practically useless for anything else we are interested in doing.\nIn general:\nx≡1+ϵerror of order ϵy≡1−x=ϵvalue of order ϵ\n\\begin{aligned}\n&x \\equiv 1 + \\epsilon \\text{error of order $\\epsilon$} \\\\\n&y \\equiv 1 - x = \\epsilon \\text{value of order $\\epsilon$} \\\\\n\\end{aligned}\n\nThat is, subtracting values close to each other (in this case, 11 and xx)\nconverts error order of magnitude into value order of magnitude. \nAlternatively, it brings earlier errors into promience as values.\n §  Analysis of subtraction\nWe can consider the subtraction:\nx=a−b;x^=a^−b^a^=a(1+Δa)b^=b(1+Δb)∣x−x^x∣=∣−aΔa−bΔba−b∣=∣−aΔa−bΔb∣∣a−b∣=∣aΔa+bΔb∣∣a−b∣≤max⁡(∣Δa∣,∣Δb∣)(∣a∣+∣b∣)∣a−b∣\n\\begin{aligned}\n&x = a - b; \\hat x = \\hat a - \\hat b \\\\\n&\\hat a = a(1 + \\Delta a) \\\\\n&\\hat b = b(1 + \\Delta b) \\\\\n&\\left| \\frac{x - \\hat x}{x} \\right|  \\\\\n&= \\left| \\frac{-a\\Delta a - b\\Delta b}{a - b} \\right| \\\\\n&= \\frac{|-a\\Delta a - b\\Delta b|}{|a - b|} \\\\\n&=  \\frac{|a\\Delta a + b\\Delta b|}{|a - b|} \\\\\n&\\leq  \\frac{\\max(|\\Delta a|, |\\Delta b|) (|a| + |b|)}{|a - b|}\n\\end{aligned}\n\nThis quantity will be large when ∣a−b∣≪∣a∣+∣b∣|a - b| \\ll |a| + |b|: that is, when\nthere is heavy cancellation in the subtraction to compute xx. \n §  Underflow\n#include <cmath>\n#include <stdio.h>\n\nint main() {\n    double x = 1000;\n    for(int i = 0; i < 60; ++i) {\n        x = sqrt(x);\n    }\n    for(int i = 0; i < 60; ++i) {\n        x = x*x;\n    }\n    printf(\"x: %10.20f\\n\", x);\n}\n\nThis produces the output:\n./sqrt-pow-1-12\n...\nx: 1.00000000000000000000\n\nThat is, even though the function is an identity function, the answer collapses\nto 1. What is happening?\n §  Computing (ex−1)/x(e^x - 1)/x\nOne way to evaluate this function is as follows:\ndouble f(double x) { return x == 0 ? 1 : (pow(M_E, x) - 1) / x; }\n\nThis can suffer from catastrophic cancellation in the numerator. When\nxx is close to 00, exe^x is close to 1, and ex−1e^x - 1 will magnify the\nerror in exe^x.\ndouble f(double x) { \n   const double y = pow(M_E, x);\n   return y == 1 ? 1 : (y - 1) / log(y);\n}\n\nThis algorithm seems crazy, but there's insight in it. We can show that\nthe errors cancel! The idea is that neither (y−1)(y - 1) nor log⁡y\\log y are\nparticularly good, the errors accumulated in them almost completely\ncancel out, leaving out a good value:\nassume y^=11=y^≡ex(1+δ)log⁡1=log⁡(ex)+log⁡(1+δ)x=−log⁡(1+δ)x=−δ+O(δ2)\n\\text{assume $\\hat y = 1$} \\\\\n1 = \\hat y \\equiv e^x(1 + \\delta) \\\\\n\\log 1 = \\log (e^x ) + \\log(1 + \\delta) \\\\\nx = -\\log(1 + \\delta) \\\\\nx = -\\delta + O(\\delta^2)\n\nIf y^≠1\\hat y \\neq 1:\nf^=(y^−1)/log⁡y^=(1+ϵ3)(y^−1)(1+ϵ+1)/(log⁡y^(1+ϵ2))\n\\hat f = (\\hat y - 1)/\\log{\\hat y} = (1+\\epsilon_3)(\\hat y - 1)(1 + \\epsilon+1)/(\\log \\hat y(1 + \\epsilon_2))\n\n §  IEEE floating point fun: +0 and -0 for complex analysis\n Rather than think of +0 and -0 as distinct numerical values, think of their sign bit as an auxiliary variable that conveys one bit of information (or misinformation) about any numerical variable that takes on 0 as its value.\nWe have two types of zeroes, +0 and -0 in IEEE-754. These are used in some\ncases. The most famous is that 1/+0=+∞1/+0 = +\\infty, while 1/−0=−∞1/-0 = -\\infty. Here,\nwe proceed to discuss some complex-analytic considerations.\n Therefore. implementers of compilers and run-time libraries bear a heavy burden of attention to detail if applications programmers are to realize the full benefit of the IEEE style of complex arithmetic. That benefit deserves Some discussion here if only to reassure implementers that their assiduity will be appreciated.\n−1+0i=+0+i−1−0i=+0−i\n\\sqrt{-1 + 0 i} = +0 + i \\\\\n\\sqrt{-1 - 0 i} = +0 - i \\\\\n\nThese will ensure that z∗=(z)∗\\sqrt{z*} = (\\sqrt{z})*:\ncopysign(1,+0)=+1copysign(1,−0)=−1\n\\texttt{copysign}(1, +0) = +1 \\\\\n\\texttt{copysign}(1, -0) = -1 \\\\\n\nThese will ensure that copysign(x,1/x)=x\\texttt{copysign}(x, 1/x) = x when x=±∞x = \\pm \\infty.\nAn example is provided where the two limits:\nf(x+i0)=lim⁡y→0−f(x+iy)f(x+i0)=lim⁡y→0−f(x+iy)\n\\begin{aligned}\n&f(x + i0) = \\lim_{y \\rightarrow 0-} f(x + i y) \\\\\n&f(x + i0) = \\lim_{y \\rightarrow 0-} f(x + i y) \\\\\n\\end{aligned}\n\n §  Complex-analytic considerations\nThe principal branch of a complex function is a way to select one branch\nof a complex-function, which tends to be multi-valued. A classical example\nis the argument function, where arg⁡(reiθ=θ\\arg(r e^{i \\theta} = \\theta. \nHowever, this is ambiguous: we can map θ↦θ+2π\\theta \\mapsto \\theta + 2 \\pi\nand still have the same complex number. So, we need to fix some standard.\nWe usually pick the \"branch\" where 0≤θ<2π0 \\leq \\theta < 2 \\pi.\nIn general, we need to carefully handle what happens to the function at\nthe discontinuity.\n What deserves to be undermined is blind faith in the power of Algebra. We should not believe that the equivalence class of expressions that all describe the same complex analytic function can be recognized by algebraic means alone, not even if relatively uncomplicated expressions are the only ones considered.\n §  References\n Accuracy and stability of numerical algorithms Branch Cuts for complex elementary functions, or much ado about Nothing's Sign Bit"
  },
  {
    "Title": "What I wish I knew before building a Shopify App",
    "Url": "https://ma.ttias.ch/what-i-wish-i-knew-before-building-a-shopify-app.html",
    "Timestamp": "2021-03-20T02:11:22",
    "Domain": "ma.ttias.ch",
    "Description": "I spent the last week building an App for the Shopify Marketplace. It was unlike anything I've done before on the Web. Knowing some of the things I'm going to share, before you start building, could save you some headache."
  },
  {
    "Title": "Linus Torvalds on how AMD and Intel are changing how processor interrupts are handled | ZDNet",
    "Url": "https://www.zdnet.com/article/linus-torvalds-on-how-amd-and-intel-are-changing-how-processor-interrupts-are-handled/",
    "Timestamp": "2021-03-20T02:11:20",
    "Domain": "www.zdnet.com",
    "Description": "AMD and Intel recently started changing how the x86 chip architecture will handle exceptions. Linus Torvalds, in turn, gave his take on their new approach for forthcoming generations of CPUs."
  },
  {
    "Title": "Release 1.0.0-preview10 · microsoft/reverse-proxy",
    "Url": "https://github.com/microsoft/reverse-proxy/releases/tag/v1.0.0-preview10",
    "Timestamp": "2021-03-20T01:09:50",
    "Domain": "github.com",
    "Description": "This release supports .NET Core 3.1 and .NET 5.0. See Getting Started.\nThe Yarp.ReverseProxy packages are available on NuGet.org.\nMajor changes and features:\n\n[Breaking change] Rebranded from Micro..."
  },
  {
    "Title": "About pull request merges - GitHub Docs",
    "Url": "https://docs.github.com/en/github/collaborating-with-issues-and-pull-requests/about-pull-request-merges",
    "Timestamp": "2021-03-20T01:09:47",
    "Domain": "docs.github.com",
    "Description": "You can merge pull requests by retaining all the commits in a feature branch, squashing all commits into a single commit, or by rebasing individual commits from the head branch onto the base branch."
  },
  {
    "Title": "Azure SDK: Mixed Reality and Event Grid Client Libraries for .NET Go GA -- Visual Studio Magazine",
    "Url": "https://visualstudiomagazine.com/articles/2021/03/19/azure-sdk.aspx",
    "Timestamp": "2021-03-20T01:09:44",
    "Domain": "visualstudiomagazine.com",
    "Description": "The latest update to the Azure SDK adds Mixed Reality and Event Grid client libraries for .NET to the cloud platform's dev tooling, along with Java Azure Core library and other features."
  }
]