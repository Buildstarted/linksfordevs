[
  {
    "Title": "Adam Storr - Azure Functions Not Loading My Dependencies, What Have I Missed?",
    "Url": "https://adamstorr.azurewebsites.net/blog/azure-functions-not-loading-my-dependencies-what-have-i-missed",
    "Timestamp": "2021-04-06T03:04:33",
    "Domain": "adamstorr.azurewebsites.net",
    "Description": "Adam Storr - Powered by coffee!"
  },
  {
    "Title": "Predictive Coding has been Unified with Backpropagation - LessWrong",
    "Url": "https://www.lesswrong.com/posts/JZZENevaLzLLeC3zn/predictive-coding-has-been-unified-with-backpropagation",
    "Timestamp": "2021-04-06T01:04:15",
    "Domain": "www.lesswrong.com",
    "Description": "Artificial Neural Networks (ANNs) are based around the backpropagation\nalgorithm. The backpropagation algorithm allows you to perform gradient descent\non a network of neurons. When we feed training data through an ANNs, we use the\nbackpropagation algorithm to tell us how the weights should change.\n\nANNs are good at inference problems. Biological Neural Networks (BNNs) are good\nat inference too. ANNs are built out of neurons. BNNs are built out of neurons\ntoo. It makes intuitive sense that ANNs and BNNs might be running similar\nalgorithms.\n\nThere is just one problem: BNNs are physically incapable of running the\nbackpropagation algorithm.\n\nWe do not know quite enough about biology to say it is impossible for BNNs to\nrun the backpropagation algorithm. However, \"a consensus has emerged that the\nbrain cannot directly implement backprop, since to do so would require\nbiologically implausible connection rules\"[1].\n\nThe backpropagation algorithm has three steps.\n\n 1. Flow information forward through a network to compute a prediction.\n 2. Compute an error by comparing the prediction to a target value.\n 3. Flow the error backward through the network to update the weights.\n\nThe backpropagation algorithm requires information to flow forward and backward\nalong the network. But biological neurons are one-directional. An action\npotential goes from the cell body down the axon to the axon terminals to another\ncell's dendrites. An axon potential never travels backward from a cell's\nterminals to its body.\n\n\n\nHEBBIAN THEORY\nPredictive coding is the idea that BNNs generate a mental model of their\nenvironment and then transmit only the information that deviates from this\nmodel. Predictive coding considers error and surprise to be the same thing.\nHebbian theory is specific mathematical formulation of predictive coding.\n\nPredictive coding is biologically plausible. It operates locally. There are no\nseparate prediction and training phases which must be synchronized. Most\nimportantly, it lets"
  }
]