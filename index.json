[
  {
    "Title": "Okay, Color Spaces — ericportis.com",
    "Url": "https://ericportis.com/posts/2024/okay-color-spaces/",
    "Timestamp": "2024-02-22T23:04:03",
    "Domain": "ericportis.com",
    "Description": "What is a “color space?”",
    "Confidence": 0.9841154
  },
  {
    "Title": "3D modeling with ChatGPT - Solidified ephemerality",
    "Url": "https://0110.be/posts/3D_modeling_with_ChatGPT_-_Solidified_ephemerality",
    "Timestamp": "2024-02-22T23:04:01",
    "Domain": "0110.be",
    "Description": "I have asked ChatGPT to generate 3D models. ChatGPT can not generate 3D models directly but 3D models can generated via intermediary OpenSCAD scripts: OpenSCAD provides a scripting language to describe objects which can be combined to form 3D models. ChatGPT understands the syntax of this scripting language and generates perfectly cromulent scripts. I have asked two versions of ChatGPT to generate a 3D model of a house, a cat, a stick figure, a chair and a tree.  The results are interesting…",
    "Confidence": 0.9865798
  },
  {
    "Title": "How well do Lc0 networks compare to the greatest transformer network from DeepMind?",
    "Url": "https://lczero.org/blog/2024/02/how-well-do-lc0-networks-compare-to-the-greatest-transformer-network-from-deepmind/",
    "Timestamp": "2024-02-22T23:04:01",
    "Domain": "lczero.org",
    "Description": "To explore the performance of Lc0 networks relative to DeepMind’s state-of-the-art transformer networks, we embarked on a comparative analysis, inspired by the methodologies detailed in DeepMind’s latest publication. Our objective was to closely align our testing approach for Lc0 networks with the evaluation framework applied by DeepMind, allowing for a direct comparison of results.",
    "Confidence": 0.98475266
  },
  {
    "Title": "No Simple Answers In Stereo",
    "Url": "https://joe-steel.com/2024-02-19-No-Simple-Answers-In-Stereo.html",
    "Timestamp": "2024-02-22T23:04:00",
    "Domain": "joe-steel.com",
    "Description": "There was some continued back and forth on Mastodon about stereo conversions. Mac Stories contributor Jonathan Reed asked a couple questions:\n\nWhat’s your view on the best converted movies? Do you think they hold up just as well vs (non-bad) native 3D movies?\n\nI am not picking on Jonathan, but crediting him with a question that seems very reasonable. It seems logical to ask for an example of what’s working, but that’s much more difficult to do than it sounds. It’s kind of like proving a negative (even though this is a positive?)\nIf there is a best conversion, you’re unlikely to be aware of it at all, because the audience usually only remembers technical errors, or discomfort. There’s nothing outwardly impressive about a good conversion, or good native stereo, and anything that was held up as a good conversion would be picked apart with intense scrutiny to prove that it’s not actually good.\nAdd on top of that the point Todd Vaziri and I were trying to make in that thread, and in our feedback to the Accidental Tech Podcast, that there are a variety of methods employed in various shots in various movies. It is not as homogeneous as it appears to be in untrustworthy marketing, or that silly “real vs. fake” site.\nThere’s no binary bit on the movie that flips if one shot in a native 3D movie is a post conversion shot, what percentage of shots need to be rendered fully 3D in an animated feature, or a blockbuster with 2,000+ effects heavy shots.\nI know that is deeply unsatisfying as an answer, and the follow up question would be for movies that don’t work well. For professional reasons I wouldn’t ever spell that out.\nUltimately, I know that just saying that it’s nuanced and complicated is not very helpful or informative to people that want to understand stereo. For those that want a quick answer on whether a movie is worth watching in 3D on their Apple Vision Pro, there’s nothing so simple as a list.\nThe best I can do is talk through common problems in stereo. To do that we need to talk about some terms. To do that I’m going to need to bore the fuck out of you.\nNative Stereo Photography\nThis is shot with two cameras. Usually this involves a cumbersome rig where the cameras have to be exactly aligned, have matching apertures, matching focal distances, and are slightly physically offset. To get them close enough together they’re often arranged with one camera pointing straight up, and it gets it’s light from a beam splitter. A semi-transparent plane of glass that lets light pass through to the main camera, but also reflects down to the vertical camera. This is an enormous pain in the ass, and it’s very easy to have something be just a little off in a way that won’t be clear until later.\nWhen the stereographer and director finish shooting, they can adjust the convergence by horizontally transforming the photography which pushes and pulls things in order out of the screen depending on where the left and right eye converge. However, they can’t adjust the interaxial without throwing away one of the eyes and doing it over with conversion. That means they might be more conservative in all of their choices to reduce the chance that there’s an error.\nThings to look for are misalignment. If the left and right eyes have an angular difference between them, or skew slightly. Your eyes are looking for horizontal disparity so vertical shifts mess it up a little. This is abundant in iPhone 15 Pro Spatial Videos because of Apple’s attempts to compensate for the mismatched lenses.\nAnother big thing is color shifts from the beam splitter. Sometimes that could manifest as a constant shift, or it could be transient if the camera rig is moving and the light catches differently. It’s possible to color correct the views to get a closer match but uncorrected differences might appear to shimmer when your brain processes the slightly different hues and values.\nSpecular reflections. Think of bright pings of light on glass or chrome, often from a distant, but bright light source. One eye might get the ping of light and the other eye doesn’t. A mismatch like that can appear to glow, or shimmer, and could be uncomfortable to look at. To correct for this in native stereo the ping might be artificially copied and offset to the other eye, or the value of the ping might be knocked down so it doesn’t draw attention.\nIf you have a visual effects shot where native stereo plate photography is combined with rendered assets you might see issues that you wouldn’t get if it was a post conversion. Like a bluescreen or greenscreen shot where the work done to extract the photographic element from the screen color where the extraction is not an exact match. A common issue is flyaway hairs, those thin wisps of hair that are always difficult, could be in one eye but not the other, or trimmed in an odd way.\nFlyaway hairs in non-VFX native stereo shots should always look pretty good, but depending on how deep the background is behind them you might be surprised to notice them more than you would in a 2D movie.\nThis doubling of work - and the need for it to match - is also what makes something like wire removal paint much more difficult. It’s easy to make each single view of paint be internally consistent and work, but then to make sure those paint adjustments match between both photographic plates is a pain that you don’t have to deal with in conversion.\nIt used to be very difficult to get 3D matchmove solves that were rock solid for both eyes. Meaning something could appear to float away where native stereo photography and CG met. Very rare, but maybe if you’re watching something old things might seem to drift or breathe.\nAnother thing 2D VFX artists take for granted is being able to use masks/mattes/rotosplines - and only having to do it once without thinking about where the matte sits in depth. The matte could be used to grade the background, or it could be to help extract a person. Those rotosplines need to be done for two eyes, and they need to match the plate photography, and their companion spline, including motionblur. A soft mask extending back along an angled surface will need to have depth that matches that angle in the other eye. So you end up doing the post conversion kinds of steps on the mattes applied to your native stereo left and right images to make them match and sit in depth, but are constrained to the native stereo plates as well.\nNative Stereo Renders\nNative stereo renders in animated movies, or for shots in a VFX heavy movie that don’t have photography, have their own pros and cons. Even those “all CG” shots are not always fully rendered in stereo for left and right eyes. The flat version of the movie will be done while the stereo version of the movie lags behind a little bit. That means that rendering the offset eye might reveal issues where an old version of a shader was used, or an asset changed since the original shot completed. It can be much more of a puzzle.\nPeople also can do anything they want to with their cameras because they are no longer constrained by physics. That means you either get mind-bending stuff, like stuff sticking out of the screen that would really be a considerable distance away from the audience, requiring enormous interaxial camera offsets, or sometimes they’ll just make it really flat, even though they have the ability to do whatever they want.\nYou also still have some of the same issues presented by bright specular pings being in one eye, but not the other, but also that they might sizzle because bright, distant light sources need more raytracing samples (tiny thing far away gets more missed rays than hits).\nYou might be like, so what? Just turn up the samples, right? That’s easier said than done in some cases, especially if the 2D version of the movie is done already, or the rendering engine just can’t resolve some very bright, distant point of light with enough samples that won’t take 3 months to render. The sample noise will sizzle differently between the two eyes and appear to glow. There are ways to cut out pixels from the other eye, or median filter it, or what have you, but if it’s uncorrected you’ll see sizzling pixels.\nNative stereo renders do have one fun trick and that’s the depth map (Z) channel that is normally used for depth of field focus effects. It is a image where every pixel corresponds to how far away something is from camera. It can be used to create an exact offset based on the stereo camera pair. This makes it kind of like post conversion where fake depth is used to offset 2D data from one camera view to the other. That means you can offset things like rotosplines, or other 2D elements, to match the depth of your 3D exactly. I do mean, exactly, since it will be at exactly the depth from the depth channel. Effectively like using a projector from the location of your left eye camera, and then viewing it from the location of your right eye camera.\nThis also means that parts of the render from the left or the right eye can be offset by the depth data to patch or supplement renders from the other eye. Think of it like sneaking in a little conversion. This can save render time, and help with various problems matching the eyes.\nIt can be as specific as using a render for parts of a character (eyes, fur, screen-right edges), or parts of lighting components (just the specular, just the reflection, refraction, or just the diffuse). \nTo a purist, it might sound like an anathema to mix and match, because a purist would assume that the highest quality is from matching renders. Really most people would fail a Pepsi challenge on fully rendered shots vs. hybridized shots. The philosophical concerns don’t matter as much as the final set of images being coherent.\nFor this reason it is absolute bunk to call all animated movies “real” 3D, or to be able to claim from your seat in an audience what’s rendered from scratch and what’s not.\nPost Conversion\nConversions are popular because they require less time on set, use more flexible camera setups, and cause fewer problems for the crew that’s mainly concerned with the 2D version of the movie. That also means they can adjust the depth of everything ad infinitum. That can mean a more creative, and thoughtful use of stereo because they can evaluate the results and change it in a way they can’t do easily on set, where they are more likely to be conservative, or stuck with what they shot.\nConversions are also associated with people looking to make a quick buck on ticket sales, and reducing labor costs on the conversion to get as much profit as possible.\nThat means it’s likely you’ll see the places where conversions fall apart because of time and budget constraints, which was very common in earlier post conversions when studio execs felt like they needed to rush. You might recall movies where only part of the film was in stereo, and they wanted you to take on or off your glasses in the theater.\nThere was also the quality issue from the assumption that people were going to watch these in theaters were they couldn’t hit pause. The home video part never panned out - but maybe it will with products like the Apple Vision Pro.\nMajor issues stem from the approach a conversion house takes when presented with 2D footage. Most places will create a 3D space in the computer and camera in order to “accurately” produce the offset eye. I’ve heard of some places where people just cut out and move stuff around to wherever it feels right for them, or use image based algorithms to create a fake depth map to drive the stereo offset, but the map might have holes and errors where the algorithm guessed wrong. I’ve never worked at a place that did these things so I don’t have insight to share about their thought process, so let’s move on to placed-in-3D stuff.\nTo do that, matchmove needs to be done where the camera is solved for, and elements in the shot get rough geometry. Since this often needs to happen for the 2D VFX portion of a movie, this is considered some synergistic cost savings. The plate can be projected on that rough geometry in a 3D software package, and then an offset camera can be dialed for the interaxial and convergence values that feel right for the shot, and in the context of the sequence. That projection on to the hard geo is really just to dial things, the geo won’t be used raw, it’ll be cut by rotos, blurred in the depth channel, etc. to make something that’s softer than the hard facets of rough geo.\nThe photography does get rotoed, however only one eye needs to be rotoed, not a complicated matching pair. The photography also needs some degree of paint work to be done to it to clean up the area that was occluded by the foreground. This can be as simple as painting out a sliver, or halo, around where a character occludes the background, or it can be a more extensive affair.\nThat means the same paint needs to be used in both eyes to account for any minor variance between the paint and the original plate. While the audience could never tell that the background was painted (really, I absolutely promise you can’t tell because shit is painted all the time in regular-old-vanilla shots and people truly don’t have an inkling) the audience can tell if there’s only paint in one eye’s view for the same reason as it would be a problem to have mismatched paint in native stereo.\nPaint removal includes things like flyaway hairs which will be painted out and rotoed, or luminance keyed, to bring them back. They will match exactly between the two eyes, unlike mismatched keys of native stereo, but they will need to be placed in depth.\nIf you want to tell anything about the quality of a conversion, look for those flyaway hairs. They should be there, and they should also be at a sensible depth relative to the rest of the hair. not way behind, or in front of the actor.\nThe actor should have internal depth, which is usually derived from the rough matchmove geometry. They should have a nose past their eyes, and their ears and neck should be back. They should never feel like a cardboard cut-out unless they are far away from the camera, like background actors, or a really wide shot of them in an environment.\nSpeaking of environments, the two biggest problems there are highly reflective and refractive surfaces. If there’s a shop window, with reflections, and the name of the shop painted on the glass, the reflections should not be at the depth we see through the window or they will look like they’re at the depth of the walls and surfaces inside the shop. They need to be at the depth of whatever they’re reflecting. That means the reflections must be painted out, along with the lettering on the glass. The lettering needs to then be placed over the shop interior at the depth of where the glass plane is on the facade, and then the reflections need to be added (reflections are additive, but that is a rant for another day). Then that reconstructed window is used for both the left and right eye. No, you won’t be able to tell that work was done because you, in the audience, don’t have the 2D version of the movie to look at and compare it to, and the work will all be so internally consistent that it wouldn’t register for you to check without the knowledge that this kind of work needed to happen. In the abstract this knowledge might cause philosophical conflict — unclean! Impure! But I assure you the director isn’t anywhere near as precious about this as you might think.\nIf a conversion house omits that level of work, and just lets the shop window be flat to the depth of the building facade, or lets everything in the window go deeper, including the reflections and lettering, then it’s going to look wrong to any casual viewer.\nThis can also be applied to things like shiny cars, or reflective bodies of water. \nAs for refraction, that will be most obvious with things like thick, curved glass, and glassware filled with liquids. Bottles, wine glasses, thick reading glasses, etc. The edges of the glass, where the index of refraction creates that defined shape that’s almost solid, should be at the depth of the glass in 3D space. The interior core of the glass, where you see through bent light of the objects behind it should be closer to the depth of the object behind it (accounting for any magnification). Then there should be an artful blend from that edge depth to that core depth in whatever fake depth channel is being used. Anything like reflections should be painted out and added on top; like the shop window.\nWhat you do not want is for the glass object to feel like everything inside of it is at the depth of the glass surface. It will look painted on, not like you’re seeing through the glass.\nThis also goes for lens flares, which are reflections and refractions from light hitting the lens element at certain angles and then the filmback. The lens flare needs to be painted out and reconstructed exactly, then the source of the flare needs to be offset to match the location of the light source, and then the little bits of the lens flare need to be offset based on where the light source moved to in relation to the center of frame, which would be the center of the lens. Oftentimes a lens flare plugin in a compositing package will be used to help replicate the original flare, or at least used as a guide for placement.\nThis leaves other camera based effects like grain, and heavy vignetting. The entire plate needs to be degrained as step 0 in this process, and then regrained, taking into account any extra reconstruction work, and also offseting the grain timing for the offset eye. You should never have a stereo offset in your grain (meaning the same pattern reproduced and moved in X) because that puts the grain in depth. If you leave the original grain on your left and right eyes, and do your offsets, then your grain will be painted on to the depth of all the surfaces you reconstructed. That’s extremely bad, and extremely obvious when it happens.\nGrain should be offset in time (effectively randomized noise seed) so there is never a matching pattern your brain will try to place in depth. The result is a fuzz that exists around screen depth. Your eye doesn’t identify it really having any depth unless the grain is heavy and can almost take on the quality of an atmosphere, that flattens things, in which case the decision maybe made to reduce grain for both left and right eyes.\nUsually people can get away without treating vignetting, unless it’s heavy —the real artsy stuff. Then the conversion house needs to remove the original vignetting and add it at screen depth (no offset) with everything else in stereo being placed behind it. You don’t want something popping out through vignetting —that doesn’t make any sense.\nThe really good news is that because this uses the same greenscreen and bluescreen, the edges don’t get screwy, and any combination with CG can be made exactly, because everything can be placed together in the same shared space. When done well it help the director shoot how they’re comfortable shooting, and get the results they want for both 2D and 3D.\nHybrid\nReally it doesn’t do any good to make any sweeping statements about quality based on method, and especially after taking into account that films will blend various parts in ways that are often invisible to you.\nIdealogical purity really doesn’t exist in either the realm of home video or stereo video, so try not to get too wound up about it. Always try to watch the best version of something you can, and suits your current situation, but don’t get yourself upset about something in the abstract.\nIf you really want to understand the quality of the 3D work inspect those common problem spots I mentioned. Pause your movie and open and close your left and right eyes. Look at the refraction, the reflections, the flyaway hairs.\nSeparately, judge whether it was worth seeing it in 3D at all. Did that add to your experience for this particular film? Was anything about it essential, or memorable? People talk about the 3D of the Avatar movies because James Cameron made it a part of the experience, not just because the native stereo checkbox was ticked.\nNo one is under any obligation to like 3D movies whatsoever, but it’s important that we don’t justify or define that dislike based on a simple binary that isn’t true.",
    "Confidence": 0.9654946
  },
  {
    "Title": "Modding Plugins back into Xcode",
    "Url": "https://bryce.co/xcode-plugin-loader/",
    "Timestamp": "2024-02-22T23:04:00",
    "Domain": "bryce.co",
    "Description": "Adding plugin support to Xcode 14+, the hard way",
    "Confidence": 0.99807054
  },
  {
    "Title": "Women in mathematics – a case study",
    "Url": "https://liorpachter.wordpress.com/2024/02/22/women-in-mathematics-a-case-study/",
    "Timestamp": "2024-02-22T23:04:00",
    "Domain": "liorpachter.wordpress.com",
    "Description": "The following describes harassment experienced by a woman who is a professor of mathematics, whose words I’m posting here (anonymously and with names changed) with her permission. ===========…",
    "Confidence": 0.59147656
  },
  {
    "Title": "Ruby could use a Heap",
    "Url": "https://homo-sapiens-reviewed.bearblog.dev/ruby-needs-a-heap/",
    "Timestamp": "2024-02-22T23:04:00",
    "Domain": "homo-sapiens-reviewed.bearblog.dev",
    "Description": "If you’re studying for coding interviews, I recommend Python, which is a shame because I am a Ruby programmer. The only major drawback to using Ruby is that ...",
    "Confidence": 0.9971148
  },
  {
    "Title": "In a GenAI world. Only Identity Matters.",
    "Url": "https://calebsima.com/2024/02/08/in-a-genai-world-only-identity-matters/",
    "Timestamp": "2024-02-22T23:04:00",
    "Domain": "calebsima.com",
    "Description": "I recently had to do some thinking about Identity and with the recent surge of deepfakes, the one thing that is clear is that Identity has become the most critical problem. Obviously identity has a…",
    "Confidence": 0.96219796
  },
  {
    "Title": "Making LLMs worth every penny | Tom Hipwell",
    "Url": "https://tomhipwell.co/reading/making_llms_worth_every_penny/",
    "Timestamp": "2024-02-22T23:03:59",
    "Domain": "tomhipwell.co",
    "Description": "Learning in the open | Tom Hipwell",
    "Confidence": 0.97125924
  },
  {
    "Title": "Productivity Tools are Taxed - Adam Grant",
    "Url": "https://www.adamgrant.info/adhd-tools-are-taxed",
    "Timestamp": "2024-02-22T23:03:59",
    "Domain": "www.adamgrant.info",
    "Description": "Productivity Tools are Taxed - Adam Grant",
    "Confidence": 0.9460183
  },
  {
    "Title": "Matryoshka Representation Learning (MRL) from the Ground Up",
    "Url": "https://aniketrege.github.io/blog/2024/mrl/",
    "Timestamp": "2024-02-22T23:03:58",
    "Domain": "aniketrege.github.io",
    "Description": "What do these scary sounding words mean?",
    "Confidence": 0.9885729
  },
  {
    "Title": "i need some hackers",
    "Url": "https://breadchris.com/blog/i-need-some-hackers/",
    "Timestamp": "2024-02-22T23:03:58",
    "Domain": "breadchris.com",
    "Description": "TL;DR - I need money for a non-profit international cybersecurity competition (open-source, too). Donations here, email chris@breadchris.com for sponsorship opportunities.\nI am running an annual cyber security competition (mcpshsf, soon to be xctf) with my high school computer science teacher, celebrating its 8th? year, on March 14th. When run at NYU, it attracted incredible talent from around the US. Some of the alumni went on to form or join notable security companies/teams ( TrailofBits, Zellic, Google, Uber, etc.",
    "Confidence": 0.97062695
  },
  {
    "Title": "What I Learned Developing with LLMs",
    "Url": "https://www.opslevel.com/resources/what-i-learned-developing-with-llms",
    "Timestamp": "2024-02-22T23:03:58",
    "Domain": "www.opslevel.com",
    "Description": "Peek behind the curtain at how an engineer at OpsLevel developed our own AI assistant.",
    "Confidence": 0.8717809
  },
  {
    "Title": "Large Language Models Are Drunk at the Wheel",
    "Url": "https://matt.si/2024-02/llms-overpromised/",
    "Timestamp": "2024-02-22T23:03:57",
    "Domain": "matt.si",
    "Description": "As an Artificial Intelligence proponent, I want to see the field succeed and go on to do great things. That is precisely why the current exaggerated publicity…",
    "Confidence": 0.9512996
  },
  {
    "Title": "Subprime Intelligence",
    "Url": "https://www.wheresyoured.at/sam-altman-fried/",
    "Timestamp": "2024-02-22T23:03:56",
    "Domain": "www.wheresyoured.at",
    "Description": "Please scroll to the bottom for news on my next big project, Better Offline, coming this Wednesday!\n\nLast week, Sam Altman debuted OpenAI's \"Sora,\" a text-to-video AI model that turns strings of text into full-blown videos, much like how OpenAI's DALL-E turns text into images. These videos — which are usually",
    "Confidence": 0.85336435
  },
  {
    "Title": "That time I almost added Tetris to htop - hisham.hm ",
    "Url": "https://hisham.hm/2024/02/12/that-time-i-almost-added-tetris-to-htop/",
    "Timestamp": "2024-02-22T23:03:56",
    "Domain": "hisham.hm",
    "Description": "Personal webpage of Hisham Muhammad, developer of htop, LuaRocks and GoboLinux.",
    "Confidence": 0.9872236
  },
  {
    "Title": "How to Optimally Trap Points in High-Dimensional Spaces Inside Ellipsoids",
    "Url": "https://www.adrianriv.com/blog/2024/02/19/minimum_volume_ellipsoid/",
    "Timestamp": "2024-02-22T23:03:56",
    "Domain": "www.adrianriv.com",
    "Description": "Adrian Rivera Cardoso is a researcher and software engineer.",
    "Confidence": 0.98301524
  },
  {
    "Title": "Interesting Uses of Ansible’s ternary filter",
    "Url": "https://www.zufallsheld.de/2024/02/21/interesting-use-of-ansible-ternary-filter/",
    "Timestamp": "2024-02-22T23:03:56",
    "Domain": "www.zufallsheld.de",
    "Description": "Some time ago I discovered an interesting use of the ternary-filter in Ansible. A ternary-filter in Ansible is a filter that takes three arguments: a condition, a value if the condition is true and an alternative value if the condition is false. Here’s a simple example straight from Ansible …",
    "Confidence": 0.99295175
  },
  {
    "Title": "Measuring Patterns To Boost Productivity · @jimmyislive",
    "Url": "https://jimmyislive.dev/posts/measuring-patterns-to-boost-productivity/",
    "Timestamp": "2024-02-22T23:03:56",
    "Domain": "jimmyislive.dev",
    "Description": "Written by\n        Jimmy John\n        \n        on February 18, 2024",
    "Confidence": 0.99645543
  },
  {
    "Title": "Open sourcing your games as solo game developer - a game changer",
    "Url": "https://simondalvai.org/blog/open-source-games/",
    "Timestamp": "2024-02-22T23:03:55",
    "Domain": "simondalvai.org",
    "Description": "Why making your games Open Source can help you as a solo independent game developer",
    "Confidence": 0.9896287
  },
  {
    "Title": "Typing A Little Faster",
    "Url": "https://sophiestavely.com/2024/02/22/typing-a-little-faster/",
    "Timestamp": "2024-02-22T23:03:55",
    "Domain": "sophiestavely.com",
    "Description": "I taught myself to type a bit faster over the course of a couple of months. I’d estimate that I spent 5 hours of effort to gain 10 to 20 words per minute (WPM) in speed. Before I started, I mostly …",
    "Confidence": 0.935668
  },
  {
    "Title": "CSS-only DVD Screensaver animation",
    "Url": "https://www.javiermorales.dev/blog/dvd",
    "Timestamp": "2024-02-22T23:03:55",
    "Domain": "www.javiermorales.dev",
    "Description": "An in-depth look at how I created a DVD screensaver animation without any Javascript, presented with dynamic examples and code snippets.",
    "Confidence": 0.99219894
  },
  {
    "Title": "Water cooling is overkill for Pi 5",
    "Url": "https://www.jeffgeerling.com/blog/2024/water-cooling-overkill-pi-5",
    "Timestamp": "2024-02-22T23:03:55",
    "Domain": "www.jeffgeerling.com",
    "Description": "tl;dr: 52Pi and Seeed Studio's water cooling solution for the Raspberry Pi 5 can be fun, and works better than any other solution—but at a steep price, and with a number of annoying quirks.",
    "Confidence": 0.9607063
  },
  {
    "Title": "View transitions: Handling aspect ratio changes",
    "Url": "https://jakearchibald.com/2024/view-transitions-handling-aspect-ratio-changes/",
    "Timestamp": "2024-02-22T23:03:55",
    "Domain": "jakearchibald.com",
    "Description": "Tips and tricks to get the transition you want",
    "Confidence": 0.9843467
  },
  {
    "Title": "Spring Rites",
    "Url": "https://dantanner.com/post/spring-rites/",
    "Timestamp": "2024-02-22T23:03:55",
    "Domain": "dantanner.com",
    "Description": "A caution against annotation-based web frameworks",
    "Confidence": 0.9884966
  },
  {
    "Title": "Burke Learns Blazor - Bug fixing, download shrinking, prepping to ship!",
    "Url": "https://youtube.com/watch?v=q85cPfEFzkg",
    "Timestamp": "2024-02-22T22:03:53",
    "Domain": "youtube.com",
    "Description": "We make some last fixes and tweaks to get this thing out the door! Let's fix some API bugs and shrink the download size, then talk about data migration.Featu...",
    "Confidence": 0.9190921
  },
  {
    "Title": "Using Figma Plugin Generated C# Markup in an Uno Platform project | Uno Tech Bites",
    "Url": "https://youtube.com/watch?v=rV4TClFB6iM",
    "Timestamp": "2024-02-22T22:03:52",
    "Domain": "youtube.com",
    "Description": "📄Uno Platform Docs: https://platform.uno/docs/articles/external/figma-docs/get-started/create-an-app.html?tabs=dotnet-cliSubscribe for more: https://www.you...",
    "Confidence": 0.99568844
  },
  {
    "Title": "Our Company Is Doing So Well That You’re All Fired",
    "Url": "https://www.mcsweeneys.net/articles/our-company-is-doing-so-well-that-youre-all-fired",
    "Timestamp": "2024-02-22T21:03:48",
    "Domain": "www.mcsweeneys.net",
    "Description": "“Paramount Global lays off about 800 employees, a day after announcing record Super Bowl ratings.” — CNBC\n- - -Thank you for jumping on this last-m...",
    "Confidence": 0.94965
  },
  {
    "Title": "Bluesky: An Open Social Web - Bluesky",
    "Url": "https://bsky.social/about/blog/02-22-2024-open-social-web",
    "Timestamp": "2024-02-22T21:03:48",
    "Domain": "bsky.social",
    "Description": "We’re excited to announce that the Bluesky network is federating and opening up in a way that allows you to host your own data.",
    "Confidence": 0.9903534
  },
  {
    "Title": "WinForms in a 64-Bit world - our strategy going forward - .NET Blog",
    "Url": "https://devblogs.microsoft.com/dotnet/winforms-designer-64-bit-path-forward/",
    "Timestamp": "2024-02-22T19:03:32",
    "Domain": "devblogs.microsoft.com",
    "Description": "32-bit components can impose challenges for WinForms developers in a 64-bit Visual Studio environment, but there are options to solve this. Component modernization, migrating to .NET 6+ and a new option to use the out-of-process Designer for Framework are the key to a feasible way forward!",
    "Confidence": 0.99465287
  },
  {
    "Title": "What is .NET Conf?",
    "Url": "https://youtube.com/watch?v=y6oqAZ9a5Ig",
    "Timestamp": "2024-02-22T18:04:07",
    "Domain": "youtube.com",
    "Description": "Continuing our series from the #dotNETConf: So what is .NET Conf you ask? Jeff Fitz shares what kind of content you'll find in the virtual event's pages.Find...",
    "Confidence": 0.9910123
  },
  {
    "Title": "Gotcha: Don't forget to shut down your dispatcher queues - The Old New Thing",
    "Url": "https://devblogs.microsoft.com/oldnewthing/20240222-11/?p=109434",
    "Timestamp": "2024-02-22T18:04:02",
    "Domain": "devblogs.microsoft.com",
    "Description": "Keep that dispatcher queue controller around, or you'll never be able to clean up.",
    "Confidence": 0.9918474
  },
  {
    "Title": ".NET Data Community Standup - Database concurrency and EF Core: Beyond optimistic concurrency",
    "Url": "https://youtube.com/watch?v=0eVTR5up2RY",
    "Timestamp": "2024-02-22T17:04:21",
    "Domain": "youtube.com",
    "Description": "In the 3rd installment of our series on concurrency, we'll dive deep into SQL isolation levels, how they work (and work differently!) across SQL Server and P...",
    "Confidence": 0.97621197
  },
  {
    "Title": "That One Time Keygen Went Down for 5 Hours (Twice)",
    "Url": "https://keygen.sh/blog/that-one-time-keygen-went-down-for-5-hours-twice/",
    "Timestamp": "2024-02-22T17:04:17",
    "Domain": "keygen.sh",
    "Description": "A postmortem of the dashboard and API outage that occurred on February 5th and 6th, 2024.",
    "Confidence": 0.97798294
  },
  {
    "Title": "Building with Blazor",
    "Url": "https://youtube.com/watch?v=hLKi9eagQ3w",
    "Timestamp": "2024-02-22T16:03:52",
    "Domain": "youtube.com",
    "Description": "Powered by Restream https://restream.ioLet's build new features for our open source web applications TagzApp. You can find it at https://github.com/FritzAndF...",
    "Confidence": 0.984569
  },
  {
    "Title": "Creating an E-Book OnBoarding screen in .NET MAUI",
    "Url": "https://askxammy.com/creating-an-e-book-onboarding-screen-in-net-maui/",
    "Timestamp": "2024-02-22T15:03:45",
    "Domain": "askxammy.com",
    "Description": "Hello! 🙋‍♀️ In this article, we'll replicate an E-Book Onboarding UI, a design sourced from Dribbble. This piece will help you strengthen your skills in managing XAML in a quick and easy way. The most fun part is that you'll learn to build it yourself! 😍\nExplanation Insights\n🗒 Before diving in, here are some guidelines to enhance your experience while replicating the UI:\n▪ Reference Image: Initially, you’ll encounter an image displaying the original UI. This image is segmented into blocks, mirroring how we’ll approach each design element.\n▪ Design Blocks: Every block features a specific design element we’ll focus on. These elements are accentuated within a highlighted box.\n▪ Code Annotations: Within the coding sections, lookout for comments stating, “Insert the following code here.” These comments signal that the upcoming code explanation corresponds directly to that marked location.\n\n\n\n\nLet’s start!\nBreaking Down the Original Design into Blocks\nFor clarity and a structured approach, I’ve segmented the original design into specific blocks. We’ll tackle each block in the sequence presented below:\n\n\n\n\n\nMain structure\nIn this case, we want to keep the same colors for both light and dark mode, let’s start by adding a white background color to the ContentPage.\n\n\n\n\n\n\n\n🚀 For those keen on making their applications adaptable to both light and dark themes, the article “Handling Light and Dark Mode With .NET MAUI” is a must-read.\nAdding the main layout: Now we will see how to add the layout that will contain all the elements of this UI. In this case, we will use the Grid.\n\n\n\n\n\n\n\nIf you want to know more information about Grid, I invite you to read the articles working with GridLayout  and latest Grid structure.\n\n\n\n\n\n\n\nLet's start by developing the first step, which includes three elements: a main image, a title, and a blue decorative bar. We are going to add the code progressively. To know exactly where to insert it, reference the code block above marked <!-- Insert Step 1 code here -->.\nNow let’s explore how to add each UI element:\n🔹 Main image\n\n\n\n🔹 Title\n\nFor the title we will use the default font. However, if you want to learn how to add external text fonts, I recommend the article “Adding Fonts in .NET MAUI”.\n\n💡 If you're wondering about the idea of using an external font but are having trouble identifying the font name, don't worry! WhatTheFont can help you. Just upload a screenshot of your UI and it will identify the source for you. 😎\n\n\n\n✍️ Have you noticed the presence of these characters ‘&#10;’ in the text? They serve a purpose. You can use them when you want to insert a line break in the text.\n🔹 Blue bar\nTo conclude this first step, let's add the blue bar found in our design. You can create it using BoxView or Line Shape. In this case, we will use the Line Shape.\n\n\n\n\n\n\n\nContinuing with step two, we have three elements: a description for clarity, a skip option, and a rounded button. Let's see how to add them to the code:\n🔹 Description & skip\n\n🔹 Rounded button\nCreating a rounded button is easy! Just add the HeightRequest and WidthRequest properties with the same value, then add the CornerRadius property with a value that's exactly half of the previous one. For more on circular buttons, check out the article “Easy way to create circle buttons”.\n\n\n\n\nAnd our E-Book onboarding screen  is done!  \n\n\n\nTo see the complete code structure, you can enter my Github repository \nhttps://github.com/LeomarisReyes/EbookOnBoardingScreen/\n\n\nThanks for reading! 👋 See you next time! 💚💕\n\n\nSpanish article: https://es.askxammy.com/creando-un-e-book-onboarding-ui-en-net-maui/\n\n\n",
    "Confidence": 0.9855838
  },
  {
    "Title": " Stable Diffusion 3 — Stability AI",
    "Url": "https://stability.ai/news/stable-diffusion-3",
    "Timestamp": "2024-02-22T15:03:42",
    "Domain": "stability.ai",
    "Description": "Announcing Stable Diffusion 3 in early preview, our most capable text-to-image model with greatly improved performance in multi-subject prompts, image quality, and spelling abilities.",
    "Confidence": 0.9939407
  },
  {
    "Title": "ASP.NET Core Full Course For Beginners",
    "Url": "https://youtube.com/watch?v=AhAxLiGC7Pc",
    "Timestamp": "2024-02-22T14:04:13",
    "Domain": "youtube.com",
    "Description": "💻Get the source code: https://go.dotnetacademy.io/aspnet8-full-courseLearn how to build a complete web application backend, step by step, using ASP.NET Core...",
    "Confidence": 0.99730706
  },
  {
    "Title": "C# for Beginners – 5 Simplified Concepts in C#",
    "Url": "https://www.devleader.ca/2024/02/22/c-for-beginners-5-simplified-concepts-in-c/",
    "Timestamp": "2024-02-22T14:04:12",
    "Domain": "www.devleader.ca",
    "Description": "Considering C# as you get started on your programming journey? This article is focused on C# for beginners, and I've included 5 concepts from C# with code!",
    "Confidence": 0.9940606
  },
  {
    "Title": "Azure Functions Extensibility - Runtime Scaling",
    "Url": "https://www.tpeczek.com/2024/02/azure-functions-extensibility-runtime.html",
    "Timestamp": "2024-02-22T14:04:12",
    "Domain": "www.tpeczek.com",
    "Description": "This post explores how the Azure Functions extensions participate in scaling activities through runtime scaling and target-based scaling features.",
    "Confidence": 0.99687296
  },
  {
    "Title": "Useful Uses of cat",
    "Url": "https://two-wrongs.com/useful-uses-of-cat",
    "Timestamp": "2024-02-22T13:04:17",
    "Domain": "two-wrongs.com",
    "Description": "When I write shell one-liners that transform the contents of some file, they\noften look something like",
    "Confidence": 0.9954073
  },
  {
    "Title": "Unexpected responses from ChatGPT",
    "Url": "https://status.openai.com/incidents/ssg8fh7sfyz3",
    "Timestamp": "2024-02-22T13:04:16",
    "Domain": "status.openai.com",
    "Description": "OpenAI's Status Page - Unexpected responses from ChatGPT.",
    "Confidence": 0.76016724
  },
  {
    "Title": ".NET 8 🔥🚀 : Guide to Web API AOT - Exploring new Features",
    "Url": "https://youtube.com/watch?v=xpZVO6qw964",
    "Timestamp": "2024-02-22T12:03:15",
    "Domain": "youtube.com",
    "Description": "In this video we will discussed the new features with AOT in .NET 8 and the difference between JIT and AOTSupport me on Patreon to access the source code:htt...",
    "Confidence": 0.99901533
  },
  {
    "Title": "Atuin - Magical Shell History",
    "Url": "https://atuin.sh/",
    "Timestamp": "2024-02-22T09:04:26",
    "Domain": "atuin.sh",
    "Description": "Sync, search and backup shell history with Atuin",
    "Confidence": 0.99711984
  },
  {
    "Title": "How IDisposable and Using Statements Work Together in C#",
    "Url": "https://youtube.com/watch?v=C4qzrFeYHgs",
    "Timestamp": "2024-02-22T06:03:22",
    "Domain": "youtube.com",
    "Description": "IDisposable is a really powerful tool for ensuring proper resource management and safety for your application. In this video, we are going to look at how IDi...",
    "Confidence": 0.9846846
  },
  {
    "Title": "A Streamlined Extension Manager comes to Visual Studio 17.9 - Visual Studio Blog",
    "Url": "https://devblogs.microsoft.com/visualstudio/a-streamlined-extension-manager-comes-to-visual-studio-17-9/",
    "Timestamp": "2024-02-22T06:03:21",
    "Domain": "devblogs.microsoft.com",
    "Description": "Discover the enhanced Extension Manager in Visual Studio 17.9! Streamline your development workflow with improved extension management features. Explore the latest updates and boost productivity.",
    "Confidence": 0.9820953
  },
  {
    "Title": "How To Use AI Assistant to Generate Test Data For .NET Applications | The .NET Tools Blog",
    "Url": "https://blog.jetbrains.com/dotnet/2024/02/21/jetbrains-ai-assistant-generate-test-data-for-dotnet/",
    "Timestamp": "2024-02-22T06:03:20",
    "Domain": "blog.jetbrains.com",
    "Description": "How To Use AI Assistant to Generate Test Data For .NET Applications for better unit tests.",
    "Confidence": 0.9985415
  },
  {
    "Title": "Method Injection in ASP.NET Core: API Controllers vs. MVC Controllers",
    "Url": "https://jeremybytes.blogspot.com/2024/02/method-injection-in-aspnet-core-api.html",
    "Timestamp": "2024-02-22T06:03:19",
    "Domain": "jeremybytes.blogspot.com",
    "Description": "   Method Injection in ASP.NET Core got a little bit easier in .NET 7. Before   .NET 7, we had to use the [FromServices] attribute on a meth...",
    "Confidence": 0.9964687
  },
  {
    "Title": "Visual Studio Presentation Mode",
    "Url": "https://bartwullems.blogspot.com/2024/02/visual-studio-presentation-mode.html",
    "Timestamp": "2024-02-22T06:03:19",
    "Domain": "bartwullems.blogspot.com",
    "Description": "I think we all have seen presentations where Visual Studio was used during the demos but were we had a hard time because the fonts were too ...",
    "Confidence": 0.99253124
  },
  {
    "Title": "Circle STARKs",
    "Url": "https://eprint.iacr.org/2024/278",
    "Timestamp": "2024-02-22T05:03:34",
    "Domain": "eprint.iacr.org",
    "Description": "David Levit, StarkWare",
    "Confidence": 0.9822083
  },
  {
    "Title": "Generate PDF Files in Your .NET MAUI App or ANY .NET Application!",
    "Url": "https://youtube.com/watch?v=JOJRzVqTmBY",
    "Timestamp": "2024-02-22T03:03:52",
    "Domain": "youtube.com",
    "Description": "PDF is still the standard for exchanging documents. In this video we'll learn how to generate a PDF report from .NET MAUI. Curious about that PDF Viewer I'm ...",
    "Confidence": 0.99297464
  },
  {
    "Title": "Data Binding in Blazor Web Apps [Pt 7] | Front-end Web Development with .NET for Beginners",
    "Url": "https://youtube.com/watch?v=HfW9J6CXKZc",
    "Timestamp": "2024-02-22T03:03:52",
    "Domain": "youtube.com",
    "Description": "Learn how to bind data to your UI elements and components using Blazor's powerful data binding capabilities. We'll also take everything we've learned so far ...",
    "Confidence": 0.9929832
  },
  {
    "Title": "Getting Started With MassTransit (Beginner Friendly)",
    "Url": "https://youtube.com/watch?v=4squjqvE8g0",
    "Timestamp": "2024-02-22T01:04:23",
    "Domain": "youtube.com",
    "Description": "📌 Accelerate your Clean Architecture skills: https://bit.ly/3PupkOJ🚀 Support me on Patreon to access the source code: https://www.patreon.com/milanjovanovi...",
    "Confidence": 0.9665591
  },
  {
    "Title": "Why would anyone do something as absurd as mob programming?!",
    "Url": "https://shiftmag.dev/mob-programming-why-do-it-882/",
    "Timestamp": "2024-02-22T01:04:21",
    "Domain": "shiftmag.dev",
    "Description": "My team was enjoying a coffee break when I uttered a question I assumed I would regret: \"Why don't we try mob programming?\"   ",
    "Confidence": 0.8263084
  },
  {
    "Title": "Is GitHub Copilot the new rubber duck?",
    "Url": "https://youtube.com/watch?v=PmWVlbKNvxo",
    "Timestamp": "2024-02-22T00:03:59",
    "Domain": "youtube.com",
    "Description": "#dotNETConf 2023 introduced .NET 8 and much more. We're highlighting some of the conference's golden nuggets in this new series.This series of One Dev Questi...",
    "Confidence": 0.8467322
  },
  {
    "Title": "JetBrains Rider and the .NET Aspire Plugin | The .NET Tools Blog",
    "Url": "https://rideride.net/aspire-plugin",
    "Timestamp": "2024-02-22T00:03:58",
    "Domain": "rideride.net",
    "Description": "A look at the .NET Aspire plugin in JetBrains Rider",
    "Confidence": 0.9989998
  }
]