[
  {
    "Title": "Conduit - Your own chat server",
    "Url": "https://conduit.rs/",
    "Timestamp": "2023-07-31T10:03:29",
    "Domain": "conduit.rs",
    "Description": "Conduit is a simple, fast and reliable\n    chat server powered by Matrix. Conduit is an alternative to Synapse and\n    tries to be lightweight and easy to install, but it is still in development.",
    "Confidence": 0.986416
  },
  {
    "Title": "What is Locking and How to Use a Locking Mechanism in C#",
    "Url": "https://code-maze.com/csharp-locking-mechanism/",
    "Timestamp": "2023-07-31T08:35:56",
    "Domain": "code-maze.com",
    "Description": "In this article, we will discuss the locking mechanism in C#. Including exclusive and non-exclusive locking mechanisms.",
    "Confidence": 0.99779224
  },
  {
    "Title": "Introduction of Auth0 Templates for .NET ",
    "Url": "https://www.infoq.com/news/2023/07/intro-of-auth0-net/",
    "Timestamp": "2023-07-31T08:35:56",
    "Domain": "www.infoq.com",
    "Description": "Auth0 Templates for .NET offers pre-built project templates with integrated Auth0 support for authentication and authorization. The development process is simplified, enabling the creation of Auth0-integrated .NET projects through familiar approaches from built-in templates. The project is open-source.",
    "Confidence": 0.99877393
  },
  {
    "Title": "369: Scanning real world objects into ready-to-use 3D models with RealityKit",
    "Url": "https://www.mergeconflict.fm/369",
    "Timestamp": "2023-07-31T08:35:56",
    "Domain": "www.mergeconflict.fm",
    "Description": "We are diving back into RealityKit which got some nice updates and potentially some visionOS optimizations as well as we start to scan real world objects and turn them into digital goods! ",
    "Confidence": 0.9847059
  },
  {
    "Title": "Scanning real world objects into ready-to-use 3D models with RealityKit | Merge Conflict ep. 369",
    "Url": "https://youtube.com/watch?v=t2yeoDT-pBI",
    "Timestamp": "2023-07-31T08:03:04",
    "Domain": "youtube.com",
    "Description": "We are diving back into RealityKit which got some nice updates and potentially some visionOS optimizations as well as we start to scan real world objects and...",
    "Confidence": 0.9218891
  },
  {
    "Title": "Happy 30th Birthday Windows Server!",
    "Url": "https://techcommunity.microsoft.com/t5/itops-talk-blog/happy-30th-birthday-windows-server/ba-p/3886101",
    "Timestamp": "2023-07-31T06:02:15",
    "Domain": "techcommunity.microsoft.com",
    "Description": "Stroll down memory lane and celebrate 30 years of Windows Server innovation\n",
    "Confidence": 0.98293346
  },
  {
    "Title": "How to Use TypeScript Generics with Functional React Components",
    "Url": "https://www.freecodecamp.org/news/typescript-generics-with-functional-react-components/",
    "Timestamp": "2023-07-31T06:02:11",
    "Domain": "www.freecodecamp.org",
    "Description": "In this article, we will explore the powerful synergy between TypeScript generics and functional React components. Generics allow you to define flexible components that can adapt to different data structures and enforce type safety throughout your codebase. By leveraging Generics in functional components, you can create highly reusable and adaptable",
    "Confidence": 0.99566877
  },
  {
    "Title": "A Git Query Language written in Rust",
    "Url": "https://www.codeproject.com/Tips/5365674/A-Git-Query-Language-written-in-Rust",
    "Timestamp": "2023-07-31T06:02:11",
    "Domain": "www.codeproject.com",
    "Description": "Introduce GQL (A Git Query Language) to perform SQL like queries on .git files",
    "Confidence": 0.998228
  },
  {
    "Title": "New: Improved flexibility when configuring endpoint URLs with the AWS SDKs and tools | Amazon Web Services",
    "Url": "https://aws.amazon.com/blogs/developer/new-improved-flexibility-when-configuring-endpoint-urls-with-the-aws-sdks-and-tools/",
    "Timestamp": "2023-07-31T06:02:10",
    "Domain": "aws.amazon.com",
    "Description": "The AWS SDKs and Tools team is excited to announce improvements for configuration of the endpoint URL used for API service requests through the shared SDK configuration file and environment variables with the AWS SDKs and Tools. Previously, you could specify the endpoint URL used for AWS requests by setting the --endpoint-url command line parameter [‚Ä¶]",
    "Confidence": 0.9978405
  },
  {
    "Title": "What's up, Python? The GIL removed, a new compiler, optparse deprecated...",
    "Url": "https://www.bitecode.dev/p/whats-up-python-the-gil-removed-a",
    "Timestamp": "2023-07-31T06:02:10",
    "Domain": "www.bitecode.dev",
    "Description": "July 2023",
    "Confidence": 0.9763546
  },
  {
    "Title": "Stream videos from Azure Blob storage with ASP.NET Core",
    "Url": "https://markheath.net/post/stream-video-blobs-aspnetcore",
    "Timestamp": "2023-07-31T06:02:10",
    "Domain": "markheath.net",
    "Description": "Mark Heath's Development Blog",
    "Confidence": 0.99655056
  },
  {
    "Title": "Let‚Äôs look at Wolfenstein 3D in F#",
    "Url": "https://www.compositional-it.com/news-blog/lets-have-a-look-at-wolfenstein-3d-made-in-f/",
    "Timestamp": "2023-07-31T06:02:10",
    "Domain": "www.compositional-it.com",
    "Description": "Introduction If you have seen some of my other blog posts you know I always find an excuse to talk about games made in F#, so once I saw James Randall's talk at fsharpConf about his F# port of Wolfstein 3D I knew I had to talk about it in this blog post. Overview This‚Ä¶",
    "Confidence": 0.9977892
  },
  {
    "Title": "LINUX AIR COMBAT free flight simulator",
    "Url": "https://askmisterwizard.com/2019/LinuxAirCombat/LinuxAirCombat.htm",
    "Timestamp": "2023-07-31T04:02:21",
    "Domain": "askmisterwizard.com",
    "Description": "LINUX AIR COMBAT",
    "Confidence": 0.9852272
  },
  {
    "Title": "GitHub - khoj-ai/khoj: An AI personal assistant for your digital brain",
    "Url": "https://github.com/khoj-ai/khoj",
    "Timestamp": "2023-07-31T01:03:15",
    "Domain": "github.com",
    "Description": "An AI personal assistant for your digital brain. Contribute to khoj-ai/khoj development by creating an account on GitHub.",
    "Confidence": 0.96316206
  },
  {
    "Title": "The San Francisco Compute Company",
    "Url": "https://sfcompute.org/",
    "Timestamp": "2023-07-30T22:03:09",
    "Domain": "sfcompute.org",
    "Description": "The San Francisco Compute Group",
    "Confidence": 0.897399
  },
  {
    "Title": "JasperFx Software is Open for Business!",
    "Url": "https://jeremydmiller.com/2023/07/30/jasperfx-software-is-open-for-business/",
    "Timestamp": "2023-07-30T21:03:32",
    "Domain": "jeremydmiller.com",
    "Description": "JasperFx Software is open for business!",
    "Confidence": 0.95347106
  },
  {
    "Title": "GPT4 can play chess ‚Äì David Chudzicki",
    "Url": "https://www.davidchudzicki.com/posts/gpt4-can-play-chess/",
    "Timestamp": "2023-07-30T21:03:22",
    "Domain": "www.davidchudzicki.com",
    "Description": "David Chudzicki",
    "Confidence": 0.97881377
  },
  {
    "Title": "7 Essential Tech Talks Every Developer Should Watch",
    "Url": "https://blog.mihaisafta.com/posts/talks-every-developer-should-watch/",
    "Timestamp": "2023-07-30T21:03:21",
    "Domain": "blog.mihaisafta.com",
    "Description": "Whether you‚Äôre a seasoned developer or just starting your coding journey, these seven talks are packed with insights that will challenge and inspire you.\nIn the rapidly evolving world of software development, it‚Äôs easy to get lost in the sea of new languages, frameworks, and techniques. But as I journeyed through my career as a developer, I found that revisiting seminal talks from industry visionaries consistently provides me with fresh perspectives and enduring wisdom.",
    "Confidence": 0.9360132
  },
  {
    "Title": "Stove and Searle on the rhetorical subversion of common sense",
    "Url": "http://edwardfeser.blogspot.com/2023/07/stove-and-searle-on-rhetorical.html",
    "Timestamp": "2023-07-30T21:03:21",
    "Domain": "edwardfeser.blogspot.com",
    "Description": "One of the stranger aspects of contemporary political and intellectual life is the frequency with which commentators put forward extremely d...",
    "Confidence": 0.56656796
  },
  {
    "Title": "How to create a Django form in 2mins using Alpine.js",
    "Url": "https://www.photondesigner.com/articles/create-django-form-with-alpine-js",
    "Timestamp": "2023-07-30T21:03:21",
    "Domain": "www.photondesigner.com",
    "Description": "Create an async Django form simply and neatly with Alpine.js. Boost speed, enhance functionality.",
    "Confidence": 0.98914045
  },
  {
    "Title": "Documentation as Code for Cloud - PlantUML - blog.dornea.nu",
    "Url": "https://blog.dornea.nu/2023/07/30/documentation-as-code-for-cloud-plantuml/",
    "Timestamp": "2023-07-30T21:03:20",
    "Domain": "blog.dornea.nu",
    "Description": "Basics I‚Äôve become a huge fan of PlantUML even before I came across the concept of ‚Äúdocumentation as code‚Äù I also code for presentations. So the term presentation as code is also a thing.  and it instantly won me over with its capabilities. I have used it in many different roles (software engineer, security engineer, security architect) extensively to draw diagrams (components, sequences) and mind maps.\nThough initially, the general syntax might seem a bit challenging to understand, I believe that with some dedication, the learning curve becomes quite manageable.",
    "Confidence": 0.9983339
  },
  {
    "Title": "I made a Quotebacks plugin for Marked",
    "Url": "https://johnwhiles.com/posts/marked-quotebacks",
    "Timestamp": "2023-07-30T21:03:20",
    "Domain": "johnwhiles.com",
    "Description": "Quotebacks is a combination of a chrome extension which can be used to collect quotes from websites, and a little library which renders those quotes in a particular style. I wanted to use it for this website, but I didn‚Äôt want to use the existing rendering code1, so I built a plugin for the popular JavaScript library marked.",
    "Confidence": 0.9684642
  },
  {
    "Title": "Finding my balance: An evolved and simplified task management system",
    "Url": "https://pankajpipada.com/posts/2023-07-30-taskmgmt/",
    "Timestamp": "2023-07-30T21:03:19",
    "Domain": "pankajpipada.com",
    "Description": "An evolution in work related task management over the years, that has worked for me, and has personally shaped my professional life.",
    "Confidence": 0.9604847
  },
  {
    "Title": "SwiftUI Data Flow 2023",
    "Url": "https://troz.net/post/2023/swiftui-data-flow-2023/",
    "Timestamp": "2023-07-30T21:03:19",
    "Domain": "troz.net",
    "Description": "Various ways to pass data around your SwiftUI apps, after WWDC 2023.",
    "Confidence": 0.9913385
  },
  {
    "Title": "What we really need is a CDN that deploys to ALL edge nodes - Junaid Bhura",
    "Url": "https://junaid.dev/what-we-really-need-is-a-cdn-that-deploys-to-all-edge-nodes/",
    "Timestamp": "2023-07-30T21:03:18",
    "Domain": "junaid.dev",
    "Description": "What we actually need from CDNs are configurable static deployments of our content to ALL edge nodes!",
    "Confidence": 0.97018397
  },
  {
    "Title": "Blog Writing for Developers",
    "Url": "https://rmoff.net/2023/07/19/blog-writing-for-developers/",
    "Timestamp": "2023-07-30T21:03:18",
    "Domain": "rmoff.net",
    "Description": "",
    "Confidence": 0.9940992
  },
  {
    "Title": "Why do consultants invent words?",
    "Url": "https://jevgeni.tarassov.ch/topics/management/consulting-reification/",
    "Timestamp": "2023-07-30T21:03:18",
    "Domain": "jevgeni.tarassov.ch",
    "Description": "I talk about why consultants tend to invent words and how that is used as a marketing gimmick. The results will shock you!",
    "Confidence": 0.8713963
  },
  {
    "Title": "You Need More Constraints",
    "Url": "https://borretti.me/article/you-need-more-constraints",
    "Timestamp": "2023-07-30T21:03:18",
    "Domain": "borretti.me",
    "Description": "A checklist of useful SQL constraints.",
    "Confidence": 0.9994139
  },
  {
    "Title": "Large Language Models (LLMs) as noise generators",
    "Url": "https://unmonoqueteclea.github.io/2023-07-30-large-language-models-(llms)-as-noise-generators.html",
    "Timestamp": "2023-07-30T21:03:17",
    "Domain": "unmonoqueteclea.github.io",
    "Description": "Allow me to introduce you to Jane Doe, a seasoned Software Engineer\nwith 10 years of experience. Currently, Jane is exploring various job\nopportunities and has come across a few enticing offers that demand a\ncover letter as part of the application process.",
    "Confidence": 0.9924691
  },
  {
    "Title": "Emacs 29.1 released",
    "Url": "https://lists.gnu.org/archive/html/emacs-devel/2023-07/msg00879.html",
    "Timestamp": "2023-07-30T21:03:17",
    "Domain": "lists.gnu.org",
    "Description": null,
    "Confidence": 0.9258675
  },
  {
    "Title": "The web I want ‚Äì Manu",
    "Url": "https://manuelmoreale.com/the-web-i-want",
    "Timestamp": "2023-07-30T21:03:17",
    "Domain": "manuelmoreale.com",
    "Description": "The web I want is powered by passion, not money. The web I want doesn't need to ask for permission to track me because they know that tracking me is ‚Ä¶",
    "Confidence": 0.9258675
  },
  {
    "Title": "New ‚Äì AWS Public IPv4 Address Charge + Public IP Insights | Amazon Web Services",
    "Url": "https://aws.amazon.com/blogs/aws/new-aws-public-ipv4-address-charge-public-ip-insights/",
    "Timestamp": "2023-07-30T19:02:10",
    "Domain": "aws.amazon.com",
    "Description": "We are introducing a new charge for public IPv4 addresses. Effective February 1, 2024 there will be a charge of $0.005 per IP per hour for all public IPv4 addresses, whether attached to a service or not (there is already a charge for public IPv4 addresses you allocate in your account but don‚Äôt attach to [‚Ä¶]",
    "Confidence": 0.9649122
  },
  {
    "Title": "EffectiveDebugging/SourceCode/StockMarket at master ¬∑ chrisnas/EffectiveDebugging",
    "Url": "https://github.com/chrisnas/EffectiveDebugging/tree/master/SourceCode/StockMarket",
    "Timestamp": "2023-07-30T18:03:10",
    "Domain": "github.com",
    "Description": "Contribute to chrisnas/EffectiveDebugging development by creating an account on GitHub.",
    "Confidence": 0.99189436
  },
  {
    "Title": "ffmprovisr",
    "Url": "https://amiaopensource.github.io/ffmprovisr/",
    "Timestamp": "2023-07-30T18:03:09",
    "Domain": "amiaopensource.github.io",
    "Description": "FFmpeg is a powerful tool for manipulating audiovisual files. Unfortunately, it also has a steep learning curve, especially for users unfamiliar with a command line interface. This app helps users through the command generation process so that more people can reap the benefits of FFmpeg.",
    "Confidence": 0.9919511
  },
  {
    "Title": "Containers in Azure Container Apps: Simplifying app orchestration with Sidecar and Init Containers",
    "Url": "https://luismts.com/azure-container-apps-sidecar-and-init-containers/",
    "Timestamp": "2023-07-30T17:02:00",
    "Domain": "luismts.com",
    "Description": "One of the powerful features of Azure Container Apps is the ability to use sidecar containers and init containers to improve the",
    "Confidence": 0.99662167
  },
  {
    "Title": "Emacs 29.1 Released",
    "Url": "https://emacsredux.com/blog/2023/07/30/emacs-29-1-released/",
    "Timestamp": "2023-07-30T17:01:58",
    "Domain": "emacsredux.com",
    "Description": "Today is a great day for Emacs - Emacs 29.1 has just been released1! Every Emacs release is special, but I haven‚Äôt been so excited about a new version of Emacs in ages. Why so? Reason #1 - pure GTK front-end (a.k.a. pgtk). This also means that now Emacs supports natively Wayland. Which in tern means that it‚Äôs easier than ever to run Emacs in Windows‚Äôs WSL. This is huge! Reason #2 - built-in support for the massively popular Language Server Protocol via eglot. eglot has existed for a while, but it‚Äôs nice to see it bundled with Emacs going forward. This will certainly make Emacs better positioned to complete with ‚Äúmodern‚Äù editors like VS Code. Reason #3 - built-in support for TreeSitter. This means that a few years down the road we‚Äôll have many Emacs major modes that are much faster, robust and feature-rich. It‚Äôs infinitely easier to built a major mode using a real parser instead of using regular expressions. Lots of built-in modes have already been updated to have a version using TreeSitter internally. Frankly, I can‚Äôt think of a bigger improvement in Emacs in the almost 20 years I‚Äôve been an Emacs user. Exciting times ahead! You can read all about the new release here. I‚Äôll likely write a few articles about some of the new features in the weeks and months to come. In Emacs We Trust! M-x Forever! P.S. Feel free to share in the comments what are you most excited about. You can read the official release announcement here.¬†‚Ü©",
    "Confidence": 0.9897194
  },
  {
    "Title": "Wikifunctions",
    "Url": "https://www.wikifunctions.org/wiki/Wikifunctions:Main_Page",
    "Timestamp": "2023-07-30T17:01:58",
    "Domain": "www.wikifunctions.org",
    "Description": "",
    "Confidence": 0.9855905
  },
  {
    "Title": "Elevate Your .NET Testing Strategy #2: InMemoryDatabase",
    "Url": "https://goatreview.com/dotnet-testing-inmemorydatabase-for-component-tests/",
    "Timestamp": "2023-07-30T16:02:15",
    "Domain": "goatreview.com",
    "Description": "The development environment in C#/.NET offers a myriad of tools that optimize testing efficiency. Among these are the InMemoryDatabase and WebApplicationFactory, both possessing the potential to drastically streamline testing strategies and development processes.\n\nAs a sequel to our previous discussion on unit testing in .NET Core, this article aims",
    "Confidence": 0.99906915
  },
  {
    "Title": "Contenedores en Azure Container Apps: Simplificando la orquestaci√≥n de aplicaciones con Sidecar e Init Containers",
    "Url": "https://luismts.com/es/azure-container-apps-sidecar-e-init-containers/",
    "Timestamp": "2023-07-30T16:02:14",
    "Domain": "luismts.com",
    "Description": "Una de las caracter√≠sticas poderosas de Azure Container Apps es la capacidad de utilizar sidecar containers e init containers para mejorar la",
    "Confidence": 0.9941143
  },
  {
    "Title": "Understanding Python Data Types | Pro Code Guide",
    "Url": "https://procodeguide.com/python-tutorials/understanding-python-data-types/",
    "Timestamp": "2023-07-30T15:02:40",
    "Domain": "procodeguide.com",
    "Description": "In this article, we will learn about Python data types. In Python, data types are used to define the type of a variable. There are different types of built-in",
    "Confidence": 0.982725
  },
  {
    "Title": "Understanding Python Numeric Data Types | Pro Code Guide",
    "Url": "https://procodeguide.com/python-tutorials/python-numeric-data-types/",
    "Timestamp": "2023-07-30T15:02:40",
    "Domain": "procodeguide.com",
    "Description": "In this article we will learn about Python numeric data types, what are different numeric data types available in Python and how to use those numeric data",
    "Confidence": 0.9898251
  },
  {
    "Title": "Migrating An API from Newtonsoft.Json to System.Text.Json",
    "Url": "https://www.andybutland.dev/2023/07/202307migrating-api-from-newtonsoftjson-to-system-text-json.html",
    "Timestamp": "2023-07-30T15:02:37",
    "Domain": "www.andybutland.dev",
    "Description": "In some recent work with Umbraco I‚Äôve been looking to migrate an API from using the Newtonsoft.Json  serialization library to the newer, Mic...",
    "Confidence": 0.9990011
  },
  {
    "Title": "Workplace advice for Programmers (Q&A)",
    "Url": "https://youtube.com/watch?v=2V5zRHSN5PE",
    "Timestamp": "2023-07-30T13:03:22",
    "Domain": "youtube.com",
    "Description": "Advice for programmers on how to face problems in the workplace.The Clean Coder:Extreme Ownership: Jocko Podcast: Support my Work  ü§ù https://www.patreon.com...",
    "Confidence": 0.99623585
  },
  {
    "Title": "The best 5¬†.NET and ASP.NET courses for eCommerce developers",
    "Url": "https://medium.com/@mwaseemzakir/the-best-5-net-and-asp-net-courses-for-ecommerce-developers-196be4ebd1fb",
    "Timestamp": "2023-07-30T12:02:27",
    "Domain": "medium.com",
    "Description": "To help eCommerce developers with their careers, we reviewed significant training courses and added free resources for ASP.NET development.",
    "Confidence": 0.9950817
  },
  {
    "Title": "Weird Things You Can Do In C#: True & False Operators #shorts",
    "Url": "https://youtube.com/watch?v=uuSOlTrFLrg",
    "Timestamp": "2023-07-30T12:02:27",
    "Domain": "youtube.com",
    "Description": "üöÄ Support me on Patreon to access the source code: https://www.patreon.com/milanjovanovicJoin my weekly .NET newsletter:https://www.milanjovanovic.techRead ...",
    "Confidence": 0.8028209
  },
  {
    "Title": "Libreboot ‚Äì GNU Boot",
    "Url": "https://libreboot.org/news/gnuboot.html#gnu-boot-cease-and-desist-email",
    "Timestamp": "2023-07-30T12:02:25",
    "Domain": "libreboot.org",
    "Description": "Libreboot ‚Äì GNU Boot",
    "Confidence": 0.9703285
  },
  {
    "Title": "Functions are Vectors",
    "Url": "https://thenumb.at/Functions-are-Vectors/",
    "Timestamp": "2023-07-30T11:03:15",
    "Domain": "thenumb.at",
    "Description": "Conceptualizing functions as infinite-dimensional vectors lets us apply the tools of linear algebra to a vast landscape of new problems, from image and geometry processing to curve fitting, light transport, and machine learning.\n\nPrerequisites: introductory linear algebra, introductory calculus, introductory differential equations.\n\n\n  Functions as Vectors\n    \n      Vector Spaces\n      Linear Operators\n      Diagonalization\n      Inner Product Spaces\n      The Spectral Theorem\n    \n  \n  Applications\n    \n      Fourier Series\n      Image Compression\n      Geometry Processing\n      Further Reading\n    \n  \n\n\n\n\nFunctions as Vectors\n\nVectors are often first introduced as lists of real numbers‚Äîi.e. the familiar notation we use for points, directions, and more.\n\n\n\n$$ \\mathbf{v} = \\begin{bmatrix}x\\\\y\\\\z\\end{bmatrix} $$\n\n\n\n\n\n\nYou may recall that this representation is only one example of an abstract vector space.\nThere are many other types of vectors, such as lists of complex numbers, graph cycles, and even magic squares.\n\nHowever, all of these vector spaces have one thing in common: a finite number of dimensions.\nThat is, each kind of vector can be represented as a collection of \\(N\\) numbers, though the definition of ‚Äúnumber‚Äù varies.\n\nIf any \\(N\\)-dimensional vector is essentially a length-\\(N\\) list, we could also consider a vector to be a mapping from an index to a value.\n\n\\[\\begin{align*}\n\\mathbf{v}_1 &= x\\\\\n\\mathbf{v}_2 &= y\\\\\n\\mathbf{v}_3 &= z\n\\end{align*}\\ \\iff\\ \\mathbf{v} = \\begin{bmatrix}x \\\\ y \\\\ z\\end{bmatrix}\\]\n\nWhat does this perspective hint at as we increase the number of dimensions?\n\n\n\n\n    Dimensions\n    \n\n\n\n\nIn higher dimensions, vectors start to look more like functions!\n\nCountably Infinite Indices\n\nOf course, a finite-length vector only specifies a value at a limited number of indices.\nCould we instead define a vector that contains infinitely many values?\n\nWriting down a vector representing a function on the natural numbers (\\(\\mathbb{N}\\))‚Äîor any other countably infinite domain‚Äîis straightforward: just extend the list indefinitely.\n\n\n\n$$ \\begin{align*}\\mathbf{v}_1 &= 1\\\\\\mathbf{v}_2 &= 2\\\\ &\\vdots \\\\ \\mathbf{v}_i &= i\\end{align*}\\ \\iff\\ \\mathbf{v} = \\begin{bmatrix}1 \\\\ 2 \\\\ 3 \\\\ \\vdots \\end{bmatrix} $$\n\n\n\n\n\n\nThis vector could represent the function \\(f(x) = x\\), where \\(x \\in \\mathbb{N}\\).1\n\nUncountably Infinite Indices\n\nMany interesting functions are defined on the real numbers (\\(\\mathbb{R}\\)), so may not be representable as a countably infinite vector.\nTherefore, we will have to make a larger conceptual leap: not only will our set of indices be infinite, it will be uncountably infinite.\n\nThat means we can‚Äôt write down vectors as lists at all‚Äîit is impossible to assign an integer index to each element of an uncountable set.\nSo, how can we write down a vector mapping a real index to a certain value?\n\nNow, a vector really is just an arbitrary function:\n\n\n\n$$ \\mathbf{v}_{x} = x^2\\ \\iff\\ \\mathbf{v} = \\begin{bmatrix} x \\mapsto x^2 \\end{bmatrix} $$\n\n\n\n\n\n\nPrecisely defining how and why we can represent functions as infinite-dimensional vectors is the purview of functional analysis.\nIn this post, we won‚Äôt attempt to prove our results in infinite dimensions: we will focus on building intuition via analogies to finite-dimensional linear algebra.\n\n\n\nVector Spaces\n\n\n  Review: Abstract vector spaces | Chapter 16, Essence of linear algebra.\n\n\nFormally, a vector space is defined by choosing a set of vectors \\(\\mathcal{V}\\), a scalar field \\(\\mathbb{F}\\), and a zero vector \\(\\mathbf{0}\\).\nThe field \\(\\mathbb{F}\\) is often the real numbers (\\(\\mathbb{R}\\)), complex numbers (\\(\\mathbb{C}\\)), or a finite field such as the integers modulo a prime (\\(\\mathbb{Z}_p\\)).\n\nAdditionally, we must specify how to add two vectors and how to multiply a vector by a scalar.\n\n\\[\\begin{align*}\n(+)\\ &:\\ \\mathcal{V}\\times\\mathcal{V}\\mapsto\\mathcal{V}\\\\\n(\\cdot)\\ &:\\ \\mathbb{F}\\times\\mathcal{V} \\mapsto \\mathcal{V}\n\\end{align*}\\]\n\nTo describe a vector space, our definitions must entail several vector space axioms.\n\nA Functional Vector Space\n\nIn the following sections, we‚Äôll work with the vector space of real functions.\nTo avoid ambiguity, square brackets are used to denote function application.\n\n\n  The scalar field \\(\\mathbb{F}\\) is the real numbers \\(\\mathbb{R}\\).\n  The set of vectors \\(\\mathcal{V}\\) contains functions from \\(\\mathbb{R}\\) to \\(\\mathbb{R}\\).2\n  \\(\\mathbf{0}\\) is the zero function, i.e. \\(\\mathbf{0}[x] = 0\\).\n\n\nAdding functions corresponds to applying the functions separately and summing the results.\n\n\n$$ (f + g)[x] = f[x] + g[x] $$\n\n\n\n\n\n\nThis definition generalizes the typical element-wise addition rule‚Äîit‚Äôs like adding the two values at each index.\n\n\\[f+g = \\begin{bmatrix}f_1 + g_1 \\\\ f_2 + g_2 \\\\ \\vdots \\end{bmatrix}\\]\n\nMultiplying a function by a scalar corresponds to applying the function and scaling the result.\n\n\n\n$$ (\\alpha f)[x] = \\alpha f[x] $$\n\n\n\n\n\n\nThis rule similarly generalizes element-wise multiplication‚Äîit‚Äôs like scaling the value at each index.\n\n\\[\\alpha f = \\begin{bmatrix}\\alpha f_1 \\\\ \\alpha f_2 \\\\ \\vdots \\end{bmatrix}\\]\n\nProofs\n\nGiven these definitions, we can now prove all necessary vector space axioms.\nWe will illustrate the analog of each property in \\(\\mathbb{R}^2\\), the familiar vector space of two-dimensional arrows.\n\n\n  \n    Vector Addition is Commutative\n  \n  \n  \nFor all vectors $$\\mathbf{u}, \\mathbf{v} \\in \\mathcal{V}$$:\n\n\n\n$$\\mathbf{u} + \\mathbf{v} = \\mathbf{v} + \\mathbf{u}$$\n\n\n\n\n\n\nSince real addition is commutative, this property follows directly from our definition of vector addition:\n\n$$\\begin{align*}\n(f + g)[x] &= f[x] + g[x]\\\\\n&= g[x] + f[x]\\\\\n&= (g + f)[x]\n\\end{align*}$$\n  \n  \n\n\n\n  \n    Vector Addition is Associative\n  \n  \n  \nFor all vectors $$\\mathbf{u}, \\mathbf{v}, \\mathbf{w} \\in \\mathcal{V}$$:\n\n\n\n$$(\\mathbf{u} + \\mathbf{v}) + \\mathbf{w} = \\mathbf{u} + (\\mathbf{v} + \\mathbf{w})$$\n\n\n\n\n\n\nThis property also follows from our definition of vector addition:\n\n$$\\begin{align*}\n((f + g) + h)[x] &= (f + g)[x] + h[x]\\\\\n&= f[x] + g[x] + h[x]\\\\\n&= f[x] + (g[x] + h[x])\\\\\n&= f[x] + (g + h)[x]\\\\\n&= (f + (g + h))[x]\n\\end{align*}$$\n\n  \n  \n\n\n\n  \n    $$\\mathbf{0}$$ is an Additive Identity\n  \n  \n  \nFor all vectors $$\\mathbf{u} \\in \\mathcal{V}$$:\n\n\n\n$$\\mathbf{0} + \\mathbf{u} = \\mathbf{u} $$\n\n\n\n\n\n\nThis one is easy:\n\n$$\\begin{align*}\n(\\mathbf{0} + f)[x] &= \\mathbf{0}[x] + f[x]\\\\\n&= 0 + f[x]\\\\\n&= f[x]\n\\end{align*}$$\n\n  \n  \n\n\n\n  \n    Additive Inverses Exist\n  \n  \n  \nFor all vectors $$\\mathbf{u} \\in \\mathcal{V}$$, there exists a vector $$-\\mathbf{u} \\in \\mathcal{V}$$ such that:\n\n\n\n$$\\mathbf{u} + (-\\mathbf{u}) = \\mathbf{0}$$\n\n\n\n\n\n\nNegation is defined as applying $$f$$ and negating the result: $$(-f)[x] = -f[x]$$.\nClearly, $$-f$$ is also in $$\\mathcal{V}$$.\n\n$$\\begin{align*}\n(f + (-f))[x] &= f[x] + (-f)[x]\\\\\n&= f[x] - f[x]\\\\\n&= 0\\\\\n&= \\mathbf{0}[x]\n\\end{align*}$$\n\n  \n  \n\n\n\n  \n    $$1$$ is a Multiplicative Identity\n  \n  \n  \nFor all vectors $$\\mathbf{u} \\in \\mathcal{V}$$:\n\n\n\n$$1\\mathbf{u} = \\mathbf{u}$$\n\n\n\n\n\n\nNote that $$1$$ is specified by the choice of $$\\mathbb{F}$$.\nIn our case, it is simply the real number $$1$$.\n\n$$\\begin{align*}\n(1 f)[x] &= 1 f[x]\\\\\n&= f[x]\n\\end{align*}$$\n\n  \n  \n\n\n\n  \n    Scalar Multiplication is Associative\n  \n  \n  \nFor all vectors $$\\mathbf{u} \\in \\mathcal{V}$$ and scalars $$\\alpha, \\beta \\in \\mathbb{F}$$:\n\n\n\n$$(\\alpha \\beta)\\mathbf{u} = \\alpha(\\beta\\mathbf{u})$$\n\n\n\n\n\n\nThis property follows from our definition of scalar multiplication:\n\n$$\\begin{align*}\n((\\alpha\\beta) f)[x] &= (\\alpha\\beta)f[x]\\\\\n&= \\alpha(\\beta f[x])\\\\\n&= \\alpha(\\beta f)[x]\n\\end{align*}$$\n\n  \n  \n\n\n\n  \n    Scalar Multiplication Distributes Over Vector Addition\n  \n  \n  \nFor all vectors $$\\mathbf{u}, \\mathbf{v} \\in \\mathcal{V}$$ and scalars $$\\alpha \\in \\mathbb{F}$$:\n\n\n\n$$\\alpha(\\mathbf{u} + \\mathbf{v}) = \\alpha\\mathbf{u} + \\alpha\\mathbf{v}$$\n\n\n\n\n\n\nAgain using our definitions of vector addition and scalar multiplication:\n\n$$\\begin{align*}\n(\\alpha (f + g))[x] &= \\alpha(f + g)[x]\\\\\n&= \\alpha(f[x] + g[x])\\\\\n&= \\alpha f[x] + \\alpha g[x]\\\\\n&= (\\alpha f)[x] + (\\alpha g)[x]\\\\\n&= (\\alpha f + \\alpha g)[x]\n\\end{align*}$$\n\n  \n  \n\n\n\n  \n    Scalar Multiplication Distributes Over Scalar Addition\n  \n  \n  \nFor all vectors $$\\mathbf{u} \\in \\mathcal{V}$$ and scalars $$\\alpha, \\beta \\in \\mathbb{F}$$:\n\n\n\n $$(\\alpha + \\beta)\\mathbf{u} = \\alpha\\mathbf{u} + \\beta\\mathbf{u}$$\n\n\n\n\n\n\nAgain using our definitions of vector addition and scalar multiplication:\n\n$$\\begin{align*}\n((\\alpha + \\beta)f)[x] &= (\\alpha + \\beta)f[x]\\\\\n&= \\alpha f[x] + \\beta f[x] \\\\\n&= (\\alpha f)[x] + (\\beta f)[x]\n\\end{align*}$$\n\n  \n  \n\n\nTherefore, we‚Äôve built a vector space of functions!3\nIt may not be immediately obvious why this result is useful, but bear with us through a few more definitions‚Äîwe will spend the rest of this post exploring powerful techniques arising from this perspective.\n\nA Standard Basis for Functions\n\n\n  Review: Linear combinations, span, and basis vectors | Chapter 2, Essence of linear algebra.\n\n\nUnless specified otherwise, vectors are written down with respect to the standard basis.\nIn \\(\\mathbb{R}^2\\), the standard basis consists of the two coordinate axes.\n\n\n\n$$ \\mathbf{e}_1 = \\begin{bmatrix}1 \\\\ 0\\end{bmatrix},\\,\\, \\mathbf{e}_2 = \\begin{bmatrix}0 \\\\ 1\\end{bmatrix} $$\n\n\n\n\n\n\nHence, vector notation is shorthand for a linear combination of the standard basis vectors.\n\n\n\n$$ \\mathbf{u} = \\begin{bmatrix}\\alpha \\\\ \\beta\\end{bmatrix} = \\alpha\\mathbf{e}_1 + \\beta\\mathbf{e}_2 $$\n\n\n\n\n\n\nAbove, we represented functions as vectors by assuming each dimension of an infinite-length vector contains the function‚Äôs result for that index.\nThis construction points to a natural generalization of the standard basis.\n\nJust like the coordinate axes, each standard basis function contains a \\(1\\) at one index and \\(0\\) everywhere else.\nMore precisely, for every \\(\\alpha \\in \\mathbb{R}\\),\n\n\n\\[\\mathbf{e}_\\alpha[x] = \\begin{cases} 1 & \\text{if } x = \\alpha \\\\ 0 & \\text{otherwise} \\end{cases}\\]\n\n\n\nWe can then express any real function \\(f\\) as a linear combination of these basis functions:\n\n\\[\\begin{align*} f[x] &= f[\\alpha]\\mathbf{e}_\\alpha[x] \\\\ &= f[1]\\mathbf{e}_1[x] + f[2]\\mathbf{e}_2[x] + f[\\pi]\\mathbf{e}_\\pi[x] + \\dots \\end{align*}\\]\n\nIf you evaluate this sum at \\(x\\), you‚Äôll find that all terms are zero‚Äîexcept \\(\\mathbf{e}_x\\), making the result \\(f[x]\\).\n\n\n\nLinear Operators\n\n\n  Review: Change of basis | Chapter 13, Essence of linear algebra.\n\n\nNow that we can manipulate functions as vectors, let‚Äôs start transferring the tools of linear algebra to the functional perspective.\n\nOne ubiquitous operation on finite-dimensional vectors is transforming them with matrices.\nA matrix \\(\\mathbf{A}\\) encodes a linear transformation, meaning multiplication preserves linear combinations.\n\n\\[\\mathbf{A}(\\alpha \\mathbf{x} + \\beta \\mathbf{y}) = \\alpha \\mathbf{A}\\mathbf{x} + \\beta \\mathbf{A}\\mathbf{y}\\]\n\nMultiplying a vector by a matrix can be intuitively interpreted as defining a new set of coordinate axes from the matrix‚Äôs column vectors.\nThe result is a linear combination of the columns:\n\n\n\n\n\n\n\\[\\mathbf{Ax} = \\begin{bmatrix} \\vert & \\vert & \\vert \\\\ \\mathbf{u} & \\mathbf{v} & \\mathbf{w} \\\\ \\vert & \\vert & \\vert \\end{bmatrix} \\begin{bmatrix}x_1 \\\\ x_2 \\\\ x_3\\end{bmatrix} = x_1\\mathbf{u} + x_2\\mathbf{v} + x_3\\mathbf{w}\\]\n\n    \n\n\n\\[\\begin{align*}\n\\mathbf{Ax} &= \\begin{bmatrix} \\vert & \\vert & \\vert \\\\ \\mathbf{u} & \\mathbf{v} & \\mathbf{w} \\\\ \\vert & \\vert & \\vert \\end{bmatrix} \\begin{bmatrix}x_1 \\\\ x_2 \\\\ x_3\\end{bmatrix} \\\\ &= x_1\\mathbf{u} + x_2\\mathbf{v} + x_3\\mathbf{w}\n\\end{align*}\\]\n\n    \n\n\n\n\n\n\n\nWhen all vectors can be expressed as a linear combination of \\(\\mathbf{u}\\), \\(\\mathbf{v}\\), and \\(\\mathbf{w}\\), the columns form a basis for the underlying vector space.\nHere, the matrix \\(\\mathbf{A}\\) transforms a vector from the \\(\\mathbf{uvw}\\) basis into the standard basis.\n\nSince functions are vectors, we could imagine transforming a function by a matrix.\nSuch a matrix would be infinite-dimensional, so we will instead call it a linear operator and denote it with \\(\\mathcal{L}\\).\n\n\n\n\\[\\mathcal{L}f = \\begin{bmatrix} \\vert & \\vert & \\vert & \\\\ \\mathbf{f} & \\mathbf{g} & \\mathbf{h} & \\cdots \\\\ \\vert & \\vert & \\vert & \\end{bmatrix} \\begin{bmatrix}f_1\\\\ f_2 \\\\ f_3\\\\ \\vdots\\end{bmatrix} = f_1\\mathbf{f} + f_2\\mathbf{g} + f_3\\mathbf{h} + \\cdots\\]\n\n\n\n\n\\[\\begin{align*}\n\\mathcal{L}f &= \\begin{bmatrix} \\vert & \\vert & \\vert & \\\\ \\mathbf{f} & \\mathbf{g} & \\mathbf{h} & \\cdots \\\\ \\vert & \\vert & \\vert & \\end{bmatrix} \\begin{bmatrix}f_1\\\\ f_2\\\\ f_3 \\\\ \\vdots\\end{bmatrix} \\\\ &= f_1\\mathbf{f} + f_2\\mathbf{g} + f_3\\mathbf{h} + \\cdots\n\\end{align*}\\]\n\n\n\nThis visualization isn‚Äôt very accurate‚Äîwe‚Äôre dealing with uncountably infinite-dimensional vectors, so we can‚Äôt actually write out an operator in matrix form.\nNonetheless, the structure is suggestive: each ‚Äúcolumn‚Äù of the operator describes a new basis function for our functional vector space.\nJust like we saw with finite-dimensional vectors, \\(\\mathcal{L}\\) represents a change of basis.\n\nDifferentiation\n\n\n  Review: Derivative formulas through geometry | Chapter 3, Essence of calculus.\n\n\nSo, what‚Äôs an example of a linear operator on functions?\nYou might recall that differentiation is linear:\n\n\\[\\frac{\\partial}{\\partial x} \\left(\\alpha f[x] + \\beta g[x]\\right) = \\alpha\\frac{\\partial f}{\\partial x} + \\beta\\frac{\\partial g}{\\partial x}\\]\n\nIt‚Äôs hard to visualize differentiation on general functions, but it‚Äôs feasible for the subspace of polynomials, \\(\\mathcal{P}\\).\nLet‚Äôs take a slight detour to examine this smaller space of functions.\n\n\\[\\mathcal{P} = \\{ p[x] = a + bx + cx^2 + dx^3 + \\cdots \\}\\]\n\nWe typically write down polynomials as a sequence of powers, i.e. \\(1, x, x^2, x^3\\), etc.\nAll polynomials are linear combinations of the functions \\(\\mathbf{e}_i[x] = x^i\\), so they constitute a countably infinite basis for \\(\\mathcal{P}\\).4\n\nThis basis provides a convenient vector notation:\n\n\n\n\\[\\begin{align*} p[x] &= a + bx + cx^2 + dx^3 + \\cdots \\\\ &= a\\mathbf{e}_0 + b\\mathbf{e}_1 + c \\mathbf{e}_2 + d\\mathbf{e}_3 + \\dots \\end{align*}\\ \\iff\\ \\mathbf{p} = \\begin{bmatrix}a\\\\ b\\\\ c\\\\ d\\\\ \\vdots\\end{bmatrix}\\]\n\n\n\n\n\\[\\begin{align*} p[x] &= a + bx + cx^2 + dx^3 + \\cdots \\\\ &= a\\mathbf{e}_0 + b\\mathbf{e}_1 + c \\mathbf{e}_2 + d\\mathbf{e}_3 + \\dots \\\\& \\iff\\ \\mathbf{p} = \\begin{bmatrix}a\\\\ b\\\\ c\\\\ d\\\\ \\vdots\\end{bmatrix} \\end{align*}\\]\n\n\n\nSince differentiation is linear, we‚Äôre able to apply the rule \\(\\frac{\\partial}{\\partial x} x^n = nx^{n-1}\\) to each term.\n\n\n\n\\[\\begin{align*}\\frac{\\partial}{\\partial x}p[x] &= \\vphantom{\\Bigg\\vert}a\\frac{\\partial}{\\partial x}1 + b\\frac{\\partial}{\\partial x}x + c\\frac{\\partial}{\\partial x}x^2 + d\\frac{\\partial}{\\partial x}x^3 + \\dots \\\\ &= b + 2cx + 3dx^2 + \\cdots\\\\ &= b\\mathbf{e}_0 + 2c\\mathbf{e}_1 + 3d\\mathbf{e}_2 + \\dots\\end{align*}  \\ \\iff\\ \\frac{\\partial}{\\partial x}\\mathbf{p} = \\begin{bmatrix}b\\\\ 2c\\\\ 3d\\\\ \\vdots\\end{bmatrix}\\]\n\n\n\n\n\\[\\begin{align*}\\frac{\\partial}{\\partial x}p[x] &= \\vphantom{\\Bigg\\vert}a\\frac{\\partial}{\\partial x}1 + b\\frac{\\partial}{\\partial x}x + c\\frac{\\partial}{\\partial x}x^2\\, +\\\\ & \\phantom{=} d\\frac{\\partial}{\\partial x}x^3 + \\dots \\\\ &= b + 2cx + 3dx^2 + \\cdots\\\\ &= b\\mathbf{e}_0 + 2c\\mathbf{e}_1 + 3d\\mathbf{e}_2 + \\dots  \\\\ &\\iff\\ \\frac{\\partial}{\\partial x}\\mathbf{p} = \\begin{bmatrix}b\\\\ 2c\\\\ 3d\\\\ \\vdots\\end{bmatrix}\\end{align*}\\]\n\n\n\nWe‚Äôve performed a linear transformation on the coefficients, so we can represent differentiation as a matrix!\n\n\\[\\frac{\\partial}{\\partial x}\\mathbf{p} = \\begin{bmatrix}0 & 1 & 0 & 0 & \\cdots\\\\ 0 & 0 & 2 & 0 & \\cdots\\\\ 0 & 0 & 0 & 3 & \\cdots\\\\ \\vdots & \\vdots & \\vdots & \\vdots & \\ddots \\end{bmatrix}\\begin{bmatrix}a\\\\ b\\\\ c\\\\ d\\\\ \\vdots\\end{bmatrix} = \\begin{bmatrix}b\\\\ 2c\\\\ 3d\\\\ \\vdots\\end{bmatrix}\\]\n\nEach column of the differentiation operator is itself a polynomial, so this matrix represents a change of basis.\n\n\\[\\frac{\\partial}{\\partial x} = \\begin{bmatrix} \\vert & \\vert & \\vert & \\vert & \\vert &  \\\\ 0 & 1 & 2x & 3x^2 & 4x^3 & \\cdots \\\\ \\vert & \\vert & \\vert & \\vert & \\vert &  \\end{bmatrix}\\]\n\nAs we can see, the differentiation operator simply maps each basis function to its derivative.\n\nThis result also applies to the larger space of analytic real functions, which includes polynomials, exponential functions, trigonometric functions, logarithms, and other familiar names.\nBy definition, an analytic function can be expressed as a Taylor series about \\(0\\):\n\n\\[f[x] = \\sum_{n=0}^\\infty \\frac{f^{(n)}[0]}{n!}x^n = \\sum_{n=0}^\\infty \\alpha_n x^n\\]\n\nWhich is a linear combination of our polynomial basis functions.\nThat means a Taylor expansion is essentially a change of basis into the sequence of powers, where our differentiation operator is quite simple.5\n\n\n\nDiagonalization\n\n\n  Review: Eigenvectors and eigenvalues | Chapter 14, Essence of linear algebra.\n\n\nMatrix decompositions are arguably the crowning achievement of linear algebra.\nTo get started, let‚Äôs review what diagonalization means for a \\(3\\times3\\) real matrix \\(\\mathbf{A}\\).\n\nEigenvectors\n\nA vector \\(\\mathbf{u}\\) is an eigenvector of the matrix \\(\\mathbf{A}\\) when the following condition holds:\n\n\n\n$$ \\mathbf{Au} = \\lambda \\mathbf{u} $$\n\n\n\n\n\n\nThe eigenvalue \\(\\lambda\\) may be computed by solving the characteristic polynomial of \\(\\mathbf{A}\\).\nEigenvalues may be real or complex.\n\nThe matrix \\(\\mathbf{A}\\) is diagonalizable when it admits three linearly independent eigenvectors, each with a corresponding real eigenvalue.\nThis set of eigenvectors constitutes an eigenbasis for the underlying vector space, indicating that we can express any vector \\(\\mathbf{x}\\) via their linear combination.\n\n\n\n$$ \\mathbf{x} = \\alpha\\mathbf{u}_1 + \\beta\\mathbf{u}_2 + \\gamma\\mathbf{u}_3 $$\n\n\n\n\n\n\nTo multiply \\(\\mathbf{x}\\) by \\(\\mathbf{A}\\), we just have to scale each component by its corresponding eigenvalue.\n\n\n\n$$ \\begin{align*} \\mathbf{Ax} &= \\alpha\\mathbf{A}\\mathbf{u}_1 + \\beta\\mathbf{A}\\mathbf{u}_2 + \\gamma\\mathbf{A}\\mathbf{u}_3 \\\\\n&= \\alpha\\lambda_1\\mathbf{u}_1 + \\beta\\lambda_2\\mathbf{u}_2 + \\gamma\\lambda_3\\mathbf{u}_3 \\end{align*} $$\n\n\n\n\n\n\nFinally, re-combining the eigenvectors expresses the result in the standard basis.\n\n\n\nIntuitively, we‚Äôve shown that multiplying by \\(\\mathbf{A}\\) is equivalent to a change of basis, a scaling, and a change back.\nThat means we can write \\(\\mathbf{A}\\) as the product of an invertible matrix \\(\\mathbf{U}\\) and a diagonal matrix \\(\\mathbf{\\Lambda}\\).\n\n\n\n\\[\\begin{align*} \\mathbf{A} &= \\mathbf{U\\Lambda U^{-1}} \\\\\n &= \\begin{bmatrix}\\vert & \\vert & \\vert \\\\ \\mathbf{u}_1 & \\mathbf{u}_2 & \\mathbf{u}_3 \\\\ \\vert & \\vert & \\vert \\end{bmatrix}\n \\begin{bmatrix}\\lambda_1 & 0 & 0 \\\\ 0 & \\lambda_2 & 0 \\\\ 0 & 0 & \\lambda_3 \\end{bmatrix}\n \\begin{bmatrix}\\vert & \\vert & \\vert \\\\ \\mathbf{u}_1 & \\mathbf{u}_2 & \\mathbf{u}_3 \\\\ \\vert & \\vert & \\vert \\end{bmatrix}^{-1}\n\\end{align*}\\]\n\n\n\n\n\\[\\begin{align*} \\mathbf{A} &= \\mathbf{U\\Lambda U^{-1}} \\\\\n &= \\begin{bmatrix}\\vert & \\vert & \\vert \\\\ \\mathbf{u}_1 & \\mathbf{u}_2 & \\mathbf{u}_3 \\\\ \\vert & \\vert & \\vert \\end{bmatrix}\n \\\\ & \\phantom{=} \\begin{bmatrix}\\lambda_1 & 0 & 0 \\\\ 0 & \\lambda_2 & 0 \\\\ 0 & 0 & \\lambda_3 \\end{bmatrix}\n \\\\ & \\phantom{=} \\begin{bmatrix}\\vert & \\vert & \\vert \\\\ \\mathbf{u}_1 & \\mathbf{u}_2 & \\mathbf{u}_3 \\\\ \\vert & \\vert & \\vert \\end{bmatrix}^{-1}\n\\end{align*}\\]\n\n\n\nNote that \\(\\mathbf{U}\\) is invertible because its columns (the eigenvectors) form a basis for \\(\\mathbb{R}^3\\).\nWhen multiplying by \\(\\mathbf{x}\\), \\(\\mathbf{U}^{-1}\\) converts \\(\\mathbf{x}\\) to the eigenbasis, \\(\\mathbf{\\Lambda}\\) scales by the corresponding eigenvalues, and \\(\\mathbf{U}\\) takes us back to the standard basis.\n\nIn the presence of complex eigenvalues, \\(\\mathbf{A}\\) may still be diagonalizable if we allow \\(\\mathbf{U}\\) and \\(\\mathbf{\\Lambda}\\) to include complex entires.\nIn this case, the decomposition as a whole still maps real vectors to real vectors, but the intermediate values become complex.\n\nEigenfunctions\n\n\n  Review: What‚Äôs so special about Euler‚Äôs number e? | Chapter 5, Essence of calculus.\n\n\nSo, what does diagonalization mean in a vector space of functions?\nGiven a linear operator \\(\\mathcal{L}\\), you might imagine a corresponding definition for eigenfunctions:\n\n\\[\\mathcal{L}f = \\psi f\\]\n\nThe scalar \\(\\psi\\) is again known as an eigenvalue.\nSince \\(\\mathcal{L}\\) is infinite-dimensional, it doesn‚Äôt have a characteristic polynomial‚Äîthere‚Äôs not a straightforward method for computing \\(\\psi\\).\n\nNevertheless, let‚Äôs attempt to diagonalize differentiation on analytic functions.\nThe first step is to find the eigenfunctions.\nStart by applying the above condition to our differentiation operator in the power basis:\n\n\\[\\begin{align*}\n&& \\frac{\\partial}{\\partial x}\\mathbf{p} = \\psi \\mathbf{p} \\vphantom{\\Big|}& \\\\\n&\\iff& \\begin{bmatrix}0 & 1 & 0 & 0 & \\cdots\\\\ 0 & 0 & 2 & 0 & \\cdots\\\\ 0 & 0 & 0 & 3 & \\cdots\\\\ \\vdots & \\vdots & \\vdots & \\vdots & \\ddots \\end{bmatrix}\\begin{bmatrix}p_0\\\\ p_1\\\\ p_2\\\\ p_3\\\\ \\vdots\\end{bmatrix}\n&= \\begin{bmatrix}\\psi p_0\\\\ \\psi p_1 \\\\ \\psi p_2 \\\\ \\psi p_3 \\\\ \\vdots \\end{bmatrix} \\\\\n&\\iff& \\begin{cases} p_1 &= \\psi p_0 \\\\ p_2 &= \\frac{\\psi}{2} p_1 \\\\ p_3 &= \\frac{\\psi}{3} p_2 \\\\ &\\dots \\end{cases} &\n\\end{align*}\\]\n\nThis system of equations implies that all coefficients are determined solely by our choice of constants \\(p_0\\) and \\(\\psi\\).\nWe can explicitly write down their relationship as \\(p_i = \\frac{\\psi^i}{i!}p_0\\).\n\nNow, let‚Äôs see what this class of polynomials actually looks like.\n\n\n\n\\[p[x] = p_0 + p_0\\psi x + p_0\\frac{\\psi^2}{2}x^2 + p_0\\frac{\\psi^3}{6}x^3 + p_0\\frac{\\psi^4}{24}x^4 + \\dots\\]\n\n\n\n\n\\[\\begin{align*}\np[x] &= p_0 + p_0\\psi x + p_0\\frac{\\psi^2}{2}x^2\\, +\\\\ &\\phantom{=} p_0\\frac{\\psi^3}{6}x^3 + p_0\\frac{\\psi^4}{24}x^4 + \\dots\n\\end{align*}\\]\n\n\n\nDifferentiation shows that this function is, in fact, an eigenfunction for the eigenvalue \\(\\psi\\).\n\n\n\n\\[\\begin{align*} \\frac{\\partial}{\\partial x} p[x] &= 0 + p_0\\psi + p_0 \\psi^2 x + p_0\\frac{\\psi^3}{2}x^2 + p_0\\frac{\\psi^4}{6}x^3 + \\dots \\\\\n&= \\psi p[x] \\end{align*}\\]\n\n\n\n\n\\[\\begin{align*}\n\\frac{\\partial}{\\partial x} p[x] &= 0 + p_0\\psi + p_0 \\psi^2 x\\, +\\\\ &\\phantom{=} p_0\\frac{\\psi^3}{2}x^2 + p_0\\frac{\\psi^4}{6}x^3 + \\dots \\\\\n&= \\psi p[x]\n\\end{align*}\\]\n\n\n\nWith a bit of algebraic manipulation, the definition of \\(e^{x}\\) pops out:\n\n\n\n\\[\\begin{align*} p[x] &= p_0 + p_0\\psi x + p_0\\frac{\\psi^2}{2}x^2 + p_0\\frac{\\psi^3}{6}x^3 + p_0\\frac{\\psi^4}{24}x^4 + \\dots \\\\\n&= p_0\\left((\\psi x) + \\frac{1}{2!}(\\psi x)^2 + \\frac{1}{3!}(\\psi x)^3 + \\frac{1}{4!}(\\psi x)^4 + \\dots\\right) \\\\\n&= p_0 e^{\\psi x} \\end{align*}\\]\n\n\n\n\n\\[\\begin{align*}\np[x] &= p_0 + p_0\\psi x + p_0\\frac{\\psi^2}{2}x^2\\, +\\\\ &\\phantom{=} p_0\\frac{\\psi^3}{6}x^3 + p_0\\frac{\\psi^4}{24}x^4 + \\dots \\\\\n&= p_0\\Big((\\psi x) + \\frac{1}{2!}(\\psi x)^2\\, +\\\\ &\\phantom{=p_0\\Big((} \\frac{1}{3!}(\\psi x)^3 + \\frac{1}{4!}(\\psi x)^4 + \\dots\\Big) \\\\\n&= p_0 e^{\\psi x}\n\\end{align*}\\]\n\n\n\nTherefore, functions of the form \\(p_0e^{\\psi x}\\) are eigenfunctions for the eigenvalue \\(\\psi\\), including when \\(\\psi=0\\).\n\nDiagonalizing Differentiation\n\nWe‚Äôve found the eigenfunctions of the derivative operator, but can we diagonalize it?\nIdeally, we would express differentiation as the combination of an invertible operator \\(\\mathcal{L}\\) and a diagonal operator \\(\\mathcal{D}\\).\n\n\n\n\\[\\begin{align*} \\frac{\\partial}{\\partial x} &= \\mathcal{L} \\mathcal{D} \\mathcal{L}^{-1}  \\\\\n&=\n\\begin{bmatrix} \\vert & \\vert &  & \\\\ \\alpha e^{\\psi_1 x} & \\beta e^{\\psi_2 x} & \\dots \\\\ \\vert & \\vert &  \\end{bmatrix}\n\\begin{bmatrix} \\psi_1 & 0 & \\dots \\\\ 0 & \\psi_2 & \\dots \\\\ \\vdots & \\vdots & \\ddots \\end{bmatrix}\n{\\color{red} \\begin{bmatrix} \\vert & \\vert &  & \\\\ \\alpha e^{\\psi_1 x} & \\beta e^{\\psi_2 x} & \\dots \\\\ \\vert & \\vert &  \\end{bmatrix}^{-1} }\n\\end{align*}\\]\n\n\n\n\n\\[\\begin{align*} \\frac{\\partial}{\\partial x} &= \\mathcal{L} \\mathcal{D} \\mathcal{L}^{-1}  \\\\\n&=\n\\begin{bmatrix} \\vert & \\vert &  & \\\\ \\alpha e^{\\psi_1 x} & \\beta e^{\\psi_2 x} & \\dots \\\\ \\vert & \\vert &  \\end{bmatrix}\n\\\\ & \\phantom{=} \\begin{bmatrix} \\psi_1 & 0 & \\dots \\\\ 0 & \\psi_2 & \\dots \\\\ \\vdots & \\vdots & \\ddots \\end{bmatrix}\n\\\\ & \\phantom{=} {\\color{red} \\begin{bmatrix} \\vert & \\vert &  & \\\\ \\alpha e^{\\psi_1 x} & \\beta e^{\\psi_2 x} & \\dots \\\\ \\vert & \\vert &  \\end{bmatrix}^{-1} }\n\\end{align*}\\]\n\n\n\nDiagonalization is only possible when our eigenfunctions form a basis.\nThis would be true if all analytic functions are expressible as a linear combination of exponentials.\nHowever‚Ä¶\n\n\n  \n    Counterexample: $$f[x] = x$$\n  \n  \n    \n      First assume that \\(f[x] = x\\) can be represented as a linear combination of exponentials.\nSince analytic functions have countably infinite dimensionality, we should only need a countably infinite sum:\n\n\\[f[x] = x = \\sum_{n=0}^\\infty \\alpha_n e^{\\psi_n x}\\]\n\n      Differentiating both sides:\n\n\\[\\begin{align*} f^{\\prime}[x] &= 1 = \\sum_{n=0}^\\infty \\psi_n\\alpha_n e^{\\psi_n x} \\\\\n f^{\\prime\\prime}[x] &= 0 = \\sum_{n=0}^\\infty \\psi_n^2\\alpha_n e^{\\psi_n x} \\end{align*}\\]\n\n      Since \\(e^{\\psi_n x}\\) and \\(e^{\\psi_m x}\\) are linearly independent when \\(n\\neq m\\), the final equation implies that all \\(\\alpha = 0\\), except possibly the \\(\\alpha_\\xi\\) corresponding to \\(\\psi_\\xi = 0\\).\nTherefore:\n\n\\[\\begin{align*}\n1 &= \\sum_{n=0}^\\infty \\psi_n\\alpha_n e^{\\psi_n x}\\\\\n&= \\psi_\\xi \\alpha_\\xi + \\sum_{n\\neq \\xi} 0\\psi_n e^{\\psi_n x} \\\\\n&= 0\n\\end{align*}\\]\n\n      That‚Äôs a contradiction‚Äîthe linear combination representing \\(f[x] = x\\) does not exist.\n\n      A similar argument shows that we can‚Äôt represent any non-constant function whose \\(n\\)th derivative is zero, nor periodic functions like sine and cosine.\n    \n  \n\n\nReal exponentials don‚Äôt constitute a basis, so we cannot construct an invertible \\(\\mathcal{L}\\).\n\nThe Laplace Transform\n\nWe previously mentioned that more matrices can be diagonalized if we allow the decomposition to contain complex numbers.\nAnalogously, more linear operators are diagonalizable in the larger vector space of functions from \\(\\mathbb{R}\\) to \\(\\mathbb{C}\\).\n\nDifferentiation works the same way in this space; we‚Äôll still find that its eigenfunctions are exponential.\n\n\\[\\frac{\\partial}{\\partial x} e^{(a+bi)x} = (a+bi)e^{(a+bi)x}\\]\n\nHowever, the new eigenfunctions have complex eigenvalues, so we still can‚Äôt diagonalize.\nWe‚Äôll need to consider the still larger space of functions from \\(\\mathbb{C}\\) to \\(\\mathbb{C}\\).\n\n\\[\\frac{\\partial}{\\partial x} : (\\mathbb{C}\\mapsto\\mathbb{C}) \\mapsto (\\mathbb{C}\\mapsto\\mathbb{C})\\]\n\nIn this space, differentiation can be diagonalized via the Laplace transform.\nAlthough useful for solving differential equations, the Laplace transform is non-trivial to invert, so we won‚Äôt discuss it further.\nIn the following sections, we‚Äôll delve into an operator that can be easily diagonalized in \\(\\mathbb{R}\\mapsto\\mathbb{C}\\): the Laplacian.\n\n\n\nInner Product Spaces\n\n\n  Review: Dot products and duality | Chapter 9, Essence of linear algebra.\n\n\nBefore we get to the spectral theorem, we‚Äôll need to understand one more topic: inner products.\nYou‚Äôre likely already familiar with one example of an inner product‚Äîthe Euclidean dot product.\n\n\\[\\begin{bmatrix}x\\\\ y\\\\ z\\end{bmatrix} \\cdot \\begin{bmatrix}a\\\\ b\\\\ c\\end{bmatrix} = ax + by + cz\\]\n\nAn inner product describes how to measure a vector along another vector.\nFor example, \\(\\mathbf{u}\\cdot\\mathbf{v}\\) is proportional to the length of the projection of \\(\\mathbf{u}\\) onto \\(\\mathbf{v}\\).\n\n\n\n$$ \\mathbf{u} \\cdot \\mathbf{v} =\\|\\mathbf{u}\\|\\|\\mathbf{v}\\|\\cos[\\theta] $$\n\n\n\n\n\n\nWith a bit of trigonometry, we can show that the dot product is equivalent to multiplying the vectors‚Äô lengths with the cosine of their angle.\nThis relationship suggests that the product of a vector with itself produces the square of its length.\n\n\\[\\begin{align*} \\mathbf{u}\\cdot\\mathbf{u} &= \\|\\mathbf{u}\\|\\|\\mathbf{u}\\|\\cos[0] \\\\\n&= \\|\\mathbf{u}\\|^2\n\\end{align*}\\]\n\nSimilarly, when two vectors form a right angle (are orthogonal), their dot product is zero.\n\n\n\n$$ \\begin{align*}  \\mathbf{u} \\cdot \\mathbf{v} &= \\|\\mathbf{u}\\|\\|\\mathbf{v}\\|\\cos[90^\\circ] \\\\ &= 0 \\end{align*} $$\n\n\n\n\n\n\nOf course, the Euclidean dot product is only one example of an inner product.\nIn more general spaces, the inner product is denoted using angle brackets, such as \\(\\langle \\mathbf{u}, \\mathbf{v} \\rangle\\).\n\n\n  The length (also known as the norm) of a vector is defined as \\(\\|\\mathbf{u}\\| = \\sqrt{\\langle \\mathbf{u}, \\mathbf{u} \\rangle}\\).\n  Two vectors are orthogonal if their inner product is zero: \\(\\ \\mathbf{u} \\perp \\mathbf{v}\\ \\iff\\ \\langle \\mathbf{u}, \\mathbf{v} \\rangle = 0\\).\n\n\nA vector space augmented with an inner product is known as an inner product space.\n\nA Functional Inner Product\n\nWe can‚Äôt directly apply the Euclidean dot product to our space of real functions, but its \\(N\\)-dimensional generalization is suggestive.\n\n\\[\\begin{align*} \\mathbf{u} \\cdot \\mathbf{v} &= u_1v_1 + u_2v_2 + \\dots + u_Nv_N \\\\ &= \\sum_{i=1}^N u_iv_i \\end{align*}\\]\n\nGiven countable indices, we simply match up the values, multiply them, and add the results.\nWhen indices are uncountable, we can convert the discrete sum to its continuous analog: an integral!\n\n\\[\\langle f, g \\rangle = \\int_a^b f[x]g[x] \\, dx\\]\n\nWhen \\(f\\) and \\(g\\) are similar, multiplying them produces a larger function; when they‚Äôre different, they cancel out.\nIntegration measures their product over some domain to produce a scalar result.\n\n\n\n\n\n\n\n\n\n\nOf course, not all functions can be integrated.\nOur inner product space will only contain functions that are square integrable over the domain \\([a, b]\\), which may be \\([-\\infty, \\infty]\\).\nLuckily, the important properties of our inner product do not depend on the choice of integration domain.\n\nProofs\n\nBelow, we‚Äôll briefly cover functions from \\(\\mathbb{R}\\) to \\(\\mathbb{C}\\).\nIn this space, our intuitive notion of similarity still applies, but we‚Äôll use a slightly more general inner product:\n\n\\[\\langle f,g \\rangle = \\int_a^b f[x]\\overline{g[x]}\\, dx\\]\n\nWhere \\(\\overline{x}\\) denotes conjugation, i.e. \\(\\overline{a + bi} = a - bi\\).\n\nLike other vector space operations, an inner product must satisfy several axioms:\n\n\n  \n    Conjugate Symmetry\n  \n  \n  \n  For all vectors $$\\mathbf{u}, \\mathbf{v} \\in \\mathcal{V}$$:\n\n  $$\\langle \\mathbf{u}, \\mathbf{v} \\rangle = \\overline{\\langle \\mathbf{v}, \\mathbf{u} \\rangle}$$\n\n  Conjugation may be taken outside the integral, making this one easy:\n\n  $$\\begin{align*} \\langle f, g \\rangle &= \\int_a^b f[x]\\overline{g[x]} \\, dx \\\\\n  &= \\int_a^b \\overline{g[x]\\overline{f[x]}} \\, dx \\\\\n  &= \\overline{\\int_a^b g[x]\\overline{f[x]} \\, dx} \\\\\n  &= \\overline{\\langle g, f \\rangle}\n  \\end{align*}$$\n\n  Note that we require conjugate symmetry because it implies $$\\langle\\mathbf{u}, \\mathbf{u}\\rangle = \\overline{\\langle\\mathbf{u}, \\mathbf{u}\\rangle}$$, i.e. the inner product of a vector with itself is real.\n  \n  \n\n\n\n  \n    Linearity in the First Argument\n  \n  \n  \n  For all vectors $$\\mathbf{u}, \\mathbf{v}, \\mathbf{w} \\in \\mathcal{V}$$ and scalars $$\\alpha, \\beta \\in \\mathbb{F}$$:\n\n  $$\\langle \\alpha \\mathbf{u} + \\beta \\mathbf{v}, \\mathbf{w} \\rangle = \\alpha\\langle \\mathbf{u}, \\mathbf{w} \\rangle + \\beta\\langle \\mathbf{v}, \\mathbf{w} \\rangle $$\n\n  The proof follows from linearity of integration, as well as our vector space axioms:\n\n\n\n\\[\\begin{align*} \\langle \\alpha f + \\beta g, h \\rangle &= \\int_a^b (\\alpha f + \\beta g)[x]\\overline{h[x]} \\, dx \\\\\n&= \\int_a^b (\\alpha f[x] + \\beta g[x])\\overline{h[x]} \\, dx \\\\\n&= \\int_a^b \\alpha f[x]\\overline{h[x]} + \\beta g[x]\\overline{h[x]} \\, dx \\\\\n&= \\alpha\\int_a^b f[x]\\overline{h[x]}\\, dx + \\beta\\int_a^b g[x]\\overline{h[x]} \\, dx \\\\\n&= \\alpha\\langle f, h \\rangle + \\beta\\langle g, h \\rangle\n\\end{align*}\\]\n\n      \n\n\n\\[\\begin{align*} &\\langle \\alpha f + \\beta g, h \\rangle\\\\ &= \\int_a^b (\\alpha f + \\beta g)[x]\\overline{h[x]} \\, dx \\\\\n&= \\int_a^b (\\alpha f[x] + \\beta g[x])\\overline{h[x]} \\, dx \\\\\n&= \\int_a^b \\alpha f[x]\\overline{h[x]} + \\beta g[x]\\overline{h[x]} \\, dx \\\\\n&= \\alpha\\int_a^b f[x]\\overline{h[x]}\\, dx\\, +\\\\&\\hphantom{==} \\beta\\int_a^b g[x]\\overline{h[x]} \\, dx \\\\\n&= \\alpha\\langle f, h \\rangle + \\beta\\langle g, h \\rangle\n\\end{align*}\\]\n\n      \n\n  Given conjugate symmetry, an inner product is also antilinear in the second argument.\n  \n  \n\n\n\n  \n    Positive-Definiteness\n  \n  \n  \n  For all $$\\mathbf{u} \\in \\mathcal{V}$$:\n\n  $$ \\begin{cases} \\langle \\mathbf{u}, \\mathbf{u} \\rangle = 0 & \\text{if } \\mathbf{u} = \\mathbf{0} \\\\ \\langle \\mathbf{u}, \\mathbf{u} \\rangle > 0 & \\text{otherwise} \\end{cases} $$\n\n  By conjugate symmetry, we know $$\\langle f, f \\rangle$$ is real, so we can compare it with zero.\n  \n  However, rigorously proving this result requires measure-theoretic concepts beyond the scope of this post.\n  In brief, we redefine $$\\mathbf{0}$$ not as specifically $$\\mathbf{0}[x] = 0$$, but as an equivalence class of functions that are zero ",
    "Confidence": 0.9806124
  }
]