[
  {
    "Title": "Functions are Vectors",
    "Url": "https://thenumb.at/Functions-are-Vectors/",
    "Timestamp": "2023-07-30T11:03:15",
    "Domain": "thenumb.at",
    "Description": "Conceptualizing functions as infinite-dimensional vectors lets us apply the tools of linear algebra to a vast landscape of new problems, from image and geometry processing to curve fitting, light transport, and machine learning.\n\nPrerequisites: introductory linear algebra, introductory calculus, introductory differential equations.\n\n\n  Functions as Vectors\n    \n      Vector Spaces\n      Linear Operators\n      Diagonalization\n      Inner Product Spaces\n      The Spectral Theorem\n    \n  \n  Applications\n    \n      Fourier Series\n      Image Compression\n      Geometry Processing\n      Further Reading\n    \n  \n\n\n\n\nFunctions as Vectors\n\nVectors are often first introduced as lists of real numbers—i.e. the familiar notation we use for points, directions, and more.\n\n\n\n$$ \\mathbf{v} = \\begin{bmatrix}x\\\\y\\\\z\\end{bmatrix} $$\n\n\n\n\n\n\nYou may recall that this representation is only one example of an abstract vector space.\nThere are many other types of vectors, such as lists of complex numbers, graph cycles, and even magic squares.\n\nHowever, all of these vector spaces have one thing in common: a finite number of dimensions.\nThat is, each kind of vector can be represented as a collection of \\(N\\) numbers, though the definition of “number” varies.\n\nIf any \\(N\\)-dimensional vector is essentially a length-\\(N\\) list, we could also consider a vector to be a mapping from an index to a value.\n\n\\[\\begin{align*}\n\\mathbf{v}_1 &= x\\\\\n\\mathbf{v}_2 &= y\\\\\n\\mathbf{v}_3 &= z\n\\end{align*}\\ \\iff\\ \\mathbf{v} = \\begin{bmatrix}x \\\\ y \\\\ z\\end{bmatrix}\\]\n\nWhat does this perspective hint at as we increase the number of dimensions?\n\n\n\n\n    Dimensions\n    \n\n\n\n\nIn higher dimensions, vectors start to look more like functions!\n\nCountably Infinite Indices\n\nOf course, a finite-length vector only specifies a value at a limited number of indices.\nCould we instead define a vector that contains infinitely many values?\n\nWriting down a vector representing a function on the natural numbers (\\(\\mathbb{N}\\))—or any other countably infinite domain—is straightforward: just extend the list indefinitely.\n\n\n\n$$ \\begin{align*}\\mathbf{v}_1 &= 1\\\\\\mathbf{v}_2 &= 2\\\\ &\\vdots \\\\ \\mathbf{v}_i &= i\\end{align*}\\ \\iff\\ \\mathbf{v} = \\begin{bmatrix}1 \\\\ 2 \\\\ 3 \\\\ \\vdots \\end{bmatrix} $$\n\n\n\n\n\n\nThis vector could represent the function \\(f(x) = x\\), where \\(x \\in \\mathbb{N}\\).1\n\nUncountably Infinite Indices\n\nMany interesting functions are defined on the real numbers (\\(\\mathbb{R}\\)), so may not be representable as a countably infinite vector.\nTherefore, we will have to make a larger conceptual leap: not only will our set of indices be infinite, it will be uncountably infinite.\n\nThat means we can’t write down vectors as lists at all—it is impossible to assign an integer index to each element of an uncountable set.\nSo, how can we write down a vector mapping a real index to a certain value?\n\nNow, a vector really is just an arbitrary function:\n\n\n\n$$ \\mathbf{v}_{x} = x^2\\ \\iff\\ \\mathbf{v} = \\begin{bmatrix} x \\mapsto x^2 \\end{bmatrix} $$\n\n\n\n\n\n\nPrecisely defining how and why we can represent functions as infinite-dimensional vectors is the purview of functional analysis.\nIn this post, we won’t attempt to prove our results in infinite dimensions: we will focus on building intuition via analogies to finite-dimensional linear algebra.\n\n\n\nVector Spaces\n\n\n  Review: Abstract vector spaces | Chapter 16, Essence of linear algebra.\n\n\nFormally, a vector space is defined by choosing a set of vectors \\(\\mathcal{V}\\), a scalar field \\(\\mathbb{F}\\), and a zero vector \\(\\mathbf{0}\\).\nThe field \\(\\mathbb{F}\\) is often the real numbers (\\(\\mathbb{R}\\)), complex numbers (\\(\\mathbb{C}\\)), or a finite field such as the integers modulo a prime (\\(\\mathbb{Z}_p\\)).\n\nAdditionally, we must specify how to add two vectors and how to multiply a vector by a scalar.\n\n\\[\\begin{align*}\n(+)\\ &:\\ \\mathcal{V}\\times\\mathcal{V}\\mapsto\\mathcal{V}\\\\\n(\\cdot)\\ &:\\ \\mathbb{F}\\times\\mathcal{V} \\mapsto \\mathcal{V}\n\\end{align*}\\]\n\nTo describe a vector space, our definitions must entail several vector space axioms.\n\nA Functional Vector Space\n\nIn the following sections, we’ll work with the vector space of real functions.\nTo avoid ambiguity, square brackets are used to denote function application.\n\n\n  The scalar field \\(\\mathbb{F}\\) is the real numbers \\(\\mathbb{R}\\).\n  The set of vectors \\(\\mathcal{V}\\) contains functions from \\(\\mathbb{R}\\) to \\(\\mathbb{R}\\).2\n  \\(\\mathbf{0}\\) is the zero function, i.e. \\(\\mathbf{0}[x] = 0\\).\n\n\nAdding functions corresponds to applying the functions separately and summing the results.\n\n\n$$ (f + g)[x] = f[x] + g[x] $$\n\n\n\n\n\n\nThis definition generalizes the typical element-wise addition rule—it’s like adding the two values at each index.\n\n\\[f+g = \\begin{bmatrix}f_1 + g_1 \\\\ f_2 + g_2 \\\\ \\vdots \\end{bmatrix}\\]\n\nMultiplying a function by a scalar corresponds to applying the function and scaling the result.\n\n\n\n$$ (\\alpha f)[x] = \\alpha f[x] $$\n\n\n\n\n\n\nThis rule similarly generalizes element-wise multiplication—it’s like scaling the value at each index.\n\n\\[\\alpha f = \\begin{bmatrix}\\alpha f_1 \\\\ \\alpha f_2 \\\\ \\vdots \\end{bmatrix}\\]\n\nProofs\n\nGiven these definitions, we can now prove all necessary vector space axioms.\nWe will illustrate the analog of each property in \\(\\mathbb{R}^2\\), the familiar vector space of two-dimensional arrows.\n\n\n  \n    Vector Addition is Commutative\n  \n  \n  \nFor all vectors $$\\mathbf{u}, \\mathbf{v} \\in \\mathcal{V}$$:\n\n\n\n$$\\mathbf{u} + \\mathbf{v} = \\mathbf{v} + \\mathbf{u}$$\n\n\n\n\n\n\nSince real addition is commutative, this property follows directly from our definition of vector addition:\n\n$$\\begin{align*}\n(f + g)[x] &= f[x] + g[x]\\\\\n&= g[x] + f[x]\\\\\n&= (g + f)[x]\n\\end{align*}$$\n  \n  \n\n\n\n  \n    Vector Addition is Associative\n  \n  \n  \nFor all vectors $$\\mathbf{u}, \\mathbf{v}, \\mathbf{w} \\in \\mathcal{V}$$:\n\n\n\n$$(\\mathbf{u} + \\mathbf{v}) + \\mathbf{w} = \\mathbf{u} + (\\mathbf{v} + \\mathbf{w})$$\n\n\n\n\n\n\nThis property also follows from our definition of vector addition:\n\n$$\\begin{align*}\n((f + g) + h)[x] &= (f + g)[x] + h[x]\\\\\n&= f[x] + g[x] + h[x]\\\\\n&= f[x] + (g[x] + h[x])\\\\\n&= f[x] + (g + h)[x]\\\\\n&= (f + (g + h))[x]\n\\end{align*}$$\n\n  \n  \n\n\n\n  \n    $$\\mathbf{0}$$ is an Additive Identity\n  \n  \n  \nFor all vectors $$\\mathbf{u} \\in \\mathcal{V}$$:\n\n\n\n$$\\mathbf{0} + \\mathbf{u} = \\mathbf{u} $$\n\n\n\n\n\n\nThis one is easy:\n\n$$\\begin{align*}\n(\\mathbf{0} + f)[x] &= \\mathbf{0}[x] + f[x]\\\\\n&= 0 + f[x]\\\\\n&= f[x]\n\\end{align*}$$\n\n  \n  \n\n\n\n  \n    Additive Inverses Exist\n  \n  \n  \nFor all vectors $$\\mathbf{u} \\in \\mathcal{V}$$, there exists a vector $$-\\mathbf{u} \\in \\mathcal{V}$$ such that:\n\n\n\n$$\\mathbf{u} + (-\\mathbf{u}) = \\mathbf{0}$$\n\n\n\n\n\n\nNegation is defined as applying $$f$$ and negating the result: $$(-f)[x] = -f[x]$$.\nClearly, $$-f$$ is also in $$\\mathcal{V}$$.\n\n$$\\begin{align*}\n(f + (-f))[x] &= f[x] + (-f)[x]\\\\\n&= f[x] - f[x]\\\\\n&= 0\\\\\n&= \\mathbf{0}[x]\n\\end{align*}$$\n\n  \n  \n\n\n\n  \n    $$1$$ is a Multiplicative Identity\n  \n  \n  \nFor all vectors $$\\mathbf{u} \\in \\mathcal{V}$$:\n\n\n\n$$1\\mathbf{u} = \\mathbf{u}$$\n\n\n\n\n\n\nNote that $$1$$ is specified by the choice of $$\\mathbb{F}$$.\nIn our case, it is simply the real number $$1$$.\n\n$$\\begin{align*}\n(1 f)[x] &= 1 f[x]\\\\\n&= f[x]\n\\end{align*}$$\n\n  \n  \n\n\n\n  \n    Scalar Multiplication is Associative\n  \n  \n  \nFor all vectors $$\\mathbf{u} \\in \\mathcal{V}$$ and scalars $$\\alpha, \\beta \\in \\mathbb{F}$$:\n\n\n\n$$(\\alpha \\beta)\\mathbf{u} = \\alpha(\\beta\\mathbf{u})$$\n\n\n\n\n\n\nThis property follows from our definition of scalar multiplication:\n\n$$\\begin{align*}\n((\\alpha\\beta) f)[x] &= (\\alpha\\beta)f[x]\\\\\n&= \\alpha(\\beta f[x])\\\\\n&= \\alpha(\\beta f)[x]\n\\end{align*}$$\n\n  \n  \n\n\n\n  \n    Scalar Multiplication Distributes Over Vector Addition\n  \n  \n  \nFor all vectors $$\\mathbf{u}, \\mathbf{v} \\in \\mathcal{V}$$ and scalars $$\\alpha \\in \\mathbb{F}$$:\n\n\n\n$$\\alpha(\\mathbf{u} + \\mathbf{v}) = \\alpha\\mathbf{u} + \\alpha\\mathbf{v}$$\n\n\n\n\n\n\nAgain using our definitions of vector addition and scalar multiplication:\n\n$$\\begin{align*}\n(\\alpha (f + g))[x] &= \\alpha(f + g)[x]\\\\\n&= \\alpha(f[x] + g[x])\\\\\n&= \\alpha f[x] + \\alpha g[x]\\\\\n&= (\\alpha f)[x] + (\\alpha g)[x]\\\\\n&= (\\alpha f + \\alpha g)[x]\n\\end{align*}$$\n\n  \n  \n\n\n\n  \n    Scalar Multiplication Distributes Over Scalar Addition\n  \n  \n  \nFor all vectors $$\\mathbf{u} \\in \\mathcal{V}$$ and scalars $$\\alpha, \\beta \\in \\mathbb{F}$$:\n\n\n\n $$(\\alpha + \\beta)\\mathbf{u} = \\alpha\\mathbf{u} + \\beta\\mathbf{u}$$\n\n\n\n\n\n\nAgain using our definitions of vector addition and scalar multiplication:\n\n$$\\begin{align*}\n((\\alpha + \\beta)f)[x] &= (\\alpha + \\beta)f[x]\\\\\n&= \\alpha f[x] + \\beta f[x] \\\\\n&= (\\alpha f)[x] + (\\beta f)[x]\n\\end{align*}$$\n\n  \n  \n\n\nTherefore, we’ve built a vector space of functions!3\nIt may not be immediately obvious why this result is useful, but bear with us through a few more definitions—we will spend the rest of this post exploring powerful techniques arising from this perspective.\n\nA Standard Basis for Functions\n\n\n  Review: Linear combinations, span, and basis vectors | Chapter 2, Essence of linear algebra.\n\n\nUnless specified otherwise, vectors are written down with respect to the standard basis.\nIn \\(\\mathbb{R}^2\\), the standard basis consists of the two coordinate axes.\n\n\n\n$$ \\mathbf{e}_1 = \\begin{bmatrix}1 \\\\ 0\\end{bmatrix},\\,\\, \\mathbf{e}_2 = \\begin{bmatrix}0 \\\\ 1\\end{bmatrix} $$\n\n\n\n\n\n\nHence, vector notation is shorthand for a linear combination of the standard basis vectors.\n\n\n\n$$ \\mathbf{u} = \\begin{bmatrix}\\alpha \\\\ \\beta\\end{bmatrix} = \\alpha\\mathbf{e}_1 + \\beta\\mathbf{e}_2 $$\n\n\n\n\n\n\nAbove, we represented functions as vectors by assuming each dimension of an infinite-length vector contains the function’s result for that index.\nThis construction points to a natural generalization of the standard basis.\n\nJust like the coordinate axes, each standard basis function contains a \\(1\\) at one index and \\(0\\) everywhere else.\nMore precisely, for every \\(\\alpha \\in \\mathbb{R}\\),\n\n\n\\[\\mathbf{e}_\\alpha[x] = \\begin{cases} 1 & \\text{if } x = \\alpha \\\\ 0 & \\text{otherwise} \\end{cases}\\]\n\n\n\nWe can then express any real function \\(f\\) as a linear combination of these basis functions:\n\n\\[\\begin{align*} f[x] &= f[\\alpha]\\mathbf{e}_\\alpha[x] \\\\ &= f[1]\\mathbf{e}_1[x] + f[2]\\mathbf{e}_2[x] + f[\\pi]\\mathbf{e}_\\pi[x] + \\dots \\end{align*}\\]\n\nIf you evaluate this sum at \\(x\\), you’ll find that all terms are zero—except \\(\\mathbf{e}_x\\), making the result \\(f[x]\\).\n\n\n\nLinear Operators\n\n\n  Review: Change of basis | Chapter 13, Essence of linear algebra.\n\n\nNow that we can manipulate functions as vectors, let’s start transferring the tools of linear algebra to the functional perspective.\n\nOne ubiquitous operation on finite-dimensional vectors is transforming them with matrices.\nA matrix \\(\\mathbf{A}\\) encodes a linear transformation, meaning multiplication preserves linear combinations.\n\n\\[\\mathbf{A}(\\alpha \\mathbf{x} + \\beta \\mathbf{y}) = \\alpha \\mathbf{A}\\mathbf{x} + \\beta \\mathbf{A}\\mathbf{y}\\]\n\nMultiplying a vector by a matrix can be intuitively interpreted as defining a new set of coordinate axes from the matrix’s column vectors.\nThe result is a linear combination of the columns:\n\n\n\n\n\n\n\\[\\mathbf{Ax} = \\begin{bmatrix} \\vert & \\vert & \\vert \\\\ \\mathbf{u} & \\mathbf{v} & \\mathbf{w} \\\\ \\vert & \\vert & \\vert \\end{bmatrix} \\begin{bmatrix}x_1 \\\\ x_2 \\\\ x_3\\end{bmatrix} = x_1\\mathbf{u} + x_2\\mathbf{v} + x_3\\mathbf{w}\\]\n\n    \n\n\n\\[\\begin{align*}\n\\mathbf{Ax} &= \\begin{bmatrix} \\vert & \\vert & \\vert \\\\ \\mathbf{u} & \\mathbf{v} & \\mathbf{w} \\\\ \\vert & \\vert & \\vert \\end{bmatrix} \\begin{bmatrix}x_1 \\\\ x_2 \\\\ x_3\\end{bmatrix} \\\\ &= x_1\\mathbf{u} + x_2\\mathbf{v} + x_3\\mathbf{w}\n\\end{align*}\\]\n\n    \n\n\n\n\n\n\n\nWhen all vectors can be expressed as a linear combination of \\(\\mathbf{u}\\), \\(\\mathbf{v}\\), and \\(\\mathbf{w}\\), the columns form a basis for the underlying vector space.\nHere, the matrix \\(\\mathbf{A}\\) transforms a vector from the \\(\\mathbf{uvw}\\) basis into the standard basis.\n\nSince functions are vectors, we could imagine transforming a function by a matrix.\nSuch a matrix would be infinite-dimensional, so we will instead call it a linear operator and denote it with \\(\\mathcal{L}\\).\n\n\n\n\\[\\mathcal{L}f = \\begin{bmatrix} \\vert & \\vert & \\vert & \\\\ \\mathbf{f} & \\mathbf{g} & \\mathbf{h} & \\cdots \\\\ \\vert & \\vert & \\vert & \\end{bmatrix} \\begin{bmatrix}f_1\\\\ f_2 \\\\ f_3\\\\ \\vdots\\end{bmatrix} = f_1\\mathbf{f} + f_2\\mathbf{g} + f_3\\mathbf{h} + \\cdots\\]\n\n\n\n\n\\[\\begin{align*}\n\\mathcal{L}f &= \\begin{bmatrix} \\vert & \\vert & \\vert & \\\\ \\mathbf{f} & \\mathbf{g} & \\mathbf{h} & \\cdots \\\\ \\vert & \\vert & \\vert & \\end{bmatrix} \\begin{bmatrix}f_1\\\\ f_2\\\\ f_3 \\\\ \\vdots\\end{bmatrix} \\\\ &= f_1\\mathbf{f} + f_2\\mathbf{g} + f_3\\mathbf{h} + \\cdots\n\\end{align*}\\]\n\n\n\nThis visualization isn’t very accurate—we’re dealing with uncountably infinite-dimensional vectors, so we can’t actually write out an operator in matrix form.\nNonetheless, the structure is suggestive: each “column” of the operator describes a new basis function for our functional vector space.\nJust like we saw with finite-dimensional vectors, \\(\\mathcal{L}\\) represents a change of basis.\n\nDifferentiation\n\n\n  Review: Derivative formulas through geometry | Chapter 3, Essence of calculus.\n\n\nSo, what’s an example of a linear operator on functions?\nYou might recall that differentiation is linear:\n\n\\[\\frac{\\partial}{\\partial x} \\left(\\alpha f[x] + \\beta g[x]\\right) = \\alpha\\frac{\\partial f}{\\partial x} + \\beta\\frac{\\partial g}{\\partial x}\\]\n\nIt’s hard to visualize differentiation on general functions, but it’s feasible for the subspace of polynomials, \\(\\mathcal{P}\\).\nLet’s take a slight detour to examine this smaller space of functions.\n\n\\[\\mathcal{P} = \\{ p[x] = a + bx + cx^2 + dx^3 + \\cdots \\}\\]\n\nWe typically write down polynomials as a sequence of powers, i.e. \\(1, x, x^2, x^3\\), etc.\nAll polynomials are linear combinations of the functions \\(\\mathbf{e}_i[x] = x^i\\), so they constitute a countably infinite basis for \\(\\mathcal{P}\\).4\n\nThis basis provides a convenient vector notation:\n\n\n\n\\[\\begin{align*} p[x] &= a + bx + cx^2 + dx^3 + \\cdots \\\\ &= a\\mathbf{e}_0 + b\\mathbf{e}_1 + c \\mathbf{e}_2 + d\\mathbf{e}_3 + \\dots \\end{align*}\\ \\iff\\ \\mathbf{p} = \\begin{bmatrix}a\\\\ b\\\\ c\\\\ d\\\\ \\vdots\\end{bmatrix}\\]\n\n\n\n\n\\[\\begin{align*} p[x] &= a + bx + cx^2 + dx^3 + \\cdots \\\\ &= a\\mathbf{e}_0 + b\\mathbf{e}_1 + c \\mathbf{e}_2 + d\\mathbf{e}_3 + \\dots \\\\& \\iff\\ \\mathbf{p} = \\begin{bmatrix}a\\\\ b\\\\ c\\\\ d\\\\ \\vdots\\end{bmatrix} \\end{align*}\\]\n\n\n\nSince differentiation is linear, we’re able to apply the rule \\(\\frac{\\partial}{\\partial x} x^n = nx^{n-1}\\) to each term.\n\n\n\n\\[\\begin{align*}\\frac{\\partial}{\\partial x}p[x] &= \\vphantom{\\Bigg\\vert}a\\frac{\\partial}{\\partial x}1 + b\\frac{\\partial}{\\partial x}x + c\\frac{\\partial}{\\partial x}x^2 + d\\frac{\\partial}{\\partial x}x^3 + \\dots \\\\ &= b + 2cx + 3dx^2 + \\cdots\\\\ &= b\\mathbf{e}_0 + 2c\\mathbf{e}_1 + 3d\\mathbf{e}_2 + \\dots\\end{align*}  \\ \\iff\\ \\frac{\\partial}{\\partial x}\\mathbf{p} = \\begin{bmatrix}b\\\\ 2c\\\\ 3d\\\\ \\vdots\\end{bmatrix}\\]\n\n\n\n\n\\[\\begin{align*}\\frac{\\partial}{\\partial x}p[x] &= \\vphantom{\\Bigg\\vert}a\\frac{\\partial}{\\partial x}1 + b\\frac{\\partial}{\\partial x}x + c\\frac{\\partial}{\\partial x}x^2\\, +\\\\ & \\phantom{=} d\\frac{\\partial}{\\partial x}x^3 + \\dots \\\\ &= b + 2cx + 3dx^2 + \\cdots\\\\ &= b\\mathbf{e}_0 + 2c\\mathbf{e}_1 + 3d\\mathbf{e}_2 + \\dots  \\\\ &\\iff\\ \\frac{\\partial}{\\partial x}\\mathbf{p} = \\begin{bmatrix}b\\\\ 2c\\\\ 3d\\\\ \\vdots\\end{bmatrix}\\end{align*}\\]\n\n\n\nWe’ve performed a linear transformation on the coefficients, so we can represent differentiation as a matrix!\n\n\\[\\frac{\\partial}{\\partial x}\\mathbf{p} = \\begin{bmatrix}0 & 1 & 0 & 0 & \\cdots\\\\ 0 & 0 & 2 & 0 & \\cdots\\\\ 0 & 0 & 0 & 3 & \\cdots\\\\ \\vdots & \\vdots & \\vdots & \\vdots & \\ddots \\end{bmatrix}\\begin{bmatrix}a\\\\ b\\\\ c\\\\ d\\\\ \\vdots\\end{bmatrix} = \\begin{bmatrix}b\\\\ 2c\\\\ 3d\\\\ \\vdots\\end{bmatrix}\\]\n\nEach column of the differentiation operator is itself a polynomial, so this matrix represents a change of basis.\n\n\\[\\frac{\\partial}{\\partial x} = \\begin{bmatrix} \\vert & \\vert & \\vert & \\vert & \\vert &  \\\\ 0 & 1 & 2x & 3x^2 & 4x^3 & \\cdots \\\\ \\vert & \\vert & \\vert & \\vert & \\vert &  \\end{bmatrix}\\]\n\nAs we can see, the differentiation operator simply maps each basis function to its derivative.\n\nThis result also applies to the larger space of analytic real functions, which includes polynomials, exponential functions, trigonometric functions, logarithms, and other familiar names.\nBy definition, an analytic function can be expressed as a Taylor series about \\(0\\):\n\n\\[f[x] = \\sum_{n=0}^\\infty \\frac{f^{(n)}[0]}{n!}x^n = \\sum_{n=0}^\\infty \\alpha_n x^n\\]\n\nWhich is a linear combination of our polynomial basis functions.\nThat means a Taylor expansion is essentially a change of basis into the sequence of powers, where our differentiation operator is quite simple.5\n\n\n\nDiagonalization\n\n\n  Review: Eigenvectors and eigenvalues | Chapter 14, Essence of linear algebra.\n\n\nMatrix decompositions are arguably the crowning achievement of linear algebra.\nTo get started, let’s review what diagonalization means for a \\(3\\times3\\) real matrix \\(\\mathbf{A}\\).\n\nEigenvectors\n\nA vector \\(\\mathbf{u}\\) is an eigenvector of the matrix \\(\\mathbf{A}\\) when the following condition holds:\n\n\n\n$$ \\mathbf{Au} = \\lambda \\mathbf{u} $$\n\n\n\n\n\n\nThe eigenvalue \\(\\lambda\\) may be computed by solving the characteristic polynomial of \\(\\mathbf{A}\\).\nEigenvalues may be real or complex.\n\nThe matrix \\(\\mathbf{A}\\) is diagonalizable when it admits three linearly independent eigenvectors, each with a corresponding real eigenvalue.\nThis set of eigenvectors constitutes an eigenbasis for the underlying vector space, indicating that we can express any vector \\(\\mathbf{x}\\) via their linear combination.\n\n\n\n$$ \\mathbf{x} = \\alpha\\mathbf{u}_1 + \\beta\\mathbf{u}_2 + \\gamma\\mathbf{u}_3 $$\n\n\n\n\n\n\nTo multiply \\(\\mathbf{x}\\) by \\(\\mathbf{A}\\), we just have to scale each component by its corresponding eigenvalue.\n\n\n\n$$ \\begin{align*} \\mathbf{Ax} &= \\alpha\\mathbf{A}\\mathbf{u}_1 + \\beta\\mathbf{A}\\mathbf{u}_2 + \\gamma\\mathbf{A}\\mathbf{u}_3 \\\\\n&= \\alpha\\lambda_1\\mathbf{u}_1 + \\beta\\lambda_2\\mathbf{u}_2 + \\gamma\\lambda_3\\mathbf{u}_3 \\end{align*} $$\n\n\n\n\n\n\nFinally, re-combining the eigenvectors expresses the result in the standard basis.\n\n\n\nIntuitively, we’ve shown that multiplying by \\(\\mathbf{A}\\) is equivalent to a change of basis, a scaling, and a change back.\nThat means we can write \\(\\mathbf{A}\\) as the product of an invertible matrix \\(\\mathbf{U}\\) and a diagonal matrix \\(\\mathbf{\\Lambda}\\).\n\n\n\n\\[\\begin{align*} \\mathbf{A} &= \\mathbf{U\\Lambda U^{-1}} \\\\\n &= \\begin{bmatrix}\\vert & \\vert & \\vert \\\\ \\mathbf{u}_1 & \\mathbf{u}_2 & \\mathbf{u}_3 \\\\ \\vert & \\vert & \\vert \\end{bmatrix}\n \\begin{bmatrix}\\lambda_1 & 0 & 0 \\\\ 0 & \\lambda_2 & 0 \\\\ 0 & 0 & \\lambda_3 \\end{bmatrix}\n \\begin{bmatrix}\\vert & \\vert & \\vert \\\\ \\mathbf{u}_1 & \\mathbf{u}_2 & \\mathbf{u}_3 \\\\ \\vert & \\vert & \\vert \\end{bmatrix}^{-1}\n\\end{align*}\\]\n\n\n\n\n\\[\\begin{align*} \\mathbf{A} &= \\mathbf{U\\Lambda U^{-1}} \\\\\n &= \\begin{bmatrix}\\vert & \\vert & \\vert \\\\ \\mathbf{u}_1 & \\mathbf{u}_2 & \\mathbf{u}_3 \\\\ \\vert & \\vert & \\vert \\end{bmatrix}\n \\\\ & \\phantom{=} \\begin{bmatrix}\\lambda_1 & 0 & 0 \\\\ 0 & \\lambda_2 & 0 \\\\ 0 & 0 & \\lambda_3 \\end{bmatrix}\n \\\\ & \\phantom{=} \\begin{bmatrix}\\vert & \\vert & \\vert \\\\ \\mathbf{u}_1 & \\mathbf{u}_2 & \\mathbf{u}_3 \\\\ \\vert & \\vert & \\vert \\end{bmatrix}^{-1}\n\\end{align*}\\]\n\n\n\nNote that \\(\\mathbf{U}\\) is invertible because its columns (the eigenvectors) form a basis for \\(\\mathbb{R}^3\\).\nWhen multiplying by \\(\\mathbf{x}\\), \\(\\mathbf{U}^{-1}\\) converts \\(\\mathbf{x}\\) to the eigenbasis, \\(\\mathbf{\\Lambda}\\) scales by the corresponding eigenvalues, and \\(\\mathbf{U}\\) takes us back to the standard basis.\n\nIn the presence of complex eigenvalues, \\(\\mathbf{A}\\) may still be diagonalizable if we allow \\(\\mathbf{U}\\) and \\(\\mathbf{\\Lambda}\\) to include complex entires.\nIn this case, the decomposition as a whole still maps real vectors to real vectors, but the intermediate values become complex.\n\nEigenfunctions\n\n\n  Review: What’s so special about Euler’s number e? | Chapter 5, Essence of calculus.\n\n\nSo, what does diagonalization mean in a vector space of functions?\nGiven a linear operator \\(\\mathcal{L}\\), you might imagine a corresponding definition for eigenfunctions:\n\n\\[\\mathcal{L}f = \\psi f\\]\n\nThe scalar \\(\\psi\\) is again known as an eigenvalue.\nSince \\(\\mathcal{L}\\) is infinite-dimensional, it doesn’t have a characteristic polynomial—there’s not a straightforward method for computing \\(\\psi\\).\n\nNevertheless, let’s attempt to diagonalize differentiation on analytic functions.\nThe first step is to find the eigenfunctions.\nStart by applying the above condition to our differentiation operator in the power basis:\n\n\\[\\begin{align*}\n&& \\frac{\\partial}{\\partial x}\\mathbf{p} = \\psi \\mathbf{p} \\vphantom{\\Big|}& \\\\\n&\\iff& \\begin{bmatrix}0 & 1 & 0 & 0 & \\cdots\\\\ 0 & 0 & 2 & 0 & \\cdots\\\\ 0 & 0 & 0 & 3 & \\cdots\\\\ \\vdots & \\vdots & \\vdots & \\vdots & \\ddots \\end{bmatrix}\\begin{bmatrix}p_0\\\\ p_1\\\\ p_2\\\\ p_3\\\\ \\vdots\\end{bmatrix}\n&= \\begin{bmatrix}\\psi p_0\\\\ \\psi p_1 \\\\ \\psi p_2 \\\\ \\psi p_3 \\\\ \\vdots \\end{bmatrix} \\\\\n&\\iff& \\begin{cases} p_1 &= \\psi p_0 \\\\ p_2 &= \\frac{\\psi}{2} p_1 \\\\ p_3 &= \\frac{\\psi}{3} p_2 \\\\ &\\dots \\end{cases} &\n\\end{align*}\\]\n\nThis system of equations implies that all coefficients are determined solely by our choice of constants \\(p_0\\) and \\(\\psi\\).\nWe can explicitly write down their relationship as \\(p_i = \\frac{\\psi^i}{i!}p_0\\).\n\nNow, let’s see what this class of polynomials actually looks like.\n\n\n\n\\[p[x] = p_0 + p_0\\psi x + p_0\\frac{\\psi^2}{2}x^2 + p_0\\frac{\\psi^3}{6}x^3 + p_0\\frac{\\psi^4}{24}x^4 + \\dots\\]\n\n\n\n\n\\[\\begin{align*}\np[x] &= p_0 + p_0\\psi x + p_0\\frac{\\psi^2}{2}x^2\\, +\\\\ &\\phantom{=} p_0\\frac{\\psi^3}{6}x^3 + p_0\\frac{\\psi^4}{24}x^4 + \\dots\n\\end{align*}\\]\n\n\n\nDifferentiation shows that this function is, in fact, an eigenfunction for the eigenvalue \\(\\psi\\).\n\n\n\n\\[\\begin{align*} \\frac{\\partial}{\\partial x} p[x] &= 0 + p_0\\psi + p_0 \\psi^2 x + p_0\\frac{\\psi^3}{2}x^2 + p_0\\frac{\\psi^4}{6}x^3 + \\dots \\\\\n&= \\psi p[x] \\end{align*}\\]\n\n\n\n\n\\[\\begin{align*}\n\\frac{\\partial}{\\partial x} p[x] &= 0 + p_0\\psi + p_0 \\psi^2 x\\, +\\\\ &\\phantom{=} p_0\\frac{\\psi^3}{2}x^2 + p_0\\frac{\\psi^4}{6}x^3 + \\dots \\\\\n&= \\psi p[x]\n\\end{align*}\\]\n\n\n\nWith a bit of algebraic manipulation, the definition of \\(e^{x}\\) pops out:\n\n\n\n\\[\\begin{align*} p[x] &= p_0 + p_0\\psi x + p_0\\frac{\\psi^2}{2}x^2 + p_0\\frac{\\psi^3}{6}x^3 + p_0\\frac{\\psi^4}{24}x^4 + \\dots \\\\\n&= p_0\\left((\\psi x) + \\frac{1}{2!}(\\psi x)^2 + \\frac{1}{3!}(\\psi x)^3 + \\frac{1}{4!}(\\psi x)^4 + \\dots\\right) \\\\\n&= p_0 e^{\\psi x} \\end{align*}\\]\n\n\n\n\n\\[\\begin{align*}\np[x] &= p_0 + p_0\\psi x + p_0\\frac{\\psi^2}{2}x^2\\, +\\\\ &\\phantom{=} p_0\\frac{\\psi^3}{6}x^3 + p_0\\frac{\\psi^4}{24}x^4 + \\dots \\\\\n&= p_0\\Big((\\psi x) + \\frac{1}{2!}(\\psi x)^2\\, +\\\\ &\\phantom{=p_0\\Big((} \\frac{1}{3!}(\\psi x)^3 + \\frac{1}{4!}(\\psi x)^4 + \\dots\\Big) \\\\\n&= p_0 e^{\\psi x}\n\\end{align*}\\]\n\n\n\nTherefore, functions of the form \\(p_0e^{\\psi x}\\) are eigenfunctions for the eigenvalue \\(\\psi\\), including when \\(\\psi=0\\).\n\nDiagonalizing Differentiation\n\nWe’ve found the eigenfunctions of the derivative operator, but can we diagonalize it?\nIdeally, we would express differentiation as the combination of an invertible operator \\(\\mathcal{L}\\) and a diagonal operator \\(\\mathcal{D}\\).\n\n\n\n\\[\\begin{align*} \\frac{\\partial}{\\partial x} &= \\mathcal{L} \\mathcal{D} \\mathcal{L}^{-1}  \\\\\n&=\n\\begin{bmatrix} \\vert & \\vert &  & \\\\ \\alpha e^{\\psi_1 x} & \\beta e^{\\psi_2 x} & \\dots \\\\ \\vert & \\vert &  \\end{bmatrix}\n\\begin{bmatrix} \\psi_1 & 0 & \\dots \\\\ 0 & \\psi_2 & \\dots \\\\ \\vdots & \\vdots & \\ddots \\end{bmatrix}\n{\\color{red} \\begin{bmatrix} \\vert & \\vert &  & \\\\ \\alpha e^{\\psi_1 x} & \\beta e^{\\psi_2 x} & \\dots \\\\ \\vert & \\vert &  \\end{bmatrix}^{-1} }\n\\end{align*}\\]\n\n\n\n\n\\[\\begin{align*} \\frac{\\partial}{\\partial x} &= \\mathcal{L} \\mathcal{D} \\mathcal{L}^{-1}  \\\\\n&=\n\\begin{bmatrix} \\vert & \\vert &  & \\\\ \\alpha e^{\\psi_1 x} & \\beta e^{\\psi_2 x} & \\dots \\\\ \\vert & \\vert &  \\end{bmatrix}\n\\\\ & \\phantom{=} \\begin{bmatrix} \\psi_1 & 0 & \\dots \\\\ 0 & \\psi_2 & \\dots \\\\ \\vdots & \\vdots & \\ddots \\end{bmatrix}\n\\\\ & \\phantom{=} {\\color{red} \\begin{bmatrix} \\vert & \\vert &  & \\\\ \\alpha e^{\\psi_1 x} & \\beta e^{\\psi_2 x} & \\dots \\\\ \\vert & \\vert &  \\end{bmatrix}^{-1} }\n\\end{align*}\\]\n\n\n\nDiagonalization is only possible when our eigenfunctions form a basis.\nThis would be true if all analytic functions are expressible as a linear combination of exponentials.\nHowever…\n\n\n  \n    Counterexample: $$f[x] = x$$\n  \n  \n    \n      First assume that \\(f[x] = x\\) can be represented as a linear combination of exponentials.\nSince analytic functions have countably infinite dimensionality, we should only need a countably infinite sum:\n\n\\[f[x] = x = \\sum_{n=0}^\\infty \\alpha_n e^{\\psi_n x}\\]\n\n      Differentiating both sides:\n\n\\[\\begin{align*} f^{\\prime}[x] &= 1 = \\sum_{n=0}^\\infty \\psi_n\\alpha_n e^{\\psi_n x} \\\\\n f^{\\prime\\prime}[x] &= 0 = \\sum_{n=0}^\\infty \\psi_n^2\\alpha_n e^{\\psi_n x} \\end{align*}\\]\n\n      Since \\(e^{\\psi_n x}\\) and \\(e^{\\psi_m x}\\) are linearly independent when \\(n\\neq m\\), the final equation implies that all \\(\\alpha = 0\\), except possibly the \\(\\alpha_\\xi\\) corresponding to \\(\\psi_\\xi = 0\\).\nTherefore:\n\n\\[\\begin{align*}\n1 &= \\sum_{n=0}^\\infty \\psi_n\\alpha_n e^{\\psi_n x}\\\\\n&= \\psi_\\xi \\alpha_\\xi + \\sum_{n\\neq \\xi} 0\\psi_n e^{\\psi_n x} \\\\\n&= 0\n\\end{align*}\\]\n\n      That’s a contradiction—the linear combination representing \\(f[x] = x\\) does not exist.\n\n      A similar argument shows that we can’t represent any non-constant function whose \\(n\\)th derivative is zero, nor periodic functions like sine and cosine.\n    \n  \n\n\nReal exponentials don’t constitute a basis, so we cannot construct an invertible \\(\\mathcal{L}\\).\n\nThe Laplace Transform\n\nWe previously mentioned that more matrices can be diagonalized if we allow the decomposition to contain complex numbers.\nAnalogously, more linear operators are diagonalizable in the larger vector space of functions from \\(\\mathbb{R}\\) to \\(\\mathbb{C}\\).\n\nDifferentiation works the same way in this space; we’ll still find that its eigenfunctions are exponential.\n\n\\[\\frac{\\partial}{\\partial x} e^{(a+bi)x} = (a+bi)e^{(a+bi)x}\\]\n\nHowever, the new eigenfunctions have complex eigenvalues, so we still can’t diagonalize.\nWe’ll need to consider the still larger space of functions from \\(\\mathbb{C}\\) to \\(\\mathbb{C}\\).\n\n\\[\\frac{\\partial}{\\partial x} : (\\mathbb{C}\\mapsto\\mathbb{C}) \\mapsto (\\mathbb{C}\\mapsto\\mathbb{C})\\]\n\nIn this space, differentiation can be diagonalized via the Laplace transform.\nAlthough useful for solving differential equations, the Laplace transform is non-trivial to invert, so we won’t discuss it further.\nIn the following sections, we’ll delve into an operator that can be easily diagonalized in \\(\\mathbb{R}\\mapsto\\mathbb{C}\\): the Laplacian.\n\n\n\nInner Product Spaces\n\n\n  Review: Dot products and duality | Chapter 9, Essence of linear algebra.\n\n\nBefore we get to the spectral theorem, we’ll need to understand one more topic: inner products.\nYou’re likely already familiar with one example of an inner product—the Euclidean dot product.\n\n\\[\\begin{bmatrix}x\\\\ y\\\\ z\\end{bmatrix} \\cdot \\begin{bmatrix}a\\\\ b\\\\ c\\end{bmatrix} = ax + by + cz\\]\n\nAn inner product describes how to measure a vector along another vector.\nFor example, \\(\\mathbf{u}\\cdot\\mathbf{v}\\) is proportional to the length of the projection of \\(\\mathbf{u}\\) onto \\(\\mathbf{v}\\).\n\n\n\n$$ \\mathbf{u} \\cdot \\mathbf{v} =\\|\\mathbf{u}\\|\\|\\mathbf{v}\\|\\cos[\\theta] $$\n\n\n\n\n\n\nWith a bit of trigonometry, we can show that the dot product is equivalent to multiplying the vectors’ lengths with the cosine of their angle.\nThis relationship suggests that the product of a vector with itself produces the square of its length.\n\n\\[\\begin{align*} \\mathbf{u}\\cdot\\mathbf{u} &= \\|\\mathbf{u}\\|\\|\\mathbf{u}\\|\\cos[0] \\\\\n&= \\|\\mathbf{u}\\|^2\n\\end{align*}\\]\n\nSimilarly, when two vectors form a right angle (are orthogonal), their dot product is zero.\n\n\n\n$$ \\begin{align*}  \\mathbf{u} \\cdot \\mathbf{v} &= \\|\\mathbf{u}\\|\\|\\mathbf{v}\\|\\cos[90^\\circ] \\\\ &= 0 \\end{align*} $$\n\n\n\n\n\n\nOf course, the Euclidean dot product is only one example of an inner product.\nIn more general spaces, the inner product is denoted using angle brackets, such as \\(\\langle \\mathbf{u}, \\mathbf{v} \\rangle\\).\n\n\n  The length (also known as the norm) of a vector is defined as \\(\\|\\mathbf{u}\\| = \\sqrt{\\langle \\mathbf{u}, \\mathbf{u} \\rangle}\\).\n  Two vectors are orthogonal if their inner product is zero: \\(\\ \\mathbf{u} \\perp \\mathbf{v}\\ \\iff\\ \\langle \\mathbf{u}, \\mathbf{v} \\rangle = 0\\).\n\n\nA vector space augmented with an inner product is known as an inner product space.\n\nA Functional Inner Product\n\nWe can’t directly apply the Euclidean dot product to our space of real functions, but its \\(N\\)-dimensional generalization is suggestive.\n\n\\[\\begin{align*} \\mathbf{u} \\cdot \\mathbf{v} &= u_1v_1 + u_2v_2 + \\dots + u_Nv_N \\\\ &= \\sum_{i=1}^N u_iv_i \\end{align*}\\]\n\nGiven countable indices, we simply match up the values, multiply them, and add the results.\nWhen indices are uncountable, we can convert the discrete sum to its continuous analog: an integral!\n\n\\[\\langle f, g \\rangle = \\int_a^b f[x]g[x] \\, dx\\]\n\nWhen \\(f\\) and \\(g\\) are similar, multiplying them produces a larger function; when they’re different, they cancel out.\nIntegration measures their product over some domain to produce a scalar result.\n\n\n\n\n\n\n\n\n\n\nOf course, not all functions can be integrated.\nOur inner product space will only contain functions that are square integrable over the domain \\([a, b]\\), which may be \\([-\\infty, \\infty]\\).\nLuckily, the important properties of our inner product do not depend on the choice of integration domain.\n\nProofs\n\nBelow, we’ll briefly cover functions from \\(\\mathbb{R}\\) to \\(\\mathbb{C}\\).\nIn this space, our intuitive notion of similarity still applies, but we’ll use a slightly more general inner product:\n\n\\[\\langle f,g \\rangle = \\int_a^b f[x]\\overline{g[x]}\\, dx\\]\n\nWhere \\(\\overline{x}\\) denotes conjugation, i.e. \\(\\overline{a + bi} = a - bi\\).\n\nLike other vector space operations, an inner product must satisfy several axioms:\n\n\n  \n    Conjugate Symmetry\n  \n  \n  \n  For all vectors $$\\mathbf{u}, \\mathbf{v} \\in \\mathcal{V}$$:\n\n  $$\\langle \\mathbf{u}, \\mathbf{v} \\rangle = \\overline{\\langle \\mathbf{v}, \\mathbf{u} \\rangle}$$\n\n  Conjugation may be taken outside the integral, making this one easy:\n\n  $$\\begin{align*} \\langle f, g \\rangle &= \\int_a^b f[x]\\overline{g[x]} \\, dx \\\\\n  &= \\int_a^b \\overline{g[x]\\overline{f[x]}} \\, dx \\\\\n  &= \\overline{\\int_a^b g[x]\\overline{f[x]} \\, dx} \\\\\n  &= \\overline{\\langle g, f \\rangle}\n  \\end{align*}$$\n\n  Note that we require conjugate symmetry because it implies $$\\langle\\mathbf{u}, \\mathbf{u}\\rangle = \\overline{\\langle\\mathbf{u}, \\mathbf{u}\\rangle}$$, i.e. the inner product of a vector with itself is real.\n  \n  \n\n\n\n  \n    Linearity in the First Argument\n  \n  \n  \n  For all vectors $$\\mathbf{u}, \\mathbf{v}, \\mathbf{w} \\in \\mathcal{V}$$ and scalars $$\\alpha, \\beta \\in \\mathbb{F}$$:\n\n  $$\\langle \\alpha \\mathbf{u} + \\beta \\mathbf{v}, \\mathbf{w} \\rangle = \\alpha\\langle \\mathbf{u}, \\mathbf{w} \\rangle + \\beta\\langle \\mathbf{v}, \\mathbf{w} \\rangle $$\n\n  The proof follows from linearity of integration, as well as our vector space axioms:\n\n\n\n\\[\\begin{align*} \\langle \\alpha f + \\beta g, h \\rangle &= \\int_a^b (\\alpha f + \\beta g)[x]\\overline{h[x]} \\, dx \\\\\n&= \\int_a^b (\\alpha f[x] + \\beta g[x])\\overline{h[x]} \\, dx \\\\\n&= \\int_a^b \\alpha f[x]\\overline{h[x]} + \\beta g[x]\\overline{h[x]} \\, dx \\\\\n&= \\alpha\\int_a^b f[x]\\overline{h[x]}\\, dx + \\beta\\int_a^b g[x]\\overline{h[x]} \\, dx \\\\\n&= \\alpha\\langle f, h \\rangle + \\beta\\langle g, h \\rangle\n\\end{align*}\\]\n\n      \n\n\n\\[\\begin{align*} &\\langle \\alpha f + \\beta g, h \\rangle\\\\ &= \\int_a^b (\\alpha f + \\beta g)[x]\\overline{h[x]} \\, dx \\\\\n&= \\int_a^b (\\alpha f[x] + \\beta g[x])\\overline{h[x]} \\, dx \\\\\n&= \\int_a^b \\alpha f[x]\\overline{h[x]} + \\beta g[x]\\overline{h[x]} \\, dx \\\\\n&= \\alpha\\int_a^b f[x]\\overline{h[x]}\\, dx\\, +\\\\&\\hphantom{==} \\beta\\int_a^b g[x]\\overline{h[x]} \\, dx \\\\\n&= \\alpha\\langle f, h \\rangle + \\beta\\langle g, h \\rangle\n\\end{align*}\\]\n\n      \n\n  Given conjugate symmetry, an inner product is also antilinear in the second argument.\n  \n  \n\n\n\n  \n    Positive-Definiteness\n  \n  \n  \n  For all $$\\mathbf{u} \\in \\mathcal{V}$$:\n\n  $$ \\begin{cases} \\langle \\mathbf{u}, \\mathbf{u} \\rangle = 0 & \\text{if } \\mathbf{u} = \\mathbf{0} \\\\ \\langle \\mathbf{u}, \\mathbf{u} \\rangle > 0 & \\text{otherwise} \\end{cases} $$\n\n  By conjugate symmetry, we know $$\\langle f, f \\rangle$$ is real, so we can compare it with zero.\n  \n  However, rigorously proving this result requires measure-theoretic concepts beyond the scope of this post.\n  In brief, we redefine $$\\mathbf{0}$$ not as specifically $$\\mathbf{0}[x] = 0$$, but as an equivalence class of functions that are zero ",
    "Confidence": 0.9806124
  },
  {
    "Title": "How to benchmark different .NET versions",
    "Url": "https://steven-giesel.com/blogPost/59cfb6f8-8b87-4707-a99e-e372541b696a",
    "Timestamp": "2023-07-30T09:02:25",
    "Domain": "steven-giesel.com",
    "Description": "With the famous BenchmarkDotNet library you can benchmark a lot - but it doesn't stop with a single .NET version. You can benchmark multiple versions of the same code that targets different runtimes!",
    "Confidence": 0.9830188
  },
  {
    "Title": "GitHub - AUTOMATIC1111/stable-diffusion-webui: Stable Diffusion web UI",
    "Url": "https://github.com/automatic1111/stable-diffusion-webui",
    "Timestamp": "2023-07-30T06:03:22",
    "Domain": "github.com",
    "Description": "Stable Diffusion web UI. Contribute to AUTOMATIC1111/stable-diffusion-webui development by creating an account on GitHub.",
    "Confidence": 0.99600905
  },
  {
    "Title": "Four ways to shoot yourself in the foot with Redis",
    "Url": "https://philbooth.me/blog/four-ways-to-shoot-yourself-in-the-foot-with-redis",
    "Timestamp": "2023-07-30T06:03:21",
    "Domain": "philbooth.me",
    "Description": "Production outages are great\nat teaching you how not to cause production outages.\nI've caused plenty and hope that by sharing them publicly,\nit might help some people bypass part one\nof the production outage learning syllabus.\nPreviously I discussed ways I've broken prod\nwith PostgreSQL\nand with healthchecks.\nNow I'll show you how I've done it with Redis too.",
    "Confidence": 0.95499897
  },
  {
    "Title": "How critical theory is radicalizing high school debate",
    "Url": "https://www.slowboring.com/p/how-critical-theory-is-radicalizing",
    "Timestamp": "2023-07-30T05:05:31",
    "Domain": "www.slowboring.com",
    "Description": "New rhetorical tactics are creating a generation of nihilists",
    "Confidence": 0.6973946
  },
  {
    "Title": "Grand Kadooment 2023 - Biggest Carnival Event of Barbados",
    "Url": "https://www.visitbarbados.org/grand-kadooment",
    "Timestamp": "2023-07-30T02:03:17",
    "Domain": "www.visitbarbados.org",
    "Description": "Celebrate Grand Kadooment 2023 to experience the grand finale of the Crop Over festival. An event showcasing masquerade bands make their way to the Grynner Highway.",
    "Confidence": 0.5919308
  },
  {
    "Title": "Error",
    "Url": "https://nt.vern.cc/cards/18ce54bi3fv/1rc3n",
    "Timestamp": "2023-07-30T01:01:56",
    "Domain": "nt.vern.cc",
    "Description": "",
    "Confidence": 0.8634593
  },
  {
    "Title": "Codifying a ChatGPT workflow into a malleable GUI",
    "Url": "https://www.geoffreylitt.com/2023/07/25/building-personal-tools-on-the-fly-with-llms.html",
    "Timestamp": "2023-07-30T00:03:03",
    "Domain": "www.geoffreylitt.com",
    "Description": "Wouldn't it be neat if you could use LLMs to create little personal utility apps as the need arises? Here's a story where I did just that...",
    "Confidence": 0.990423
  },
  {
    "Title": "The transformer model, explained clearly - DeriveIt",
    "Url": "https://www.deriveit.org/notes/119",
    "Timestamp": "2023-07-30T00:03:03",
    "Domain": "www.deriveit.org",
    "Description": "In this note, we'll come up with GPT and the TRANSFORMER architecture, from scratch.\n\n\n",
    "Confidence": 0.9466522
  },
  {
    "Title": "How to read inference rules",
    "Url": "https://cohost.org/prophet/post/2248211-how-to-read-inferenc",
    "Timestamp": "2023-07-30T00:03:02",
    "Domain": "cohost.org",
    "Description": "The notation used is probably one of the largest barriers of entry to type inference papers, but it is rarely explained explicitly, so... I'm going to do just that!\n\nFor starters, inference rules are really nothing more than implications. The inference rule\n\nAB(Name)\n\nreally just means \"if A then B\". These are usually given a name (in this case, creatively, Name) to make it easier to refer to them in the rest of the paper.\n\nNow, even though these are technically just implications, it's usually not a great idea to read them from top to bottom. Inference rules denote relations, but it usually usually makes more sense to read them as (possibly non-deterministic) functions.\nFor example, a judgement for typing function application might look like this. (where Γ⊢e:τ means \"In a context Γ, the type of an expression e is infered to τ\")\n\nΓ⊢e1  :  τ1→τ2      Γ⊢e2  :  τ1Γ⊢e1(e2)  :  τ2(App)\n\nNaively, one might read this as\n\n> If e1 has type τ1→τ2 in a context Γ and e2 has type τ1 in Γ, then e1(e2) has type τ2 in Γ\n\nbut a much better way to read it, that is much closer to an actual implementation, would be\n\n> In order to infer a type for e1(e2) in a context Γ, one first needs to infer a type for e1 with shape τ1→τ2 in Γ. Now e2 also needs to infer to type τ1 in Γ, so that the result (i.e. the type of e1(e2)) is τ2.\n\nRead this way, the inference rule maps very closely onto an actual implementation! Seriously, compare the corresponding pseudocode to that second description\n\ninfer Γ (App e1 e2) =\n    let (τ1 -> τ2) = infer Γ e1\n    let τ3 = infer Γ e2\n    unify τ1 τ3\n    return τ2\n\n\nThe only major difference between this code (which skips error handling, just like inference rules) and the inference rule is that the fact that the type of e2 needs to be equal to τ1 is explicit in the code (unify τ1 τ3).\n\nReading off the algorithm like this is possible if the inference rules are syntax directed, i.e. if there is only ever a single rule that might match on a given expression. This is not always the case, so sometimes it's better to imagine non-deterministically choosing the correct rule to apply, rather than just pattern matching.\n\nAnd that's... pretty much all you need to know to read inference rules!\n\nThere are a few common conventions in type systems that might be a bit surprising, so let's go over those as well\n\n\nENVIRONMENTS AND EXTENSION\n\nType inference needs an environment to keep track of the types of variables. This is usually called Γ and extended as Γ,x  :  τ.\n\nFor example, this inference rule for (annotated) let bindings checks e2 under the environment Γ, extended with the binding x  :  τ1.\n\nΓ⊢e1  :  τ1      Γ,x  :  τ1⊢e2  :  τ2Γ⊢let x  :  τ1=e1 in e2  :  τ2Let\n\nExtracting information from the environment is achieved through \"pattern matching\" on the environment, for example in this inference rule for variables.\n\nΓ,x  :  τ⊢x  :  τVar\n\n\nUNIFICATION VARIABLES\n\nUnification variables [https://cohost.org/prophet/post/2220730-if-there-is-one-piec] don't exist in theoretical type systems, but they still map very directly onto a similar concept. Instead of generating a fresh unification variable, inference rules just \"guess\" a new type (they're relations, remember?).\n\nFor example, this typing rule for (unannotated) lambdas just pulls the type τ out of thin air.\n\nΓ,x  :  τ⊢e  :  τ1Γ⊢λx→e  :  τ→τ1Lambda\n\n\nLISTS\n\nSomething you will see pretty often in papers by Simon Peyton Jones are lists that are represented by an overline. E.g. the syntax for uncurried function application might be e1(e‾), where e‾ consists of 0 or more expressions.\n\n\nSKOLEMS\n\nSimilarly, skolems don't exist as a separate concept. Instead, \"unbound\" type variables are treated as skolems, although these obviously cannot conflict with any other type variables in scope!\nIn an implementation, this would be achieved by generating a fresh skolem, but in inference rules, this is expressed by the side condition that the type variable should not occur \"free in the environment\", written a∉ftv(Γ), where ftv denotes the set of free type variables (= skolems) in Γ.\n\nFor example, a rule for let bindings with polymorphic types (that need to be skolemized) might look like this\n\nΓ⊢e1  :  τ1      a‾∉ftv(Γ)      Γ,x  :  ∀a‾.τ1⊢e2  :  τ2Γ⊢let x  :  ∀a‾.τ1=e1 in e2  :  τ2\n\n\nWHERE TO GO FROM HERE\n\nGreat, with a little practice, you should be able to read inference rules now! I would recommend you read Practical type inference for higher rank types [https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/putting.pdf], which is a great, relatively beginner friendly paper about type inference that even contains a full implementation at the end! (And despite the name, is not just about higher rank types).",
    "Confidence": 0.99551284
  },
  {
    "Title": "Sampling at scale with OpenTelemetry",
    "Url": "https://www.gouthamve.dev/sampling-at-scale-with-opentelemetry/",
    "Timestamp": "2023-07-30T00:03:02",
    "Domain": "www.gouthamve.dev",
    "Description": "Thoughts on different sampling strategies at scale when using OpenTelemetry. With experience running tracing at scale at Grafana Labs.",
    "Confidence": 0.98975974
  },
  {
    "Title": "Hamel’s Blog - Optimizing LLM latency",
    "Url": "https://hamel.dev/notes/llm/03_inference.html",
    "Timestamp": "2023-07-30T00:03:02",
    "Domain": "hamel.dev",
    "Description": "An exploration of inference tools for open source LLMs focused on latency.",
    "Confidence": 0.9940068
  },
  {
    "Title": "Building a BitTorrent client in Elixir",
    "Url": "https://kochika.me/posts/torrent/",
    "Timestamp": "2023-07-30T00:03:01",
    "Domain": "kochika.me",
    "Description": "\nIn this post, we delve headfirst into the BitTorrent protocol, understanding the process of downloading a torrent by building a minimal torrent client from scratch.",
    "Confidence": 0.99518687
  },
  {
    "Title": "The Illustrated Transformer",
    "Url": "https://jalammar.github.io/illustrated-transformer/",
    "Timestamp": "2023-07-30T00:03:01",
    "Domain": "jalammar.github.io",
    "Description": "Discussions:\nHacker News (65 points, 4 comments), Reddit r/MachineLearning (29 points, 3 comments)\n\n\nTranslations: Arabic, Chinese (Simplified) 1, Chinese (Simplified) 2, French 1, French 2, Italian, Japanese, Korean, Persian, Russian, Spanish 1, Spanish 2, Vietnamese\n\nWatch: MIT’s Deep Learning State of the Art lecture referencing this post\n\nIn the previous post, we looked at Attention – a ubiquitous method in modern deep learning models. Attention is a concept that helped improve the performance of neural machine translation applications. In this post, we will look at The Transformer – a model that uses attention to boost the speed with which these models can be trained. The Transformer outperforms the Google Neural Machine Translation model in specific tasks. The biggest benefit, however, comes from how The Transformer lends itself to parallelization. It is in fact Google Cloud’s recommendation to use The Transformer as a reference model to use their Cloud TPU offering. So let’s try to break the model apart and look at how it functions.\n\nThe Transformer was proposed in the paper Attention is All You Need. A TensorFlow implementation of it is available as a part of the Tensor2Tensor package. Harvard’s NLP group created a guide annotating the paper with PyTorch implementation. In this post, we will attempt to oversimplify things a bit and introduce the concepts one by one to hopefully make it easier to understand to people without in-depth knowledge of the subject matter.\n\n2020 Update: I’ve created a “Narrated Transformer” video which is a gentler approach to the topic:\n\n\n\n\nA High-Level Look\nLet’s begin by looking at the model as a single black box. In a machine translation application, it would take a sentence in one language, and output its translation in another.\n\n\n  \n\n\n",
    "Confidence": 0.98625076
  },
  {
    "Title": "GPT-4 Code Interpreter and Pillow",
    "Url": "https://metastable.org/pillow.html",
    "Timestamp": "2023-07-30T00:03:01",
    "Domain": "metastable.org",
    "Description": "Getting ChatGPT to draw whether it wants to or not!",
    "Confidence": 0.97616756
  },
  {
    "Title": "Digital Bum: Finding a Home/lessness on the Internet",
    "Url": "https://aartaka.me/blog/digital-bum",
    "Timestamp": "2023-07-30T00:03:01",
    "Domain": "aartaka.me",
    "Description": "Internet grew out of a non-commercial academic network with free resources for everyone. Can one get back to this dream of free Internet and build a lifestyle out of it? Well yeah I guess so, kinda worked for me.",
    "Confidence": 0.983472
  },
  {
    "Title": "Defcon: Preventing Overload with Graceful Feature Degradation",
    "Url": "https://www.micahlerner.com/2023/07/23/defcon-preventing-overload-with-graceful-feature-degradation.html",
    "Timestamp": "2023-07-30T00:03:01",
    "Domain": "www.micahlerner.com",
    "Description": "Defcon: Preventing Overload with Graceful Feature Degradation",
    "Confidence": 0.97766197
  },
  {
    "Title": "LN 035: The Messy Desktop",
    "Url": "https://alexanderobenauer.com/labnotes/035/",
    "Timestamp": "2023-07-30T00:03:00",
    "Domain": "alexanderobenauer.com",
    "Description": "When I got to college, I learned a lot about computing fairly quickly, before I even stepped into my first computer science course.",
    "Confidence": 0.9931991
  },
  {
    "Title": "A Lock-Free Vector",
    "Url": "https://ibraheem.ca/posts/a-lock-free-vector/",
    "Timestamp": "2023-07-30T00:03:00",
    "Domain": "ibraheem.ca",
    "Description": "Designing a fast, lock-free vector.",
    "Confidence": 0.99211615
  },
  {
    "Title": "Git says I am adding '^M' but core.autocrlf is true",
    "Url": "https://stackoverflow.com/questions/34729978/git-says-i-am-adding-m-but-core-autocrlf-is-true",
    "Timestamp": "2023-07-29T22:04:07",
    "Domain": "stackoverflow.com",
    "Description": "The problem is simple enough. Git is saying I've added CR (^M) to a file,\n\nme@myComp MINGW64 /c/workspace/service (develop)\n$ git diff --check\nengine/src/main/java/someFile.java:18: trailing whites...",
    "Confidence": 0.9897082
  },
  {
    "Title": "If We Want a Shift to Walking, We Need to Prioritize Dignity",
    "Url": "https://streets.mn/2023/07/19/if-we-want-a-shift-to-walking-we-need-to-prioritize-dignity/",
    "Timestamp": "2023-07-29T21:03:30",
    "Domain": "streets.mn",
    "Description": "To make walking and rolling a desirable, everyday activity, we need facilities that are compliant, safe and dignified.",
    "Confidence": 0.9138811
  },
  {
    "Title": "So you want to build your own open source chatbot… – Mozilla Hacks - the Web developer blog",
    "Url": "https://hacks.mozilla.org/2023/07/so-you-want-to-build-your-own-open-source-chatbot/",
    "Timestamp": "2023-07-29T21:03:30",
    "Domain": "hacks.mozilla.org",
    "Description": "A small team within Mozilla’s innovation group recently undertook a hackathon to build a trustworthy internal chatbot prototype.",
    "Confidence": 0.98040175
  },
  {
    "Title": "A spectacular superconductor claim is making news. Here’s why experts are doubtful",
    "Url": "https://www.science.org/content/article/spectacular-superconductor-claim-making-news-here-s-why-experts-are-doubtful",
    "Timestamp": "2023-07-29T16:03:27",
    "Domain": "www.science.org",
    "Description": "Skepticism abounds for claim that lead-based material perfectly conducts electricity at room temperature and pressure",
    "Confidence": 0.65150666
  },
  {
    "Title": "It's There But It Isn't - EF Shadow Property #shorts",
    "Url": "https://youtube.com/watch?v=FX9fbokvWZQ",
    "Timestamp": "2023-07-29T12:02:12",
    "Domain": "youtube.com",
    "Description": "🚀 Support me on Patreon to access the source code: https://www.patreon.com/milanjovanovicJoin my weekly .NET newsletter:https://www.milanjovanovic.techRead ...",
    "Confidence": 0.9637698
  }
]