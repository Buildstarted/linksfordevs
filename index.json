[
  {
    "Title": "Nick Chapsas",
    "Url": "https://video.weiler.rocks/channel/UCrkPsvLGln62OMZRO6K-llg",
    "Timestamp": "2023-06-30T03:02:58",
    "Domain": "video.weiler.rocks",
    "Description": "Hello everybody I’m Nick Chapsas and this is my YouTube channel. ",
    "Confidence": 0.9805384
  },
  {
    "Title": "Creating a VS Code editor extension",
    "Url": "https://timheuer.com/blog/resx-editor-for-visual-studio-code/",
    "Timestamp": "2023-06-30T02:02:06",
    "Domain": "timheuer.com",
    "Description": "I created a resx editor to learn how to write VS Code custom editor extensions. Check it out and the code.",
    "Confidence": 0.99662954
  },
  {
    "Title": "Jason Bock Explains What's New in .NET 7 APIs -- Visual Studio Magazine",
    "Url": "https://visualstudiomagazine.com/articles/2023/06/29/net-7-apis.aspx",
    "Timestamp": "2023-06-30T01:03:28",
    "Domain": "visualstudiomagazine.com",
    "Description": "Jason Bock will explain what's new in .NET 7 APIs at a big August developer conference in San Diego.",
    "Confidence": 0.9935014
  },
  {
    "Title": "Burke Learns Blazor - Finishing the API and starting the UI",
    "Url": "https://youtube.com/watch?v=qONPW7N_r8k",
    "Timestamp": "2023-06-30T01:03:25",
    "Domain": "youtube.com",
    "Description": "Last week we got the API started - let's finish it up and start on the UI!Community Links: https://www.theurlist.com/burke-learns-blazorFeaturing: Jon Gallow...",
    "Confidence": 0.98972166
  },
  {
    "Title": "Enable Un-typed within ASP.NET Core OData - OData",
    "Url": "https://devblogs.microsoft.com/odata/enable-un-typed-within-asp-net-core-odata/",
    "Timestamp": "2023-06-29T23:02:36",
    "Domain": "devblogs.microsoft.com",
    "Description": "Introduction The latest ASP.NET Core OData supports the following two built-in OData abstract types: Edm.Untyped Collection(Edm.Untyped) Developers can use them to advertise a property in OData metadata schema (aka, Edm model) so that such property is declared with a particular name present,",
    "Confidence": 0.99800456
  },
  {
    "Title": "Programming Languages Going Above and Beyond",
    "Url": "https://whileydave.com/2023/06/27/programming-languages-going-above-and-beyond/",
    "Timestamp": "2023-06-29T23:02:35",
    "Domain": "whileydave.com",
    "Description": "Dafny is a programming language which often genuinely amazes me.",
    "Confidence": 0.97128564
  },
  {
    "Title": "GitHub Quick Reviews",
    "Url": "https://youtube.com/watch?v=H8P2wTqBtKM",
    "Timestamp": "2023-06-29T21:02:35",
    "Domain": "youtube.com",
    "Description": "Powered by Restream https://restream.io",
    "Confidence": 0.9870885
  },
  {
    "Title": "Making movie trailers with Zeroscope, ElevenLabs, and GPT-4 - Charlie Holtz",
    "Url": "https://www.charlieholtz.com/articles/how-i-make-ai-movies",
    "Timestamp": "2023-06-29T21:02:34",
    "Domain": "www.charlieholtz.com",
    "Description": "My new favorite hobby is making trailers for films that don't exist.",
    "Confidence": 0.9631553
  },
  {
    "Title": "Engineering Guardrails",
    "Url": "https://ashishb.net/all/engineering-guardrails/",
    "Timestamp": "2023-06-29T21:02:34",
    "Domain": "ashishb.net",
    "Description": "Guardrails are meant to protect us from tripping over. The same can be said about engineering guardrails. Like most engineering decisions, adding guardrails is a trade-off. There are multiple levels of adding guardrails and one has to decide which ones and how many should be added. Source code Production deployments Data Information Security Source code",
    "Confidence": 0.9929646
  },
  {
    "Title": "I built a weird keyboard",
    "Url": "https://nathanfriend.io/2023/06/26/i-built-a-weird-keyboard.html",
    "Timestamp": "2023-06-29T21:02:33",
    "Domain": "nathanfriend.io",
    "Description": " I spent most of my free time over the last 10 months building this bizarre keyboard from scratch. It’s a Dactyl Manuform - a split keyboard with a highly sculpted design that is somehow simulataneously the ugliest and most eye-catching object I’ve ever seen. The goal of this keyboard design is to place keys exactly along each finger’s natural axis of motion. The consequences of this approach are downward-sloping (A.K.A “tented”) rows to match the natural angle of the wrists, a deeper middle finger column to compensate for this finger’s relative length, an offset pinky column to minimize stretching, and a thumb cluster with multiple keys to take advantage of its opposable nature. There’s a reason most keyboards don’t look like this. It’s difficult to mass-produce curved keyboards since they can’t use the stiff, flat PCBs that most keyboards use to wire the keys to the microcontroller. Every Dactyl Manuform is a unique piece of art, painstakingly hand-wired by a human being. Here’s how I built mine: Build log The first step was to design and 3D print the case. There are a number of Dactyl Manuform model generators out there; I ended up using this one since it had a few extra features I wanted (e.g. wide pinky keys). Some things I was looking for in my design: A key layout similar to the ErgoDox EZ keyboards I already own An aggressive tent angle (I went with π/8 = 22.5°) Hot-swap sockets I gave up on the hot-swap sockets after failing to coax the generator script to produce valid hot-swap socket holders. I’m glad I did - in the end I don’t think they would have worked anyway. Once I had a model that looked good on the screen, I printed a draft version to see it in real life. Printed with a .8mm nozzle, .32mm layer height, and lightning infill Overall, I was really happy with this first draft. I only made a few tweaks to the model before printing the real thing: I enabled the “external microcontroller” option, which let me kick the can on deciding which microcontroller and connection types to use I added my own screw holes; the holes generated by the script were awkwardly placed To mount the base plates to the body of the keyboard, I used these heat set inserts. It’s a neat system; the threaded inserts are melted into the 3D print using a soldering gun, producing threaded screw holes much stronger and smaller than anything that would be possible using only 3D printing. To determine the ideal hole size for these inserts, I made a test print. A test print with insert holes ranging from 3.8mm to 4.2mm in diameter I’m glad I tested this; all of the hole options were too small! A second print with bigger holes was more successful. A second test print with insert holes ranging from 4.2mm to 4.6mm in diameter. I decided to go with 4.5mm With this last detail resolved, I began printing the real halves. I used my favorite material - wood PLA - which looks (and even smells!) quite similar to real wood, once properly post-processed. The final STL file being sliced in Cura. 24 hours per half! The left half, fresh off the printer Both halves printed and slightly cleaned up I melted the inserts into the holes, which was nerve-wracking; one bad insert would have likely ruined the whole print. It was awkward to try and hold the keyboard and the insert in place while pressing the insert into the plastic with the soldering gun. Somehow I managed to install all ten without issue! An installed heat-set insert I ordered the transparent acrylic base plates from ponoko.com and was quite happy with the result. Acrylic base plate from ponoko.com Next up was post-processing. I sanded each half with 80-grit sandpaper, which was a ton of work - there are a lot of nooks and crannies that make this a tedious job. Both halves, sanded In order to sand the inside of the key holes, I printed a little attachment that I could wrap with sandpaper and fit on my screwdriver. My custom SuperSander™ (patent pending) Normally this would be an unnecessary step - no one sees the inside of the key holes - but the fit was a bit too tight; most of the holes required some sanding before the switch would fit properly. (This is why I’m glad I didn’t bother with hot-swap sockets - the fit is so tight, I’d never be able to get the switches out anyway.) Another shot, because sanding these took too much time not to show off The next step was to stain and clear coat the prints. I used this gel stain and glossy polyurethane to give the prints a rich, polished wood color. The finished product Some nice faux woodgrain This was a time consuming step, as each half required three coats of stain (minimum 24 hours to dry per coat) and at least 3 coats of polyurethane (a few hours to dry per coat). I did this in the dead of winter which made drying these in the outdoors challenging (a space heater may have been involved). The hard work paid off, though - I’m really happy with how these look! The layer lines even give the illusion of a wood grain. The next decision was which key switches to use. I’m a clicky switch guy; the noisier the better. I bought a Kailh switch tester so I could make an informed decision and decided on Kailh Box Whites. Regular and low-profile Kailh switch testers. Not pictured: Gateron and Boba testers Compared to other clicky switches (e.g. Cherry MX Blues), Box Whites are extra clicky (they click twice per key press). Perfect for working remotely! If I ever make a silent/office-friendly version of this board, I’ll go with Boba U4 Silents. At this point, I was able to set the switches and keycaps in place and get a feel for what it would be like to type on this monstrosity. I’ll admit it felt about as weird as it looks. Switches and keycaps (temporarily) installed There was one last detail to work out before I could begin wiring up the halves. I wanted to install a rotary encoder (“volume knob”) on each half, but these don’t click into a standard keyboard hole out of the box. I had to 3D print a special adapter for each. The end result, with chunks of glass attached to the bottoms I printed these with PETG and learned the hard way to always use glue stick when printing with PETG. The adapters adhesed to the bed so strongly they took chunks out of my glass bed when I finally pried them off. I was able to salvage the adapters with some sanding, but the printer bed was unfortunately beyond repair. 😭 Despite their rather violent effects on my printer, the adapters did their job quite nicely! Rotary encoder with and without the cap Finally, it was time to start wiring it up! First, I wrapped diodes around one pin on each switch. Diodes wrapped I soldered the diodes into place and snipped the extra leg. Diodes soldered and snipped I soldered the remaining diode legs together to form the rows of the key matrix. Diode legs soldered together into rows I used small, individual pieces of insulated wire to form the columns. Don't look too closely; I'm really bad at soldering I installed DuPont connectors so I didn’t have to solder directly to the microcontroller. This saved me a lot of headache since it took quite a bit of trial and error to get all pins in the right spot. DuPont connectors installed I flashed a basic QMK firmware to the microcontroller and had the incredibly satisfying experience of seeing a letter appear on the screen when I pressed a key. I also got the LED strip working! IT LIVES!! I was getting really close at this point. I designed a custom holder for the microcontroller since the one that was supposed to be compatible with my case didn’t fit for some reason. Custom-designed microcontroller holder I spliced some wires together since a few of the microcontroller pins had to be shared by more than one connection. I'm frankly shocked this thing works After assembling all the pieces, a bit of software configuration, and a lot of trial and error… I had a working keyboard! Finally. Done. This was way too much work. Never again. Probably. ALERT How does it feel? Weird, but good! I’ve only been typing on it for a few work days, so my muscle memory hasn’t fully adjusted. I keep reaching for keys in the wrong places; in particular, my fingers naturally stretch too far when reaching for the bottom row. I also made a few modifications to my QMK layout to take advantage of the more accessible thumb clusters compared to my ErgoDox EZ. I think I’ll really like it once I’m used to it. Was it worth it? Umm… I think so? The end result was fantastic, but it was an insane amount of work. I don’t recommend this project to anyone who isn’t interested in the process itself. If you’re just looking for a great ergonomic keyboard, I’d recommend buying an ErgoDox EZ, a Moonlander, a Kinesis Advantage360 or a prebuilt Dactyl Manuform, all of which will cost about the same as this project (see below). Cost breakdown Cost of all items, including tax and shipping. Description Cost (CAD) Link Wood PLA filament for 3D printed case $40.44 amazon.ca Kailh switch tester $21.46 aliexpress.com Kailh low-profile switch tester $9.68 aliexpress.com Pro Micro controller (x2) $44.98 amazon.ca Kailh BOX White switches (x90) $51.26 aliexpress.com M3 heat-seated inserts (x100) $12.42 amazon.ca M3 screws (x100) $12.02 amazon.ca EC11 rotary encoder (x4) $15.80 amazon.ca LED strip (1m) $16.37 aliexpress.com 1N4148 Diode (x100) $8.80 digikey.ca Reset button (x3) $10.82 digikey.ca 22AWG Wire (25’) $7.24 digikey.ca TRRS jack, female (x3) $12.65 digikey.ca Jumper wire (x60) $11.74 digikey.ca Soldering iron $59.87 homedepot.ca Solder $28.23 homedepot.ca Wire stripper $11.29 canadiantire.ca Acrylic base plate (x4) $54.88 ponoko.com Gel wood stain $19.93 homedepot.ca Glossy polyurethane $27.11 amazon.ca Keycap set (x2) $57.32 amazon.ca Electrical tape $5.37 amazon.ca Rubber feet $13.55 amazon.ca Only keyboard materials $450.93 ≈ $340 USD All items (including tools, testers, etc.) $553.23 ≈ $417 USD As you can see, building your own keyboard is not a good way to save money 💸 Links/resources All .stl and .svg files: https://www.thingiverse.com/thing:6099418 A helpful Hacker News comment: https://news.ycombinator.com/item?id=23445208 The generator I used to generate the keyboard case model: https://github.com/ibnuda/dactyl-keyboard/tree/refaktor The firmware that powers the keyboard: https://qmk.fm/ Some helpul build logs: https://sachee.medium.com/building-my-first-keyboard-and-you-can-too-512c0f8a4c5f https://youtu.be/UerP5bxGL3c (and subsequent videos) https://nickgreen.info/dactyl-manuform-build-log/ https://medium.com/swlh/complete-idiot-guide-for-building-a-dactyl-manuform-keyboard-53454845b065 ",
    "Confidence": 0.97592616
  },
  {
    "Title": "Have we reached the Generative AI peak? - Ritza Articles",
    "Url": "https://ritza.co/articles/have-we-reached-the-peak/",
    "Timestamp": "2023-06-29T21:02:33",
    "Domain": "ritza.co",
    "Description": "None",
    "Confidence": 0.99136025
  },
  {
    "Title": "Welcome to Peter's DevLog - NXP has messed up their password form validation",
    "Url": "https://peterme.net/scraps/nxp-has-messed-up-their-password-form-validation.html",
    "Timestamp": "2023-06-29T21:02:32",
    "Domain": "peterme.net",
    "Description": "I've been using a password manager for years now, and from time to time this leads to strange discoveries with password validation. They've ranged from crashing the server to simply ignoring valid passwords. I might write this out as a proper article some time in the future, but my latest discovery has me puzzled as to what is actually going on and I wanted to share it.",
    "Confidence": 0.9175809
  },
  {
    "Title": "Creating an ORM-less framework",
    "Url": "https://withinboredom.info/2023/06/28/creating-an-orm-less-framework/",
    "Timestamp": "2023-06-29T21:02:32",
    "Domain": "withinboredom.info",
    "Description": "Years ago, I enjoyed working with Durable Functions on Azure. There were some aspects that I fell in love with, like how “close” I felt to pure DDD, how simple it was to scale, and how …",
    "Confidence": 0.99365896
  },
  {
    "Title": "Goodbye MongoDB - Stuart Spence Blog",
    "Url": "https://blog.stuartspence.ca/2023-05-goodbye-mongo.html",
    "Timestamp": "2023-06-29T21:02:32",
    "Domain": "blog.stuartspence.ca",
    "Description": "After years of strange problems and bad performance with MongoDB, this week I successfully migrated chesscraft.ca to PostgreSQL. This took lots of preparations, went smoothly, and is performing great, so I thought I'd write about it. Writing here is cathartic. Also, others might find it useful.",
    "Confidence": 0.98126066
  },
  {
    "Title": "Lili’s pieces - CV4Animals 2023",
    "Url": "https://writings.lambdaloop.com/posts/cv4animals-2023/",
    "Timestamp": "2023-06-29T21:02:32",
    "Domain": "writings.lambdaloop.com",
    "Description": "A writeup of the CV4Animals 2023 workshop",
    "Confidence": 0.98406893
  },
  {
    "Title": "Why I chose php for my new side project",
    "Url": "https://commit.pizza/2023/06/28/why-i-chose-php-for-my-new-side-project/",
    "Timestamp": "2023-06-29T21:02:32",
    "Domain": "commit.pizza",
    "Description": "The reasons I picked php in 2023 for a completely new side project.",
    "Confidence": 0.90686417
  },
  {
    "Title": "The Many Ways that Digital Minds Can Know",
    "Url": "https://moultano.wordpress.com/2023/06/28/the-many-ways-that-digital-minds-can-know/",
    "Timestamp": "2023-06-29T21:02:32",
    "Domain": "moultano.wordpress.com",
    "Description": "LLMs do all the things people say they do at once, even if those people vociferously disagree, and they’re all useful.",
    "Confidence": 0.98139334
  },
  {
    "Title": "Hands on example of ChatGPT as a programming tool",
    "Url": "https://blog.scottlogic.com/2023/06/29/hands-on-example-of-chatgpt-as-a-programming-tool.html",
    "Timestamp": "2023-06-29T21:02:32",
    "Domain": "blog.scottlogic.com",
    "Description": "ChatGPT is put through its paces to see how the AI fares as a productivity tool for developing a small project. We look at where it helps, where it doesn't, and where AI tooling might go in the future.",
    "Confidence": 0.9946148
  },
  {
    "Title": "The busy bubble",
    "Url": "https://blog.liallen.me/the-busy-bubble",
    "Timestamp": "2023-06-29T21:02:32",
    "Domain": "blog.liallen.me",
    "Description": "Thoughts from Allen",
    "Confidence": 0.96150446
  },
  {
    "Title": "Towards an Adaptable Systems Architecture for Memory Tiering at Warehouse-Scale",
    "Url": "https://www.micahlerner.com/2023/06/29/towards-an-adaptable-systems-architecture-for-memory-tiering-at-warehouse-scale.html",
    "Timestamp": "2023-06-29T21:02:31",
    "Domain": "www.micahlerner.com",
    "Description": "This is one in a series of papers I’m reading from ASPLOS. These paper reviews can be delivered weekly to your inbox, or you can subscribe to the Atom feed. As always, feel free to reach out on Twitter with feedback or suggestions!",
    "Confidence": 0.9614376
  },
  {
    "Title": "CLI tools hidden in the Python standard library",
    "Url": "https://til.simonwillison.net/python/stdlib-cli-tools",
    "Timestamp": "2023-06-29T19:03:58",
    "Domain": "til.simonwillison.net",
    "Description": "Seth Michael Larson pointed out that the Python gzip module can be used as a CLI tool like this:",
    "Confidence": 0.99479395
  },
  {
    "Title": "Let's Learn .NET | .NET Live TV",
    "Url": "https://dotnet.microsoft.com/en-us/live/lets-learn-dotnet",
    "Timestamp": "2023-06-29T18:02:31",
    "Domain": "dotnet.microsoft.com",
    "Description": "This monthly beginner series will walk through the fundamentals of using C# and .NET to build real world applications. Come learn something new and leave with something that we all built, together, live with experts.",
    "Confidence": 0.9752842
  },
  {
    "Title": "Microsoft 365 Developer Proxy v0.9 with over-consenting guidance - Microsoft 365 Developer Blog",
    "Url": "https://devblogs.microsoft.com/microsoft365dev/microsoft-365-developer-proxy-v0-9-with-over-consenting-guidance/?ocid=M365_202306_Organic&Channel=Twitter&Product=Graph&Intent=Announcement&Audience=Developer",
    "Timestamp": "2023-06-29T17:06:46",
    "Domain": "devblogs.microsoft.com",
    "Description": "Microsoft 365 Developer Proxy introduces the preview ability to detect over-consented apps that use Microsoft Graph.",
    "Confidence": 0.99708617
  },
  {
    "Title": "Clean Architecture vs Vertical Slice Architecture",
    "Url": "https://youtube.com/watch?v=_yJJRn2_SFg",
    "Timestamp": "2023-06-29T15:04:07",
    "Domain": "youtube.com",
    "Description": "Vertical Slice Architecture vs Clean Architecture: Which one is the best for your project?💎 Be a Patreon to get the source code: https://patreon.com/gsferre...",
    "Confidence": 0.994043
  },
  {
    "Title": "Rx.NET v6.0: Enhancing Compatibility, Trimming Support, and Many More ",
    "Url": "https://www.infoq.com/news/2023/06/rx-dotnet-6/",
    "Timestamp": "2023-06-29T15:04:05",
    "Domain": "www.infoq.com",
    "Description": "Last month, the team behind Rx.NET announced the release of the 6.0 version. The latest version of the library brings several improvements and aligns itself with the current .NET ecosystem. While the update doesn't introduce significant new functionality, it focuses on enhancing compatibility, supporting the latest versions of .NET, and addressing common pain points for developers.",
    "Confidence": 0.99660486
  },
  {
    "Title": "What Is Infrastructure as Code?",
    "Url": "https://youtube.com/watch?v=a_mf8H7eQVc",
    "Timestamp": "2023-06-29T14:04:48",
    "Domain": "youtube.com",
    "Description": "As a .NET developer, I host most of my applications on Microsoft Azure. I have a few simple web apps, and I also run a SaaS product on Azure.I don't know muc...",
    "Confidence": 0.99683434
  },
  {
    "Title": "Mastering .NET MAUI SwipeView for Enhanced App Experience",
    "Url": "https://youtube.com/watch?v=BHBYHC_9URc",
    "Timestamp": "2023-06-29T14:04:48",
    "Domain": "youtube.com",
    "Description": "Let's look at all the features of adding swipe actions to your application with .NET MAUI's built in SwipeView. Swipe up, down, left, right on any control th...",
    "Confidence": 0.97973347
  },
  {
    "Title": "How to wait for multiple C++ coroutines to complete before propagating failure, peeling away at a tuple - The Old New Thing",
    "Url": "https://devblogs.microsoft.com/oldnewthing/20230629-00/?p=108380",
    "Timestamp": "2023-06-29T14:04:45",
    "Domain": "devblogs.microsoft.com",
    "Description": "Iterating over a tuple recursively.",
    "Confidence": 0.99671674
  },
  {
    "Title": "The New “Interceptors” Feature of C# 12 Is WILD!",
    "Url": "https://youtube.com/watch?v=91xir2oUQPg",
    "Timestamp": "2023-06-29T13:03:50",
    "Domain": "youtube.com",
    "Description": "Use code SUMMER23 until the 20th of July for a 15% discount on any bundle at https://dometrain.com/bundlesBecome a Patreon and get source code access: https:...",
    "Confidence": 0.96144474
  },
  {
    "Title": "runtime/docs/coding-guidelines/vectorization-guidelines.md at main · dotnet/runtime",
    "Url": "https://github.com/dotnet/runtime/blob/main/docs/coding-guidelines/vectorization-guidelines.md",
    "Timestamp": "2023-06-29T13:03:49",
    "Domain": "github.com",
    "Description": ".NET is a cross-platform runtime for cloud, mobile, desktop, and IoT apps. - runtime/docs/coding-guidelines/vectorization-guidelines.md at main · dotnet/runtime",
    "Confidence": 0.99332756
  },
  {
    "Title": "how (not) to write a pipeline",
    "Url": "https://cohost.org/tef/post/1764930-how-not-to-write-a",
    "Timestamp": "2023-06-29T12:03:09",
    "Domain": "cohost.org",
    "Description": "let me boldly assert that there are two types of programmer (at least as far as this post is concerned):\n\n 1. a programmer who builds something out of message queues and calls it a pipeline\n 2. a programmer who has had to maintain and operate a type-1 made pipeline\n\nalthough both types of programmer are kept awake by thoughts of code at night, only one of them is on the pager rota. every programmer gets to fuck around, but only a handful are blessed/cursed with finding out.\n\nthis essay is for programmers of the second variety. i'm not sure what anyone else will get, but hopefully the people who have already suffered will receive validation, and maybe a few lucky people will pick up some useful framing and vocabulary to fight the good fight later.\n\nit's also for people who won't whine about the lack of capitalization in a rough blog post\n\n----------------------------------------\n\n\nPRELUDE: CHARACTER SELECTION\n\nyou are:\n\n * ☐ A long time industry expert, with twelve acronyms on your business card.\n * ☐ Someone with a kanban, can-do attitude. Party planning your way to success!\n * ☑ A well rounded burnout who got hired through word of mouth.\n\nyou are\n\n * ☐ in a small engineering team, building out features for a website\n * ☑ employee number four and the ceo keeps writing code on weekends\n * ☐ in a large enterprise company with an hour and a half long daily standup\n\nyou are\n\n * ☐ in charge of building out backend systems\n * ☐ all working in the same repo\n * ☑ accidentally in ops, after fixing one too many builds\n\n\nTHE FIRE IS LIT: A PIPELINE IS COMING\n\nit's 3pm, it's friday, and your coworker drops a link in the company chat. it's a 2000 line change request, and they're desperate for a +1. despite the length, the code is relatively straight forwards. a lot of it is just yaml.\n\nthere's a thumbnail generator that's starting to take too long. originally it just cropped and resized things down, but for some ungodly reason, there's video support and transcoding now. requests to the website are timing out because this process is taking too long to complete.\n\nyour coworker, bored as hell, desperate to get on with real work, volunteers as tribute to fix the bug. now the code calls \"StartThumbNailer(user, file)\", a message gets put in a queue, and another process elsewhere calls \"user, file := queue.ThumbNextNail()\". problem solved.\n\nalas, it isn't icarus' fate to know his future.\n\n\nSTEP 1: CLEANING UP AFTER DROPPED MESSAGES\n\nyou open a dm, it's best to avoid an audience. people get touchy about their code.\n\n> \"This is great work, it's good to prototype these things out\"\n\nRemember: Don't be a dick about it. Don't squeal and wail, not matter how much you want to. People really don't like being told \"You can't do it that way. You do not understand why.\" It's a bad look all round, even if it's true.\n\nEstablish common ground, reframe problem, work towards common goals. Then you can be a dick about it, later. Remember: It's only a little bit less of a dick to be Socratic about it, and ask questions you already know the answer to, so try and be nice where you can.\n\n> \"I don't see a lot of error handling.\"\n\nThere's never any error handling. The message broker is always running, the queue always exists, and the workers never make a mistake, either. That's how prototypes look, sure, but that's how pipelines will look, years later.\n\nThe only thing that changes as pipelines age is the number of graphs on the ops dashboard.\n\n> \"It will be good to work out precisely which things we can leave out of a first implementation. We don't want to lose future feature dev time to operations.\"\n\nYou're not trying to \"shut it down\", you're working out the minimum level of work needed to ship it. You're justifying why with a business case.\n\n> \"What happens when message delivery fails? Maybe there's a network blip, maybe someone tripped over the cable at the data centre?\"\n\nIt's important not to blame the broker, and focus more on \"networks are bad and haunted.\" People will just tell you the broker saves messages to disk, so they're even harder to lose, ignoring the actual problem at hand.\n\n> \"If the send fails, we can just log it instead of doing nothing, and I guess we can restart those thumbnail jobs by hand.\"\n\nAgain, it's time to move on. A log message is fine enough for a first prototype, so don't get hung up on it. A lack of error handling isn't the important point. The important point is \"how much manual work is it going to be to fix things after they go wrong?\"\n\n> \"If something else went wrong, Is there a way to tell which users are missing thumbnails? Or would we have to add logging elsewhere?\"\n\nThe answer's usually something like \"a timestamp in the database\" or something, and another klaxon starts ringing. Resist the urge to explain why timestamps are a bad choice for now, you can revisit it later.\n\nThe important bit is that \"there's some state, in a database\", and for now it only has two states \"needs doing, and has been done\". It'll end up with seven or eight states later on, and that's when you'll have \"the timestamp talk\"\n\n> \"I think the important question is: Can write some .NeedsThumbnail() or .HasThumbnail() function, and then resend those messages? We can worry about the implementation details at the end.\"\n\nThat's right. \"At the end\". Now your coworker should realise it's time for a long haul discussion about the system. Don't miss your chance to recap things.\n\n * you recognise that there's limits to error handling in a prototype\n * you're worried about the manual work required to fix errors\n * with a bit more of a plan, things can move forward\n * working out manual steps required for recovery & seeing if there's low hanging fruit\n\nyou open a shared document. it's called \"background work: operations guide\", and drop a link in the change request.\n\n>  1. lost message\n> \n>  * problem: the service might fail to send a message for a variety of reasons\n>  * solution: we can discover this through logs and alarms\n>  * solution: we can store some state in the database, so it's easy to tell the \"needs thumbnails\" from \"has thumbnails\"\n>  * resolution: we can write scripts to restart lost jobs, or do it manually\n\nwith some aplomb, you write up a series of headings underneath.\n\n>  2. duplicate messages\n>  3. failed thumbnailer\n>  4. head of line\n>  5. statistics / monitoring / debugging\n\nyour coworker is not impressed, but hasn't lost hope yet. maybe they'll ship it by the end of the week. meanwhile, there's a bit of a shit-eating-grin on your face.\n\nerror handling sometimes means running the same thing twice. duplicate handling means avoiding just that. the statistics part is yet another \"tricking them into using the database\" section, broken down into smaller bite sized chunks.\n\nit is time to make your coworker eat their vegetables.\n\n\nSTEP 2: HANDLING DUPLICATES\n\nthe original code didn't really have any deduplication. the worker assumes it's the only one running for that user's thumbnails.\n\nduplication is a always a problem because the recovery steps for \"lost messages\" involve resending potentially lost messages. for example:\n\n * workers stop running overnight\n * message queue builds up\n * workers restart in morning\n * user notices thumbnail is missing, gets manually added to queue by hand\n * now there's two messages in the queue.\n\nit isn't the worst thing in the world if two processes get run at once, but no-one's really sure if it will cause problems, writing to the same files at the same time.\n\nagain, the point of raising this isn't \"this has to be fixed\" but \"we need to understand how it can fail, and how much time will we waste fixing it.\"\n\nsometimes the answer is \"make the process idempotent\", but usually the answer is \"locks\", shortly followed by \"leases\". you'll need to timeout locks eventually, if a worker crashes midway through thumbnailing.\n\n\"well, fuck it, we'll use redis. put a worker_id, timestamp in a hash, and if that timestamp is old, or missing, i can write my own one in.\"\n\nit's not perfect by any means, but it will reduce the operations headache. you update the file and move on.\n\nsometimes the queue has a magical transactional mode, where you can hold onto a message until you've finished processing it, and after a timeout it'll be available for other workers.\n\nit's almost the same thing you're doing with redis, but there's still a little more work to do:\n\n\nSTEP 3: HANDLING FAILED THUMBNAILER.\n\n> \"if a worker takes a message, asks for a lease, and then crashes, how do we retry work?\n> Do we scan the logs by hand again? What if it crashes before being able to log anything?\"\n\nThe usual answer to \"How do we fix it\" is \"Something puts the message back in the queue.\" Your coworker points out that we can fix it in the same way as before. Putting some timestamps in the database. Writing an automated script that restarts old jobs.\n\nYou nod once more, it's still not the right time to argue about timestamps.\n\n> \"Let's call it a message pump. Since it pumps messages into the queue. Also, we should probably keep track of the last error message, and the number of attempts when we do it.\"\n\nIt's ok, it's only adding one JSONB column. One we'll make use of later. Heh heh heh.\n\n> \"There's still one problem left, but I'm not sure it applies.\"\n\n\nSTEP 4: HANDLING HEAD OF LINE BLOCKING (OPTIONAL)\n\nthankfully your coworker was sensible, and suggested using the same automated script as before. if they'd chosen the fancy transactional queue, there'd be a little surprise for them in store: head of line blocking\n\neveryone who has worked with queues usually has a story like this, give or take some details:\n\n 1. for error handling, we used a transactional queue: if a process failed, it returned to the queue.\n 2. one job kept killing all the workers during a period of high load. the worker would crash, and the bad job went back to the top of the list. two people had a screaming match over zoom\n 3. instead of limiting retries, or a time-to-live on the job message, retries are put in a new \"error queue\", with their own workers\n 4. the error queue ends up full of duplicate messages, and all progress on retries is blocked.\n\nyou write \"head of line blocking: not in scope\" and move on. It's at this point you pray for a lightbulb above your coworkers head. Handling lost messages, handling broken thumbnailer runs, ensuring two jobs don't overlap, these are all interlinked. It's far too easy to write error handling that causes more errors.\n\nwith the error handling wrapped up, it's time to move on to the last section. unfortunately for your coworker, this is where the magic happens.\n\nIt's finally time to talk about timestamps.\n\n\nSTEP 6: DEBUGGING / INTROSPECTION\n\nnow you can start asking questions like \"How do we tell if a job has been enqueued or not? how do we tell if a job is being worked on?\" before moving onto \"Actually, how many states can a process be in?\"\n\nwith the queue, it's not possible to find out what things are running, if anything has crashed, and instead all you have is a graph that has three settings:\n\n * Queue's empty. Fine. Or maybe it's broken and no work is happening.\n * Burst of work, but line is going down. Actually fine.\n * Line is going up. Something is not working, and whatever it is, the problem is only getting worse.\n\nMeanwhile, the message broker queue thing as it stands only tracks two things:\n\n * If it's in the queue, this job will be assigned to a worker next.\n * If it's not in the queue, this job will not be assigned to a worker.\n\nIn other words:\n\n * We can't really tell if things are working reliably, outside of a few cases\n * The only thing we know for sure is \"this work is about to be done\"\n\nYour coworker points out that the timestamp thing from earlier would help, and it does. Now you can track two more things:\n\n * When did I last send a message\n * When did a job last complete.\n\nNotice how neither of those two states really tell you anything about what's going on for a particular thumbnail job. You can kinda infer \"this should be run\" if the last send message is older than the job last completing. You can kinda infer \"this ran, but i don't know if it succeeded or not\" if the last send message is earlier than the job completeing.\n\nThat's not a lot to infer. The underlying problem here is that a process can be in way more than two or three states, especially now error handling has come into the mix, and a handful of timestamps won't fix it.\n\nTo be clear: This is the time to yell about timestamps.\n\nYell about having one field called state is way easier than started_at last_active_at completed_at last_error_at, and a nasty series of comparisons. A field with three booleans and a truth table isn't any better, either.\n\nAfter sketching things out with your coworker, you decide that a job can be in several different states:\n\n * initialised: an id or record exists but there's no data inside\n * created: everything needed to start the job is available.\n * enqueued: in the process of being assigned to a worker\n * active: a worker is doing the job\n * failed: a worker encountered a known error, and can be restarted\n * panic: a worker encountered an unknown error\n * timeout: a worker failed to complete the job\n * skipped: no work actually needed\n\n> \"We'll probably combine some of them together, but it's good practice to split things up as much as possible.\"\n\n\nSTEP 7: BUILD A REVISED PLAN\n\nNow you've committed to tracking state in the database, it's time to clean up the proposal.\n\n * we'll have a table of thumbnail jobs in the database\n * each one has a state field, a free text field, and a jsonb column for error handling and other details\n\n> \"We could have a table of thumbnail changes in the database, that gets a new record every time the state changes. It might be more work now, but it's probably easier to have one DB migration than two.\"\n\n * workers read from a queue to find new work, and use a redis lock to exclude others\n * there's a message pump that reads from the database and writes to the queue\n * we don't need to persist any messages in the queue\n\nIt all feels a little overkill, but it really does make a big difference. Unlike before, there's handling in place for all sorts of errors. It's easy to recover from a crashed queue, you just empty it and restart the workers. Plus one quite lovely feature: the message pump can check the queue length, and alert if things stop going down. There's no need to configure an alerting system.\n\nIt's not a lot of code to write. The redis lock code (in lua) is somewhere on stack overflow. The database migration is a pain in the ass to deploy, but it's not difficult to write. Your coworker even adds in the second table, and starts collecting statistical data in the jsonb field to show off.\n\n\nINTERMISSION: YOU HAVE NOT BUILT A PIPELINE, YET\n\nit's a good point to sigh and take a break. you've successfully got a coworker to build a reliable, robust system. one that has automated ways to handle failures, one that doesn't require human intervention on the regular, and one that doesn't remind you of all the awful systems you've suffered so far.\n\nit's a good moment to take a step back and ask \"how come it worked out this time\"\n\n * your coworker actually believes you when you share your experience\n * you aren't forcing people to reinvent your exact solution\n * not every issue is fixed, despite being identified\n * it wasn't about someone being right, or someone being wrong, it was about lowering operational costs\n\nsure enough, sometimes it doesn't work out\n\n * your coworker doesn't care about the operational cost\n * no-one's getting a bonus for taking time to ship things carefully\n * the right thing will only matter at scale, and by that time, who knows what we'll be needing\n\nsometimes it's a little bit like solving a race condition. no-one believes it can be fixed, and when people ask for help, they just want to move the problem elsewhere. turns out \"have you tried explicitly ordering the operations on the shared mutable state\" is not a popular answer, despite being correct. people hate eating their vegetables.\n\neven so, you do win the argument eventually.\n\nthe queue keeps exploding, duplicate messages keep breaking everything. the error handling caused a denial of service. things tend to steer towards \"tracking the state of a job\".\n\nthe broker grows from \"a new central point of failure that always needs more disks\" into \"a lightweight service discovery and work assignment service\", and instead of \"just fire and regret a message\" you have \"the queue buffers the results of a more expensive database query.\"\n\nin fact, we could get rid of the queue entirely, and just use a load balancer\n\n * workers connect to a load balancer instead of a broker\n * scheduler sends http request for each item of work until told to slow down\n * jobs complete in the background and update the database\n\nwe could even get rid of the load balancer, too:\n\n * workers connect to the scheduler, and ask for the next job to perform\n * scheduler hands out jobs one by one as workers scale up or finish tasks\n * like before, scheduler amortises the expensive database query\n * but now? you don't need a lock server\n\nin some ways, you've just moved the queue and locking data structures inside the scheduler, they haven't really gone away. it does, however, let us do a variety of nice things:\n\n * like before, the scheduler can be killed and restarted, as all state is kept in the database\n * scheduler can even spin up more workers as and when required by the load\n * scheduler can offer api for starting jobs, listing jobs, getting status, or watching for changes\n * workers can even report back to the scheduler instead of to the database directly, which lets scheduler check worker is trying to update an active job.\n\nthe best part? the system doesn't really look much like a pipeline.\n\nnow the message pump, the lock, the queue, the worker registration, and the worker assignment, and the recovery processes are all happening in the same place, it feels quite different from slapping messages into a queue and hoping for the best.\n\nit might feel like more work than wrapping shit together in redis, but in some ways there's less complexity going around. as far as the worker is concerned, there's no lock manager, there's no queue, there's no database, just a scheduler api for getting a job and another for progress updates.\n\nit's almost important enough to recap\n\n * keep all the process state in a database table\n * use a text field for state, there's a lot of them\n * use another table if you want to track changes\n * scheduler amortises expensive database queries\n * scheduler keeps track of which worker is active on what\n * workers poll scheduler to be assigned work\n * workers update scheduler, not database, with progress updates / heartbeat messages\n * scheduler orders work to be done by most overdue\n\n[that last one is how you handle restarting errored jobs or missing jobs]\n\nintermission almost over, and we're nearly halfway to the (non) goal.\n\n\nROUND 2: OH NO, MORE PIPELINE!\n\neven if you don't go full tilt and implement the all-in-one scheduler, it might feel like the battle is won. with just a message pump, you're handling all sorts of errors automatically, and it didn't take two sprints to write.\n\nthere's just one problem: someone has suggested splitting up the thumbnailer into seperate processes. that same someone suggests tying the parts back together with a message broker. the pipeline is back, and the same problems are back too.\n\nnot just head of line blocking, but the whole \"have you tried reading the logs\" school of state management.\n\nit's at this point that you start to ask \"why does everyone keep doing it this way?\" and it doesn't take too long to realise that the answer is unix. when people think about batch processing, they think about being at the command line, and chaining up steps together in a unix pipeline\n\nand much like a pipeline, you end up writing something different to make things robust.\n\nat some point, the unix pipeline fails, and you're faced with re-running the entire thing again, or repeating that time honoured unix tradition of breaking up the pipeline a series of Makefile steps, using temporary files for each program's input and output.\n\npipelines aren't really designed for reliability, and in some ways, they're not really designed for task level parallelism either: it's a lot easier to run a 1000 calls to process(item) than it is to run one worker pool for each step in process and wire them up together with queues.\n\nanyway, back to the problem at hand: someone has had the bright idea of connecting things up with a message broker. this time, it's a new coworker, and the argument's a bit different. sure enough, there's the same old story about service discover, or \"free error handling for a problem created by using a broker\" stuff you heard the last time, but now there's \"can scale up and down each worker queue to adjust for load\" too\n\ni've already elaborated at length about why \"don't join things together with queues\" elsewhere, and this post is already quite lengthy in the \"don't use a queue to store background work\" department, so let's cut to the meat of it.\n\n> \"There's no real end-to-end management of the task state, unless you have a database. \"\n\nThe state tracking needs to be back, or messages will fall into the void. The message pump needs to be back, or work won't get restarted after failure. We can add new pipeline-like steps to our improved system, without having to implement the all-in-one scheduler.\n\n> \"We can add new logic to the message pump, and new states to the database table.\"\n\nFine. Keep your queues and your worker pool, but you tell the database when you're done with your step, and the message pump will send off a new task. It's still a little janky, each worker has to know the name of the worker after it to glue everything together, but it'll work fine enough.\n\nThe important part is to not be woken up at night because the queue's exploded. Using a message pump keeps that queue length bounded.\n\nThat's the real problem with queues: There's never any flow control, or a way for a queue to tell clients to slow down. There's always persistence, so you can't drop messages on the floor. There's never bounds on the size of a queue, either. A queue is a machine for turning rpc calls into full disks.\n\nLook, to keep a distributed system running, you need to build a system that aims towards an equilibrium state, slowing things down when other parts can't keep up. A message broker does the exact opposite, allowing parts to get wildly out of sync until you run out of disk space to keep up appearances.\n\nOnce you have backpressure or load shedding atop of a queue, you can't just fire a message away and hope for the best. You need to implement flow control, or error handling. You have to stop pretending the queue is a magic wand.\n\nIt's fine to glue things together with queues. You've just got to avoid persistence, and demand backpressure or load shedding, and you won't end up in the same mess over and over and over again. That's the important bit. That and \"if you run things, you need to keep track of their states\"\n\nAnyway, back to the coworker.\n\n> \"That's not the only route we can go. Your way is to have six worker pools, one for each part of the thumbnail task, shared across every task. Could we have one worker pool, for the whole thumbnail task, and just run the different steps inside the same worker?\"\n\nIt's not much of a change from before. Now the worker updates the database, then runs the next step, then updates the database, and runs the next step, until it runs out of steps. Like before, it's way, way easier to run task() 1,000 times, than it is to run six different pools and queues between them, but not every pipeline can benefit from task-level-parallelism.\n\n> \"... but if we have to do stuff like aggregation, we might have to do something different\"\n\nlet's stop thinking about the janky-ass setup we have so far. remember the all in one scheduler earlier? for task level parallelism, it's pretty straight forwards, we just run all the thumbnail components in one worker, and use the same setup from before:\n\n * we have one big worker pool\n * each worker asks for a task, and which state it's in\n * the worker runs that one part of the code, and reports the output\n * the scheduler decides if that worker runs the next step\n * in other words: the scheduler can decide how to parition the work at runtime\n\nthat's a kinda nice thing to have, but it doesn't clear up running things with task-level parallelism. in this case, the thumbnailer can run one step in parallel for each of the output files it generates.\n\nyour coworker, now fully baptised in the church of state tracking, decides to step in. they're saying stuff like \"it's not a background job system, it's a series of persistent state machines executed in parallel across a shared worker pool\", so you know they've got this.\n\n> \"We just run another task, right? There's one table for thumbnail tasks, right, one per top level thumbnail job. Let's add a second table for subtasks, and a given task can have lots of\n> active subtasks running.\"\n\nIn the all in one scheduler, it looks something like this\n\n * The worker tells the scheduler to start a number of subtasks\n * As the worker asks the scheduler to create a subtask, the scheduler can choose to assign it immediately\n * The original worker polls the scheduler to find out when that subtask completes\n * The original worker moves onto other work.\n * If there's an error, the worker restarts the subtask\n\nApplying it to the redis-and-message-pump prototype, we get:\n\n * There's a redis server providing a lock and a queue\n * The database has a table for the thumbnail task and another table for subtasks\n * There's a message pump, which scans both tables and tops up the message queue with jobs\n * Workers pull out a task or a subtask from the queue, and run it\n * The top level tasks end up just starting other subtasks and waiting for most of the time.\n\nCongratulations: You are the proud author of a not-pipeline. Sure enough, there's task level parallelism, and there's even inter-task level parallelism too. There's duplicate handling, end-to-end error handling with state tracking, and it's easy to ask questions like \"What process is running?\" and \"What error did this thing return with.\" Throw on a web interface, and you can probably move a lot of the support burden to another team.\n\nIt might still smell like a pipeline, but there's those all important differences.\n\n * The queue isn't persistent. You can restart the broker at any time, or drop all messages to go back to a known state.\n * There's backpressure. The message pump can hold off on writing messages if the queue gets too big.\n * There's end-to-end error handling, it doesn't matter if an individual part fails.\n * There's even process supervision. The top level task watches over the smaller ones. Errors in subprocesses can be handled and managed, too.\n * There's task level parallelism. One broken process in one thumbnail doesn't affect other thumbnail tasks.\n * There's inter-task level parallelism. One broken substep doesn't impeded other substeps from running to completetion.\n * The system is always in recovery. Instead of assuming things work, it constantly looks out for overdue tasks, trying to push things back into equilibrium,\n * It's even pretty easy to adapt. Adding new states doesn't require migrations, and the JSONB column gets used to smuggle variables into subtasks. Nice.\n\nMost importantly? This system is the winner of the \"least likely to wake you up at night\" award, handed out by a select set of judges. Not bad for \"a lock and and a queue in redis to amortise an expensive postgres query\"\n\nNot that anyone else notices. The problem with building robust systems is that it's a thankless task that everyone eventually takes for granted. Give it a few months, the new hires will be exclaiming \"Couldn't we just use a message broker. It's so much easier than this mess of queue and redis and database.\"\n\nI blame unix, personally.\n\n\nFINALE: THE PRODUCT\n\nyour coworker has been nerd sniped. they went off and finished the all-in-one scheduler for fun. it turned out to be a little less code than the redis duct tape sandwich in production. they've made some changes\n\nthere's a process table and a 'procedure' table, some of the other names have changed too. they renamed the scheduler to \"operator\". it's a kubernetes term for a daemon that watches a database and runs set actions, very similarly to how an operator works. in some ways, your coworker has written a toolkit for writing a scheduler in.\n\nthey've also started going on about \"microsoft orleans\" and \"entity component systems\" and \"the blackboard pattern.\" you even heard \"tuplespace\" once. they're all \"here's a database that tracks state, persists objects, and here's a series of workers that collaborative update this database to achieve tasks in a distributed or parallel or concurrent style\" looking things, and your coworker really, really wants to keep talking about turning the rather fancy state machines they can build.\n\nwell, at least it's not a pipeline. you hate pipelines, remember?\n\n\nPOSTSCRIPT\n\nyou might wonder and ask me \"is this software something you want to write\", and the answer is \"i have already written this software ten times over, in slightly different forms, all in different jobs\"\n\nin one job, we had a web api for all the little tasks associated with a background process, and we'd even coded a few buttons in. i suggested to coworker that it might be easier to just return a list of actions each thingy could do, over hardcoding each and every one. we even started doing things like returning human facing values in api. a \"state_en\" along with a \"state\" with things like \"Process is running for N hours\" inside.\n\nin another job, the big database of state got copied around from place to place. we ended up using version numbers instead of timestamps. yep, that's right, there's still more timestamp content. the problem with timestamps is that you can't tell if you've missed a version, there's no easy way to handle clock skew, and you don't really know how many times a thing has changed, either. a version number handles all these things, but you'll still need a timestamp to know when a version changed, and you won't care so much about clock skew, then.\n\ni even got to write my own rpc toolkit one of the times. as much as i lament having to rebuild the same internal product in each and every job, i'm not sure i'd have learned so much about distributed systems without it. that, and i'd probably be out of a job if people could actually build batch processing systems as easily as they build websites.\n\nthankfully, this post will change nothing",
    "Confidence": 0.942045
  },
  {
    "Title": "Test Data Generation With AutoFixture in .NET - Code Maze",
    "Url": "https://code-maze.com/csharp-test-data-generation-with-autofixture/",
    "Timestamp": "2023-06-29T11:04:52",
    "Domain": "code-maze.com",
    "Description": "In this article, we will explore how to automate test data generation and enhance productivity with a single library called AutoFixture.",
    "Confidence": 0.9974616
  },
  {
    "Title": "Valve is not willing to publish games with AI generated content anymore",
    "Url": "https://old.reddit.com/r/aigamedev/comments/142j3yt/valve_is_not_willing_to_publish_games_with_ai/",
    "Timestamp": "2023-06-29T09:05:41",
    "Domain": "old.reddit.com",
    "Description": "Hey all, I tried to release a game about a month ago, with a few assets that were fairly obviously AI generated. My plan was to just submit a...",
    "Confidence": 0.8706505
  }
]