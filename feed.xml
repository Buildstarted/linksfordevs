<?xml version="1.0" encoding="utf-16"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <atom:link rel="self" type="application/rss+xml" href="https://linksfor.dev/" />
    <title>linksfor.dev(s)</title>
    <link>https://linksfor.dev/</link>
    <description>Curated links for devs</description>
    <language>en</language>
    <item>
      <title>Beyond A*: Better Planning with Transformers via Search Dynamics Bootstrapping</title>
      <link>https://arxiv.org/abs/2402.14083</link>
      <description>While Transformers have enabled tremendous progress in various application settings, such architectures still lag behind traditional symbolic planners for solving complex decision making tasks. In this work, we demonstrate how to train Transformers to solve complex planning tasks and present Searchformer, a Transformer model that optimally solves previously unseen Sokoban puzzles 93.7% of the time, while using up to 26.8% fewer search steps than standard $A^*$ search. Searchformer is an encoder-decoder Transformer model trained to predict the search dynamics of $A^*$. This model is then fine-tuned via expert iterations to perform fewer search steps than $A^*$ search while still generating an optimal plan. In our training method, $A^*$&amp;#39;s search dynamics are expressed as a token sequence outlining when task states are added and removed into the search tree during symbolic planning. In our ablation studies on maze navigation, we find that Searchformer significantly outperforms baselines that predict the optimal plan directly with a 5-10$	imes$ smaller model size and a 10$	imes$ smaller training dataset. We also demonstrate how Searchformer scales to larger and more complex decision making tasks like Sokoban with improved percentage of solved tasks and shortened search dynamics.</description>
      <author> ([Submitted on 21 Feb 2024])</author>
      <guid>https://arxiv.org/abs/2402.14083</guid>
      <pubDate>Fri, 23 Feb 2024 20:03:58 GMT</pubDate>
    </item>
    <item>
      <title>After 14 years in the industry, I still find programming difficult</title>
      <link>https://www.piglei.com/articles/en-programming-is-still-hard-after-14-years/</link>
      <description>piglei&amp;#39;s blog</description>
      <author> (piglei,piglei2007@gmail.com)</author>
      <guid>https://www.piglei.com/articles/en-programming-is-still-hard-after-14-years/</guid>
      <pubDate>Fri, 23 Feb 2024 19:04:00 GMT</pubDate>
    </item>
    <item>
      <title>Scrutor vs Autofac in C#: What You Need To Know</title>
      <link>https://www.devleader.ca/2024/02/23/scrutor-vs-autofac-in-c-what-you-need-to-know/</link>
      <description>Scrutor vs Autofac in C#! Which one comes out on top between these two solutions for dependency injection in dotnet? See how each are used with code examples!</description>
      <author> (https://www.facebook.com/devleaderca)</author>
      <guid>https://www.devleader.ca/2024/02/23/scrutor-vs-autofac-in-c-what-you-need-to-know/</guid>
      <pubDate>Fri, 23 Feb 2024 18:03:26 GMT</pubDate>
    </item>
    <item>
      <title>Gotcha: Be careful how you shut down your dispatcher queues - The Old New Thing</title>
      <link>https://devblogs.microsoft.com/oldnewthing/20240223-00/?p=109438</link>
      <description>The dispatcher queue thread isn&amp;#39;t useful after it has shut down, so don&amp;#39;t try anything.</description>
      <author> (Raymond Chen)</author>
      <guid>https://devblogs.microsoft.com/oldnewthing/20240223-00/?p=109438</guid>
      <pubDate>Fri, 23 Feb 2024 18:03:25 GMT</pubDate>
    </item>
    <item>
      <title>Intel Processor Instability Causing Oodle Decompression Failures</title>
      <link>https://www.radgametools.com/oodleintel.htm</link>
      <description>RAD Game Tools&amp;#39; web page. RAD makes Bink
Video, the Telemetry Performance Visualization System, and Oodle Data Compression - all popular video game middleware.</description>
      <author> ()</author>
      <guid>https://www.radgametools.com/oodleintel.htm</guid>
      <pubDate>Fri, 23 Feb 2024 18:03:25 GMT</pubDate>
    </item>
    <item>
      <title>The Billion Row Challenge (1BRC) - Step-by-step from 71s to 1.7s</title>
      <link>https://questdb.io/blog/billion-row-challenge-step-by-step/</link>
      <description>I took part in the Billion Row Challenge. Enjoy a deep, step-by-step summary of how you get from a Parallel Java Streams implementation that takes 71 seconds to a super-optimized version that takes 1.7 seconds. Example code and walkthroughs included!</description>
      <author> (QuestDB)</author>
      <guid>https://questdb.io/blog/billion-row-challenge-step-by-step/</guid>
      <pubDate>Fri, 23 Feb 2024 17:03:23 GMT</pubDate>
    </item>
    <item>
      <title>Cecilifier - Learn Mono.Cecil by example</title>
      <link>https://cecilifier.me/</link>
      <description>Cecilifier is a tool aiming to make learning using Mono.Cecil easier by taking code snippets and generating Cecil APIs calls that creates an assembly equivalent to the compiled code snippet. It may also help you to learn more about MSIL (also known simply as IL).</description>
      <author> ()</author>
      <guid>https://cecilifier.me/</guid>
      <pubDate>Fri, 23 Feb 2024 16:04:10 GMT</pubDate>
    </item>
    <item>
      <title>ChatGPT vs. Advent of Code 2023 Day 22: Sand Slabs</title>
      <link>https://youtube.com/watch?v=Yd3emlhHl2E</link>
      <description>Can ChatGPT and its Code interpreter solve a programming puzzle resembling Jenga? Let&amp;#39;s see!</description>
      <author> (Martin Zikmund
  
  
  
    Martin Zikmund
  






    •)</author>
      <guid>https://youtube.com/watch?v=Yd3emlhHl2E</guid>
      <pubDate>Fri, 23 Feb 2024 14:03:52 GMT</pubDate>
    </item>
    <item>
      <title>Commits &amp;#183; sigma0-xyz/zkbitcoin</title>
      <link>https://github.com/sigma0-xyz/zkbitcoin/commits/v0.1.0</link>
      <description>zkBitcoin: zero-knowledge proofs on Bitcoin! Contribute to sigma0-xyz/zkbitcoin development by creating an account on GitHub.</description>
      <author> (sigma0-xyz)</author>
      <guid>https://github.com/sigma0-xyz/zkbitcoin/commits/v0.1.0</guid>
      <pubDate>Fri, 23 Feb 2024 13:04:36 GMT</pubDate>
    </item>
    <item>
      <title>ZK Mini Summit &amp;#183; Luma</title>
      <link>https://lu.ma/ZKMiniSummit_Denver</link>
      <description>The ZK Mini Summit &amp;#128640; is gearing up to be a thrilling ride &amp;#127775;. We’ll dive deep into the innovative universe of zero-knowledge proofs (ZKPs) &amp;#129504;&amp;#128171;&amp;#160;From a whimsical zero-knowledge proofs...</description>
      <author> ()</author>
      <guid>https://lu.ma/ZKMiniSummit_Denver</guid>
      <pubDate>Fri, 23 Feb 2024 13:04:36 GMT</pubDate>
    </item>
    <item>
      <title>v0.1.0 &amp;#183; sigma0-xyz/zkbitcoin &amp;#183; Discussion #44</title>
      <link>https://github.com/sigma0-xyz/zkbitcoin/discussions/44</link>
      <description>Hey &amp;#128075; This is our first release &amp;#127881; What&amp;#39;s Changed (since we came out of stealth &amp;#129399;) ci: run bitcoin core regtest node for tests by @xJonathanLEI in #17 Keepalive with Fibonacci Backoff, fault toleran...</description>
      <author> (sigma0-xyz)</author>
      <guid>https://github.com/sigma0-xyz/zkbitcoin/discussions/44</guid>
      <pubDate>Fri, 23 Feb 2024 13:04:35 GMT</pubDate>
    </item>
    <item>
      <title>What is Scrum and is Scrum good for startups? — todo.space blog</title>
      <link>https://blog.todo.space/2019/03/04/what-is-scrum-in-agile-and-is-scrum-good-for-startups/</link>
      <description>TLDR: No, it&amp;#39;s not lean, it&amp;#39;s not agile, and, therefore, it is very expensive, therefore, not good for startups.</description>
      <author> ()</author>
      <guid>https://blog.todo.space/2019/03/04/what-is-scrum-in-agile-and-is-scrum-good-for-startups/</guid>
      <pubDate>Fri, 23 Feb 2024 11:03:51 GMT</pubDate>
    </item>
    <item>
      <title>nanoFramework: Unleashing the Power of C# in Embedded Systems and IoT with Jos&amp;#233; Sim&amp;#245;es</title>
      <link>https://youtube.com/watch?v=G1WOQKT6pgg</link>
      <description>Avalonia XPF This episode of The Modern .NET Show is supported, in part, by Avalonia XPF, a binary-compatible cross-platform fork of WPF, enables WPF apps to...</description>
      <author> (The No-Frauds Club
  
  
  
    The No-Frauds Club
  






    •)</author>
      <guid>https://youtube.com/watch?v=G1WOQKT6pgg</guid>
      <pubDate>Fri, 23 Feb 2024 09:04:10 GMT</pubDate>
    </item>
    <item>
      <title>JavaScript Bloat in 2024</title>
      <link>https://tonsky.me/blog/js-bloat/</link>
      <description>What is the average size of JavaScript code downloaded per website? Fuck around and find out!</description>
      <author> (https://www.facebook.com/nikitonsky)</author>
      <guid>https://tonsky.me/blog/js-bloat/</guid>
      <pubDate>Fri, 23 Feb 2024 09:04:09 GMT</pubDate>
    </item>
    <item>
      <title>Mark Oliver&amp;#39;s World</title>
      <link>https://blog.markoliver.website/Recording-Video-From-A-Nintendo-Switch</link>
      <description>Posted: 23/02/2024</description>
      <author> ()</author>
      <guid>https://blog.markoliver.website/Recording-Video-From-A-Nintendo-Switch</guid>
      <pubDate>Fri, 23 Feb 2024 09:04:08 GMT</pubDate>
    </item>
    <item>
      <title>ASP.NET Community Standup Topic - Assessing your app&amp;#39;s Azure readiness</title>
      <link>https://youtube.com/watch?v=lhoNVsxRIP4</link>
      <description>Mike Rousos joins us for an update on Azure Migrate Application and Code Assessment, and how you can use it to check your scan your application&amp;#39;s .NET source...</description>
      <author> (dotnet
  
  
  
    dotnet
  






    •)</author>
      <guid>https://youtube.com/watch?v=lhoNVsxRIP4</guid>
      <pubDate>Fri, 23 Feb 2024 08:04:12 GMT</pubDate>
    </item>
    <item>
      <title>SDXL Lightning - by fal.ai</title>
      <link>https://fastsdxl.ai/</link>
      <description>Lightning fast SDXL API demo by fal.ai</description>
      <author> (fal.ai)</author>
      <guid>https://fastsdxl.ai/</guid>
      <pubDate>Fri, 23 Feb 2024 08:04:07 GMT</pubDate>
    </item>
    <item>
      <title>IA responsable y mecanismos de protecci&amp;#243;n</title>
      <link>https://youtube.com/watch?v=Z4PVYfXSFuw</link>
      <description>Trataremos el tema de moda la IA con un experto en la materia como lo es Matias Cordero para adentrarnos en la IA responsable y mecanismos de protecci&amp;#243;n.Beco...</description>
      <author> (Javier Su&amp;#225;rez
  
  
  
    Javier Su&amp;#225;rez
  






    •)</author>
      <guid>https://youtube.com/watch?v=Z4PVYfXSFuw</guid>
      <pubDate>Fri, 23 Feb 2024 07:02:52 GMT</pubDate>
    </item>
    <item>
      <title>Introducing Modular Monoliths: The Goldilocks Architecture</title>
      <link>https://ardalis.com/introducing-modular-monoliths-goldilocks-architecture/</link>
      <description>This article introduces the concept of Modular Monoliths, an architectural approach that combines the simplicity of monolithic applications with the flexibility of microservices, offering a balanced solution for software development. It outlines the key characteristics, advantages, and suitable scenarios for adopting this Goldilocks architecture, providing a pragmatic path for developers and architects aiming for maintainability, simplicity, and scalability in their projects.</description>
      <author> (Ardalis)</author>
      <guid>https://ardalis.com/introducing-modular-monoliths-goldilocks-architecture/</guid>
      <pubDate>Fri, 23 Feb 2024 06:03:26 GMT</pubDate>
    </item>
    <item>
      <title>An Active Local NuGet Server</title>
      <link>https://ardalis.com/active-local-nuget-server/</link>
      <description>As I&amp;#39;m writing this the Internet is out. When that happens, it makes it very difficult to work on development projects that have NuGet dependencies, especially when it comes to adding anything new to a project. A local NuGet server that kept up-to-date with my commonly used packages would be helpful right now.</description>
      <author> (Ardalis)</author>
      <guid>https://ardalis.com/active-local-nuget-server/</guid>
      <pubDate>Fri, 23 Feb 2024 06:03:26 GMT</pubDate>
    </item>
    <item>
      <title>Announcing TypeScript 5.4 RC - TypeScript</title>
      <link>https://devblogs.microsoft.com/typescript/announcing-typescript-5-4-rc/</link>
      <description>Today we’re excited to announce our Release Candidate of TypeScript 5.4! Between now and the stable release of TypeScript 5.4, we expect no further changes apart from critical bug fixes. To get started using the RC, you can get it through NuGet,</description>
      <author> (Daniel Rosenwasser)</author>
      <guid>https://devblogs.microsoft.com/typescript/announcing-typescript-5-4-rc/</guid>
      <pubDate>Fri, 23 Feb 2024 06:03:24 GMT</pubDate>
    </item>
    <item>
      <title>Minimal APIs vs Controller APIs: SerializerOptions.WriteIndented = true</title>
      <link>https://jeremybytes.blogspot.com/2024/02/minimal-apis-vs-controller-apis.html</link>
      <description>Another thing added in ASP.NET Core / .NET 7.0 (yeah, I know it&amp;#39;s been a   while since it was released), is the &amp;quot;ConfigureHttpJsonOptions&amp;quot; e...</description>
      <author> ()</author>
      <guid>https://jeremybytes.blogspot.com/2024/02/minimal-apis-vs-controller-apis.html</guid>
      <pubDate>Fri, 23 Feb 2024 06:03:23 GMT</pubDate>
    </item>
    <item>
      <title>The case for an application-level tracing API in .NET</title>
      <link>https://nblumhardt.com/2024/02/tracing-with-message-templates/</link>
      <description>If you want to record a log event from your application in .NET, you can do that today without a lot of noise or ceremony using Microsoft.Extensions.Logging:</description>
      <author> ()</author>
      <guid>https://nblumhardt.com/2024/02/tracing-with-message-templates/</guid>
      <pubDate>Fri, 23 Feb 2024 06:03:23 GMT</pubDate>
    </item>
    <item>
      <title>.NET API Middleware for common functionality [Pt 5] | Back-end Web Development w/ .NET for Beginners</title>
      <link>https://youtube.com/watch?v=2QvUwxcHCBc</link>
      <description>This video covers implementing common behavior for ASP.NET Core APIs using middlewares. Middlewares are a piece of code that can run before and after each re...</description>
      <author> (@ventoshe)</author>
      <guid>https://youtube.com/watch?v=2QvUwxcHCBc</guid>
      <pubDate>Fri, 23 Feb 2024 04:04:08 GMT</pubDate>
    </item>
    <item>
      <title>Azure API Center and ASP.NET Web API Integration: A Developer&amp;#39;s Guide | .NET Conf 2023</title>
      <link>https://youtube.com/watch?v=o4gMJfee4iA</link>
      <description>In a rapidly evolving digital landscape, the use of APIs is becoming increasingly popular. One key tool that has emerged as a game-changer is the ability to ...</description>
      <author> (@acodersjourney
          
        
        
        
        3 months ago)</author>
      <guid>https://youtube.com/watch?v=o4gMJfee4iA</guid>
      <pubDate>Fri, 23 Feb 2024 04:04:08 GMT</pubDate>
    </item>
    <item>
      <title>Install Teams Toolkit in Visual Studio - Teams</title>
      <link>https://learn.microsoft.com/en-us/microsoftteams/platform/toolkit/toolkit-v4/install-teams-toolkit-vs?pivots=visual-studio-v17-7</link>
      <description>Learn about installation of Teams Toolkit of different versions in Visual Studio, and marketplace.</description>
      <author> (zyxiaoyuer)</author>
      <guid>https://learn.microsoft.com/en-us/microsoftteams/platform/toolkit/toolkit-v4/install-teams-toolkit-vs?pivots=visual-studio-v17-7</guid>
      <pubDate>Fri, 23 Feb 2024 04:04:04 GMT</pubDate>
    </item>
    <item>
      <title>Weekly Update 388</title>
      <link>https://youtube.com/watch?v=-4lxnZK6S2s</link>
      <description>LockBit Burned to the Ground by Law Enforcement; Phishy FedEx SMS; Have I Been Pwned UX Redesign; Sponsored by Report URI</description>
      <author> (Yahoo Finance
  
  
  
    Yahoo Finance
  





    •)</author>
      <guid>https://youtube.com/watch?v=-4lxnZK6S2s</guid>
      <pubDate>Fri, 23 Feb 2024 02:04:00 GMT</pubDate>
    </item>
    <item>
      <title>Easy Guide to Creating Minimal APIs in ASP.NET</title>
      <link>https://youtube.com/watch?v=AuKKFVSMxJc</link>
      <description>Discover the simplicity and versatility of Minimal APIs in ASP.NET with my step-by-step tutorial! Learn how to create HTTP GET, PUT, POST, and DELETE methods...</description>
      <author> (Nick Proud
  
  
  
    Nick Proud
  






    •)</author>
      <guid>https://youtube.com/watch?v=AuKKFVSMxJc</guid>
      <pubDate>Fri, 23 Feb 2024 01:03:55 GMT</pubDate>
    </item>
    <item>
      <title>.NET 8 Is Now Available on AWS Lambda</title>
      <link>https://youtube.com/watch?v=ZKkZuzGfijs</link>
      <description>Hi, I&amp;#39;m James! It&amp;#39;s finally here, the .NET 8 managed runtime is now available on AWS Lambda. And in this video, you&amp;#39;re going to learn how to take your existi...</description>
      <author> (James Eastham
  
  
  
    James Eastham
  






    •)</author>
      <guid>https://youtube.com/watch?v=ZKkZuzGfijs</guid>
      <pubDate>Fri, 23 Feb 2024 00:04:06 GMT</pubDate>
    </item>
    <item>
      <title>Okay, Color Spaces — ericportis.com</title>
      <link>https://ericportis.com/posts/2024/okay-color-spaces/</link>
      <description>What is a “color space?”</description>
      <author> (Eric Portis)</author>
      <guid>https://ericportis.com/posts/2024/okay-color-spaces/</guid>
      <pubDate>Thu, 22 Feb 2024 23:04:03 GMT</pubDate>
    </item>
    <item>
      <title>3D modeling with ChatGPT - Solidified ephemerality</title>
      <link>https://0110.be/posts/3D_modeling_with_ChatGPT_-_Solidified_ephemerality</link>
      <description>I have asked ChatGPT to generate 3D models. ChatGPT can not generate 3D models directly but 3D models can generated via intermediary OpenSCAD scripts: OpenSCAD provides a scripting language to describe objects which can be combined to form 3D models. ChatGPT understands the syntax of this scripting language and generates perfectly cromulent scripts. I have asked two versions of ChatGPT to generate a 3D model of a house, a cat, a stick figure, a chair and a tree.  The results are interesting…</description>
      <author> ()</author>
      <guid>https://0110.be/posts/3D_modeling_with_ChatGPT_-_Solidified_ephemerality</guid>
      <pubDate>Thu, 22 Feb 2024 23:04:01 GMT</pubDate>
    </item>
    <item>
      <title>How well do Lc0 networks compare to the greatest transformer network from DeepMind?</title>
      <link>https://lczero.org/blog/2024/02/how-well-do-lc0-networks-compare-to-the-greatest-transformer-network-from-deepmind/</link>
      <description>To explore the performance of Lc0 networks relative to DeepMind’s state-of-the-art transformer networks, we embarked on a comparative analysis, inspired by the methodologies detailed in DeepMind’s latest publication. Our objective was to closely align our testing approach for Lc0 networks with the evaluation framework applied by DeepMind, allowing for a direct comparison of results.</description>
      <author> ()</author>
      <guid>https://lczero.org/blog/2024/02/how-well-do-lc0-networks-compare-to-the-greatest-transformer-network-from-deepmind/</guid>
      <pubDate>Thu, 22 Feb 2024 23:04:01 GMT</pubDate>
    </item>
    <item>
      <title>No Simple Answers In Stereo</title>
      <link>https://joe-steel.com/2024-02-19-No-Simple-Answers-In-Stereo.html</link>
      <description>There was some continued back and forth on Mastodon about stereo conversions. Mac Stories contributor Jonathan Reed asked a couple questions:

What’s your view on the best converted movies? Do you think they hold up just as well vs (non-bad) native 3D movies?

I am not picking on Jonathan, but crediting him with a question that seems very reasonable. It seems logical to ask for an example of what’s working, but that’s much more difficult to do than it sounds. It’s kind of like proving a negative (even though this is a positive?)
If there is a best conversion, you’re unlikely to be aware of it at all, because the audience usually only remembers technical errors, or discomfort. There’s nothing outwardly impressive about a good conversion, or good native stereo, and anything that was held up as a good conversion would be picked apart with intense scrutiny to prove that it’s not actually good.
Add on top of that the point Todd Vaziri and I were trying to make in that thread, and in our feedback to the Accidental Tech Podcast, that there are a variety of methods employed in various shots in various movies. It is not as homogeneous as it appears to be in untrustworthy marketing, or that silly “real vs. fake” site.
There’s no binary bit on the movie that flips if one shot in a native 3D movie is a post conversion shot, what percentage of shots need to be rendered fully 3D in an animated feature, or a blockbuster with 2,000+ effects heavy shots.
I know that is deeply unsatisfying as an answer, and the follow up question would be for movies that don’t work well. For professional reasons I wouldn’t ever spell that out.
Ultimately, I know that just saying that it’s nuanced and complicated is not very helpful or informative to people that want to understand stereo. For those that want a quick answer on whether a movie is worth watching in 3D on their Apple Vision Pro, there’s nothing so simple as a list.
The best I can do is talk through common problems in stereo. To do that we need to talk about some terms. To do that I’m going to need to bore the fuck out of you.
Native Stereo Photography
This is shot with two cameras. Usually this involves a cumbersome rig where the cameras have to be exactly aligned, have matching apertures, matching focal distances, and are slightly physically offset. To get them close enough together they’re often arranged with one camera pointing straight up, and it gets it’s light from a beam splitter. A semi-transparent plane of glass that lets light pass through to the main camera, but also reflects down to the vertical camera. This is an enormous pain in the ass, and it’s very easy to have something be just a little off in a way that won’t be clear until later.
When the stereographer and director finish shooting, they can adjust the convergence by horizontally transforming the photography which pushes and pulls things in order out of the screen depending on where the left and right eye converge. However, they can’t adjust the interaxial without throwing away one of the eyes and doing it over with conversion. That means they might be more conservative in all of their choices to reduce the chance that there’s an error.
Things to look for are misalignment. If the left and right eyes have an angular difference between them, or skew slightly. Your eyes are looking for horizontal disparity so vertical shifts mess it up a little. This is abundant in iPhone 15 Pro Spatial Videos because of Apple’s attempts to compensate for the mismatched lenses.
Another big thing is color shifts from the beam splitter. Sometimes that could manifest as a constant shift, or it could be transient if the camera rig is moving and the light catches differently. It’s possible to color correct the views to get a closer match but uncorrected differences might appear to shimmer when your brain processes the slightly different hues and values.
Specular reflections. Think of bright pings of light on glass or chrome, often from a distant, but bright light source. One eye might get the ping of light and the other eye doesn’t. A mismatch like that can appear to glow, or shimmer, and could be uncomfortable to look at. To correct for this in native stereo the ping might be artificially copied and offset to the other eye, or the value of the ping might be knocked down so it doesn’t draw attention.
If you have a visual effects shot where native stereo plate photography is combined with rendered assets you might see issues that you wouldn’t get if it was a post conversion. Like a bluescreen or greenscreen shot where the work done to extract the photographic element from the screen color where the extraction is not an exact match. A common issue is flyaway hairs, those thin wisps of hair that are always difficult, could be in one eye but not the other, or trimmed in an odd way.
Flyaway hairs in non-VFX native stereo shots should always look pretty good, but depending on how deep the background is behind them you might be surprised to notice them more than you would in a 2D movie.
This doubling of work - and the need for it to match - is also what makes something like wire removal paint much more difficult. It’s easy to make each single view of paint be internally consistent and work, but then to make sure those paint adjustments match between both photographic plates is a pain that you don’t have to deal with in conversion.
It used to be very difficult to get 3D matchmove solves that were rock solid for both eyes. Meaning something could appear to float away where native stereo photography and CG met. Very rare, but maybe if you’re watching something old things might seem to drift or breathe.
Another thing 2D VFX artists take for granted is being able to use masks/mattes/rotosplines - and only having to do it once without thinking about where the matte sits in depth. The matte could be used to grade the background, or it could be to help extract a person. Those rotosplines need to be done for two eyes, and they need to match the plate photography, and their companion spline, including motionblur. A soft mask extending back along an angled surface will need to have depth that matches that angle in the other eye. So you end up doing the post conversion kinds of steps on the mattes applied to your native stereo left and right images to make them match and sit in depth, but are constrained to the native stereo plates as well.
Native Stereo Renders
Native stereo renders in animated movies, or for shots in a VFX heavy movie that don’t have photography, have their own pros and cons. Even those “all CG” shots are not always fully rendered in stereo for left and right eyes. The flat version of the movie will be done while the stereo version of the movie lags behind a little bit. That means that rendering the offset eye might reveal issues where an old version of a shader was used, or an asset changed since the original shot completed. It can be much more of a puzzle.
People also can do anything they want to with their cameras because they are no longer constrained by physics. That means you either get mind-bending stuff, like stuff sticking out of the screen that would really be a considerable distance away from the audience, requiring enormous interaxial camera offsets, or sometimes they’ll just make it really flat, even though they have the ability to do whatever they want.
You also still have some of the same issues presented by bright specular pings being in one eye, but not the other, but also that they might sizzle because bright, distant light sources need more raytracing samples (tiny thing far away gets more missed rays than hits).
You might be like, so what? Just turn up the samples, right? That’s easier said than done in some cases, especially if the 2D version of the movie is done already, or the rendering engine just can’t resolve some very bright, distant point of light with enough samples that won’t take 3 months to render. The sample noise will sizzle differently between the two eyes and appear to glow. There are ways to cut out pixels from the other eye, or median filter it, or what have you, but if it’s uncorrected you’ll see sizzling pixels.
Native stereo renders do have one fun trick and that’s the depth map (Z) channel that is normally used for depth of field focus effects. It is a image where every pixel corresponds to how far away something is from camera. It can be used to create an exact offset based on the stereo camera pair. This makes it kind of like post conversion where fake depth is used to offset 2D data from one camera view to the other. That means you can offset things like rotosplines, or other 2D elements, to match the depth of your 3D exactly. I do mean, exactly, since it will be at exactly the depth from the depth channel. Effectively like using a projector from the location of your left eye camera, and then viewing it from the location of your right eye camera.
This also means that parts of the render from the left or the right eye can be offset by the depth data to patch or supplement renders from the other eye. Think of it like sneaking in a little conversion. This can save render time, and help with various problems matching the eyes.
It can be as specific as using a render for parts of a character (eyes, fur, screen-right edges), or parts of lighting components (just the specular, just the reflection, refraction, or just the diffuse). 
To a purist, it might sound like an anathema to mix and match, because a purist would assume that the highest quality is from matching renders. Really most people would fail a Pepsi challenge on fully rendered shots vs. hybridized shots. The philosophical concerns don’t matter as much as the final set of images being coherent.
For this reason it is absolute bunk to call all animated movies “real” 3D, or to be able to claim from your seat in an audience what’s rendered from scratch and what’s not.
Post Conversion
Conversions are popular because they require less time on set, use more flexible camera setups, and cause fewer problems for the crew that’s mainly concerned with the 2D version of the movie. That also means they can adjust the depth of everything ad infinitum. That can mean a more creative, and thoughtful use of stereo because they can evaluate the results and change it in a way they can’t do easily on set, where they are more likely to be conservative, or stuck with what they shot.
Conversions are also associated with people looking to make a quick buck on ticket sales, and reducing labor costs on the conversion to get as much profit as possible.
That means it’s likely you’ll see the places where conversions fall apart because of time and budget constraints, which was very common in earlier post conversions when studio execs felt like they needed to rush. You might recall movies where only part of the film was in stereo, and they wanted you to take on or off your glasses in the theater.
There was also the quality issue from the assumption that people were going to watch these in theaters were they couldn’t hit pause. The home video part never panned out - but maybe it will with products like the Apple Vision Pro.
Major issues stem from the approach a conversion house takes when presented with 2D footage. Most places will create a 3D space in the computer and camera in order to “accurately” produce the offset eye. I’ve heard of some places where people just cut out and move stuff around to wherever it feels right for them, or use image based algorithms to create a fake depth map to drive the stereo offset, but the map might have holes and errors where the algorithm guessed wrong. I’ve never worked at a place that did these things so I don’t have insight to share about their thought process, so let’s move on to placed-in-3D stuff.
To do that, matchmove needs to be done where the camera is solved for, and elements in the shot get rough geometry. Since this often needs to happen for the 2D VFX portion of a movie, this is considered some synergistic cost savings. The plate can be projected on that rough geometry in a 3D software package, and then an offset camera can be dialed for the interaxial and convergence values that feel right for the shot, and in the context of the sequence. That projection on to the hard geo is really just to dial things, the geo won’t be used raw, it’ll be cut by rotos, blurred in the depth channel, etc. to make something that’s softer than the hard facets of rough geo.
The photography does get rotoed, however only one eye needs to be rotoed, not a complicated matching pair. The photography also needs some degree of paint work to be done to it to clean up the area that was occluded by the foreground. This can be as simple as painting out a sliver, or halo, around where a character occludes the background, or it can be a more extensive affair.
That means the same paint needs to be used in both eyes to account for any minor variance between the paint and the original plate. While the audience could never tell that the background was painted (really, I absolutely promise you can’t tell because shit is painted all the time in regular-old-vanilla shots and people truly don’t have an inkling) the audience can tell if there’s only paint in one eye’s view for the same reason as it would be a problem to have mismatched paint in native stereo.
Paint removal includes things like flyaway hairs which will be painted out and rotoed, or luminance keyed, to bring them back. They will match exactly between the two eyes, unlike mismatched keys of native stereo, but they will need to be placed in depth.
If you want to tell anything about the quality of a conversion, look for those flyaway hairs. They should be there, and they should also be at a sensible depth relative to the rest of the hair. not way behind, or in front of the actor.
The actor should have internal depth, which is usually derived from the rough matchmove geometry. They should have a nose past their eyes, and their ears and neck should be back. They should never feel like a cardboard cut-out unless they are far away from the camera, like background actors, or a really wide shot of them in an environment.
Speaking of environments, the two biggest problems there are highly reflective and refractive surfaces. If there’s a shop window, with reflections, and the name of the shop painted on the glass, the reflections should not be at the depth we see through the window or they will look like they’re at the depth of the walls and surfaces inside the shop. They need to be at the depth of whatever they’re reflecting. That means the reflections must be painted out, along with the lettering on the glass. The lettering needs to then be placed over the shop interior at the depth of where the glass plane is on the facade, and then the reflections need to be added (reflections are additive, but that is a rant for another day). Then that reconstructed window is used for both the left and right eye. No, you won’t be able to tell that work was done because you, in the audience, don’t have the 2D version of the movie to look at and compare it to, and the work will all be so internally consistent that it wouldn’t register for you to check without the knowledge that this kind of work needed to happen. In the abstract this knowledge might cause philosophical conflict — unclean! Impure! But I assure you the director isn’t anywhere near as precious about this as you might think.
If a conversion house omits that level of work, and just lets the shop window be flat to the depth of the building facade, or lets everything in the window go deeper, including the reflections and lettering, then it’s going to look wrong to any casual viewer.
This can also be applied to things like shiny cars, or reflective bodies of water. 
As for refraction, that will be most obvious with things like thick, curved glass, and glassware filled with liquids. Bottles, wine glasses, thick reading glasses, etc. The edges of the glass, where the index of refraction creates that defined shape that’s almost solid, should be at the depth of the glass in 3D space. The interior core of the glass, where you see through bent light of the objects behind it should be closer to the depth of the object behind it (accounting for any magnification). Then there should be an artful blend from that edge depth to that core depth in whatever fake depth channel is being used. Anything like reflections should be painted out and added on top; like the shop window.
What you do not want is for the glass object to feel like everything inside of it is at the depth of the glass surface. It will look painted on, not like you’re seeing through the glass.
This also goes for lens flares, which are reflections and refractions from light hitting the lens element at certain angles and then the filmback. The lens flare needs to be painted out and reconstructed exactly, then the source of the flare needs to be offset to match the location of the light source, and then the little bits of the lens flare need to be offset based on where the light source moved to in relation to the center of frame, which would be the center of the lens. Oftentimes a lens flare plugin in a compositing package will be used to help replicate the original flare, or at least used as a guide for placement.
This leaves other camera based effects like grain, and heavy vignetting. The entire plate needs to be degrained as step 0 in this process, and then regrained, taking into account any extra reconstruction work, and also offseting the grain timing for the offset eye. You should never have a stereo offset in your grain (meaning the same pattern reproduced and moved in X) because that puts the grain in depth. If you leave the original grain on your left and right eyes, and do your offsets, then your grain will be painted on to the depth of all the surfaces you reconstructed. That’s extremely bad, and extremely obvious when it happens.
Grain should be offset in time (effectively randomized noise seed) so there is never a matching pattern your brain will try to place in depth. The result is a fuzz that exists around screen depth. Your eye doesn’t identify it really having any depth unless the grain is heavy and can almost take on the quality of an atmosphere, that flattens things, in which case the decision maybe made to reduce grain for both left and right eyes.
Usually people can get away without treating vignetting, unless it’s heavy —the real artsy stuff. Then the conversion house needs to remove the original vignetting and add it at screen depth (no offset) with everything else in stereo being placed behind it. You don’t want something popping out through vignetting —that doesn’t make any sense.
The really good news is that because this uses the same greenscreen and bluescreen, the edges don’t get screwy, and any combination with CG can be made exactly, because everything can be placed together in the same shared space. When done well it help the director shoot how they’re comfortable shooting, and get the results they want for both 2D and 3D.
Hybrid
Really it doesn’t do any good to make any sweeping statements about quality based on method, and especially after taking into account that films will blend various parts in ways that are often invisible to you.
Idealogical purity really doesn’t exist in either the realm of home video or stereo video, so try not to get too wound up about it. Always try to watch the best version of something you can, and suits your current situation, but don’t get yourself upset about something in the abstract.
If you really want to understand the quality of the 3D work inspect those common problem spots I mentioned. Pause your movie and open and close your left and right eyes. Look at the refraction, the reflections, the flyaway hairs.
Separately, judge whether it was worth seeing it in 3D at all. Did that add to your experience for this particular film? Was anything about it essential, or memorable? People talk about the 3D of the Avatar movies because James Cameron made it a part of the experience, not just because the native stereo checkbox was ticked.
No one is under any obligation to like 3D movies whatsoever, but it’s important that we don’t justify or define that dislike based on a simple binary that isn’t true.</description>
      <author> ()</author>
      <guid>https://joe-steel.com/2024-02-19-No-Simple-Answers-In-Stereo.html</guid>
      <pubDate>Thu, 22 Feb 2024 23:04:00 GMT</pubDate>
    </item>
    <item>
      <title>Modding Plugins back into Xcode</title>
      <link>https://bryce.co/xcode-plugin-loader/</link>
      <description>Adding plugin support to Xcode 14+, the hard way</description>
      <author> (Bryce Pauken)</author>
      <guid>https://bryce.co/xcode-plugin-loader/</guid>
      <pubDate>Thu, 22 Feb 2024 23:04:00 GMT</pubDate>
    </item>
    <item>
      <title>Women in mathematics – a case study</title>
      <link>https://liorpachter.wordpress.com/2024/02/22/women-in-mathematics-a-case-study/</link>
      <description>The following describes harassment experienced by a woman who is a professor of mathematics, whose words I’m posting here (anonymously and with names changed) with her permission. ===========…</description>
      <author> (Lior Pachter)</author>
      <guid>https://liorpachter.wordpress.com/2024/02/22/women-in-mathematics-a-case-study/</guid>
      <pubDate>Thu, 22 Feb 2024 23:04:00 GMT</pubDate>
    </item>
    <item>
      <title>Ruby could use a Heap</title>
      <link>https://homo-sapiens-reviewed.bearblog.dev/ruby-needs-a-heap/</link>
      <description>If you’re studying for coding interviews, I recommend Python, which is a shame because I am a Ruby programmer. The only major drawback to using Ruby is that ...</description>
      <author> ()</author>
      <guid>https://homo-sapiens-reviewed.bearblog.dev/ruby-needs-a-heap/</guid>
      <pubDate>Thu, 22 Feb 2024 23:04:00 GMT</pubDate>
    </item>
    <item>
      <title>In a GenAI world. Only Identity Matters.</title>
      <link>https://calebsima.com/2024/02/08/in-a-genai-world-only-identity-matters/</link>
      <description>I recently had to do some thinking about Identity and with the recent surge of deepfakes, the one thing that is clear is that Identity has become the most critical problem. Obviously identity has a…</description>
      <author> ()</author>
      <guid>https://calebsima.com/2024/02/08/in-a-genai-world-only-identity-matters/</guid>
      <pubDate>Thu, 22 Feb 2024 23:04:00 GMT</pubDate>
    </item>
    <item>
      <title>Making LLMs worth every penny | Tom Hipwell</title>
      <link>https://tomhipwell.co/reading/making_llms_worth_every_penny/</link>
      <description>Learning in the open | Tom Hipwell</description>
      <author> ()</author>
      <guid>https://tomhipwell.co/reading/making_llms_worth_every_penny/</guid>
      <pubDate>Thu, 22 Feb 2024 23:03:59 GMT</pubDate>
    </item>
    <item>
      <title>Productivity Tools are Taxed - Adam Grant</title>
      <link>https://www.adamgrant.info/adhd-tools-are-taxed</link>
      <description>Productivity Tools are Taxed - Adam Grant</description>
      <author> ()</author>
      <guid>https://www.adamgrant.info/adhd-tools-are-taxed</guid>
      <pubDate>Thu, 22 Feb 2024 23:03:59 GMT</pubDate>
    </item>
    <item>
      <title>Matryoshka Representation Learning (MRL) from the Ground Up</title>
      <link>https://aniketrege.github.io/blog/2024/mrl/</link>
      <description>What do these scary sounding words mean?</description>
      <author> (Aniket  Rege)</author>
      <guid>https://aniketrege.github.io/blog/2024/mrl/</guid>
      <pubDate>Thu, 22 Feb 2024 23:03:58 GMT</pubDate>
    </item>
    <item>
      <title>i need some hackers</title>
      <link>https://breadchris.com/blog/i-need-some-hackers/</link>
      <description>TL;DR - I need money for a non-profit international cybersecurity competition (open-source, too). Donations here, email chris@breadchris.com for sponsorship opportunities.
I am running an annual cyber security competition (mcpshsf, soon to be xctf) with my high school computer science teacher, celebrating its 8th? year, on March 14th. When run at NYU, it attracted incredible talent from around the US. Some of the alumni went on to form or join notable security companies/teams ( TrailofBits, Zellic, Google, Uber, etc.</description>
      <author> (breadchris)</author>
      <guid>https://breadchris.com/blog/i-need-some-hackers/</guid>
      <pubDate>Thu, 22 Feb 2024 23:03:58 GMT</pubDate>
    </item>
    <item>
      <title>What I Learned Developing with LLMs</title>
      <link>https://www.opslevel.com/resources/what-i-learned-developing-with-llms</link>
      <description>Peek behind the curtain at how an engineer at OpsLevel developed our own AI assistant.</description>
      <author> ()</author>
      <guid>https://www.opslevel.com/resources/what-i-learned-developing-with-llms</guid>
      <pubDate>Thu, 22 Feb 2024 23:03:58 GMT</pubDate>
    </item>
    <item>
      <title>Large Language Models Are Drunk at the Wheel</title>
      <link>https://matt.si/2024-02/llms-overpromised/</link>
      <description>As an Artificial Intelligence proponent, I want to see the field succeed and go on to do great things. That is precisely why the current exaggerated publicity…</description>
      <author> ()</author>
      <guid>https://matt.si/2024-02/llms-overpromised/</guid>
      <pubDate>Thu, 22 Feb 2024 23:03:57 GMT</pubDate>
    </item>
    <item>
      <title>Subprime Intelligence</title>
      <link>https://www.wheresyoured.at/sam-altman-fried/</link>
      <description>Please scroll to the bottom for news on my next big project, Better Offline, coming this Wednesday!

Last week, Sam Altman debuted OpenAI&amp;#39;s &amp;quot;Sora,&amp;quot; a text-to-video AI model that turns strings of text into full-blown videos, much like how OpenAI&amp;#39;s DALL-E turns text into images. These videos — which are usually</description>
      <author> (Edward Zitron)</author>
      <guid>https://www.wheresyoured.at/sam-altman-fried/</guid>
      <pubDate>Thu, 22 Feb 2024 23:03:56 GMT</pubDate>
    </item>
    <item>
      <title>That time I almost added Tetris to htop - hisham.hm </title>
      <link>https://hisham.hm/2024/02/12/that-time-i-almost-added-tetris-to-htop/</link>
      <description>Personal webpage of Hisham Muhammad, developer of htop, LuaRocks and GoboLinux.</description>
      <author> ()</author>
      <guid>https://hisham.hm/2024/02/12/that-time-i-almost-added-tetris-to-htop/</guid>
      <pubDate>Thu, 22 Feb 2024 23:03:56 GMT</pubDate>
    </item>
    <item>
      <title>How to Optimally Trap Points in High-Dimensional Spaces Inside Ellipsoids</title>
      <link>https://www.adrianriv.com/blog/2024/02/19/minimum_volume_ellipsoid/</link>
      <description>Adrian Rivera Cardoso is a researcher and software engineer.</description>
      <author> (Adrian Rivera Cardoso)</author>
      <guid>https://www.adrianriv.com/blog/2024/02/19/minimum_volume_ellipsoid/</guid>
      <pubDate>Thu, 22 Feb 2024 23:03:56 GMT</pubDate>
    </item>
    <item>
      <title>Interesting Uses of Ansible’s ternary&amp;#160;filter</title>
      <link>https://www.zufallsheld.de/2024/02/21/interesting-use-of-ansible-ternary-filter/</link>
      <description>Some time ago I discovered an interesting use of the ternary-filter in Ansible. A ternary-filter in Ansible is a filter that takes three arguments: a condition, a value if the condition is true and an alternative value if the condition is&amp;#160;false. Here’s a simple example straight from Ansible …</description>
      <author> ()</author>
      <guid>https://www.zufallsheld.de/2024/02/21/interesting-use-of-ansible-ternary-filter/</guid>
      <pubDate>Thu, 22 Feb 2024 23:03:56 GMT</pubDate>
    </item>
    <item>
      <title>Measuring Patterns To Boost Productivity &amp;#183; @jimmyislive</title>
      <link>https://jimmyislive.dev/posts/measuring-patterns-to-boost-productivity/</link>
      <description>Written by
        Jimmy John
        
        on&amp;#160;February 18, 2024</description>
      <author> ()</author>
      <guid>https://jimmyislive.dev/posts/measuring-patterns-to-boost-productivity/</guid>
      <pubDate>Thu, 22 Feb 2024 23:03:56 GMT</pubDate>
    </item>
    <item>
      <title>Open sourcing your games as solo game developer - a game changer</title>
      <link>https://simondalvai.org/blog/open-source-games/</link>
      <description>Why making your games Open Source can help you as a solo independent game developer</description>
      <author> ()</author>
      <guid>https://simondalvai.org/blog/open-source-games/</guid>
      <pubDate>Thu, 22 Feb 2024 23:03:55 GMT</pubDate>
    </item>
    <item>
      <title>Typing A Little Faster</title>
      <link>https://sophiestavely.com/2024/02/22/typing-a-little-faster/</link>
      <description>I taught myself to type a bit faster over the course of a couple of months. I’d estimate that I spent 5 hours of effort to gain 10 to 20 words per minute (WPM) in speed. Before I started, I mostly …</description>
      <author> (Sophie Stavely)</author>
      <guid>https://sophiestavely.com/2024/02/22/typing-a-little-faster/</guid>
      <pubDate>Thu, 22 Feb 2024 23:03:55 GMT</pubDate>
    </item>
    <item>
      <title>CSS-only DVD Screensaver animation</title>
      <link>https://www.javiermorales.dev/blog/dvd</link>
      <description>An in-depth look at how I created a DVD screensaver animation without any Javascript, presented with dynamic examples and code snippets.</description>
      <author> (Javier Morales)</author>
      <guid>https://www.javiermorales.dev/blog/dvd</guid>
      <pubDate>Thu, 22 Feb 2024 23:03:55 GMT</pubDate>
    </item>
    <item>
      <title>Water cooling is overkill for Pi 5</title>
      <link>https://www.jeffgeerling.com/blog/2024/water-cooling-overkill-pi-5</link>
      <description>tl;dr: 52Pi and Seeed Studio&amp;#39;s water cooling solution for the Raspberry Pi 5 can be fun, and works better than any other solution—but at a steep price, and with a number of annoying quirks.</description>
      <author> ()</author>
      <guid>https://www.jeffgeerling.com/blog/2024/water-cooling-overkill-pi-5</guid>
      <pubDate>Thu, 22 Feb 2024 23:03:55 GMT</pubDate>
    </item>
    <item>
      <title>View transitions: Handling aspect ratio changes</title>
      <link>https://jakearchibald.com/2024/view-transitions-handling-aspect-ratio-changes/</link>
      <description>Tips and tricks to get the transition you want</description>
      <author> ()</author>
      <guid>https://jakearchibald.com/2024/view-transitions-handling-aspect-ratio-changes/</guid>
      <pubDate>Thu, 22 Feb 2024 23:03:55 GMT</pubDate>
    </item>
    <item>
      <title>Spring Rites</title>
      <link>https://dantanner.com/post/spring-rites/</link>
      <description>A caution against annotation-based web frameworks</description>
      <author> ()</author>
      <guid>https://dantanner.com/post/spring-rites/</guid>
      <pubDate>Thu, 22 Feb 2024 23:03:55 GMT</pubDate>
    </item>
    <item>
      <title>Burke Learns Blazor - Bug fixing, download shrinking, prepping to ship!</title>
      <link>https://youtube.com/watch?v=q85cPfEFzkg</link>
      <description>We make some last fixes and tweaks to get this thing out the door! Let&amp;#39;s fix some API bugs and shrink the download size, then talk about data migration.Featu...</description>
      <author> (dotnet
  
  
  
    dotnet
  






    •)</author>
      <guid>https://youtube.com/watch?v=q85cPfEFzkg</guid>
      <pubDate>Thu, 22 Feb 2024 22:03:53 GMT</pubDate>
    </item>
    <item>
      <title>Using Figma Plugin Generated C# Markup in an Uno Platform project | Uno Tech Bites</title>
      <link>https://youtube.com/watch?v=rV4TClFB6iM</link>
      <description>&amp;#128196;Uno Platform Docs: https://platform.uno/docs/articles/external/figma-docs/get-started/create-an-app.html?tabs=dotnet-cliSubscribe for more: https://www.you...</description>
      <author> (Uno Platform
  
  
  
    Uno Platform
  






    •)</author>
      <guid>https://youtube.com/watch?v=rV4TClFB6iM</guid>
      <pubDate>Thu, 22 Feb 2024 22:03:52 GMT</pubDate>
    </item>
    <item>
      <title>Our Company Is Doing So Well That You’re All Fired</title>
      <link>https://www.mcsweeneys.net/articles/our-company-is-doing-so-well-that-youre-all-fired</link>
      <description>“Paramount Global lays off about 800 employees, a day after announcing record Super Bowl ratings.” — CNBC
- - -Thank you for jumping on this last-m...</description>
      <author> (by Jeff Wysaski)</author>
      <guid>https://www.mcsweeneys.net/articles/our-company-is-doing-so-well-that-youre-all-fired</guid>
      <pubDate>Thu, 22 Feb 2024 21:03:48 GMT</pubDate>
    </item>
    <item>
      <title>Bluesky: An Open Social Web - Bluesky</title>
      <link>https://bsky.social/about/blog/02-22-2024-open-social-web</link>
      <description>We’re excited to announce that the Bluesky network is federating and opening up in a way that allows you to host your own data.</description>
      <author> ()</author>
      <guid>https://bsky.social/about/blog/02-22-2024-open-social-web</guid>
      <pubDate>Thu, 22 Feb 2024 21:03:48 GMT</pubDate>
    </item>
  </channel>
</rss>