[
  {
    "Title": "Error",
    "Url": "https://nt.vern.cc/cards/18ce54bi3fv/1rc3n",
    "Timestamp": "2023-07-30T01:01:56",
    "Domain": "nt.vern.cc",
    "Description": ""
  },
  {
    "Title": "Codifying a ChatGPT workflow into a malleable GUI",
    "Url": "https://www.geoffreylitt.com/2023/07/25/building-personal-tools-on-the-fly-with-llms.html",
    "Timestamp": "2023-07-30T00:03:03",
    "Domain": "www.geoffreylitt.com",
    "Description": "Wouldn't it be neat if you could use LLMs to create little personal utility apps as the need arises? Here's a story where I did just that..."
  },
  {
    "Title": "The transformer model, explained clearly - DeriveIt",
    "Url": "https://www.deriveit.org/notes/119",
    "Timestamp": "2023-07-30T00:03:03",
    "Domain": "www.deriveit.org",
    "Description": "In this note, we'll come up with GPT and the TRANSFORMER architecture, from scratch.\n\n\n"
  },
  {
    "Title": "How to read inference rules",
    "Url": "https://cohost.org/prophet/post/2248211-how-to-read-inferenc",
    "Timestamp": "2023-07-30T00:03:02",
    "Domain": "cohost.org",
    "Description": "The notation used is probably one of the largest barriers of entry to type inference papers, but it is rarely explained explicitly, so... I'm going to do just that!\n\nFor starters, inference rules are really nothing more than implications. The inference rule\n\nAB(Name)\n\nreally just means \"if A then B\". These are usually given a name (in this case, creatively, Name) to make it easier to refer to them in the rest of the paper.\n\nNow, even though these are technically just implications, it's usually not a great idea to read them from top to bottom. Inference rules denote relations, but it usually usually makes more sense to read them as (possibly non-deterministic) functions.\nFor example, a judgement for typing function application might look like this. (where Γ⊢e:τ means \"In a context Γ, the type of an expression e is infered to τ\")\n\nΓ⊢e1  :  τ1→τ2      Γ⊢e2  :  τ1Γ⊢e1(e2)  :  τ2(App)\n\nNaively, one might read this as\n\n> If e1 has type τ1→τ2 in a context Γ and e2 has type τ1 in Γ, then e1(e2) has type τ2 in Γ\n\nbut a much better way to read it, that is much closer to an actual implementation, would be\n\n> In order to infer a type for e1(e2) in a context Γ, one first needs to infer a type for e1 with shape τ1→τ2 in Γ. Now e2 also needs to infer to type τ1 in Γ, so that the result (i.e. the type of e1(e2)) is τ2.\n\nRead this way, the inference rule maps very closely onto an actual implementation! Seriously, compare the corresponding pseudocode to that second description\n\ninfer Γ (App e1 e2) =\n    let (τ1 -> τ2) = infer Γ e1\n    let τ3 = infer Γ e2\n    unify τ1 τ3\n    return τ2\n\n\nThe only major difference between this code (which skips error handling, just like inference rules) and the inference rule is that the fact that the type of e2 needs to be equal to τ1 is explicit in the code (unify τ1 τ3).\n\nReading off the algorithm like this is possible if the inference rules are syntax directed, i.e. if there is only ever a single rule that might match on a given expression. This is not always the case, so sometimes it's better to imagine non-deterministically choosing the correct rule to apply, rather than just pattern matching.\n\nAnd that's... pretty much all you need to know to read inference rules!\n\nThere are a few common conventions in type systems that might be a bit surprising, so let's go over those as well\n\n\nENVIRONMENTS AND EXTENSION\n\nType inference needs an environment to keep track of the types of variables. This is usually called Γ and extended as Γ,x  :  τ.\n\nFor example, this inference rule for (annotated) let bindings checks e2 under the environment Γ, extended with the binding x  :  τ1.\n\nΓ⊢e1  :  τ1      Γ,x  :  τ1⊢e2  :  τ2Γ⊢let x  :  τ1=e1 in e2  :  τ2Let\n\nExtracting information from the environment is achieved through \"pattern matching\" on the environment, for example in this inference rule for variables.\n\nΓ,x  :  τ⊢x  :  τVar\n\n\nUNIFICATION VARIABLES\n\nUnification variables [https://cohost.org/prophet/post/2220730-if-there-is-one-piec] don't exist in theoretical type systems, but they still map very directly onto a similar concept. Instead of generating a fresh unification variable, inference rules just \"guess\" a new type (they're relations, remember?).\n\nFor example, this typing rule for (unannotated) lambdas just pulls the type τ out of thin air.\n\nΓ,x  :  τ⊢e  :  τ1Γ⊢λx→e  :  τ→τ1Lambda\n\n\nLISTS\n\nSomething you will see pretty often in papers by Simon Peyton Jones are lists that are represented by an overline. E.g. the syntax for uncurried function application might be e1(e‾), where e‾ consists of 0 or more expressions.\n\n\nSKOLEMS\n\nSimilarly, skolems don't exist as a separate concept. Instead, \"unbound\" type variables are treated as skolems, although these obviously cannot conflict with any other type variables in scope!\nIn an implementation, this would be achieved by generating a fresh skolem, but in inference rules, this is expressed by the side condition that the type variable should not occur \"free in the environment\", written a∉ftv(Γ), where ftv denotes the set of free type variables (= skolems) in Γ.\n\nFor example, a rule for let bindings with polymorphic types (that need to be skolemized) might look like this\n\nΓ⊢e1  :  τ1      a‾∉ftv(Γ)      Γ,x  :  ∀a‾.τ1⊢e2  :  τ2Γ⊢let x  :  ∀a‾.τ1=e1 in e2  :  τ2\n\n\nWHERE TO GO FROM HERE\n\nGreat, with a little practice, you should be able to read inference rules now! I would recommend you read Practical type inference for higher rank types [https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/putting.pdf], which is a great, relatively beginner friendly paper about type inference that even contains a full implementation at the end! (And despite the name, is not just about higher rank types)."
  },
  {
    "Title": "Sampling at scale with OpenTelemetry",
    "Url": "https://www.gouthamve.dev/sampling-at-scale-with-opentelemetry/",
    "Timestamp": "2023-07-30T00:03:02",
    "Domain": "www.gouthamve.dev",
    "Description": "Thoughts on different sampling strategies at scale when using OpenTelemetry. With experience running tracing at scale at Grafana Labs."
  },
  {
    "Title": "Hamel’s Blog - Optimizing LLM latency",
    "Url": "https://hamel.dev/notes/llm/03_inference.html",
    "Timestamp": "2023-07-30T00:03:02",
    "Domain": "hamel.dev",
    "Description": "An exploration of inference tools for open source LLMs focused on latency."
  },
  {
    "Title": "Building a BitTorrent client in Elixir",
    "Url": "https://kochika.me/posts/torrent/",
    "Timestamp": "2023-07-30T00:03:01",
    "Domain": "kochika.me",
    "Description": "\nIn this post, we delve headfirst into the BitTorrent protocol, understanding the process of downloading a torrent by building a minimal torrent client from scratch."
  },
  {
    "Title": "The Illustrated Transformer",
    "Url": "https://jalammar.github.io/illustrated-transformer/",
    "Timestamp": "2023-07-30T00:03:01",
    "Domain": "jalammar.github.io",
    "Description": "Discussions:\nHacker News (65 points, 4 comments), Reddit r/MachineLearning (29 points, 3 comments)\n\n\nTranslations: Arabic, Chinese (Simplified) 1, Chinese (Simplified) 2, French 1, French 2, Italian, Japanese, Korean, Persian, Russian, Spanish 1, Spanish 2, Vietnamese\n\nWatch: MIT’s Deep Learning State of the Art lecture referencing this post\n\nIn the previous post, we looked at Attention – a ubiquitous method in modern deep learning models. Attention is a concept that helped improve the performance of neural machine translation applications. In this post, we will look at The Transformer – a model that uses attention to boost the speed with which these models can be trained. The Transformer outperforms the Google Neural Machine Translation model in specific tasks. The biggest benefit, however, comes from how The Transformer lends itself to parallelization. It is in fact Google Cloud’s recommendation to use The Transformer as a reference model to use their Cloud TPU offering. So let’s try to break the model apart and look at how it functions.\n\nThe Transformer was proposed in the paper Attention is All You Need. A TensorFlow implementation of it is available as a part of the Tensor2Tensor package. Harvard’s NLP group created a guide annotating the paper with PyTorch implementation. In this post, we will attempt to oversimplify things a bit and introduce the concepts one by one to hopefully make it easier to understand to people without in-depth knowledge of the subject matter.\n\n2020 Update: I’ve created a “Narrated Transformer” video which is a gentler approach to the topic:\n\n\n\n\nA High-Level Look\nLet’s begin by looking at the model as a single black box. In a machine translation application, it would take a sentence in one language, and output its translation in another.\n\n\n  \n\n\n"
  },
  {
    "Title": "GPT-4 Code Interpreter and Pillow",
    "Url": "https://metastable.org/pillow.html",
    "Timestamp": "2023-07-30T00:03:01",
    "Domain": "metastable.org",
    "Description": "Getting ChatGPT to draw whether it wants to or not!"
  },
  {
    "Title": "Digital Bum: Finding a Home/lessness on the Internet",
    "Url": "https://aartaka.me/blog/digital-bum",
    "Timestamp": "2023-07-30T00:03:01",
    "Domain": "aartaka.me",
    "Description": "Internet grew out of a non-commercial academic network with free resources for everyone. Can one get back to this dream of free Internet and build a lifestyle out of it? Well yeah I guess so, kinda worked for me."
  },
  {
    "Title": "Defcon: Preventing Overload with Graceful Feature Degradation",
    "Url": "https://www.micahlerner.com/2023/07/23/defcon-preventing-overload-with-graceful-feature-degradation.html",
    "Timestamp": "2023-07-30T00:03:01",
    "Domain": "www.micahlerner.com",
    "Description": "Defcon: Preventing Overload with Graceful Feature Degradation"
  },
  {
    "Title": "LN 035: The Messy Desktop",
    "Url": "https://alexanderobenauer.com/labnotes/035/",
    "Timestamp": "2023-07-30T00:03:00",
    "Domain": "alexanderobenauer.com",
    "Description": "When I got to college, I learned a lot about computing fairly quickly, before I even stepped into my first computer science course."
  },
  {
    "Title": "A Lock-Free Vector",
    "Url": "https://ibraheem.ca/posts/a-lock-free-vector/",
    "Timestamp": "2023-07-30T00:03:00",
    "Domain": "ibraheem.ca",
    "Description": "Designing a fast, lock-free vector."
  }
]