[
  {
    "Title": "Leaked Palantir S-1 shows $579M loss in 2019 – TechCrunch",
    "Url": "https://techcrunch.com/2020/08/21/leaked-s-1-screenshots-show-palantir-losing-579m-in-2019/",
    "Timestamp": "2020-08-21T23:04:02",
    "Domain": "techcrunch.com",
    "Description": "Palantir filed an S-1 confidentially to the SEC in early July, but we have so far been waiting for the final document to be published for weeks now with nary a murmur. Now, thanks to some leaked screenshots to TechCrunch from a Palantir shareholder, we might have some top-line numbers. Full-year re…"
  },
  {
    "Title": "Exclude code that follows [DoesNotReturn] from code coverage (per #898) by kevin-montrose · Pull Request #904 · coverlet-coverage/coverlet",
    "Url": "https://github.com/coverlet-coverage/coverlet/pull/904",
    "Timestamp": "2020-08-21T23:03:55",
    "Domain": "github.com",
    "Description": "Per #898\nImplementing reachability analysis, which will allow instructions that follow calls to methods that never return to be excluded from coverage metrics.\nDetermines what methods will return w..."
  },
  {
    "Title": "Kids Trap (Collection 1-15)",
    "Url": "https://open.spotify.com/playlist/4BDJsVcxauiyDxpO5pTAxt?si=J2G2L-O-SI6M0Q3seq1yfg",
    "Timestamp": "2020-08-21T23:03:51",
    "Domain": "open.spotify.com",
    "Description": "oman546 · Playlist · 149 songs · 3.1K likes"
  },
  {
    "Title": "This is your one shot",
    "Url": "https://neoslash.net/this-is-your-one-shot?guid=none",
    "Timestamp": "2020-08-21T22:04:21",
    "Domain": "neoslash.net",
    "Description": "I've worked in IT consulting for over 10 years in various roles. My current job includes meeting with prospects and discussing our IT services and how they fit into their business. This has been the first position I've held where I deal with a lot mo..."
  },
  {
    "Title": "fast.ai releases new deep learning course, four libraries, and 600-page book",
    "Url": "https://www.fast.ai/2020/08/21/fastai2-launch/",
    "Timestamp": "2020-08-21T22:04:18",
    "Domain": "www.fast.ai",
    "Description": "We're releasing Practical Deep Learning for Coders (2020), fastai v2, fastcore, fastscript, and fastgpu."
  },
  {
    "Title": "Generating Sales from YouTube micro-influencers",
    "Url": "https://austinrepp.com/youtubemicro/",
    "Timestamp": "2020-08-21T22:04:16",
    "Domain": "austinrepp.com",
    "Description": "Explore the numbers of my YouTube marketing campaign for Discord Bot Studio."
  },
  {
    "Title": "You don’t always have to be productive - WEEB TRASH LIFE",
    "Url": "https://weebtrash.ga/2020/08/21/you-dont-always-have-to-be-productive/",
    "Timestamp": "2020-08-21T22:04:12",
    "Domain": "weebtrash.ga",
    "Description": "Did you know that being constantly stuck in the productivity trap isn’t really healthy? Now you do. Wanna’ get out of it? Read these tips."
  },
  {
    "Title": "Tom Spark Talks with Troy Hunt - Owner of HaveIBeenPwned!",
    "Url": "https://www.youtube.com/watch?v=KZ195OCgH_Y",
    "Timestamp": "2020-08-21T22:04:06",
    "Domain": "www.youtube.com",
    "Description": "I talked with Troy Hunt about his website and about general privacy online. \n\n►Privacy Review/tier list website with all ratings: https://vpntierlist.com/\n\nFavorite VPN provider : TorGuard VPN http://bit.ly/tomsparkTorGuard \nFavorite browser: http://brave.com/tom352\n\nWant to see what my favorite products are? https://www.vpntierlist.com/tom-spark-favorite-products/\n\nDiscord community: https://discord.gg/9YkDAJU\nJoin the subreddit! https://www.reddit.com/r/virtualprivatenetwork/\n\nDisclaimer: This video and all my videos are solely my opinion, to provide educational content and to entertain my audience, and thus are protected by the first amendment in the USA, as well as the Consumer Review Fairness Act which protects my right to write reviews. \n\nI am affiliated, but not sponsored by any VPN. This means I do make money when you click on the links provided, but keep my own opinion to be legit and truthful without bias. I do not host sponsored content on this channel, which means I am not paid to promote VPNs in a positive manner. All of my opinions on this channel are strictly my own!"
  },
  {
    "Title": "Toward a Zoom agreement",
    "Url": "https://seths.blog/2020/08/toward-a-zoom-agreement/",
    "Timestamp": "2020-08-21T22:04:05",
    "Domain": "seths.blog",
    "Description": "If you promise not to check your email while we’re talking, we promise to not waste your time. If you agree to look me in the eye and try to absorb the gist of what I’m saying, I agree …"
  },
  {
    "Title": "Hand washing stations",
    "Url": "https://casita-colibri.blog/2020/08/20/hand-washing-stations/",
    "Timestamp": "2020-08-21T22:04:02",
    "Domain": "casita-colibri.blog",
    "Description": "If you are out, about, and going to the mercados in Oaxaca in the last couple of months, you may have seen a clever contraption like the one below set up outside the Independencia entrance to Merca…"
  },
  {
    "Title": "Why I Started Experimental Cooking — A Pursuit of Joy",
    "Url": "https://knowledgeartist.org/articles/51b7b831-515b-4ad0-ad4d-616cbbe180c1/why-i-started-experimental-cooking-a-pursuit-of-joy",
    "Timestamp": "2020-08-21T22:03:59",
    "Domain": "knowledgeartist.org",
    "Description": "Cooking to me is not about creating the best tasting dish in the world. Rather, it is an outlet of expression, a means to evoke sensations."
  },
  {
    "Title": "Why Do Technical Recruiters Even Exist? - Scott Turman",
    "Url": "https://scottturman.com/why-do-technical-recruiters-even-exist/",
    "Timestamp": "2020-08-21T22:03:55",
    "Domain": "scottturman.com",
    "Description": "We’re all familiar with the suited snakes that spend every 8-to-10-hour workday calling to convince us to leave our current, steady jobs for nothing more than a few bucks more and criminally bad insurance. With that said then, why do recruiters even exist?"
  },
  {
    "Title": "Port 5432 is open: introducing the Splitgraph Data Delivery Network",
    "Url": "https://www.splitgraph.com/blog/data-delivery-network-launch",
    "Timestamp": "2020-08-21T21:03:46",
    "Domain": "www.splitgraph.com",
    "Description": "We launch the Splitgraph Data Delivery Network: a single endpoint that lets any PostgreSQL application, client or BI tool to connect and query over 40,000 public datasets hosted or proxied by Splitgraph."
  },
  {
    "Title": "Why efficiency is dangerous and slowing down makes life better | Psyche Ideas",
    "Url": "https://psyche.co/ideas/why-efficiency-is-dangerous-and-slowing-down-makes-life-better",
    "Timestamp": "2020-08-21T21:03:43",
    "Domain": "psyche.co",
    "Description": "The urge to do everything faster and better is risky. Far wiser to do what’s good enough for the range of possible futures"
  },
  {
    "Title": "Ignoring mass reformatting commits with git blame – Rob Allen's DevNotes",
    "Url": "https://akrabat.com/ignoring-revisions-with-git-blame/",
    "Timestamp": "2020-08-21T20:03:39",
    "Domain": "akrabat.com",
    "Description": "I’ve recently merged a PR by Stephen to rst2df that reformats the entire codebase to align with PEP 8. As rst2pdf is over a decade old, this has resulted in a lot of changes to the files which now have Stephen’s name attached. This affects git blame."
  },
  {
    "Title": "An Update on MDN Web Docs – Mozilla Hacks - the Web developer blog",
    "Url": "https://hacks.mozilla.org/2020/08/an-update-on-mdn-web-docs/",
    "Timestamp": "2020-08-21T19:03:42",
    "Domain": "hacks.mozilla.org",
    "Description": "Last week, Mozilla announced some general changes in our investments and we would like to outline how they will impact our MDN platform efforts moving forward. It hurts to make ..."
  },
  {
    "Title": "Developer Advocate, Careers At EDB ",
    "Url": "https://enterprisedb.hrmdirect.com/employment/job-opening.php?req=1370410&cust_sort1=-1&nohd=#job",
    "Timestamp": "2020-08-21T19:03:40",
    "Domain": "enterprisedb.hrmdirect.com",
    "Description": "Developer Advocate Remote preferred  The world loves Postgres If you work with developers or data scientists or anyone wrangling data youll probably see a sticker with the tusks and trunk of the Postgres elephant on the lid of a nearby laptop EDB has a lot to do with that Weve been major contributors to Postgres since the beginning and we are proud to call thousands of boundary pushing customers our partners Proud though we are we are not resting on our laurels Theres plenty of work to do The good news is that everything we do will impact Postgres which is to say that it will impact the world No pressure As a Developer Advocate you will join a team of user experience and software development professionals on a mission to make working with Postgres easier for developers data scientists data wranglers operators and the ever expanding ecosystem of technologists who rely on Postgres to innovate at speed What youll do Make Postgres easier identify points of friction in Postgres and EDB products develop approaches to address them Contribute Postgres expertise to a number of communities including Nodejs Python and Go Identify and engage with new communities which stand to benefit from Postgres Publish code and content that will help others get started with and get more out of Postgres Share feedback to development teams across our portfolio Collaborate with product engineering and marketing teams Talk to Postgres users and EDB customers and listen for not only their needs but also their ideas for the future of Postgres What were looking for A strong communicator A robust online portfolio featuring code and technical content including READMEs Ideally experience working on a technical advocacy team Facility with modern communication collaboration and development tools; eg Slack Github Stack Overflow VSCode Jira Experience with agile methodologies Willingness to analyze understand and extend existing cultural and technical systems Developer Advocates will report to the Director of Developer Advocacy in the CXO organization"
  },
  {
    "Title": "Inside the Microsoft STL: The std::exception_ptr | The Old New Thing",
    "Url": "https://devblogs.microsoft.com/oldnewthing/20200820-00/?p=104097",
    "Timestamp": "2020-08-21T18:03:59",
    "Domain": "devblogs.microsoft.com",
    "Description": "When debugging, you may find yourself staring at a std::exception_ptr and want to know what exception is inside it. What you see in the MSVC header file is that a std::exception_ptr is a class that consists of two pointers enigmatically named _Data1 and _Data2."
  },
  {
    "Title": "My Stream Timer",
    "Url": "http://www.mystreamtimer.com/",
    "Timestamp": "2020-08-21T18:03:56",
    "Domain": "www.mystreamtimer.com",
    "Description": "A cool app to count up or down that writes text to file for streamers"
  },
  {
    "Title": ".NET Productivity Tips and Tricks",
    "Url": "https://www.youtube.com/watch?v=NXbpIMOJzNE",
    "Timestamp": "2020-08-21T18:03:54",
    "Domain": "www.youtube.com",
    "Description": "The .NET Productivity team (a.k.a. Roslyn) is constantly thinking of new ways to make .NET developers more productive. Roslyn PM Mika Dumont shows a number of the latest features that make your coding life better, including her favorite (IntelliSense completion in DateTime and TimeSpan string literals). \n\nSee the .NET blog post at https://devblogs.microsoft.com/dotnet/learn-about-the-latest-net-productivity-features/ for more details."
  },
  {
    "Title": "Recognizing different types of exception objects that Windows platform libraries can throw | The Old New Thing",
    "Url": "https://devblogs.microsoft.com/oldnewthing/20200821-00/?p=104112",
    "Timestamp": "2020-08-21T18:03:54",
    "Domain": "devblogs.microsoft.com",
    "Description": "Last time, we saw how to dig the exception object out of a std::exception_ptr. But what kind of exception objects might there be? For C++/CX code, you are probably going to get a Platform::Exception^. 0:007> ?? p class std::exception_ptr   +0x000 _Data1      : 0x08a5885c Void   +0x004 _Data2      : 0x08a58850 Void  We learned that the _Data1 points to an EXCEPTION_RECORD¹"
  },
  {
    "Title": "How NAT traversal works",
    "Url": "https://tailscale.com/blog/how-nat-traversal-works/",
    "Timestamp": "2020-08-21T18:03:52",
    "Domain": "tailscale.com",
    "Description": "In this post, we'll talk about how to establish a peer-to-peer connection between two machines, in spite of all the obstacles in the way."
  },
  {
    "Title": "Productivity Tips and Tricks",
    "Url": "https://channel9.msdn.com/Shows/Visual-Studio-Toolbox/Productivity-Tips-and-Tricks",
    "Timestamp": "2020-08-21T18:03:48",
    "Domain": "channel9.msdn.com",
    "Description": "The .NET Productivity team (a.k.a. Roslyn) is constantly thinking of new ways to make .NET developers more productive. Roslyn PM Mika Dumont shows a number of the latest features that make your coding"
  },
  {
    "Title": "When U.S. air force discovered the flaw of averages",
    "Url": "https://www.thestar.com/news/insight/2016/01/16/when-us-air-force-discovered-the-flaw-of-averages.html",
    "Timestamp": "2020-08-21T17:13:26",
    "Domain": "www.thestar.com",
    "Description": "In the early 1950s, a young lieutenant realized the fatal flaw in the cockpit design of U.S. air force jets. Todd Rose explains in an excerpt from his..."
  },
  {
    "Title": "GUIDs are globally unique, but substrings of GUIDs aren't | The Old New Thing",
    "Url": "https://devblogs.microsoft.com/oldnewthing/20080627-00/?p=21823",
    "Timestamp": "2020-08-21T17:03:35",
    "Domain": "devblogs.microsoft.com",
    "Description": "A customer needed to generate an 8-byte unique value, and their initial idea was to generate a GUID and throw away the second half, keeping the first eight bytes. They wanted to know if this was a good idea. No, it’s not a good idea."
  },
  {
    "Title": "Introducing the new Azure SDKs",
    "Url": "https://www.youtube.com/watch?v=38RYIx7a2M4",
    "Timestamp": "2020-08-21T16:03:46",
    "Domain": "www.youtube.com",
    "Description": "Azure SDK Website: https://aka.ms/azsdk\nAzure SDK Twitter: https://twitter.com/@AzureSDK\nAzure SDK Blog: https://aka.ms/azsdk/blog\nAzure SDK Releases: https://aka.ms/azsdk/releases\nAzure SDK Guidelines: https://aka.ms/azsdk/guide\nAzure SDK Code: https://aka.ms/azsdk/github\nAzure SDK Intro Video: https://aka.ms/azsdk/intro\nAzure SDK Intro Video Deck: https://aka.ms/azsdk/intro/deck\n\nIntroducing the new Azure SDKs\n\nBefore we introduced the new Azure SDKs in July of 2019, each Azure Service team independently created their own SDKs in disparate repositories, had their own CI process, and had varying levels of authentication, language, OS, and package support.  \n\nWith the new Azure SDKs we have established a set of common design guidelines, a centralized engineering system, centralized open-source repositories, as well as consistent authentication, language, and OS support. We also deliver packages to the most popular package managers for each language.\n\nWe currently provide libraries in .NET, Java, Python, and JavaScript/TypeScript with more languages coming soon. \n\nhttps://aka.ms/azsdk will lead you to the Azure SDK website, which a great landing spot with links to our packages, guidelines, documentation, and blog.\n\nDeveloper productivity is the primary driver for the new Azure SDK effort.\n\nWe know that you will be most productive when your experience feels natural for whatever language you code in and when you can easily download the libraries from package managers that you use every day.\n\nConsistency is also key, with the new Azure SDKs, navigating amongst the services doesn’t require you to learn a new API paradigm.  \n\nBeing approachable, diagnosable, and dependable are all pillars that go into building libraries, so we are investing heavily in these areas as well. Including ramping up our documentation efforts, integrating with standard logging frameworks, and holding API board reviews for all new libraries.\n\nYou can find all these guidelines detailed at https://aka.ms/azsdk/guide. We’ll continue to evolve these guidelines, so please review them, and provide us with feedback or suggestions via GitHub\n\nWe released our first GA libraries in the fall of 2019 and continue to release monthly.  \n\nIn GA, we currently have Storage, Key Vault, Event Hubs, App Config, Cosmos for JavaScript/TypeScript, Python and Java. Cognitive Services Text Analytics. Azure.Identity and Azure.Core which implement a consistent authentication and request/response pipeline for all libraries.\n\n\nAnd in Preview, we have Cosmos for .NET. Cognitive Services Form Recognizer, Cognitive Search, Service Bus, and the Azure SDK for Embedded C\n\nWe’ll eventually GA libraries for all Azure services, but next up on the list is Tables, Cognitive services Anomaly Detector, Azure Synapse Analytics, and the new Resource Management libraries.\n\nYou can find the full list of libraries with links to packages, code, and docs at https://aka.ms/azsdk/releases. \n\nYou can find the code for all the new Azure SDKs at https://aka.ms/azsdk/github. That will take you to our central repo which has links to all the other language specific repos and houses our common design guidelines. It’s a great place to file architecture board suggestions and questions or issues that cut across all the SDKs. The overriding theme here is everything in the open on GitHub, with an engaged engineering team to help you through any issues you discover as you build your Azure solutions.\n\n\nWe’d like to encourage you to start using the new Azure SDKs and contributing to them by submitting issues or pull-requests on GitHub. You can follow us on Twitter https://twitter.com/@AzureSDK and subscribe to our blog at https://aka.ms/azsdk/blog."
  },
  {
    "Title": "Azure SDK Latest Releases | Azure SDKs",
    "Url": "https://azure.github.io/azure-sdk/",
    "Timestamp": "2020-08-21T16:03:45",
    "Domain": "azure.github.io",
    "Description": null
  },
  {
    "Title": "Approximate results may vary",
    "Url": "https://ericlippert.com/2020/08/21/approximate-results-may-vary/",
    "Timestamp": "2020-08-21T16:03:43",
    "Domain": "ericlippert.com",
    "Description": "Part 33 of my ongoing series is coming but I did not get all the code written that I wanted to this week, so it will be delayed. In the meanwhile: Living in Canada as a child, of course I grew up l…"
  },
  {
    "Title": "reMarkable microSD",
    "Url": "http://www.davisr.me/projects/remarkable-microsd/",
    "Timestamp": "2020-08-21T16:03:41",
    "Domain": "www.davisr.me",
    "Description": "This page discusses how I added a microSD card to my reMarkable tablet. I did this because I want to develop software for my rM without wearing out the internal eMMC. I chose an external card because I want to be able to swap them easily; it also makes backups faster."
  },
  {
    "Title": "The Brussels Choice - Numberphile",
    "Url": "https://www.youtube.com/watch?v=AeqK96UX3rA",
    "Timestamp": "2020-08-21T15:03:41",
    "Domain": "www.youtube.com",
    "Description": "Neil Sloane from the OEIS discusses the Choix de Bruxelles.\nCheck out Brilliant (get 20% off their premium service): https://brilliant.org/numberphile (sponsor)\n\nMore links & stuff in full description below ↓↓↓ \n\nNeil Sloane founded the runs the OEIS: https://oeis.org/\nBrussels Choice on the OEIS: https://oeis.org/A323454\nNeil Sloane playlist on Numberphile: http://bit.ly/Sloane_Numberphile\nNeil Sloane on the Numberphile podcast: https://youtu.be/mNk_MfFKnuY\n\nNumberphile is supported by the Mathematical Sciences Research Institute (MSRI): http://bit.ly/MSRINumberphile\n\nWe are also supported by Science Sandbox, a Simons Foundation initiative dedicated to engaging everyone with the process of science. https://www.simonsfoundation.org/outreach/science-sandbox/\n\nAnd support from Math For America - https://www.mathforamerica.org/\n\nNUMBERPHILE\nWebsite: http://www.numberphile.com/\nNumberphile on Facebook: http://www.facebook.com/numberphile\nNumberphile tweets: https://twitter.com/numberphile\nSubscribe: http://bit.ly/Numberphile_Sub\n\nVideo by Brady Haran and Pete McPartlan\n\nPatreon: http://www.patreon.com/numberphile\n\nNumberphile T-Shirts and Merch: https://teespring.com/stores/numberphile\n\nBrady's videos subreddit: http://www.reddit.com/r/BradyHaran/\n\nBrady's latest videos across all channels: http://www.bradyharanblog.com/\n\nSign up for (occasional) emails: http://eepurl.com/YdjL9"
  },
  {
    "Title": "First thoughts on Rust vs OCaml",
    "Url": "https://blog.darklang.com/first-thoughts-on-rust-vs-ocaml/",
    "Timestamp": "2020-08-21T15:03:40",
    "Domain": "blog.darklang.com",
    "Description": "I'm about two weeks into Rust now, so this feels like a good time to write a critique, before I get Stockholm Syndrome'd. My main motivation in learning Rust is that I have to maintain some of Dark's Rust code. There was a recent outage related to that code, and"
  },
  {
    "Title": "Chromium’s impact on root DNS traffic | APNIC Blog",
    "Url": "https://blog.apnic.net/2020/08/21/chromiums-impact-on-root-dns-traffic/",
    "Timestamp": "2020-08-21T14:03:55",
    "Domain": "blog.apnic.net",
    "Description": "Guest Post: With 70% of market share, Chromium has had a significant impact on the total root DNS traffic."
  },
  {
    "Title": "Why Did Mozilla Remove XUL Add-ons?",
    "Url": "https://yoric.github.io/post/why-did-mozilla-remove-xul-addons/",
    "Timestamp": "2020-08-21T12:03:39",
    "Domain": "yoric.github.io",
    "Description": "During the past few days, I&rsquo;ve been chatting with Firefox users, trying to separate fact from rumor regarding the consequences of the August 2020 Mozilla layoffs. One of the topics that came back a few times was the removal of XUL-based add-ons during the move to Firefox Quantum. I was very surprised to see that, years after it happened, some community members still felt hurt by this choice.\nAnd then, as someone pointed out on reddit, I realized that we still haven&rsquo;t taken the time to explain in-depth why we had no choice but to remove XUL-based add-ons.\nSo, if you&rsquo;re ready for a dive into some of the internals of add-ons and Gecko, I&rsquo;d like to take this opportunity to try and give you a bit more detail.\n"
  },
  {
    "Title": "Fullstack Vulnerability Management | Cyber Attack Prevention",
    "Url": "https://www.edgescan.com/",
    "Timestamp": "2020-08-21T11:03:48",
    "Domain": "www.edgescan.com",
    "Description": "edgescan Fullstack Vulnerability Management™. Award winning & internationally accredited cyber attack prevention. Continuous security testing."
  },
  {
    "Title": "Building a Second Brain: The Illustrated Notes",
    "Url": "https://maggieappleton.com/basb/",
    "Timestamp": "2020-08-21T06:15:04",
    "Domain": "maggieappleton.com",
    "Description": "Maggie Appleton is an art director, anthropologist, and metaphor-making illustrator. This is her digital garden for growing visual explanations about technology, culture, and programming"
  },
  {
    "Title": "Building a Second Brain",
    "Url": "https://www.buildingasecondbrain.com/",
    "Timestamp": "2020-08-21T06:07:23",
    "Domain": "www.buildingasecondbrain.com",
    "Description": "An online bootcamp on leveraging digital tools to enhance your creativity, productivity, and learning, by Tiago Forte"
  },
  {
    "Title": "Lightroom App Update Wipes Users' Photos and Presets, Adobe Says they are 'Not Recoverable'",
    "Url": "https://petapixel.com/2020/08/20/lightroom-app-update-wipes-users-photos-and-presets-adobe-says-they-are-not-recoverable/",
    "Timestamp": "2020-08-21T06:03:54",
    "Domain": "petapixel.com",
    "Description": "This morning, multiple readers wrote in to alert us to a major Adobe gaff. It seems the latest update to the Lightroom app for iPhone and iPad"
  },
  {
    "Title": "Development quotes of the week [LWN.net]",
    "Url": "https://lwn.net/Articles/829123/",
    "Timestamp": "2020-08-21T06:03:44",
    "Domain": "lwn.net",
    "Description": "Posted Aug 19, 2020 22:57 UTC (Wed) by flussence (subscriber, #85566)\n         In reply to: Development quotes of the week by mathstuf\n        Parent article: Development quotes of the week"
  },
  {
    "Title": "How Shopify Reduced Storefront Response Times with a Rewrite",
    "Url": "https://engineering.shopify.com/blogs/engineering/how-shopify-reduced-storefront-response-times-rewrite",
    "Timestamp": "2020-08-21T06:03:41",
    "Domain": "engineering.shopify.com",
    "Description": "In January 2019, we set out to rewrite the critical software that powers all online storefronts on Shopify’s platform to offer the fastest online shopping experience possible, entirely from scratch and without downtime.\nThe Storefront Renderer is a server-side application that loads a Shopify merchant's storefront Liquid theme, along with the data required to serve the request (for example product data, collection data, inventory information, and images), and returns the HTML response back to your browser. Shaving milliseconds off response time leads to big results for merchants on the platform as buyers increasingly expect pages to load quickly, and failing to deliver on performance can hinder sales, not to mention other important signals like SEO.\nThe previous storefront implementation‘s development, started over 15 years ago when Tobi launched Snowdevil, lived within Shopify’s Ruby on Rails monolith. Over the years, we realized that the “storefront” part of Shopify is quite different from the other parts of the monolith: it has much stricter performance requirements and can accept more complexity implementation-wise to improve performance, whereas other components (such as payment processing) need to favour correctness and readability.\nIn addition to this difference in paradigm, storefront requests progressively became slower to compute as we saw more storefront traffic on the platform. This performance decline led to a direct impact on our merchant storefronts’ performance, where time-to-first-byte metrics from Shopify servers slowly crept up as time went on.\nHere’s how the previous architecture looked:\nOld Storefront Implementation \nBefore, the Rails monolith handled almost all kinds of traffic: checkout, admin, APIs, and storefront.\nWith the new implementation, traffic routing looks like this:\nNew Storefront Implementation \nThe Rails monolith still handles checkout, admin, and API traffic, but storefront traffic is handled by the new implementation.\nDesigning the new storefront implementation from the ground up allowed us to think about the guarantees we could provide: we took the opportunity of this evergreen project to set us up on strong primitives that can be extended in the future, which would have been much more difficult to retrofit in the legacy implementation. An example of these foundations is the decision to design the new implementation on top of an active-active replication setup. As a result, the new implementation always reads from dedicated read replicas, improving performance and reducing load on the primary writers.\nSimilarly, by rebuilding and extracting the storefront-related code in a dedicated application, we took the opportunity to think about building the best developer experience possible: great debugging tools, simple onboarding setup, welcoming documentation, and so on.\nFinally, with improving performance as a priority, we work to increase resilience and capacity in high load scenarios (think flash sales: events where a large number of buyers suddenly start shopping on a specific online storefront), and invest in the future of storefront development at Shopify. The end result is a fast, resilient, single-purpose application that serves high-throughput online storefront traffic for merchants on the Shopify platform as quickly as possible.\nDefining Our Success Criteria\nOnce we clearly outlined the problem we’re trying to solve and scoped out the project, we defined three main success criteria:\n\n\nEstablishing feature parity: for a given input, both implementations generate the same output.\n\nImproving performance: the new implementation runs on active-active replication setup and minimizes server response times.\n\nImproving resilience and capacity: in high-load scenarios, the new implementation generally sustains traffic without causing errors.\n\nBuilding A Verifier Mechanism\nBefore building the new implementation, we needed a way to make sure that whatever we built would behave the same way as the existing implementation. So, we built a verifier mechanism that compares the output of both implementations and returns a positive or negative result depending on the outcome of the comparison.\nThis verification mechanism runs on storefront traffic in production, and it keeps track of verification results so we can identify differences in output that need fixing. Running the verifier mechanism on production traffic (in addition to comparing the implementations locally through a formal specification and a test suite) lets us identify the most impactful areas to work on when fixing issues, and keeps us focused on the prize: reaching feature parity as quickly as possible. It’s desirable for multiple reasons:\n\ngiving us an idea of progress and spreading the risk over a large amount of time\nshortening the period of time that developers at Shopify work with two concurrent implementations at once\nproviding value to Shopify merchants as soon as possible.\n\nThere are two parts to the entire verifier mechanism implementation:\n\nA verifier service (implemented in Ruby) compares the two responses we provide and returns a positive or negative result depending on the verification outcome. Similar to a `diff` tool, it lets us identify differences between the new and legacy implementations.\nA custom nginx routing module (implemented in Lua on top of OpenResty) sends a sample of production traffic to the verifier service for verification. This module acts as a router depending on the result of the verifications for subsequent requests.\n\nThe following diagram shows how each part interacts with the rest of the architecture:\nLegacy implementation and new implementation at the same conceptual layer\nThe legacy implementation (the Rails monolith) still exists, and the new implementation (including the Verifier service) is introduced at the same conceptual layer. Both implementations are placed behind a custom routing module that decides where to route traffic based on the request attributes and the verification data for this request type. Let’s look at an example.\nWhen a buyer’s device sends an initial request for a given storefront page (for example, a product page from shop XYZ), the request is sent to Shopify’s infrastructure, at which point an nginx instance handles it. The routing module considers the request attributes to determine if other shop XYZ product page requests have previously passed verification.\nFirst request routed to Legacy implementation\nSince this is the first request of this kind in our example, the routing module sends the request to the legacy implementation to get a baseline reference that it will use for subsequent shop XYZ product page requests.\nRouting module sends original request and legacy implementation’s response to the new implementation\nOnce the response comes back from the legacy implementation, the Lua routing module sends that response to the buyer. In the background, the Lua routing module also sends both the original request and the legacy implementation’s response to the new implementation. The new implementation computes a response to the original request and feeds both its response and the forwarded legacy implementation’s response to the verifier service. This is done asynchronously to make sure we’re not adding latency to responses we send to buyers, who don’t notice anything different.\nAt this point, the verifier service received the responses from both the legacy and new implementations and is ready to compare them. Of course, the legacy implementation is assumed to be correct as it’s been running in production for years now (it acts as our reference point). We keep track of differences between the two implementations’ responses so we can debug and fix them later. The verifier service looks at both responses’ status code, headers, and body, ensuring they’re equivalent. This lets us identify any differences in the responses so we make sure our new implementation behaves like the legacy one.\nTime-related and randomness-related exceptions make it impossible to have exactly byte-equal responses, so we ignore certain patterns in the verifier service to relax the equivalence criteria. The verifier service uses a fixed time value during the comparison process and sets any random values to a known value so we reliably compare the outputs containing time-based and randomness-based differences.\nThe verifier service sends comparison result back to the Lua module\nThe verifier service sends the outcome of the comparison back to the Lua module, which keeps track of that comparison outcome for subsequent requests of the same kind.\nDynamically Routing Requests To the New Implementation\nOnce we had verified our new approach, we tested rendering a page using the new implementation instead of the legacy one. We iterated upon our verification mechanism to allow us to route traffic to the new implementation after a given number of successful verifications. Here’s how it works.\nJust like when we only verified traffic, a request arrives from a client device and hits Shopify’s architecture. The request is sent to both implementations, and both outputs are forwarded to the verifier service for comparison. The comparison result is sent back to the Lua routing module, which keeps track of it for future requests.\nWhen a subsequent storefront request arrives from a buyer and reaches the Lua routing module, it decides where to send it based on the previous verification results for requests similar to the current one (based on the request attributes\nFor subsequent storefront requests, the Lua routing module decides where to send it\nIf the request was verified multiple times in the past, and nearly all outcomes from the verifier service were “Pass”, then we consider the request safe to be served by the new implementation.\nIf most verifier service results are “Pass”, then it uses the new implementation\nIf, on the other hand, some verifications failed for this kind of request, we’ll play it safe and send the request to the legacy implementation.\nIf most verifier service results are “Fail”, then it uses the old implementation\nSuccessfully Rendering In Production\nWith the verifier mechanism and the dynamic router in place, our first goal was to render one of the simplest storefront pages that exists on the Shopify platform: the password page that protects a storefront before the merchant makes it available to the public.\nOnce we reached full parity for a single shop’s password page, we tested our implementation in production (for the first time) by routing traffic for this password page to the new implementation for a couple of minutes to test it out.\nSuccess! The new implementation worked in production. It was time to start implementing everything else.\nIncreasing Feature Parity\nAfter our success with the password page, we tackled the most frequently accessed storefront pages on the platform (product pages, collection pages, etc). Diff by diff, endpoint by endpoint, we slowly increased the parity rate between the legacy and new implementations.\nHaving both implementations running at the same time gave us a safety net to work with so that if we introduced a regression, requests would easily be routed to the legacy implementation instead. Conversely, whenever we shipped a change to the new implementation that would fix a gap in feature parity, the verifier service starts to report verification successes, and our custom routing module in nginx automatically starts sending traffic to the new implementation after a predetermined time threshold.\nDefining “Good” Performance with Apdex Scores\nWe collected Apdex (Application Performance Index) scores on server-side processing time for both the new and legacy implementations to compare them.\nTo calculate Apdex scores, we defined a parameter for a satisfactory threshold response time (this is the Apdex’s “T” parameter). Our threshold response time to define a frustrating experience would then be “above 4T” (defined by Apdex).\nWe defined our “T” parameter as 200ms, which lines up with Google’s PageSpeed Insights recommendation for server response times. We consider server processing time below 200ms as satisfying and a server processing time of 800ms or more as frustrating. Anything in between is tolerated.\nFrom there, calculating the Apdex score for a given implementation consists of setting a time frame, and counting three values:\n\nN, the total number of responses in the defined time frame\nS, the number of satisfying responses (faster than 200ms) in the time frame\nT, the number of tolerated responses (between 200ms and 800ms) in the time frame\n\nThen, we calculate the Apdex score: \n\n$$\\frac{s + t/2}{n}$$\nBy calculating Apdex scores for both the legacy and new implementations using the same T parameter, we had common ground to compare their performance.\nMethods to Improve Server-side Storefront Performance\nWe want all Shopify storefronts to be fast, and this new implementation aims to speed up what a performance-conscious theme developer can’t by optimizing data access patterns, reducing memory allocations, and implementing efficient caching layers.\nOptimizing Data Access Patterns\nThe new implementation uses optimized, handcrafted SQL multi-select statements maximizing the amount of data transferred in a single round trip. We carefully vet what we eager-load depending on the type of request and we optimize towards reducing instances of N+1 queries.\nReducing Memory Allocations\nWe reduce the number of memory allocations as much as possible so Ruby spends less time in garbage collection. We use methods that apply modifications in place (such as #map!) rather than those that allocate more memory space (like #map). This kind of performance-oriented Ruby paradigm sometimes leads to code that’s not as simple as idiomatic Ruby, but paired with proper testing and verification, this tradeoff provides big performance gains. It may not seem like much, but those memory allocations add up quickly, and considering the amount of storefront traffic Shopify handles, every optimization counts.\nImplementing Efficient Caching Layers\nWe implemented various layers of caching throughout the application to reduce expensive calls. Frequent database queries are partitioned and cached to optimize for subsequent reads in a key-value store, and in the case of extremely frequent queries, those are cached directly in application memory to reduce I/O latency. Finally, the results of full page renders are cached too, so we can simply serve a full HTTP response directly from cache if possible.\nMeasuring Performance Improvement Successes\nOnce we could measure the performance of both implementations and reach a high enough level of verified feature parity, we started migrating merchant shops. Here are some of the improvements we’re seeing with our new implementation:\n\nAcross all shops, average server response times for requests served by the new implementation are 4x to 6x faster than the legacy implementation. This is huge!\nWhen migrating a storefront to the new implementation, we see that the Apdex score for server-side processing time improves by +0.11 on average.\nWhen only considering cache misses (requests that can’t be served directly from the cache and need to be computed from scratch), the new implementation increases the Apdex score for server-side processing time by a full +0.20 on average compared to the previous implementation.\nWe heard back from merchants mentioning a 500ms improvement in time-to-first-byte metrics when the new implementation was rolled out to their storefront.\n\nSo another success! We improved store performance in production.\nNow how do we make sure this translates to our third success criteria?\nImproving Resilience and Capacity\nWhile working on the new implementation, the Verifier service identified potential parity gaps, which helped tremendously. However, a few times we shipped code to production that broke in exceedingly rare edge cases that it couldn’t catch.\nAs a safety mechanism, we made it so that whenever the new implementation would fail to successfully render a given request, we’d fall back to the legacy implementation. The response would be slower, but at least it was working properly. We used circuit breakers in our custom nginx routing module so that we’d open the circuit and start sending traffic to the legacy implementation if the new implementation was having trouble responding successfully. Read more on tuning circuit breakers in this blog post by my teammate Damian Polan.\nIncrease Capacity in High-load Scenarios\nTo ensure that the new implementation responds well to flash sales, we implemented and tweaked two mechanisms. The first one is an automatic scaling mechanism that adds or remove computing capacity in response to the amount of load on the current swarm of computers that serve traffic. If load increases as a result of an increase in traffic, the autoscaler will detect this increase and start provisioning more compute capacity to handle it.\nAdditionally, we introduced in-memory cache to reduce load on external data stores for storefronts that put a lot of pressure on the platform’s resources. This provides a buffer that reduces load on very-high traffic shops.\nFailing Fast\nWhen an external data store isn’t available, we don’t want to serve buyers an error page. If possible, we’ll try to gracefully fall back to a safe way to serve the request. It may not be as fast, or as complete as a normal, healthy response, but it’s definitely better than serving a sad error page.\nWe implemented circuit breakers on external datastores using Semian, a Shopify-developed Ruby gem that controls access to slow or unresponsive external services, avoiding cascading failures and making the new implementation more resilient to failure.\nSimilarly, if a cache store isn’t available, we’ll quickly consider the timeout as a cache miss, so instead of failing the entire request because the cache store wasn’t available, we’ll simply fetch the data from the canonical data store instead. It may take longer, but at least there’s a successful response to serve back to the buyer.\nTesting Failure Scenarios and the Limits of the New Implementation\nFinally, as a way to identify potential resilience issues, the new implementation uses Toxiproxy to generate test cases where various resources are made available or not, on demand, to generate problematic scenarios.\nAs we put these resilience and capacity mechanisms in place, we regularly ran load tests using internal tooling to see how the new implementation behaves in the face of a large amount of traffic. As time went on, we increased the new implementation’s resilience and capacity significantly, removing errors and exceptions almost completely even in high-load scenarios. With BFCM 2020 coming soon (which we consider as an organic, large-scale load test), we’re excited to see how the new implementation behaves.\nWhere We’re at Currently\nWe’re currently in the process of rolling out the new implementation to all online storefronts on the platform. This process happens automatically, without the need for any intervention from Shopify merchants. While we do this, we’re adding more features to the new implementation to bring it to full parity with the legacy implementation. The new implementation is currently at 90%+ feature parity with the legacy one, and we’re increasing that figure every day with the goal of reaching 100% parity to retire the legacy implementation.\nAs we roll out the new implementation to storefronts we are continuing to see and measure performance improvements as well. On average, server response times for the new implementation are 4x faster than the legacy implementation. Rhone Apparel, a Shopify Plus merchant, started using the new implementation in April 2020 and saw dramatic improvements in server-side performance over the previous month.\nWe learned a lot during the process of rewriting this critical piece of software. The strong foundations of this new implementation make it possible to deploy it around the world, closer to buyers everywhere, to reduce network latency involved in cross-continental networking, and we continue to explore ways to make it even faster while providing the best developer experience possible to set us up for the future.\n\nWe're always on the lookout for talent and we’d love to hear from you. Visit our Engineering career page to find out about our open positions."
  },
  {
    "Title": "Upgrade to .NET 5 by davidfowl · Pull Request #96 · davidfowl/BedrockFramework",
    "Url": "https://github.com/davidfowl/BedrockFramework/pull/96/files",
    "Timestamp": "2020-08-21T05:03:44",
    "Domain": "github.com",
    "Description": null
  },
  {
    "Title": "Guess a GUID",
    "Url": "http://guessaguid.secretgeek.net/",
    "Timestamp": "2020-08-21T05:03:41",
    "Domain": "guessaguid.secretgeek.net",
    "Description": "Guess A Guid. The fun guid guessing game."
  },
  {
    "Title": "WSL 2 Support is coming to Windows 10 Versions 1903 and 1909 | Windows Command Line",
    "Url": "https://devblogs.microsoft.com/commandline/wsl-2-support-is-coming-to-windows-10-versions-1903-and-1909/",
    "Timestamp": "2020-08-21T02:03:37",
    "Domain": "devblogs.microsoft.com",
    "Description": "Support for Windows Subsystem for Linux (WSL) 2 distros is being backported to Windows 10 version 1903, and 1909!"
  },
  {
    "Title": "Docker Desktop & WSL 2 - Backport Update - Docker Blog",
    "Url": "https://www.docker.com/blog/docker-desktop-wsl-2-backport-update/",
    "Timestamp": "2020-08-21T01:03:42",
    "Domain": "www.docker.com",
    "Description": "Learn from Docker experts to simplify and advance your app development and management with Docker. Stay up to date on Docker events and new version announcements!"
  }
]