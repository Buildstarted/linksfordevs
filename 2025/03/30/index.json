[
  {
    "Title": "Proof That Steins;Gate World Is a Running Simulation",
    "Url": "https://samyar.me/posts/steins-gate-and-hamming-code/",
    "Timestamp": "2025-03-30T00:01:27",
    "Domain": "samyar.me",
    "Description": "My thoughts on why Steins;Gate series world and probably ours too, is a simulation."
  },
  {
    "Title": "Don't Walk with Rocks in Your Shoes",
    "Url": "https://bencornia.com/blog/dont-walk-with-rocks-in-your-shoes",
    "Timestamp": "2025-03-30T00:01:27",
    "Domain": "bencornia.com",
    "Description": "Published 2025 March 29"
  },
  {
    "Title": "The case against local LLMs",
    "Url": "https://vivekhaldar.com/articles/the-case-against-local-llms/",
    "Timestamp": "2025-03-30T00:01:27",
    "Domain": "vivekhaldar.com",
    "Description": "As a geek, I love the idea of running LLMs locally, and owning my entire AI stack without depending on cloud APIs. I run LLMs locally with Ollama all the time. I’d love to see a world where the intelligence that these models provide is democratized, cheap and plentiful.\nBut if I think about it objectively there are several reasons local LLMs going mainstream might not happen.\nWidely used utilities want to be centralized\nIn the early days of electricity, factories generated their own electricity on-site. This phase was relatively short-lived, giving way to the centralized power grid we have now. The same thing happened with water, trash disposal, manufactured goods and pretty much every staple of modern life."
  },
  {
    "Title": "Adding Home Automation to KDE – David Edmundson's Web Log",
    "Url": "https://blog.davidedmundson.co.uk/blog/adding-home-automation-to-kde/",
    "Timestamp": "2025-03-30T00:01:27",
    "Domain": "blog.davidedmundson.co.uk",
    "Description": ""
  },
  {
    "Title": "My writing workflow with LLMs",
    "Url": "https://stanislas.blog/2025/02/writing-workflow-llm/",
    "Timestamp": "2025-03-30T00:01:27",
    "Domain": "stanislas.blog",
    "Description": "A blog about building things with code and computers"
  },
  {
    "Title": "Default to optimism",
    "Url": "https://dancullum.com/2025/03/default-to-optimism/",
    "Timestamp": "2025-03-30T00:01:27",
    "Domain": "dancullum.com",
    "Description": "Pessimism is easy. Anyone can come up with a risk, a downside, or a way things could go wrong. Optimism, on the other hand, is hard. The optimist sticks their neck out for an idea. They try and ima…"
  },
  {
    "Title": "Vernus",
    "Url": "https://vernus.one/the-dev-ladder/how-to-commit-refactoring-changes",
    "Timestamp": "2025-03-30T00:01:26",
    "Domain": "vernus.one",
    "Description": "Vernus is a brick in the paywall."
  },
  {
    "Title": "The Disconnect Between AI Benchmarks and Math Research | Sugaku",
    "Url": "https://sugaku.net/content/ai-benchmarks-vs-real-math-research/",
    "Timestamp": "2025-03-30T00:01:26",
    "Domain": "sugaku.net",
    "Description": "Evaluating AI systems on their ability to be a mathematical copilot"
  },
  {
    "Title": "The Value of Scarcity in the Age of A[I]bundance",
    "Url": "https://philipps.bearblog.dev/the-value-of-scarcity-in-the-age-of-abundance/",
    "Timestamp": "2025-03-30T00:01:25",
    "Domain": "philipps.bearblog.dev",
    "Description": "In a time of accelerating AI development, I think we should take a step back and understand why and for what purposes we want to use generative models.\n\nTh..."
  },
  {
    "Title": "What’s in a Nix store path",
    "Url": "https://fzakaria.com/2025/03/28/what-s-in-a-nix-store-path",
    "Timestamp": "2025-03-30T00:01:25",
    "Domain": "fzakaria.com",
    "Description": "This is a follow up to my post on nix vanity store paths. Check it out if you want to jazz-up your /nix/store paths with some vanity prefixes ✨."
  },
  {
    "Title": "Fixing White Balance on Mac Air",
    "Url": "https://nicktrevino.com/fixing-white-balance-on-mac-air.html",
    "Timestamp": "2025-03-30T00:01:25",
    "Domain": "nicktrevino.com",
    "Description": "Posted on 3/29/2025, 8:34 AM"
  },
  {
    "Title": "\"But LLMs are not deterministic!\"",
    "Url": "https://vivekhaldar.com/articles/-but-llms-are-not-deterministic--/",
    "Timestamp": "2025-03-30T00:01:23",
    "Domain": "vivekhaldar.com",
    "Description": "•\t“But LLMs are not deterministic!”\nThe most common retort to my “LLMs are compilers” framing is:\n\n“… but compilers are deterministic, LLMs just generate probabilistic output!”\n\nI think that retort is not even wrong, and I’ll explain why.\nThe analogy’s central idea was not that LLMs replicate every characteristic of compilers, but that using natural-language-driven LLMs is another large step up in abstraction. Traditional compilers enabled programmers to express themselves in high-level languages instead of assembly. Similarly, LLMs empower programmers to express their intent directly in natural language, translating it into code."
  }
]