[
  {
    "Title": "The Missing Mirror",
    "Url": "https://virtuallytd.com/posts/the-missing-mirror/",
    "Timestamp": "2025-01-04T02:05:10",
    "Domain": "virtuallytd.com",
    "Description": "Delve into the nuanced realm of morality in 'The Missing Mirror.' This article explores the profound impact of self-reflection and remorse on distinguishing 'good' people from 'bad.' Discover how empathy and the ability to recognize one's mistakes foster personal growth and redefine our understanding of moral character."
  },
  {
    "Title": "Writing About Writing",
    "Url": "https://www.matthartz.me/p/writing-about-writing",
    "Timestamp": "2025-01-04T02:02:22",
    "Domain": "www.matthartz.me",
    "Description": "Starting this blog has given me a strong incentive to organize my thoughts."
  },
  {
    "Title": "How we speed up filtered vector search with ACORN | Weaviate",
    "Url": "https://weaviate.io/blog/speed-up-filtered-vector-search",
    "Timestamp": "2025-01-04T02:02:22",
    "Domain": "weaviate.io",
    "Description": "Learn about the challenges of filtered vector search and how Weaviate tackles them with ACORN."
  },
  {
    "Title": "Farewell: Third-Generation (3G UMTS WCDMA) Mobile Service in Australia (2002-2024)",
    "Url": "https://goughlui.com/2024/12/31/farewell-third-generation-3g-umts-wcdma-mobile-service-in-australia-2002-2024/",
    "Timestamp": "2025-01-04T02:02:22",
    "Domain": "goughlui.com",
    "Description": "After 22 long years, third-generation (or better known as 3G) mobile telephone service for the general public has ended across Australia. Sometimes also known as the “Universal Mobile Telecom…"
  },
  {
    "Title": "DIY Multideck | A game system to play hundreds of board games",
    "Url": "https://diymultideck.mauri.app/manual/",
    "Timestamp": "2025-01-04T02:02:22",
    "Domain": "diymultideck.mauri.app",
    "Description": "The DIY multideck is a deck of 162 playing cards (3 standard decks) that allows you to play hundreds of existing games, not only classic card games but also modern games that use components like boards or coins. The DIY multideck is ideal for traveling and prototyping new games."
  },
  {
    "Title": "My objection(s) to the \"LLMs are just next-token predictors\" take | Alejandro Tlaie Boria, PhD",
    "Url": "https://alejandrotlaie.net/my-objections-to-the-llms-are-just-next-token-predictors-take",
    "Timestamp": "2025-01-04T02:02:21",
    "Domain": "alejandrotlaie.net",
    "Description": "An outline of why I believe this is a wrong stance to have in light of the current evidence"
  },
  {
    "Title": "Maximum Speed SQLite Inserts",
    "Url": "https://blog.julik.nl/2025/01/maximum-speed-sqlite-inserts",
    "Timestamp": "2025-01-04T02:02:20",
    "Domain": "blog.julik.nl",
    "Description": "In my work I tend to reach for SQLite more and more. The type of work I find it useful for most these days is quickly amalgamating, dissecting, collecting and analyzing large data sets. As I have outlined in my Euruko talk on scheduling, a key element of the project was writing a simulator. That simulator outputs metrics - lots and lots of metrics, which resemble what our APM solution collects. Looking at those metrics makes it possible to plot, dissect and examine the performance of various job flows. You can, of course, store those metrics in plain Ruby objects and then work with them in memory - there is nothing wrong with that. However, I find using SQL vastly superior. And since the simulator only ever runs on one machine, and every session is unique - SQLite is the perfect tool for collecting metrics. Even if it is not a specialized datastore. One challenge presented itself, though: those metrics get output in very large amounts. Every tick of the simulator can generate thousands of values. Persisting them to SQLite is fast, but with very large amounts that “fast” becomes “not that fast”. I had to go through a number of steps to make these inserts more palatable, which led to a very, very pleasant speed improvement indeed. That seems worth sharing - so strap in and let’s play. Setting the scene Let’s generate our data first and see how far we can push our little setup. rng = Random.new(42) metrics = %w( foo bar baz bad bleg ) values = (500_000).times.map do |n| {name: metrics.sample(random: rng), value: rng.rand} end We will assume we are inserting from Hash objects representing column-value mappings. We will use a fresh database for every test and keep it in memory to not even care about the filesystem performance - for now: def create_db db = SQLite3::Database.new(\":memory:\") db.execute(\"CREATE TABLE metrics (name VARCHAR NOT NULL, value FLOAT NOT NULL)\") db end and add a timing helper: def timed(&blk) t = Process.clock_gettime(Process::CLOCK_MONOTONIC) yield delta = Process.clock_gettime(Process::CLOCK_MONOTONIC) - t warn \"Took #{delta} seconds\" end First - the naive insert: timed(\"Naive\") do db = create_db first_record = values.first cols = first_record.keys.join(\", \") placeholders = ([\"?\"] * first_record.length).join(\", \") sql = \"INSERT INTO metrics (#{cols}) VALUES (#{placeholders})\" values.each do |cols_to_values| db.query(sql, cols_to_values.values) end end This gives Naive - 2.3065050000150222 seconds. Surely we can do better than that. Transactions for bulk insert are great, let’s use one: timed(\"With transaction\") do db = create_db db.transaction do first_record = values.first cols = first_record.keys.join(\", \") placeholders = ([\"?\"] * first_record.length).join(\", \") sql = \"INSERT INTO metrics (#{cols}) VALUES (#{placeholders})\" values.each do |cols_to_values| db.query(sql, cols_to_values.values) end end end That gives With transaction - 1.8898840000038035 seconds. Better, but by far not the improvement we need. Let’s use a prepared statement next: timed(\"With transaction and prepared statement\") do db = create_db first_record = values.first cols = first_record.keys.join(\", \") placeholders = ([\"?\"] * first_record.length).join(\", \") sql_stmt = \"INSERT INTO metrics (#{cols}) VALUES (#{placeholders})\" db.transaction do prepared_stmt = db.prepare(sql_stmt) values.each do |cols_to_values| prepared_stmt.execute(cols_to_values.values) end end end This gives: With transaction and prepared statement - 0.6456299999845214 seconds - much better. But we can go further. By default SQLite optimizes for durability (at least on my version). Since we are working with a local database and we do not care about a potential crash, we can “downgrade” the durability of the storage engine to get more speed: timed(\"With pragmas, transaction and prepared statement\") do db = create_db db.query(\"PRAGMA synchronous = OFF\") db.query(\"PRAGMA journal_mode = OFF\") first_record = values.first cols = first_record.keys.join(\", \") placeholders = ([\"?\"] * first_record.length).join(\", \") sql_stmt = \"INSERT INTO metrics (#{cols}) VALUES (#{placeholders})\" db.transaction do prepared_stmt = db.prepare(sql_stmt) values.each do |cols_to_values| prepared_stmt.execute(cols_to_values.values) end end end Still better: With pragmas, transaction and prepared statement - 0.6219140000175685 seconds - this is already a substantial improvement, but we can give the crank another turn. Host parameter stuffing How can we make it even faster than that? Well, the INSERT SQL statement supports multiple tuples in sequence, as long as they have the same cardinality. A bit like so: INSERT INTO metrics (name, value) VALUES ('foo', 1.0), ('bar', 2.0), ('baz', 4.0) We can assign our placeholders in the prepared statement and then pass our bound parameters in the end: db.query(\"INSERT INTO metrics (name, value) VALUES (?, ?), (?, ?)\", [\"foo\", 1.0, \"bar\", 2.0\"]) But there is a limit - the maximum number of bound variables per SQL statement, varies with the version of SQLite. Sadly, the sqlite3 gem does not support querying for sqlite3_limit(), but the info says: To prevent excessive memory allocations, the maximum value of a host parameter number is SQLITE_MAX_VARIABLE_NUMBER, which defaults to 999 for SQLite versions prior to 3.32.0 (2020-05-22) or 32766 for SQLite versions after 3.32.0. We can thus assume the value to be 999 for now (but you can query for the version using SQLite3::VERSION or try to find another way to access the sqlite3_limit API). What we need to do is figure out how many of our records we can stuff into a single INSERT - since we cannot really “split” the records, we always need to insert all values pertaining to a single record in one statement. max_bindvars = 999 first_record = values.first cardinality = first_record.length records_per_statement, _ = max_bindvars.divmod(cardinality) This shows us that we can at most stuff records_per_statement into a single INSERT (the remainder is not really useful here). We will use 2 statements, one of which we will prepare - since it is going to be reused. The first one will fit as many records as we can and bind variables for all of them - 999 bindvars or less, depending on the cardinality of our records. The second one will contain enough bindvars to fit the remaining records, and will be used only once - in fact, we do not even need to prepare it. timed(\"With multirow inserts, pragmas, transaction and prepared statement\") do db = create_db db.query(\"PRAGMA synchronous = OFF\") db.query(\"PRAGMA journal_mode = OFF\") first_record = values.first # We need to group our records into blocks of at most max_bindvars values cardinality = first_record.length row_placeholder = \"(\" + ([\"?\"] * cardinality).join(\", \") + \")\" # => (?, ?, ?) max_bindvars = 999 max_records_per_statement, _ = max_bindvars.divmod(cardinality) prepared_statement_for_max = nil cols = first_record.keys.join(\", \") db.transaction do values.each_slice(max_records_per_statement) do |records_subset| bound_params = records_subset.flat_map(&:values) if records_subset.length == max_records_per_statement prepared_statement_for_max ||= begin placeholders_for_larger_chunk = ([row_placeholder] * max_records_per_statement).join(\", \") sql_max = \"INSERT INTO metrics (#{cols}) VALUES #{placeholders_for_larger_chunk}\" db.prepare(sql_max) end prepared_statement_for_max.execute(bound_params) else # This is the last slice which is smaller placeholders_for_smaller_chunk = ([row_placeholder] * records_subset.length).join(\", \") sql_rest = \"INSERT INTO metrics (#{cols}) VALUES #{placeholders_for_smaller_chunk}\" db.query(sql_rest, bound_params) end end end end Running all of our implementations then gives us: Naive - 2.7048650000069756 seconds With transaction - 2.3600640000076964 seconds With transaction and prepared statement - 0.637083999987226 seconds With pragmas, transaction and prepared statement - 0.6406159999896772 seconds With multirow inserts, pragmas, transaction and prepared statement - 0.3141590000013821 seconds We can see that using multirow inserts gives us a 2x speedup. Splendid. Memory databases to disk Of course, this is with memory databases - so it is probably very fast because of that. But what if I told you that you can actually serialize a memory DB onto disk very quickly, just using the builtin SQLite functions? A little-known feature of SQLite called online backup can be used to prepare your database in memory, do all of the bulk operations – and then write it out onto the filesystem, in a very fast (and consistent) way. The API in the Ruby gem is not pretty - but it is there and it works, and it works well (has been for more than a decade, in fact). Let’s put it to use: def write_to_disk(source_db, filename) destination_db = SQLite3::Database.new(filename) b = SQLite3::Backup.new(destination_db, 'main', source_db, 'main') begin b.step(1) end while b.remaining > 0 b.finish destination_db.close end Running the code gives us: Naive - 2.706573000003118 seconds With a prepared statement - 0.990191999997478 seconds With transaction and prepared statement - 0.627656000026036 seconds With pragmas, transaction and prepared statement - 0.6277800000098068 seconds With multirow inserts, pragmas, transaction and prepared statement - 0.3135960000217892 seconds These timings include serialization to disk using the backup API. And produces a few SQLite3 files of exactly the same size. Comparing disk and memory performance Out of curiosity, I would like to show what kind of performance we can have if we perform the same “accelerated” inserts on a disk DB, with the same 500000 rows: Disk DBs: Naive - 474.50496300001396 seconds With transaction - 2.3882359999988694 seconds With transaction and prepared statement - 0.7706829999806359 seconds With pragmas, transaction and prepared statement - 0.7386469999910332 seconds With multirow inserts, pragmas, transaction and prepared statement - 0.3565039999957662 seconds Memory DBs: Naive - 2.3065050000150222 seconds With transaction - 1.8898840000038035 seconds With transaction and prepared statement - 0.6456299999845214 seconds With pragmas, transaction and prepared statement - 0.634888000000501 seconds With multirow inserts, pragmas, transaction and prepared statement - 0.31626500000129454 seconds This is curious: using a DB in RAM only helps is in the most pathological case with our “naive” inserts – but for other cases performance is on par. Aren’t modern SSDs marvelous? So there you have it: a roughly x8 speedup for inserts."
  },
  {
    "Title": "A Software Observability Roundup - parente.dev",
    "Url": "https://blog.parente.dev/software-observability-roundup/",
    "Timestamp": "2025-01-04T02:02:20",
    "Domain": "blog.parente.dev",
    "Description": "I spent some time recently catching up on my #to-read saves in Obsidian. More than a fewof these were blog posts from 2024 about software observability. Talk of \"redefining observability\",\"observability 2.0\", and \"try Honeycomb\" had caught my eye in a few spaces,and so I had been hoarding links on the topic. After spending a few days immersing myself in thosearticles and branching out to others, I decided to write this bullet-form roundup."
  },
  {
    "Title": "Bitmasking: Storing Multiple States in a Single Integer",
    "Url": "https://www.ika.im/posts/bitmasking",
    "Timestamp": "2025-01-04T02:02:20",
    "Domain": "www.ika.im",
    "Description": "Elegant and super-useful technique to store multiple on/off states into a single value."
  },
  {
    "Title": "On log levels",
    "Url": "https://weberdominik.com/blog/on-log-levels/",
    "Timestamp": "2025-01-04T02:02:20",
    "Domain": "weberdominik.com",
    "Description": ""
  },
  {
    "Title": "New Year resolution: sponsoring some of the open source projects I use",
    "Url": "https://www.andreagrandi.it/posts/new-year-resolution-sponsoring-opensource-projects/",
    "Timestamp": "2025-01-04T02:02:19",
    "Domain": "www.andreagrandi.it",
    "Description": "I decided to set aside a monthly budget and sponsor a few open source projects I regularly use."
  },
  {
    "Title": "Using LLMs and Cursor to become a finisher",
    "Url": "https://zohaib.me/using-llms-and-cursor-for-finishing-projects-productivity/",
    "Timestamp": "2025-01-04T02:02:19",
    "Domain": "zohaib.me",
    "Description": "Struggling to finish side projects due to limited time? In my latest blog post, I share how I improved my productivity using AI tools like LLMs and Cursor IDE. Learn to refine specs, bootstrap code, and iterate effectively to rapidly build and deploy your projects—even with a busy schedule."
  },
  {
    "Title": "The JIT calculator challenge",
    "Url": "https://ochagavia.nl/blog/the-jit-calculator-challenge/",
    "Timestamp": "2025-01-04T02:02:18",
    "Domain": "ochagavia.nl",
    "Description": "Advent of code has come and passed, what should I do now with so much free time? Fear not! The JIT calculator challenge is here.\n1. The challenge Back when Rust was in its infancy, the official website featured an example program to showcase the language’s syntax. It was a toy calculator, implemented as an interpreter in 20 lines of code.\nWhy talk about website archaeology, though? We are not here for nostalgia’s sake, are we?"
  },
  {
    "Title": "My Takeaways From 12 Months of Therapy",
    "Url": "https://cauldron.life/blog/my-takeaways-from-therapy/",
    "Timestamp": "2025-01-04T02:02:15",
    "Domain": "cauldron.life",
    "Description": "Therapy had been a buzzword amongst some people I know and on the web. Honestly, I believed it to be a waste of time and money. I gave therapy a try first in 2021, but it was short-lived and not so great. In 2024, I again got curious about the mental health space and started paying more attention to the issues I was facing in my daily life. Things that are regular and normal, but when paid attention to, could be something deep. Some of these may even look silly at first glance, like getting angry when someone questions your choices. After 12 months of work with my therapist, I feel I’ve gained much, and there’s some inner work everyone could do."
  },
  {
    "Title": "Unit Testing Clean Architecture Use Cases",
    "Url": "https://www.milanjovanovic.tech/blog/unit-testing-clean-architecture-use-cases",
    "Timestamp": "2025-01-04T00:02:12",
    "Domain": "www.milanjovanovic.tech",
    "Description": "Drawing from years of experience, I share my battle-tested approach to unit testing Clean Architecture use cases in .NET, focusing on the critical balance between code coverage and test quality. Through practical examples and real-world scenarios, I'll demonstrate how to write effective tests that not only catch bugs but also provide confidence in your code deployments."
  },
  {
    "Title": "I still don’t think companies serve you ads based on spying through your microphone",
    "Url": "https://simonwillison.net/2025/Jan/2/they-spy-on-you-but-not-like-that/",
    "Timestamp": "2025-01-04T00:02:10",
    "Domain": "simonwillison.net",
    "Description": "One of my weirder hobbies is trying to convince people that the idea that companies are listening to you through your phone’s microphone and serving you targeted ads is a …"
  }
]