<?xml version="1.0" encoding="utf-16"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <atom:link rel="self" type="application/rss+xml" href="https://linksfor.dev/" />
    <title>linksfor.dev(s)</title>
    <link>https://linksfor.dev/</link>
    <description>Curated links for devs</description>
    <language>en</language>
    <item>
      <title>On the potential of Transformers in Reinforcement Learning</title>
      <link>https://lorenzopieri.com/rl_transformers/</link>
      <description>Summary  Transformers architectures are the hottest thing in supervised and unsupervised learning, achieving SOTA results on natural language processing, vision, audio and multimodal tasks. Their key capability is to capture which elements in a long sequence are worthy of attention, resulting in great summarisation and generative skills. Can we transfer any of these skills to reinforcement learning? The answer is yes (with some caveats). I will cover how it’s possible to refactor reinforcement learning as a sequence problem and reflect on potential and limitations of this approach.  Warning: This blogpost is pretty technical, it presupposes a basic understanding of deep learning and good familiarity with reinforcement learning. Previous knowledge of transformers is not required.  Intro to Transformers  Introduced in 2017, Transformers architectures took the deep learning scene by storm: they achieved SOTA results on nearly all benchmarks, while being simpler and faster than the previous overengineered networks and scaling well with additional data. For instance by getting rid of convolutional network (CNNs) we can go from the complex Feature Pyramid Network Base RCNN of Detectron2, which uses the old fashioned anchor boxes of classic computer vision,   (Feature Pyramid Network Base RCNN Architecture, image taken from this nice blogpost)  to a transformer model based on embedding vectors extracted from the image patches and few hidden layers.   (ViT Architecture, all credits to the original paper)    Similarly, in the NLP domain, transformers get rid of recurrent neural networks (RNNs) and are able to handle sequences as a whole, rather than sequentially.  The key are the self-attention and multi-head attention layers, which roughly tell the network how elements in a long sequence correlate with each other, so that for every element (say a word, or a pixel) we know where we should pay attention to. The best example is in text translation: when translating “piace” in “Mi piace questo articolo”, the transformer needs to also pay attention to the word “Mi” to correctly translate it into “I like this article”, while there is no need to pay attention to the other words. A similar proposition such as “Le piace questo articolo” translates instead into “She likes this article”.  The transformer architecture is usually divided into encoder and decoder modules. The encoder is responsible for taking the input data and building representations of its features, usually as an embedding vector. The decoder takes these embeddings as input and generates new sequences. Depending on what we need to do, we may only need one of these modules. Encoder-only architectures are used for tasks requiring understanding of the input, such as sentiment analysis and sentence classification. Deconder-only models are generative models, for instance text or image generation. Encoder-Decoder architectures are suited for text translation and summarisation.  Famous networks of each category include:     Encoder-only: BERT, RoBERTa, ALBERT, ViT   Deconder-only: GPT2, GPT3   Encoder-Decoder: BART, T5, DETR   The de-facto standard to play with transformers is the hugginface library, which contains excellent documentation which I encourage you to check. The next big-thing for transformer architectures in supervised/unsupervised learning is likely to be multimodal transformers: networks able to ingest text, audio, images and more at the same time.  RL Transformers  We usually model reinforcement learning problems as a Markov decision process described by sequences of actions, states, transition probabilities and rewards. At every step a typical algorithm would check only the current state and the past performances encoded in a state or state-action value function to decide which action to take next. There is no notion of “previous-action”, a Markov process doesn’t care (or better, it doesn’t remember).  The key hypothesis behind RL transformers is: what if instead we take the whole sequence as the building block to base our prediction? If we do so, we mapped a 1-step Markovian problem into a sequence-to-sequence or sequence-to-action problem, which we can approach using battle tested supervised learning techniques. No value functions, no actor-critics, no bootstrapping.  Sequences are usually called trajectories in RL literature, so we will use these terms interchangeably. We will assume that trajectories are already given before-hand, so we will limit ourselves to offline-reinforcement learning. These trajectories may have been produced by another RL agent or they may be expert (human) demonstrations.  RL with transformers has been recently explored in the paper Offline Reinforcement Learning as One Big Sequence Modeling Problem and Decision Transformer: Reinforcement Learning via Sequence Modeling.  In the former work they first use a GPT-like architecture to ingest trajectories composed by actions, states, rewards  [\tau = (s_1, a_1, r_1, s_2, a_2, r_2, \dots )]  and to generate novel candidate trajectories. The setup is nearly identical to the one that we would use in natural language processing.  Finally they apply a beam-search strategy to planning, by iteratively expanding the most promising trajectories weighted by cumulative returns until a single trajectory stands out as the most promising. To avoid being too greedy they also augment trajectories with the reward-to-go $R_t$ (future cumulative reward) at each step.  [R_t = \sum_{t’ = t}^{T} \gamma^{t’-t} r_{t’}]  In the latter work they get rid of rewards at each time step and only consider the reward-to-go, directly predicting the next action given the state and a target final return.  [\tau = (s_1, a_1, R_1, s_2, a_2, R_2, \dots )]  This very simple approach is reminiscent of upside-down reinforcement learning, that is instead of asking the agent to search for an optimal policy, just ask it to “obtain so much total reward in so much time”. In essence we are doing supervised learning, asking the agent to extrapolate based on past trajectories which trajectory will lead to the highest rewards. Actually it’s possible to condition the agent to achieve arbitrary rewards (among the ones achieved in the training dataset). This can be handy if intermediate levels of rewards are still meaningful, for instance last year I used this technique to teach a food scooping robot (before knowing that upside-down RL had a name) to pick different portion sizes, having set the exact weight of scooped food as a positive reward. Basically the robot trains to scoop as much as possible, but if we ask for a small portion it should be able to do that!  From what I wrote so far you may be thinking that this is pretty much behaviour cloning on transformers steroids… and you would not be that wrong! Indeed the authors of Decision Transformer also recongnised this and benchmarked against vanilla behaviour cloning (by the way, props to how they divided their discussion session in clearly articulated questions). The conclusion is that, even though performance are often comparable, the transformers approach do outperform vanilla behaviour cloning if there is a large amount of training data.  How do RL transformers perform overall?  Pretty well, they are often on par or better than state of the art TD-learning model-free offline methods such as Conservative Q-Learning (CQL) on benchmarks like Atari and D4RL!  On Atari they have been tested on a DQN-replay Atari dataset, performing on par or better in Breakout, Pong and Seaquest, while worst on Qbert. On D4RL (an offline RL benchmark using OpenAI Gym) they have been tried on continuous tasks such as HalfCheetah, Hopper and Walker being again on par with state of the art methods. Tests on a Key-to-Door environment, in which a key must be picked well in advance of reaching the final door, also shows very good long-term credit assignment capabilities. Tests on modified D4RL benchmarks in which the reward is given only at episode’s end shows that transformers perform well also in sparse reward settings, while the performance of CQL plummets.  The Good and the Hype  So, why is this whole thing interesting? The reasons are very pragmatic, there is the opportunity of transferring progress from very well funded fields (NLP, Vision for consumer applications) to many less wealthy fields (and often harder, e.g. robotics) out of the box. Battle tested architectures trained on large quantities of data can be reused, saving engineering time on RL algorithms (no need to bootstrap value functions, no actor-critic networks, etc.). These techniques may become the standard to pretrain agents. On a more technical level, RL transformers appear to be good at long-term credit assignment and in sparse rewards scenarios, which is very often what we face in real-world RL.  That said, these methods are not a panacea, for instance no serious benchmark has been tried on online RL, the “normal” RL. Knowling that current deep learning techniques tend to handle very poorly discrepancies from identically and independently distributed statistics, current transformers architectures may not be enough. Also, for specific domains ultimately it boils down to the availability of large datasets of expert trajectories, which in fields like robotics are still missing.  All in all, the idea is pretty fresh and there may still be large room for improvement. Onwards!</description>
      <author> (Lorenzo Pieri)</author>
      <guid>https://lorenzopieri.com/rl_transformers/</guid>
      <pubDate>Sun, 19 Dec 2021 00:03:46 GMT</pubDate>
    </item>
    <item>
      <title>OSS Simple Sabotage Manual, Sections 11, 12</title>
      <link>http://svn.cacert.org/CAcert/CAcert_Inc/Board/oss/oss_sabotage.html</link>
      <description>Editor&amp;#39;s Note:  This is sections 11,12 of the OSS&amp;#39;s Simple Sabotage Field Manual,
a 1944 document that has been declassified.  The OSS became the CIA after WWII.
The full document is here.</description>
      <author> ()</author>
      <guid>http://svn.cacert.org/CAcert/CAcert_Inc/Board/oss/oss_sabotage.html</guid>
      <pubDate>Sun, 19 Dec 2021 00:03:46 GMT</pubDate>
    </item>
    <item>
      <title>Learning a technical subject</title>
      <link>http://muratbuffalo.blogspot.com/2021/12/learning-technical-subject.html</link>
      <description>I love learning.  I wanted to write about how I learn, so I can analyze if there is a method to this madness. I will first talk about what m...</description>
      <author> (Get link







Facebook







Twitter







Pinterest







Email







Other Apps)</author>
      <guid>http://muratbuffalo.blogspot.com/2021/12/learning-technical-subject.html</guid>
      <pubDate>Sun, 19 Dec 2021 00:03:46 GMT</pubDate>
    </item>
    <item>
      <title>Self-Organizing Teams - Vadim Kravcenko</title>
      <link>https://vadimkravcenko.com/en/self-organizing-teams/</link>
      <description>To mitigate this, we made some drastic changes to the way we work: We&amp;#39;ve decided to go full force on self-organization, and we&amp;#39;ve put the teams in charge with everyone else in a support role to make sure they are not blocked.</description>
      <author> ()</author>
      <guid>https://vadimkravcenko.com/en/self-organizing-teams/</guid>
      <pubDate>Sun, 19 Dec 2021 00:03:45 GMT</pubDate>
    </item>
    <item>
      <title>Telling the Time with Computer Vision - Jinay Jain</title>
      <link>https://blog.jinay.dev/posts/timekeeper/</link>
      <description>Overengineering an approach to reading an analog clock</description>
      <author> ()</author>
      <guid>https://blog.jinay.dev/posts/timekeeper/</guid>
      <pubDate>Sun, 19 Dec 2021 00:03:45 GMT</pubDate>
    </item>
    <item>
      <title>The Growth-Severity Confound</title>
      <link>https://paulromer.net/grow-severity-confound/</link>
      <description>Economist. Policy Entrepreneur. Geek. Co-recipient of the 2018 Sveriges Riksbank Prize in Economic Sciences in Memory of Alfred Nobel. University Professor at NYU. Focused on urbanization; cooperation at scale of millions and billions; science; technology; economic development; long-term growth; code as language.</description>
      <author> ()</author>
      <guid>https://paulromer.net/grow-severity-confound/</guid>
      <pubDate>Sun, 19 Dec 2021 00:03:45 GMT</pubDate>
    </item>
    <item>
      <title>In Case I Die | Jonathan Verrecchia</title>
      <link>https://www.verekia.com/in-case-i-die</link>
      <description>I am writing this in case I die of a sudden death, because it would really suck to not get a chance to say goodbye and a few last words. I am healthy, live a low-risk life, and have no plan to die, but accidents can always happen. This is for the important people of my life, my family, friends, girlfriend, and anyone who cares about me. I care about you all, and I want you to know that it&amp;#39;s fine if I&amp;#39;m dead.</description>
      <author> ()</author>
      <guid>https://www.verekia.com/in-case-i-die</guid>
      <pubDate>Sun, 19 Dec 2021 00:03:45 GMT</pubDate>
    </item>
    <item>
      <title>Inspection and the limits of trust.</title>
      <link>https://lethain.com/inspection/</link>
      <description>For a long time, the path to engineering manager began with a prolonged stint of technical leadership. Then you’d transition into an initial management role that balanced people and technical responsibilities. Some companies call this a tech lead manager role. Folks entering those sorts of managerial roles were often the senior-most technical contributor on their team. If they struggled with the transition, many of them would fall back into the familiar habit of technical leadership instead of developing the new skills of people management.</description>
      <author> ()</author>
      <guid>https://lethain.com/inspection/</guid>
      <pubDate>Sun, 19 Dec 2021 00:03:45 GMT</pubDate>
    </item>
    <item>
      <title>GitHub - VollRagm/KernelBypassSharp: C# Kernel Mode Driver to read and write memory in protected processes</title>
      <link>https://github.com/VollRagm/KernelBypassSharp</link>
      <description>C# Kernel Mode Driver to read and write memory in protected processes - GitHub - VollRagm/KernelBypassSharp: C# Kernel Mode Driver to read and write memory in protected processes</description>
      <author> (VollRagm)</author>
      <guid>https://github.com/VollRagm/KernelBypassSharp</guid>
      <pubDate>Sat, 18 Dec 2021 23:02:29 GMT</pubDate>
    </item>
    <item>
      <title>OMNIZART: MUSIC TRANSCRIPTION MADE EASY</title>
      <link>https://music-and-culture-technology-lab.github.io/omnizart-doc/</link>
      <description>Omnizart is a Python library and a streamlined solution for automatic music transcription. This library gathers the research outcomes from Music and Cultural Technology Lab, analyzing polyphonic music and transcribes musical notes of instruments [WCS20], chord progression [CS19], drum events [WWS20], frame-level vocal melody [LS18], note-level vocal melody [HS20], and beat [CS20].</description>
      <author> ()</author>
      <guid>https://music-and-culture-technology-lab.github.io/omnizart-doc/</guid>
      <pubDate>Sat, 18 Dec 2021 20:36:28 GMT</pubDate>
    </item>
    <item>
      <title>GitHub - Music-and-Culture-Technology-Lab/omnizart: Omniscient Mozart, being able to transcribe everything in the music, including vocal, drum, chord, beat, instruments, and more.</title>
      <link>https://github.com/Music-and-Culture-Technology-Lab/omnizart</link>
      <description>Omniscient Mozart, being able to transcribe everything in the music, including vocal, drum, chord, beat, instruments, and more. - GitHub - Music-and-Culture-Technology-Lab/omnizart: Omniscient Moza...</description>
      <author> (Music-and-Culture-Technology-Lab)</author>
      <guid>https://github.com/Music-and-Culture-Technology-Lab/omnizart</guid>
      <pubDate>Sat, 18 Dec 2021 20:35:57 GMT</pubDate>
    </item>
    <item>
      <title>Log4j – Apache Log4j Security Vulnerabilities</title>
      <link>https://logging.apache.org/log4j/2.x/security.html</link>
      <description>This page lists all the security vulnerabilities fixed in released versions of Apache Log4j 2. Each vulnerability is given a security impact rating by the Apache Logging security team. please note that this rating may vary from platform to platform. We also list the versions of Apache Log4j the flaw is known to affect, and where a flaw has not been verified list the version with a question mark.</description>
      <author> ()</author>
      <guid>https://logging.apache.org/log4j/2.x/security.html</guid>
      <pubDate>Sat, 18 Dec 2021 19:01:59 GMT</pubDate>
    </item>
    <item>
      <title>Accessing arrays and class fields with&amp;#160;.NET profiling APIs</title>
      <link>https://chnasarre.medium.com/accessing-arrays-and-class-fields-with-net-profiling-apis-d5ff21114a5d</link>
      <description>This post describes how to access arrays and fields of reference type instances with&amp;#160;.NET Profiler APIs.</description>
      <author> (https://chnasarre.medium.com)</author>
      <guid>https://chnasarre.medium.com/accessing-arrays-and-class-fields-with-net-profiling-apis-d5ff21114a5d</guid>
      <pubDate>Sat, 18 Dec 2021 18:01:54 GMT</pubDate>
    </item>
    <item>
      <title>Build a Password Generator Function with Vanilla JavaScript (Advent of JS Challenge)</title>
      <link>https://www.youtube.com/watch?v=O-79Cb5s9U4</link>
      <description>Let&amp;#39;s build a password generator function with Vanilla JavaScript. This is a challenge from the FREE Advent of JavaScript series which consists of 24 JavaScr...</description>
      <author> (James Q Quick
  
  
  
    James Q Quick
  




  
    
    
  



    •)</author>
      <guid>https://www.youtube.com/watch?v=O-79Cb5s9U4</guid>
      <pubDate>Sat, 18 Dec 2021 17:02:15 GMT</pubDate>
    </item>
    <item>
      <title>A primer on FeatureManagement APIs in ASP.NET Core</title>
      <link>https://www.youtube.com/watch?v=xBP2x7bR5uc</link>
      <description>The beauty of a feature flag is simplicity - it&amp;#39;s a conditional block of code that chooses between different execution paths at runtime. In this talk, we wil...</description>
      <author> (Microsoft Visual Studio
  
  
  
    Microsoft Visual Studio
  






    •)</author>
      <guid>https://www.youtube.com/watch?v=xBP2x7bR5uc</guid>
      <pubDate>Sat, 18 Dec 2021 17:02:15 GMT</pubDate>
    </item>
    <item>
      <title>Windows Wednesday</title>
      <link>https://devblogs.microsoft.com/commandline/windows-wednesday/</link>
      <description>Hey everyone! We have an exciting new web show planned to launch next year called Windows Wednesday that we’d love to tell you more about. &amp;#128578; What is Windows Wednesday? Windows Wednesday is a weekly 30-minute live web show where we’ll talk about anything related to Windows.</description>
      <author> (Kayla Cinnamon)</author>
      <guid>https://devblogs.microsoft.com/commandline/windows-wednesday/</guid>
      <pubDate>Sat, 18 Dec 2021 16:01:42 GMT</pubDate>
    </item>
    <item>
      <title>[Log4Shell] 3rd Vulnerability on Apache Log4j Utility Found</title>
      <link>https://www.cyberkendra.com/2021/12/3rd-vulnerability-on-apache-log4j.html</link>
      <description>Log4Shell third vulnerability found after CVE-2021-44228 &amp;amp; CVE-2021-45046</description>
      <author> (3 min read)</author>
      <guid>https://www.cyberkendra.com/2021/12/3rd-vulnerability-on-apache-log4j.html</guid>
      <pubDate>Sat, 18 Dec 2021 15:02:08 GMT</pubDate>
    </item>
    <item>
      <title>Supporting Remix with full stack Cloudflare Pages</title>
      <link>https://blog.cloudflare.com/remix-on-cloudflare-pages/</link>
      <description>Cloudflare Pages now natively supports full stack Remix applications.</description>
      <author> (Greg Brimble)</author>
      <guid>https://blog.cloudflare.com/remix-on-cloudflare-pages/</guid>
      <pubDate>Sat, 18 Dec 2021 14:01:32 GMT</pubDate>
    </item>
    <item>
      <title>Postgres is a great pub/sub &amp;amp; job server</title>
      <link>https://webapp.io/blog/postgres-is-the-answer/</link>
      <description>This article will introduce you to Postgres, explain the alternatives, and walk you through an example use case of pub/sub and its solution.</description>
      <author> (Lyn Chen)</author>
      <guid>https://webapp.io/blog/postgres-is-the-answer/</guid>
      <pubDate>Sat, 18 Dec 2021 08:02:07 GMT</pubDate>
    </item>
    <item>
      <title>I bought 1000 meters of wire to settle a physics debate</title>
      <link>https://www.youtube.com/watch?v=2Vrhk5OjBP8</link>
      <description>I constructed the Veritasium electricity thought experiment in real life to test the result.If you were watching my community posts a month ago, the day that...</description>
      <author> (AlphaPhoenix
  
  
  
    AlphaPhoenix)</author>
      <guid>https://www.youtube.com/watch?v=2Vrhk5OjBP8</guid>
      <pubDate>Sat, 18 Dec 2021 06:02:11 GMT</pubDate>
    </item>
    <item>
      <title>CCPA Scam November 2021 from Princeton University</title>
      <link>https://blog.freeradical.zone/post/ccpa-scam-2021-12/</link>
      <description>Get ready for the CCPA compliance scams from Princeton.</description>
      <author> (Tek)</author>
      <guid>https://blog.freeradical.zone/post/ccpa-scam-2021-12/</guid>
      <pubDate>Sat, 18 Dec 2021 05:02:32 GMT</pubDate>
    </item>
  </channel>
</rss>